{
  "crawler": "medium",
  "fetched_at": "2026-02-10T11:45:21.818680",
  "elapsed_seconds": 28.53,
  "total_articles": 70,
  "articles_with_content": 70,
  "avg_content_length": 4457,
  "articles": [
    {
      "title_en": "Coding with Cursor & Gemini: 6 Hours vs 18 Minutes",
      "url": "https://javascript.plainenglish.io/dev-log-6-hours-to-prompt-18-minutes-to-code-building-a-nook-phone-app-67ee7726f951?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-10T06:03:53",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "productivity",
        "react",
        "prompt-engineering",
        "web-development",
        "artificial-intelligence"
      ],
      "content_length": 11946,
      "content_preview": "<h4>From Coder to Architect: Why Natural Language is the NewÂ Syntax</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PwybB6qUFtxb9aYJVB2RuA.png\" /><figcaption>From Coder to Architect (ByÂ Gemini)</figcaption></figure><p>They say that in the AI era, natural language is becoming the new syntax. But does talking to an AI really make software development effortless?</p><p>I decided to put this to the test. My goal was to build a fully polished, commercial-grade web application ",
      "content_full": "<h4>From Coder to Architect: Why Natural Language is the NewÂ Syntax</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PwybB6qUFtxb9aYJVB2RuA.png\" /><figcaption>From Coder to Architect (ByÂ Gemini)</figcaption></figure><p>They say that in the AI era, natural language is becoming the new syntax. But does talking to an AI really make software development effortless?</p><p>I decided to put this to the test. My goal was to build a fully polished, commercial-grade web application without writing a single line of functional code myself. The journey revealed a stark reality: AI has turned coding into a commodity, but it has elevated architectural design and strategic planning to a premiumÂ skill.</p><p><em>This post shares my experience using an AI agent to automate the entire lifecycleÂ , from project creation to final deployment.</em></p><h3>1. Introduction: The Efficiency Paradox</h3><p>We often think of AI coding tools as magic wandsÂ , type a sentence, get an app. However, my recent experiment proved that the reality is slightly different.</p><p>I set out to build a <strong>â€˜Nook Inc.â€™ (Animal Crossing) style Todo List Web App</strong>. TheÂ result?</p><ul><li><strong>Human Time (Architecture &amp; Prompt Engineering):</strong> <strong>6 Hours</strong> (30 Iterations)</li><li><strong>AI Time (Execution &amp; Build):</strong> <strong>~18Â Minutes</strong></li></ul><p>This post is not just a success story. It is a log of the <strong>trial and error</strong> required to control a Large Language Model (LLM) precisely, and an analysis of where AI excels and where it still struggles.</p><ul><li><strong>Project:</strong> Nook TodoÂ List</li><li><strong>Stack:</strong> React, Vite, TypeScript, Tailwind CSS, Zustand, IndexedDB</li><li><strong>Tools:</strong> Cursor Agent (Implementation), Gemini CLI (Asset Generation)</li></ul><h3>2. The Evolution of the Prompt (v1 â†’Â v30)</h3><p>I didnâ€™t write a single line of production code. Instead, I spent 6 hours writing and refining a <strong>â€œMaster Prompt.â€</strong> Here is how the promptÂ evolved.</p><h4>Phase 1. Establishing Engineering Standards (v1 ~Â v13)</h4><p>The initial prompts were simple functional requirements. However, the AI lacked â€œengineering discipline.â€</p><p><strong>Issue:</strong> Mixing npm and pnpm, inconsistent commit messages, ignoring project structure.</p><p><strong>Solution:</strong> I defined the AIâ€™s persona as a <strong>â€œSenior Architect.â€</strong> I explicitly enforced <strong>Conventional Commits</strong> and a strict <strong>GitÂ Flow</strong>.</p><p><strong>Outcome:</strong> The AI began to modularize code and manage branches (feat/setup, fix/ui) systematically.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KjiU3pgtZRHRHTNwxCFCTw.png\" /><figcaption><a href=\"https://gist.github.com/naturalkei/8abe865d7c5aa51adccc6fba0c56aadd#file-vrt-todo-prompt-v13-md\">vrt-todo-prompt-v13</a></figcaption></figure><ul><li><em>âš ï¸ </em><strong><em>Note:</em></strong><em> If you canâ€™t find the â€œAddâ€ button, itâ€™s not missing. The AI styled it with </em><strong><em>White Text on a White Border</em></strong><em>, making it perfectly invisible. I honestly didnâ€™t even realize this bug existed at first because it was so well camouflaged.</em></li></ul><blockquote>ğŸ‘‰ <strong>Result (r1):</strong> <a href=\"https://naturalkei.github.io/vrt-todo-r1/\">Live Demo</a>Â , Functional, but visually raw (Engineering style)</blockquote><h4>Phase 2. The Limitation of Text-Based Design (v14 ~Â v25)</h4><p>I tried to improve the UI solely through text prompts like <em>â€œMake it cleanâ€</em> or <em>â€œAdd a filterÂ tab.â€</em></p><ul><li><strong>The Problem:</strong> The AI misinterpreted â€œCleanâ€ as â€œEmpty.â€ It frequently generated <strong>White Text on White Backgrounds</strong>, creating an â€œInvisible UIâ€ where buttons existed but couldnâ€™t beÂ seen.</li><li><strong>Realization:</strong> Describing visual aesthetics (e.g., â€œNook Phone styleâ€) in text resulted in generic, beige designs that lacked character. I realized that natural language has limits when describing pixels.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*M8q1qY5maHOy6-NfXwdk0g.png\" /><figcaption><a href=\"https://gist.github.com/naturalkei/1e44876b8a03fb4af72c27e33e76ae7c#file-vrt-todo-prompt-v25-md\">vrt-todo-prompt-v25</a></figcaption></figure><ul><li><em>âš ï¸ </em><strong><em>Note:</em></strong><em> The AI strikes again with its â€œInvisible UI.â€ In this version, both the </em><strong><em>â€˜Addâ€™ button</em></strong><em> and the </em><strong><em>â€˜Allâ€™ filter tab</em></strong><em> are styled with white text on a white background, making them practically impossible to see. It seems the AI confuses â€œClean Designâ€ with â€œBlankÂ Paper.â€</em></li></ul><blockquote>ğŸ‘‰ <strong>Result (r2):</strong> <a href=\"https://naturalkei.github.io/vrt-todo-r2/\">Live Demo</a>Â , â€˜Nookâ€™s Crannyâ€™ style (PaperÂ texture)</blockquote><h4>Phase 3. Final Polish &amp; Branding (v26 ~Â v30)</h4><p>To break the cycle of â€œInvisible UIâ€, I changed my strategy from Description to Reference.</p><ul><li><strong>Strategy:</strong> I asked <strong>Gemini</strong> to generate a â€œNook Phone UI design draftâ€Â first.</li><li><strong>Execution:</strong> I fed that specific image to <strong>Cursor</strong> with the instruction: <em>â€œImplement this design pixel-perfectly.â€</em></li><li><strong>Outcome:</strong> The synergy was instant. Gemini acted as the <strong>Designer</strong> (defining the #1CCD9E palette and rounded aesthetics), and Cursor acted as the <strong>Engineer</strong>. The vague â€œcuteâ€ request became a concrete visual spec, and the app finally looked like aÂ product.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*-jDqzfG335j-GAFz5H4RCA@2x.png\" /><figcaption><a href=\"https://gist.github.com/naturalkei/7aeec9dcb60a56ccc70ea644c1554066#file-vrt-todo-prompt-v30-md\">vrt-todo-prompt-v30</a></figcaption></figure><ul><li><em>ğŸ‰ </em><strong><em>Success:</em></strong><em> This result actually exceeded my expectations. The AI finally nailed the â€˜Nook Phoneâ€™ aesthetic with the correct mint green palette and rounded UI. The contrast issues are gone, and it even polished advanced features like â€˜Import/Exportâ€™ and â€˜Priorityâ€™ filters. It feels like a finishedÂ product.</em></li></ul><blockquote>ğŸ‘‰ <strong>Result (r3):</strong> <a href=\"https://naturalkei.github.io/vrt-todo-r3/\">Live Demo</a> (awesome!)</blockquote><h3>3. Technical Bottlenecks: Where AI Still Struggles</h3><p>Despite the success, there were moments where the AIâ€™s <strong>â€œTraining Biasâ€</strong> clashed with my specific requirements.</p><h4>1) The â€œStandard Practiceâ€ BiasÂ (CI/CD)</h4><p>The most persistent issue was the GitHub Actions configuration (deploy.yml). My template uses a release branch for deployment, but the AI kept reverting it toÂ main.</p><p><strong>AIâ€™s Logic:</strong> â€œStandard CI/CD pipelines usually deploy fromÂ main.&quot;</p><p><strong>The Conflict:</strong> Even after I added a constraint (â€œDo not change the branch triggerâ€), the AI treated my configuration as a â€œbugâ€ to beÂ fixed.</p><p><strong>Resolution:</strong> I realized that re-running the 18-minute build to test a prompt constraint was inefficient. I manually reverted the file in 3Â seconds.</p><ul><li><strong>Lesson:</strong> Sometimes, <strong>manual intervention is cheaper</strong> than prompt engineering. Conserving tokens and time is part of the strategy.</li></ul><h4>2) Stability vs. Bleeding Edge (Tailwind CSS)</h4><p>I requested <strong>Tailwind CSS v4</strong>. However, upon encountering a PostCSS configuration error, the Agent autonomously downgraded toÂ <strong>v3</strong>.</p><ul><li><strong>Observation:</strong> The AI prioritized â€œsuccessful buildâ€ over â€œspecific version requirement.â€ While practical, this behavior can be problematic if strict version compliance isÂ needed.</li></ul><h3>4. The Result: 18 Minutes of Execution</h3><p>Once the <strong>v30 Prompt</strong> was finalized, I ran it on a fresh repository. The Cursor Agent took approximately <strong>18 minutes</strong> to go from init toÂ deploy.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/540/1*0d05G0q04Kh6q7S38_ZlYg.gif\" /><figcaption><a href=\"https://www.youtube.com/shorts/ECju7R6JK1k\">AI Coding Speedrun: Building a React App in 18 Minutes(20x Timelapse)</a></figcaption></figure><pre>âœ… Project Complete!<br />&quot;Nook Todo List&quot; app has been successfully built! ğŸ‰<br />ğŸ“Š Summary of Completed Tasks<br />â€¢ Total Commits: 8 (Followed branching orders well)<br />â€¢ Tests: 15 (Passed all)<br />â€¢ Design: Nook Inc. theme applied<br />â€¢ Time Elapsed: Approx. 18 minutes (Based on v30 prompt execution)<br />ğŸ”— Result Links<br />â€¢ GitHub Repository: https://github.com/naturalkei/vrt-todo-r3<br />â€¢ Live Demo: https://naturalkei.github.io/vrt-todo-r3/</pre><h4><strong>The OutputÂ Quality:</strong></h4><p><strong>Feature Complete:</strong> CRUD, Priority, Due Dates, Recurrence, JSON Import/Export.</p><p><strong>Local-First:</strong> Robust offline support using <strong>IndexedDB</strong>.</p><p><strong>Quality Assurance (QA):</strong></p><ul><li><strong>Unit Tests:</strong> 15 tests (Vitest)Â , 100%Â Pass.</li><li><strong>E2E Tests:</strong> Playwright setup included.</li><li><strong>Accessibility:</strong> Passed axe-coreÂ checks.</li></ul><p>The AI didnâ€™t just â€œwrite codeâ€; it orchestrated the entire development lifecycle, including testing and documentation.</p><h3>5. Conclusion: From Coder to Architect</h3><p>This project changed my perspective on software development. The AI is an incredibly fast, capable, but occasionally stubborn â€œJunior Developer.â€</p><ul><li><strong>The Human Role:</strong> Defining the <em>What</em> and <em>Why</em> (Architecture, Design constraints, BusinessÂ Logic).</li><li><strong>The AI Role:</strong> Executing the <em>How</em> (Syntax, Boilerplate, Testing, Refactoring).</li></ul><p>We are transitioning from <strong>Coders</strong> who lay every brick, to <strong>Directors</strong> who provide the blueprints. However, merely giving orders is not enough. <strong>The most critical skill is now â€œStrategic Planning.â€</strong></p><p>To unlock true efficiency, you must understand the entire development lifecycle and possess the capability to <strong>design precise, constraint-based prompts</strong>. Ultimately, the quality of the output depends entirely on the developerâ€™s ability to plan the logic and structure before a single line of code is generated.</p><h3><strong>[Appendix] The Evolution over 6Â Hours</strong></h3><ul><li><strong>ğŸ£ v13 (1 Hour In):</strong> <a href=\"https://gist.github.com/naturalkei/8abe865d7c5aa51adccc6fba0c56aadd#file-vrt-todo-prompt-v13-md\">vrt-todo-prompt-v13.md</a> | <a href=\"https://github.com/naturalkei/vrt-todo-r1\">vrt-todo-r1</a></li><li><strong>ğŸƒ v25 (3 Hours In):</strong> <a href=\"https://gist.github.com/naturalkei/1e44876b8a03fb4af72c27e33e76ae7c#file-vrt-todo-prompt-v25-md\">vrt-todo-prompt-v25.md</a> | <a href=\"https://github.com/naturalkei/vrt-todo-r2\">vrt-todo-r2</a></li><li><strong>ğŸ“± v30 (6 Hours In):</strong> <a href=\"https://gist.github.com/naturalkei/7aeec9dcb60a56ccc70ea644c1554066#file-vrt-todo-prompt-v30-md\">vrt-todo-prompt-v30.md</a> | <a href=\"https://github.com/naturalkei/vrt-todo-r3\">vrt-todo-r3</a> |Â <a href=\"https://youtu.be/cwOqrMslfgw?si=VDQLRcvoLQGA4Na5\">video</a></li></ul><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=67ee7726f951\" width=\"1\" /><hr /><p><a href=\"https://javascript.plainenglish.io/dev-log-6-hours-to-prompt-18-minutes-to-code-building-a-nook-phone-app-67ee7726f951\">Coding with Cursor &amp; Gemini: 6 Hours vs 18 Minutes</a> was originally published in <a href=\"https://javascript.plainenglish.io\">JavaScript in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "The JavaScript Ecosystem Just Had Its ChatGPT Moment (And You Probably Missed It)",
      "url": "https://javascript.plainenglish.io/the-javascript-ecosystem-just-had-its-chatgpt-moment-and-you-probably-missed-it-b3c69dc7e445?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T18:04:36",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "programming",
        "web-development",
        "javascript",
        "software-development",
        "artificial-intelligence"
      ],
      "content_length": 777,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/the-javascript-ecosystem-just-had-its-chatgpt-moment-and-you-probably-missed-it-b3c69dc7e445?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*FYeQD1b78rFJBRGePUwqqw.jpeg\" width=\"3840\" /></a></p><p class=\"medium-feed-snippet\">Remember when ChatGPT dropped and suddenly everyone at your company was quietly using it, pretending they&#x2019;d written that perfect ",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/the-javascript-ecosystem-just-had-its-chatgpt-moment-and-you-probably-missed-it-b3c69dc7e445?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*FYeQD1b78rFJBRGePUwqqw.jpeg\" width=\"3840\" /></a></p><p class=\"medium-feed-snippet\">Remember when ChatGPT dropped and suddenly everyone at your company was quietly using it, pretending they&#x2019;d written that perfect email&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/the-javascript-ecosystem-just-had-its-chatgpt-moment-and-you-probably-missed-it-b3c69dc7e445?source=rss----4b3a1ed4f11c---4\">Continue reading on JavaScript in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Why This JavaScript Question Confuses Even Good Developers",
      "url": "https://javascript.plainenglish.io/why-this-javascript-question-confuses-even-good-developers-773bbf9e54ee?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:56:13",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "software-development",
        "javascript",
        "web-development",
        "interview",
        "programming"
      ],
      "content_length": 4942,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ay_eMwNRrpWNMbaCvd-Xqg.png\" /></figure><p>If you read my previous <a href=\"https://medium.com/javascript-in-plain-english/why-this-javascript-event-loop-question-appears-in-every-interview-29b75ed753f2\">blog</a>, I left you with this question.</p><pre>console.log(&quot;A&quot;);<br /><br />setTimeout(() =&gt; console.log(&quot;B&quot;), 0);<br /><br />Promise.resolve().then(() =&gt; console.log(&quot;C&quot;));<br /><br />(async",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ay_eMwNRrpWNMbaCvd-Xqg.png\" /></figure><p>If you read my previous <a href=\"https://medium.com/javascript-in-plain-english/why-this-javascript-event-loop-question-appears-in-every-interview-29b75ed753f2\">blog</a>, I left you with this question.</p><pre>console.log(&quot;A&quot;);<br /><br />setTimeout(() =&gt; console.log(&quot;B&quot;), 0);<br /><br />Promise.resolve().then(() =&gt; console.log(&quot;C&quot;));<br /><br />(async function () {<br />  console.log(&quot;D&quot;);<br />  await null;<br />  console.log(&quot;E&quot;);<br />})();<br /><br />console.log(&quot;F&quot;);</pre><p>At first glance, it looksÂ similar.</p><ul><li><strong>setTimeout</strong>: easy toÂ handle</li><li><strong>Promise:</strong> seenÂ it</li><li><strong>async/await:</strong> I use it regularly.</li></ul><p>but still a lot people get stuckÂ here,</p><p>not because the question is difficult, but because it mixes multiple concepts together.</p><p>Letâ€™s see how can we break it down and reason throughÂ it.</p><p>If you guys new to the Event loop, I highly recommend reading my previous article before continuing. <a href=\"https://medium.com/javascript-in-plain-english/why-this-javascript-event-loop-question-appears-in-every-interview-29b75ed753f2\">here</a></p><h3>Synchronous Code RunsÂ First</h3><p>JavaScript doesnâ€™t care how complex the code is, it always runs the synchronous codeÂ first.</p><p>So letâ€™s see what is synchronous here</p><pre>console.log('A');</pre><p>Inside the async function:</p><pre>console.log('D');</pre><p>Yes, this is a synchronous code, which surprises manyÂ people.</p><p>An async function is not fully asynchronous. It runs synchronously until it hits awaitÂ .</p><p>And then finally weÂ have</p><pre>console.log('F');</pre><p>So, our output so farÂ is</p><pre>A<br />D<br />F</pre><p>Pretty easy,Â right.</p><h3>Now comes the part that feelsÂ tricky</h3><pre>await null;</pre><p>This looksÂ simple.</p><ul><li>No NetworkÂ call</li><li>No timeout</li><li>No Promise</li></ul><p>But why is this important?</p><p>Because await always pauses execution, even if the value isÂ null</p><p>Under the hood, JavaScript interprets it asÂ this:</p><pre>await Promise.resolve(null);</pre><p>This means the rest of the async function is deferred and treated just like a promise callback.</p><p>Now this looks easy, and you can tell that this is a promise and the code after it is scheduled to run later and it goes first to <strong>microtask queue</strong>.</p><p>So this is a very important detail to understand.</p><h3>Now check what is left toÂ run</h3><p>At this point, all synchronous code isÂ done.</p><p>Now, weÂ have:</p><h4>Microtasks (high priority)</h4><ul><li>Promise.resolve().then(() =&gt; console.log(&quot;C&quot;));</li><li>Continuation of async function â†’ console.log(&quot;E&quot;);</li></ul><h4>Macrotasks (lower priority)</h4><ul><li>setTimeout(() =&gt; console.log(&quot;B&quot;), 0);</li></ul><p>Now, what the rule says about these twoÂ queues:</p><blockquote><strong><em>Microtasks always runs before macrotasks</em></strong></blockquote><h3>Letâ€™s evaluate Microtasks first</h3><p>The microtask queue is processed as soon as the call stack isÂ empty.</p><p>So, JavaScript startsÂ with</p><pre>console.log(&quot;C&quot;);</pre><p>And then next microtask and the paused async functionÂ resumes:</p><pre>console.log(&quot;E&quot;);</pre><p>Output:</p><pre>C<br />E</pre><h3>Finally Macrotask chance</h3><p>Now the microtask is empty, JavaScript looks at the macrotask queue.</p><pre>console.log(&quot;B&quot;);</pre><p>Output:</p><pre>B</pre><h3>Final Output</h3><pre>A<br />D<br />F<br />C<br />E<br />B</pre><p>Simple, right? No guessing, no memorization, we just understood the concept and by using that, came up with theÂ answer.</p><h3>Whatâ€™s next?</h3><p>Now, if you think you know everything and can answer any question related to event loop. Try to answer the below questionÂ next.</p><pre>console.log('start');<br /><br />setTimeout(() =&gt; console.log('timeout'), 0);<br /><br />setImmediate(() =&gt; console.log('immediate'));<br /><br />Promise.resolve().then(() =&gt; console.log('promise'));<br /><br />process.nextTick(() =&gt; console.log('nextTick'));<br /><br />console.log('end');</pre><p>This is where even experienced developers start arguing about output. Thatâ€™s where things get really interesting.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=773bbf9e54ee\" width=\"1\" /><hr /><p><a href=\"https://javascript.plainenglish.io/why-this-javascript-question-confuses-even-good-developers-773bbf9e54ee\">Why This JavaScript Question Confuses Even Good Developers</a> was originally published in <a href=\"https://javascript.plainenglish.io\">JavaScript in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "React Testing Doesnâ€™t Have to Be a Drama",
      "url": "https://javascript.plainenglish.io/react-testing-doesnt-have-to-be-a-drama-2a0e36335633?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:56:10",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "reactjs",
        "react",
        "clean-test",
        "typescript-with-react",
        "tdd"
      ],
      "content_length": 9663,
      "content_preview": "<p>â€œYou create Clean Tests as functional requirements in Markdown, where you define the expected behavior for functions and React components. From those, <a href=\"https://3tg.dev\">3TG</a> generates the actualÂ tests.â€</p><p>That sentence alone removes about 80% of the emotional baggage React testing has accumulated over the years. Because letâ€™s be honest: React testing didnâ€™t become dramatic because React is bad. It became dramatic because the expectations were unclear, the tools were loud, and t",
      "content_full": "<p>â€œYou create Clean Tests as functional requirements in Markdown, where you define the expected behavior for functions and React components. From those, <a href=\"https://3tg.dev\">3TG</a> generates the actualÂ tests.â€</p><p>That sentence alone removes about 80% of the emotional baggage React testing has accumulated over the years. Because letâ€™s be honest: React testing didnâ€™t become dramatic because React is bad. It became dramatic because the expectations were unclear, the tools were loud, and the tests showed up lateâ€Šâ€”â€Šusually right after something alreadyÂ broke.</p><p>React testing isnâ€™t supposed to feel like opening night at a theater where everyoneâ€™s nervous, the props are missing, and someone forgot their lines. Itâ€™s supposed to feel boring. Predictable. Quietly reassuring.</p><p>And yes, thatâ€™s achievable.</p><h3>Why React Testing Feels Harder Than ItÂ Should</h3><p>React itself isnâ€™t the villain. The drama comes from everything wrapped aroundÂ it.</p><ul><li>Components grow organically</li><li>State sneaks in through sideÂ doors</li><li>Hooks multiplyÂ quietly</li><li>Behavior changes without anyone writing itÂ down</li></ul><p>By the time tests are added, the code already has opinions. Strong ones. And the tests are forced to interrogate internals instead of verifying outcomes.</p><p>Thatâ€™s when React testing turns into performance art:</p><ul><li>Tests fail during harmless refactors</li><li>Snapshots scream for attention</li><li>Assertions read likeÂ riddles</li><li>Confidence dropsÂ fast</li></ul><p>Meanwhile, MS Teams is pinging about a standup youâ€™re already late for, and your test run still hasnâ€™t finished. Drama thrives in that environment.</p><h3>The Real Problem Isnâ€™t Testingâ€Šâ€”â€ŠItâ€™sÂ Timing</h3><p>Most React tests fail developers because they arrive <strong>after decisions have already beenÂ made</strong>.</p><p>By the time youâ€™re writingÂ tests:</p><ul><li>The component API isÂ fixed</li><li>The behavior isÂ implicit</li><li>The edge cases live only in someoneâ€™s head</li></ul><p>So tests end up reverse-engineering intent. Thatâ€™s stressful. AndÂ fragile.</p><p>Test-first thinking flips this timeline.</p><p>When you write behavior before implementationâ€Šâ€”â€Ševen brieflyâ€Šâ€”â€Šyou remove ambiguity. You turn opinions into expectations. And suddenly testing stops feeling like cross-examination and starts feeling like confirmation.</p><p>Thatâ€™s why writing behavior in Markdown counts as writingÂ tests.</p><h3>What React Tests Are Actually Supposed toÂ Protect</h3><p>Letâ€™s clear something up: React tests are not there to prove you used hooks correctly or structured your JSX tastefully.</p><p>They exist to protect <strong>behavior users relyÂ on</strong>.</p><p>Things like:</p><ul><li>What renders under specific conditions</li><li>How interactions changeÂ state</li><li>What must remain stable during refactors</li></ul><p>Clean Tests focus on exactlyÂ that.</p><p>They are:</p><ul><li><strong>Readable</strong>â€Šâ€”â€Šyou understand them at aÂ glance</li><li><strong>Behavior-focused</strong>â€Šâ€”â€Šoutcomes over internals</li><li><strong>Portable</strong>â€Šâ€”â€Šnot glued to a specific environment</li><li><strong>Maintainable</strong>â€Šâ€”â€Šresistant to refactors</li></ul><p>When tests protect behavior instead of implementation details, the drama disappears almost instantly.</p><h3>Where React Testing Usually Goes Off theÂ Rails</h3><p>React testing tends to spiral when teams chase the wrongÂ signals.</p><p>Common traps:</p><ul><li>Snapshot tests used as behavioral guarantees</li><li>Tests tightly coupled to DOM structure</li><li>Assertions that care <em>how</em> something happened instead of <em>what</em>Â happened</li><li>Mocking everything until nothing feelsÂ real</li></ul><p>The result? A test suite that technically exists but emotionally cannot beÂ trusted.</p><p>Confidence doesnâ€™t come from quantity. It comes fromÂ <strong>clarity</strong>.</p><h3>Where Behavior-First Testing Changes Everything</h3><p>This is the moment React testing calmsÂ down.</p><p>When you define behaviorÂ first:</p><ul><li>Components get clearer responsibilities</li><li>Edge cases surfaceÂ early</li><li>APIs become easier to reasonÂ about</li></ul><p>With 3TG, that behavior lives in Markdown, not in someoneâ€™s memory. FromÂ there:</p><ul><li>3TG proposes test values and expected results usingÂ AI</li><li>It generates mock stubs with TypeScript typings</li><li>It produces <strong>Clean Tests</strong>, not raw or fragileÂ ones</li></ul><p>This matters because <strong>AI-only test generation is unsafe and incomplete</strong>. Without human-defined expectations, AI can only guess what mattersâ€Šâ€”â€Šand guesses are a terrible foundation forÂ trust.</p><p>3TG doesnâ€™t guess. It executesÂ intent.</p><h3>Earlier Feedback, Fewer Surprises</h3><p>React teams often say, â€œWeâ€™ll test it later,â€ without realizing that â€œlaterâ€ is when changes are most expensive.</p><p>By moving expectations earlier:</p><ul><li>Problems surface before code solidifies</li><li>Refactors becomeÂ safer</li><li>Reviews getÂ calmer</li></ul><p>This isnâ€™t about slowing down. Itâ€™s about <strong>reducing surprise</strong>.</p><p>When tests fail, they should feel obvious. Not dramatic. Not mysterious. Just informative.</p><p>Thatâ€™s the difference between a safety system and a noise generator.</p><h3>Speed Matters More Than PeopleÂ Admit</h3><p>Hereâ€™s a truth every experienced React developer learns the hard way: if tests are slow, they will be skipped. Religiously.</p><p>If a test run takes longer than your coffee cooling down on your desk, developers will â€œtemporarilyâ€ stop running it locally. Temporarily, of course, meaningÂ forever.</p><p>Clean Tests prioritize fast feedback:</p><ul><li>Minimal setup</li><li>Clear intent</li><li>Focused behavior</li></ul><p>Speed isnâ€™t about optimization bragging rights. Itâ€™s about <strong>protecting focus</strong> in a world full of interruptions.</p><h3>React Testing Is Also a DesignÂ Tool</h3><p>One underrated benefit of behavior-first testing is how much it improves component design.</p><p>When you ask, â€œWhat should this component do?â€ before building it, you naturally:</p><ul><li>Reduce unnecessary state</li><li>Clarify boundaries</li><li>Avoid overloading components with responsibility</li></ul><p>If behavior is hard to describe, the component is usually doing tooÂ much.</p><p>Clean Tests act like a design constraint that nudges you toward simpler, more reliable React codeâ€Šâ€”â€Šwithout ever soundingÂ preachy.</p><h3>Why Flaky React Tests Create So MuchÂ Drama</h3><p>Flakiness is where trust goes toÂ die.</p><p>A flakyÂ test:</p><ul><li>Fails randomly</li><li>Passes onÂ rerun</li><li>Trains developers to ignoreÂ failures</li></ul><p>React apps are especially vulnerable due to timing, async behavior, and external dependencies.</p><p>Clean Tests minimize thisÂ by:</p><ul><li>Avoiding reliance on fragile internals</li><li>Keeping mocks explicit andÂ typed</li><li>Focusing on observable outcomes</li></ul><p>A test that only fails when something meaningful breaks is a test people listenÂ to.</p><h3>A Very Familiar ReactÂ Story</h3><p>A team ships a React feature quickly. It works. Everyoneâ€™s happy.</p><p>Two weeks later, a small refactor lands. Tests explode. Half the failures are unrelated. The suite gets rerun three times until it passes. Nobody investigates.</p><p>Confidence drops.</p><p>Eventually, theyÂ reset.</p><p>They start defining behavior in Markdown:</p><ul><li>What the component shouldÂ render</li><li>How it responds to interaction</li><li>What must neverÂ regress</li></ul><p>3TG generates Clean Tests around those expectations.</p><p>Slowly:</p><ul><li>Tests become readableÂ again</li><li>Refactors stop feelingÂ risky</li><li>Failures become meaningful</li></ul><p>The drama fadesâ€Šâ€”â€Šnot because React changed, but because <strong>expectations became explicit</strong>.</p><h3>This Isnâ€™t About More Testsâ€Šâ€”â€ŠItâ€™s AboutÂ Trust</h3><p>React testing doesnâ€™t need more tools, louder frameworks, or stricterÂ rules.</p><p>It needs:</p><ul><li>Clear intent</li><li>Early feedback</li><li>Calm enforcement</li></ul><p>Clean Tests donâ€™t nag. They donâ€™t overreact. They donâ€™t punish reasonable changes.</p><p>They quietly ensure that what mattered yesterday still mattersÂ today.</p><p>Thatâ€™s how testing becomes empowering instead of exhausting.</p><h3>Your NextÂ Move</h3><p>If React testing currently feels theatrical, itâ€™s time to lower theÂ volume.</p><ul><li>Visit <a href=\"https://3tg.dev\"><strong>3tg.dev</strong></a> and start using the freeÂ plan</li><li>Write one React componentâ€™s expected behavior inÂ Markdown</li><li>Let 3TG generate Clean Tests and feel the difference</li></ul><p>You can also see real walkthroughs here:<br /><a href=\"https://www.youtube.com/@codingcreedtechnologies\">https://www.youtube.com/@codingcreedtechnologies</a></p><p>Ask for a live demonstration of 3TGâ€™s capabilities on your own projectâ€Šâ€”â€Šjust write to contact @ coding-creed.tech</p><p>React testing doesnâ€™t have to be a drama.<br />It can be a quiet agreement between you and your code: <em>this behaviorÂ matters</em>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*w25dg3PiqOJbzNnEm_hR4A.png\" /></figure><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2a0e36335633\" width=\"1\" /><hr /><p><a href=\"https://javascript.plainenglish.io/react-testing-doesnt-have-to-be-a-drama-2a0e36335633\">React Testing Doesnâ€™t Have to Be a Drama</a> was originally published in <a href=\"https://javascript.plainenglish.io\">JavaScript in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "Why people are obsessed with web development",
      "url": "https://javascript.plainenglish.io/why-people-are-obsessed-with-web-development-39fe59f43da9?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:55:38",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "software-engineering",
        "web-development",
        "front-end-development",
        "programming",
        "career-development"
      ],
      "content_length": 645,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/why-people-are-obsessed-with-web-development-39fe59f43da9?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1456/1*dDfubXUjPIK10JvOGVGX8g.png\" width=\"1456\" /></a></p><p class=\"medium-feed-snippet\">A clear look at the pull, the flexing on LinkedIn, and the real problems the field faces</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/why-peop",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/why-people-are-obsessed-with-web-development-39fe59f43da9?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1456/1*dDfubXUjPIK10JvOGVGX8g.png\" width=\"1456\" /></a></p><p class=\"medium-feed-snippet\">A clear look at the pull, the flexing on LinkedIn, and the real problems the field faces</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/why-people-are-obsessed-with-web-development-39fe59f43da9?source=rss----4b3a1ed4f11c---4\">Continue reading on JavaScript in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "CSS â€œScroll-Drivenâ€ Animations: Parallax in Angular with 0 JS",
      "url": "https://javascript.plainenglish.io/css-scroll-driven-animations-parallax-in-angular-with-0-js-5613d74d4e6f?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:55:36",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "javascript",
        "angular",
        "software-engineering",
        "software-development",
        "web-development"
      ],
      "content_length": 718,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/css-scroll-driven-animations-parallax-in-angular-with-0-js-5613d74d4e6f?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*1Psugxmazt1AtogfDZpxQg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Delete your HostListener. You can now build 60fps parallax effects, reading progress bars, and reveal animations using only&#xa0;CSS.</p><p class=\"medium-feed",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/css-scroll-driven-animations-parallax-in-angular-with-0-js-5613d74d4e6f?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*1Psugxmazt1AtogfDZpxQg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Delete your HostListener. You can now build 60fps parallax effects, reading progress bars, and reveal animations using only&#xa0;CSS.</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/css-scroll-driven-animations-parallax-in-angular-with-0-js-5613d74d4e6f?source=rss----4b3a1ed4f11c---4\">Continue reading on JavaScript in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "20 Browser Extensions For Web Development & Design",
      "url": "https://javascript.plainenglish.io/20-browser-extensions-for-web-development-design-a7ae95beb184?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:55:31",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "react",
        "web-development",
        "software-development",
        "ux-design",
        "javascript"
      ],
      "content_length": 631,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/20-browser-extensions-for-web-development-design-a7ae95beb184?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*gU_5j9JDmilBujy5pV9nrw.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Tools I actually use, and why most extension lists are still wrong</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/20-browser-extensions-for-",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/20-browser-extensions-for-web-development-design-a7ae95beb184?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*gU_5j9JDmilBujy5pV9nrw.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Tools I actually use, and why most extension lists are still wrong</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/20-browser-extensions-for-web-development-design-a7ae95beb184?source=rss----4b3a1ed4f11c---4\">Continue reading on JavaScript in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "How to Migrate a Monolithic React App to a Turborepo Monorepoâ€Šâ€”â€Šand Make it Rollback-Friendly",
      "url": "https://javascript.plainenglish.io/how-to-migrate-a-monolithic-react-app-to-a-turborepo-monorepo-and-make-it-rollback-friendly-90acd00d5d14?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:55:30",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "programming",
        "reactjs",
        "react",
        "javascript",
        "micro-frontends"
      ],
      "content_length": 14222,
      "content_preview": "<h3><strong>How to Migrate a Monolithic React App to a Turborepo Monorepoâ€Šâ€”â€Šand Make it Rollback-Friendly</strong></h3><p>Deep dive into restructuring a large enterprise React application into independently versioned, lazy-loaded frontends using Turborepo.</p><p>We had a large, monolithic React frontendâ€Šâ€”â€Špowering table and graph visualisation, application analysis, and multi-authorised workspace management. Over time, the codebase grew to <strong>include 40+ feature modules</strong>, <strong>mu",
      "content_full": "<h3><strong>How to Migrate a Monolithic React App to a Turborepo Monorepoâ€Šâ€”â€Šand Make it Rollback-Friendly</strong></h3><p>Deep dive into restructuring a large enterprise React application into independently versioned, lazy-loaded frontends using Turborepo.</p><p>We had a large, monolithic React frontendâ€Šâ€”â€Špowering table and graph visualisation, application analysis, and multi-authorised workspace management. Over time, the codebase grew to <strong>include 40+ feature modules</strong>, <strong>multiple</strong> <strong>Redux reducers</strong>, and a complex web of Redux-thunk sideÂ effects.</p><p>Then came a new requirement: build another <strong>application</strong> alongside the existing app. Both needed to share authentication, routing, and utilitiesâ€Šâ€”â€Šbut evolve independently. We needed a better architecture.</p><h4>Shipping them as a single bundleÂ meant:</h4><blockquote>- A bug in application 2 could block an application 1Â release</blockquote><blockquote>- No way to roll back one app without reverting theÂ other</blockquote><blockquote>- Growing build times as the bundle ballooned</blockquote><h4><strong>The Solution: A Three-Layer Turborepo Monorepo</strong></h4><pre>main-app<br />â”œâ”€â”€ apps/<br />â”‚   â””â”€â”€ shell/                    # Layer 1: Orchestrator<br />â”œâ”€â”€ packages/<br />â”‚   â”œâ”€â”€ application-1/            # Layer 2: application - 1<br />â”‚   â”œâ”€â”€ application-2/            # Layer 3: application - 2<br />â”‚   â””â”€â”€ common/                   # Layer 5: Shared utilities, design system<br />â”œâ”€â”€ turbo.json<br />â””â”€â”€ package.json</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fl_yJ9-pNFTHiD7lMcoOTA.png\" /></figure><h4><strong>Layer 1: The Shell (Orchestrator)</strong></h4><p>The shell is the entry point. It does one thing well: <strong>route between apps. </strong>This<strong> </strong>decouples the loading decision from apps, avoids rebuilding everything on every change, and provides a single place for future cross-cutting concerns.</p><pre>function ShellApp() {<br />  return (<br />    &lt;Router&gt;<br />      &lt;Suspense fallback={&lt;LoadingFallback /&gt;}&gt;<br />        &lt;Switch&gt;<br />          &lt;Route path=&quot;/application-1&quot;&gt;<br />            &lt;ImagingApp /&gt;<br />          &lt;/Route&gt;<br />          &lt;Route path=&quot;/application-2&quot;&gt;<br />            &lt;ConsoleApp /&gt;<br />          &lt;/Route&gt;<br />        &lt;/Switch&gt;<br />      &lt;/Suspense&gt;<br />    &lt;/Router&gt;<br />  );<br />}</pre><p>Key points:</p><ul><li><strong>React.lazy() </strong>for code splittingâ€Šâ€”â€ŠApplication 1 and Application 2 load as separateÂ chunks</li><li><strong>Suspense</strong> with a loading fallbackâ€Šâ€”â€Šno blank screens during chunkÂ loads</li><li><strong>React Router v5</strong> with &lt;Switch&gt;â€Šâ€”â€Šclean URL-based routing betweenÂ apps</li><li>The shell has zero business logic. Itâ€™s a trafficÂ handler.</li></ul><h4><strong>Layer 2: Independent Applications (Application-1, Application-2)</strong></h4><p>Each app is a self-contained package that exports a single React component. This can be heavy on business logic. This helps independent builds, dependency trees, and release cycles so a failing Console test never blocks an ImagingÂ release.</p><pre>// packages/application-1/src/index.js<br />export default function App1() {<br />  const [isReady, setIsReady] = useState(i18nInitialized);<br /><br />  return (<br />    &lt;Provider store={store}&gt;<br />      &lt;Suspense fallback={&lt;LoadingComponent /&gt;}&gt;<br />        &lt;StyledEngineProvider injectFirst={true}&gt;<br />          &lt;ErrorBoundary header={true} retry={true} relogin={true}&gt;<br />            &lt;Route path=&quot;/application-1&quot;&gt;<br />              &lt;App /&gt;<br />            &lt;/Route&gt;<br />          &lt;/ErrorBoundary&gt;<br />        &lt;/StyledEngineProvider&gt;<br />      &lt;/Suspense&gt;<br />    &lt;/Provider&gt;<br />  );<br />}</pre><p>Each package ownsÂ its:</p><ul><li>Webpack configuration</li><li>Dependencies</li><li>Tests</li><li>Build output</li><li>Version number</li><li>Sub routes of each child'sÂ app</li></ul><h4><strong>Layer 3: SharedÂ Code</strong></h4><p>The <strong>@main-app-fe/common</strong> package holds utilities used by multiple apps. This help avoids code duplication and cross-package coupling, enforces a clean one-way dependency direction.</p><pre>// packages/common/src/index.js<br />export { default as useDebounce } from './utilities/useDebounce';<br />export { useScroll } from './utilities/useScroll';<br />export { useForm } from './utilities/useForm';<br />export { default as useIntersectionObserver } from './utilities/useIntersectionObserver';<br />export { default as i18n } from './i18n';<br />export {default as FormComponent} from './common-ui/Form';</pre><p>Common uses <strong>peerDependencies</strong> for React and i18next to avoid duplicate bundles.</p><pre>{<br />  &quot;name&quot;: &quot;@main-app-fe/common&quot;,<br />  &quot;version&quot;: &quot;1.0.0&quot;,<br />  &quot;description&quot;: &quot;Common utilities and component for Webapp&quot;,<br />  &quot;main&quot;: &quot;src/index.js&quot;,<br />  &quot;scripts&quot;: {<br />    &quot;lint&quot;: &quot;eslint ./src/ -f stylish&quot;,<br />    &quot;lint:fix&quot;: &quot;eslint --quiet ./src/ -f stylish --fix&quot;<br />  },<br />  &quot;peerDependencies&quot;: {<br />    &quot;react&quot;: &quot;^18.3.1&quot;,<br />    &quot;i18next&quot;: &quot;^23.14.0&quot;<br />  }<br />}</pre><h4><strong>Workspace Wiring</strong></h4><p>The root <strong>package.json</strong> declares npm workspaces</p><pre>{<br />  &quot;name&quot;: &quot;main-app-fe-monorepo&quot;,<br />  &quot;workspaces&quot;: [&quot;apps/*&quot;, &quot;packages/*&quot;]<br />}</pre><p>The shell declares workspace dependencies. The â€*â€ means â€œuse whatever is in the local workspace.â€ npm resolves these to symlinks, not registryÂ lookups.</p><pre>{<br />  &quot;dependencies&quot;: {<br />    &quot;@main-app-fe/application-1&quot;: &quot;*&quot;,<br />    &quot;@main-app-fe/application-2&quot;: &quot;*&quot;<br />  }<br />}</pre><h4><strong>Turborepo: Build Orchestration</strong></h4><p><strong>turbo.json</strong> defines the task dependency graph:</p><pre>{<br />  &quot;$schema&quot;: &quot;https://turbo.build/schema.json&quot;,<br />  &quot;tasks&quot;: {<br />    &quot;build&quot;: {<br />      &quot;dependsOn&quot;: [&quot;^build&quot;],<br />      &quot;outputs&quot;: [&quot;dist/**&quot;, &quot;build/**&quot;]<br />    },<br />    &quot;dev&quot;: {<br />      &quot;persistent&quot;: true,<br />      &quot;cache&quot;: false<br />    },<br />    &quot;test&quot;: {<br />      &quot;dependsOn&quot;: [&quot;^build&quot;],<br />      &quot;outputs&quot;: [&quot;coverage/**&quot;]<br />    },<br />    &quot;lint&quot;: {}<br />  }<br />}</pre><p>The <strong>dependsOn: [^build]</strong> is the key line. The `^` prefix means <strong>build my dependencies first</strong>. So when you run `turbo runÂ build`:</p><p>1. <strong>@main-app-fe/common</strong> builds first (no dependencies)</p><p>2. <strong>@main-app-fe/application-1</strong> and <strong>@main-app-fe/application-2</strong> build in <strong>parallel</strong> (both depend onÂ common)</p><p>3. <strong>shell</strong> builds last (depends on <strong>application-1</strong> and <strong>application-2</strong>)</p><p>Turbo handles caching, parallelisation, and dependency ordering automatically.</p><h4><strong>Development Server Architecture</strong></h4><p>Each package has its own webpack dev server, but the <strong>shell acts as the gateway</strong> in development</p><pre>Browser â†’ Shell (port 3000)<br />              â”œâ”€â”€ /application-1/* â†’ application-1 chunks (lazy-loaded)<br />              â”œâ”€â”€ /application-2/* â†’ application-1 chunks (lazy-loaded)<br />              â”œâ”€â”€ /api/*     â†’ Backend proxy (port 8090)</pre><p>The shellâ€™s webpack config handles proxy routing for both applicationsâ€™ backends.</p><pre>// apps/shell/config/webpack.deploy.js<br />devServer: {<br />  port: 3000,<br />  proxy: {<br />    '/api/': { target: process.env.APPLICATION_BACKEND_URL },<br />    '/app-1/api/': { target: process.env.APPLICATION_1_BACKEND_URL },<br />    '/app-2/api/': { target: process.env.APPLICATION_2_BACKEND_URL },<br />  },<br />  static: [<br />    { directory: '../packages/application-1/src/assets', publicPath: '/application-1/assets' },<br />    { directory: '../packages/application-1/build/locales', publicPath: '/application-1/locales' },<br />  ]<br />}</pre><p>We have also added that selective development is possible via TurboÂ filters.</p><pre>npm run dev:shell     # Full stack with shell orchestrating<br />npm run dev:imaging   # Imaging standalone on port 3002<br />npm run dev:console   # Console standalone on port 3001</pre><h4><strong>Independent Versioning andÂ Rollback</strong></h4><p>This is where the architecture pays off. Each package has its own version. We use <strong>package-prefixed git tags</strong> for release tracking.</p><pre>// packages/imaging/package.json<br />{ &quot;name&quot;: &quot;@imaging-fe/imaging&quot;, &quot;version&quot;: &quot;3.6.0&quot; }<br /><br />// packages/console/package.json<br />{ &quot;name&quot;: &quot;@imaging-fe/console&quot;, &quot;version&quot;: &quot;1.0.0&quot; }<br /><br />tag_package() {<br />    local package=$1<br />    local version=$2<br />    local tag=&quot;${package}-v${version}&quot;<br /><br />    git tag -a &quot;${tag}&quot; -m &quot;Release ${package} v${version}&quot;<br />    git push origin &quot;${tag}&quot;<br />}<br /><br /># Release independently<br />./release.sh imaging    # Creates tag: imaging-v3.6.0<br />./release.sh console    # Creates tag: console-v1.0.0</pre><p><strong>Rolling back</strong> application 1 is a single command. Application 2 remains completely untouched. No collateral damage</p><pre># List available versions<br />git tag | grep imaging-v<br /># imaging-v3.4.0<br /># imaging-v3.5.0<br /># imaging-v3.6.0<br /><br /># Revert imaging to a known-good version<br />git checkout imaging-v3.5.0 -- packages/imaging/<br />git add packages/imaging/<br />git commit -m &quot;Revert imaging to v3.5.0&quot;</pre><h4><strong>Cross-App Navigation</strong></h4><p>Users move between apps seamlesslyâ€Šâ€”â€Šno pageÂ reloads</p><pre>// In Console â€” link to Imaging<br />&lt;Link to=&quot;/application-1/home/default/MyApp/overview&quot;&gt;<br />  Open in Imaging<br />&lt;/Link&gt;<br /><br />// In Imaging â€” link to Console<br />&lt;Link to=&quot;/application-2/dashboard&quot;&gt;<br />  Back to Console<br />&lt;/Link&gt;</pre><p>React Router in the shell detects the path change and swaps the active app via lazy loading. The user experiences a single-page application.</p><h4>In ShortÂ â€”</h4><ol><li><strong>Keep the shell dumb: </strong>The shell should have zero business logic. No Redux, no API calls, no state. It routes and loads chunks. Thatâ€™sÂ it.</li><li><strong>Common packages need peer dependencies: </strong>If a <strong>common</strong> bundles its own React, you get two React instances at runtime. Use <strong>peerDependencies</strong> for shared libraries.</li><li><strong>Workspace `â€*â€` is a feature, not a bug: </strong>In a monorepo, <strong><em>â€*â€</em></strong> means â€œuse the local version.â€ This is intentionalâ€Šâ€”â€Šyou always want the local workspace package, not a published one. Version numbers in package.json are metadata for tagging, not for dependency resolution.</li><li><strong>Git tags beat npm versions for rollback: </strong>In a monorepo, you canâ€™t `npm install <strong>@main-app-fe/application-1@3.5.0</strong>â€Šâ€”â€Šitâ€™s not published. Package-prefixed git tags (`application-1-v3.5.0`) give you the same rollback capability with zero infrastructure.</li><li><strong>Turboâ€™s `^` operator is essential: dependsOn: [^build]</strong> ensures packages build in dependency order. Without it, the shell might try to import imaging before itâ€™sÂ built.</li><li><strong>Separate proxy configs per app: </strong>Each app talks to different backends. The shellâ€™s webpack dev server needs separate proxy blocks for `/api/` vs `/application-2/api/`. Environment variables per backend keep configsÂ clean.</li><li>P<strong>ackage structure is a stepping stone to true micro-frontends: </strong>Because each package is self-containedâ€Šâ€”â€Šwith its own routes, state management, dependencies, and build configâ€Šâ€”â€Šextracting one into a separate repository is straightforward.</li></ol><h4>You can explore some of my other articlesÂ here:</h4><ol><li><a href=\"https://medium.com/javascript-in-plain-english/stop-the-drain-a-practical-guide-to-fixing-memory-leaks-in-frontend-apps-with-chrome-devtools-aeca19a3bdbb\">A Practical Guide to Fixing Memory Leaks in Frontend Apps with ChromeÂ DevTools</a></li><li><a href=\"https://medium.com/javascript-in-plain-english/nielsen-mts3-ui-interview-experience-e8de5d906bd8\">Nielsen MTS3 UI Interview Experience</a></li><li><a href=\"https://medium.com/javascript-in-plain-english/cast-interview-experience-senior-software-engineer-fullstack-frontend-heavy-9bf3e116558e\">CAST Interview Experience Senior Software Engineer FullStack (Frontend heavy)</a></li><li><a href=\"https://medium.com/@diptom/platform9-senior-frontend-engineer-interview-experience-d15faa1566db\">Platform9 Senior Frontend Engineer Interview Experience</a></li></ol><h3>ğŸ’¡ Need Personalised Guidance?</h3><p>If youâ€™d like <strong>1:1 guidance, Resume reviews, or a mock interview</strong>, you can book a session with me viaÂ <a href=\"https://topmate.io/diptom_saha\"><strong>Topmate</strong></a>.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=90acd00d5d14\" width=\"1\" /><hr /><p><a href=\"https://javascript.plainenglish.io/how-to-migrate-a-monolithic-react-app-to-a-turborepo-monorepo-and-make-it-rollback-friendly-90acd00d5d14\">How to Migrate a Monolithic React App to a Turborepo Monorepoâ€Šâ€”â€Šand Make it Rollback-Friendly</a> was originally published in <a href=\"https://javascript.plainenglish.io\">JavaScript in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "Angular Isnâ€™t â€œHeavyâ€. Your Architecture Is.",
      "url": "https://javascript.plainenglish.io/angular-isnt-heavy-your-architecture-is-3850ed242ebf?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:55:29",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "angular",
        "frontend",
        "web-development",
        "programming",
        "software-development"
      ],
      "content_length": 597,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/angular-isnt-heavy-your-architecture-is-3850ed242ebf?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/0*sau_X3lxIV-0JAZJ\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">If Your Angular App Needs NgRx Everywhere, You Already Lost.</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/angular-isnt-heavy-your-architecture-is-3850ed242eb",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/angular-isnt-heavy-your-architecture-is-3850ed242ebf?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/0*sau_X3lxIV-0JAZJ\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">If Your Angular App Needs NgRx Everywhere, You Already Lost.</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/angular-isnt-heavy-your-architecture-is-3850ed242ebf?source=rss----4b3a1ed4f11c---4\">Continue reading on JavaScript in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Bun.js vs Node.js: Beyond Benchmarksâ€Šâ€”â€ŠWhat Really Matters in Production",
      "url": "https://javascript.plainenglish.io/bun-js-vs-node-js-beyond-benchmarks-what-really-matters-in-production-3b163ec0a5d6?source=rss----4b3a1ed4f11c---4",
      "source": "medium",
      "published_at": "2026-02-09T17:55:28",
      "external_id": null,
      "tags": [
        "javascript-in-plain-english",
        "bunjs",
        "software-development",
        "javascript",
        "nodejs",
        "programming"
      ],
      "content_length": 740,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/bun-js-vs-node-js-beyond-benchmarks-what-really-matters-in-production-3b163ec0a5d6?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/700/0*JRgBFRpGfEI3HjNi.png\" width=\"700\" /></a></p><p class=\"medium-feed-snippet\">When Bun.js first came into the spotlight, it made waves with bold claims: 3x faster than Node.js, blazing benchmarks, and JavaScript&#x2026;</p><p class=\"",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://javascript.plainenglish.io/bun-js-vs-node-js-beyond-benchmarks-what-really-matters-in-production-3b163ec0a5d6?source=rss----4b3a1ed4f11c---4\"><img src=\"https://cdn-images-1.medium.com/max/700/0*JRgBFRpGfEI3HjNi.png\" width=\"700\" /></a></p><p class=\"medium-feed-snippet\">When Bun.js first came into the spotlight, it made waves with bold claims: 3x faster than Node.js, blazing benchmarks, and JavaScript&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://javascript.plainenglish.io/bun-js-vs-node-js-beyond-benchmarks-what-really-matters-in-production-3b163ec0a5d6?source=rss----4b3a1ed4f11c---4\">Continue reading on JavaScript in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "I Built a Self-Hosted Google Trends Alternative with DuckDB",
      "url": "https://python.plainenglish.io/i-built-a-self-hosted-google-trends-alternative-with-duckdb-624a19bcab65?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-10T08:24:40",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "technology",
        "web-development",
        "data-science",
        "python",
        "programming"
      ],
      "content_length": 47383,
      "content_preview": "<h4>Track SERP rankings, title changes, and competitor data Google Trends wonâ€™t show. Built with Python, DuckDB, and a CLI-first approach.</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2PiTUTbiujJZ8YutGdMwdw.png\" /></figure><p>Google Trends will tell you if people are searching for â€œreactâ€ or â€œnextjsâ€. But it wonâ€™t tell you that Stack Overflow just got bumped from position #2 to #7, or that Vercel changed their landing page title five times this month trying to improve ",
      "content_full": "<h4>Track SERP rankings, title changes, and competitor data Google Trends wonâ€™t show. Built with Python, DuckDB, and a CLI-first approach.</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2PiTUTbiujJZ8YutGdMwdw.png\" /></figure><p>Google Trends will tell you if people are searching for â€œreactâ€ or â€œnextjsâ€. But it wonâ€™t tell you that Stack Overflow just got bumped from position #2 to #7, or that Vercel changed their landing page title five times this month trying to improve click-through rate.</p><p>If, say, youâ€™re an indie dev launching a product, needing every edge available, <em>thatâ€™s </em>the data that actually matters toÂ you.</p><p>So I spent a weekend building a tool to track it. I couldâ€™ve just paid for Ahrefs/Semrush etc. But building this taughtÂ me:</p><ul><li>How SERP APIs work under theÂ hood</li><li>How to model time-series data in SQL (and itsÂ gotchas)</li><li>How to calculate derived metrics (interest score) from rawÂ data</li><li>How DuckDB handles analytical queries</li></ul><p>â€¦and also because I didnâ€™t really want to spend anywhere near <em>thatÂ </em>much.ğŸ˜…</p><p>Ironically, focusing on CLI only made the tool <em>more</em> usefulâ€Šâ€”â€ŠI can use this, then pipe results to jq, schedule fetches with cron, and script complex workflows without fighting a web framework.</p><p>If I want a dashboard later, I can always add FastAPI in ~50 lines. But for now, the CLI is enough. Hereâ€™s how I builtÂ it.</p><blockquote>If youâ€™d like to tinker, the full code to this is on GitHub. Feel free to star, clone, fork, whatever: <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/\">https://github.com/sixthextinction/duckdb-google-trends-basic/</a></blockquote><h3>Why Google Trends Isnâ€™t Enough (And Why SEO Tools Cost $200/Month)</h3><p>Google Trends answers one question really well: â€œHow many people are searching forÂ X?â€</p><p>But if youâ€™re building a product, writing technical content, or trying to rank for competitive keywords, you need to answer different questions:</p><ul><li>Which competitors are winning in search results rightÂ now?</li><li>When did that tutorial site enter the topÂ 10?</li><li>Is my rank drop because Google reshuffled the entire SERP, or justÂ me?</li><li>What headlines are competitors A/BÂ testing?</li></ul><p>Tools like Ahrefs and SEMrush answer these questions. They cost $99â€“500/month, but I just wanted something I could self-host for the cost of API calls + would be doable as a weekendÂ project.</p><h3><strong>Why I Use Google Results Volatility as a Proxy for SearchÂ Interest</strong></h3><p>This works because of ONE reasonâ€Šâ€”â€Šwhen search interest in a term rises, Googleâ€™s top 10 results become <em>volatile</em>.</p><p>â€œVolatilityâ€ here simply means that new domains enter, rankings shift around, sites update their titles and snippets to capture more clicks, etc. You get the pictureâ€Šâ€”â€Šessentially, the search engine results page becomesÂ chaotic.</p><p>Conversely, when interest in a term is stable or declining, Googleâ€™s top 10 ossifies. The same Wikipedia article, the same W3Schools tutorial, the same official docs sit in positions 1â€“3 forÂ months.</p><p>So I donâ€™t really need to track raw search volume (which I canâ€™t have access toâ€Šâ€”â€ŠIâ€™m not Google), I can just track these threeÂ things:</p><ul><li><strong>New domains entering top 10</strong> because itâ€™s a signal of rising interest or new content opportunities</li><li><strong>Average rank improvement</strong> because itâ€™s a signal of SERP instability</li><li><strong>Domain overlap ratio</strong> because it measures how many domains persist between snapshots (complementing newÂ domains)</li></ul><p>Turns out, if I aggregate these three signals into a single 0â€“100 score (Iâ€™ll talk about the formula in just a bit), I get something that behaves <em>remarkably </em>like Google Trendsâ€Šâ€”â€Šbut tells me a <em>lot </em>more than just how many are searching.</p><h3>The System Architecture</h3><p>The entire system is about ~1000 lines of Python and runs locally with no server required.</p><p>Hereâ€™s how itÂ works:</p><figure><img alt=\"Data flow diagram showing two paths: fetch path flows from CLI commands through scraper and SERP client to Bright Data API, then stores results in DuckDB database tables (serp_snapshots and interest_scores). Analytics path reads from the database tables and returns results to CLI output. Interest scores are automatically calculated during snapshot insertion by comparing with previous dayâ€™s data\" src=\"https://cdn-images-1.medium.com/max/812/1*rjuc3ZqEuvrV-tUK7Gc23Q.jpeg\" /></figure><p>I started small with this oneâ€Šâ€”â€Šusing only daily snapshots, not live queries. Each run appends point-in-time data instead of overwriting. This way, over 7â€“30 days, I could build a local historical dataset that I could queryÂ freely.</p><p>I use <a href=\"https://duckdb.org/\">DuckDB</a> for this. For this workload (rank comparisons, volatility calculations, detecting new entrants), DuckDBâ€™s SQL engine isÂ ideal.</p><ul><li>Itâ€™s columnar, so analytical queries over time-series data are fast. (If you want to know more, I covered columnar formats vs. JSON <a href=\"https://medium.com/python-in-plain-english/stop-paying-the-json-tax-build-faster-data-pipelines-in-python-with-apache-arrow-a37ce670a1f1\">in this blogÂ post</a>)</li><li>It handles indexing, window functions (LAG(), PARTITION BYâ€Šâ€”â€Šwhich we will use extensively), and aggregations without needing a server or cloud warehouse.</li><li>It has an in-process design, meaning no separate database serverâ€Šâ€”â€Šour project will need just a Python library and aÂ file.</li><li>Plus, since itâ€™s a single file (Our &quot;databaseâ€ just lives in data/serp_data.duckdb), backups are trivial and thereâ€™s zero configuration overhead.</li></ul><p><a href=\"https://duckdb.org/docs/stable/\">Documentation</a></p><h3>My Interest ScoreÂ Formula</h3><p>Every day, for each keyword, the system calculates a 0â€“100 â€œSearch Interest Scoreâ€ based on how much the SERP moved compared to the previousÂ day.</p><p>Iâ€™m not gonna get into the math, but basically, I did some research on Google Trends scoring, adapted it for my needs, and split my scoring logic into 3 weightedÂ parts:</p><h4>1. New Domains Entering Top 10 (0â€“40Â points)</h4><blockquote><em>new_domains = current_top10â€Šâ€”â€Šprevious_top10</em></blockquote><blockquote><em>new_domains_score = min(len(new_domains) * 4,Â 40)</em></blockquote><p>If 3 new sites enter the top 10, thatâ€™s 12 points. If 10 new sites appear (rare but possible during breaking news or major updates), that maxes out at 40Â points.</p><h4>2. Average Rank Improvement (0â€“30Â points)</h4><p>For each domain that appears in both snapshots</p><blockquote><em>rank_improvement = previous_rankâ€Šâ€”â€Šcurrent_rank</em></blockquote><p>A positive value here means it movedÂ up.</p><p>Now, average across all domains, normalized to -10 to +10Â range</p><blockquote><em>avg_improvement = mean(rank_improvements)</em></blockquote><blockquote><em>rank_improvement_score = min(max((avg_improvement + 10) / 20 * 30, 0),Â 30)</em></blockquote><p>If the average site improved by 2 positions, thatâ€™s roughly 18 points. If rankings barely moved, this stays close to 15 (neutral).</p><h4>3. Domain Overlap Ratio (0â€“30Â points)</h4><p>Finally, how many of todayâ€™s top 10 domains <em>also</em> appeared in yesterdayâ€™s topÂ 10?</p><blockquote>reshuffle_count = count(domains present in both current and previous topÂ 10)</blockquote><blockquote>reshuffle_frequency = reshuffle_count / max(len(current_domains_set), 1)</blockquote><blockquote>reshuffle_score = reshuffle_frequency *Â 30</blockquote><p>Letâ€™s say 8 out of 10 domains carry over from yesterday, thatâ€™s 24 points. If only 3 carry over (meaning 7 are newâ€Šâ€”â€Ša massive reshuffle), thatâ€™s 9 points. This complements the new domains score by capturing continuity.</p><h4>Total Score</h4><p>So, taking all three parts togetherâ€¦</p><blockquote><em>interest_score = new_domains_score + rank_improvement_score + reshuffle_score</em></blockquote><p>High scores (60â€“100) = lots of movement = rising interest or major SERP disruption.</p><p>Low scores (0â€“40) = stable, ossified rankings = same old, same old. Established content is dominant.</p><h3>What This Looks Like inÂ Practice</h3><p>I tested this by tracking â€œnextjsâ€ for 7 days likeÂ this.</p><pre>python main.py scores --query &quot;nextjs&quot; --days 7</pre><p>Hereâ€™s what the output lookedÂ like:</p><pre>=== Interest Scores for 'nextjs' (last 7 days) ===<br /><br />Found 7 scores:<br /><br />| snapshot_date | interest_score | new_domains | avg_rank_improvement | reshuffle_freq |<br />|---------------|----------------|-------------|----------------------|----------------|<br />| 2026-02-01    | 45.2           | 2           | 1.5                  | 0.6            |<br />| 2026-02-02    | 52.3           | 3           | 2.1                  | 0.7            |<br />| 2026-02-03    | 38.7           | 1           | 0.8                  | 0.5            |<br />| 2026-02-04    | 61.4           | 4           | 3.2                  | 0.8            |<br />| 2026-02-05    | 42.1           | 2           | 1.2                  | 0.6            |<br />| 2026-02-06    | 55.8           | 3           | 2.5                  | 0.7            |<br />| 2026-02-07    | 48.3           | 2           | 1.8                  | 0.6            |<br /><br />Chart saved to: nextjs_trend.png<br /><br />Summary: Min: 38.7  Max: 61.4  Avg: 49.1</pre><p>Hereâ€™s that generated chart (Iâ€™m using basic matplotlib forÂ these):</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*F4f5rB4cZlkCn-OucbB4Mw.png\" /><figcaption>Matplotlib generated Search Interest Trend graph for the term â€œnextjsâ€. I tracked this over 7 days in the tool, running once perÂ day.</figcaption></figure><p>That spike on Feb 4 (interest_score = 61.4) indicates a major SERP reshuffleâ€Šâ€”â€Šprobably a Google algorithm update or a major new tutorial entering the rankings.</p><h3>Actually BuildingÂ It</h3><p>Before diving into the code, hereâ€™s the birdâ€™s-eye view of how the system fits together.</p><p>The entire project is driven by a small CLI (powered by <a href=\"https://docs.python.org/3/library/argparse.html\">argparse</a>) in main.py. This file doesnâ€™t contain any scraping or analytics logicâ€Šâ€”â€Šitâ€™s just the orchestration layer that wires everything together.</p><blockquote>Read the full code for <strong>main.py</strong> here: <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/main.py\">https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/main.py</a></blockquote><p>You run specific commands (fetch to get SERP data for a keyword, volatility to analyze rank volatility for a keyword over a period of time, scores to view interest scores for a keyword over time, etc.) likeÂ so:</p><pre>python main.py fetch --keywords &quot;python&quot;<br />python main.py volatility --query &quot;python&quot; --days 30<br />python main.py scores --query &quot;python&quot; --days 90</pre><p>The CLI dispatch uses argparse subcommands to wire everything together:</p><pre>def main():  <br />    parser = argparse.ArgumentParser(description=&quot;DuckDB Google Trends&quot;)  <br />    subparsers = parser.add_subparsers(dest='command', help='Commands')  <br />      <br />    # Each command gets its own parser with relevant arguments  <br />    fetch_parser = subparsers.add_parser('fetch', help='Fetch SERP snapshots')  <br />    fetch_parser.add_argument('--keywords', nargs='+', help='Keywords to track')  <br />    fetch_parser.add_argument('--num-results', type=int, default=10)  <br />    fetch_parser.add_argument('--delay', type=float, default=1.0)  <br />      <br />    scores_parser = subparsers.add_parser('scores', help='Show interest scores')  <br />    scores_parser.add_argument('--query', required=True)  <br />    scores_parser.add_argument('--days', type=int, default=90)  <br />    scores_parser.add_argument('--output', type=str)  <br />      <br />    # ... similar parsers for analyze, volatility, new-entrants, changes, calculate-scores  <br />      <br />    args = parser.parse_args()  <br />    commands = {  <br />        'fetch': cmd_fetch,  <br />        'analyze': cmd_analyze,  <br />        'volatility': cmd_volatility,  <br />        'new-entrants': cmd_new_entrants,  <br />        'changes': cmd_changes,  <br />        'calculate-scores': cmd_calculate_scores,  <br />        'scores': cmd_scores  <br />    }  <br />    commands[args.command](args)</pre><p>Our main.py defines commands that map directly to the questions we want toÂ ask:</p><ul><li><strong>fetchâ€Š</strong>â€”â€Šcollect todayâ€™s SERP results for a set ofÂ keywords</li><li><strong>analyzeâ€Š</strong>â€”â€Šinspect the shape of the collected data</li><li><strong>volatilityâ€Š</strong>â€”â€Šmeasure how rankings change overÂ time</li><li><strong>new-entrants</strong>â€Šâ€”â€Šdetect URLs appearing for the firstÂ time</li><li><strong>changesâ€Š</strong>â€”â€Štrack title and snippetÂ updates</li><li><strong>calculate-scores</strong>â€Šâ€”â€Šrecalculate interest scores for existing snapshots</li><li><strong>scoresâ€Š</strong>â€”â€Šview the calculated interest scoreÂ trend</li></ul><p>For example, hereâ€™s the key command handler for the scores command (the other handlers follow the same pattern):</p><pre># Usage: python main.py scores --query &quot;python&quot; --days 90<br />def cmd_scores(args):  <br />    &quot;&quot;&quot;Show interest scores for a query&quot;&quot;&quot;  <br />    with SERPAnalytics() as analytics:  <br />        result = analytics.interest_scores(args.query, days=args.days)  <br />          <br />        print(f&quot;\\n=== Interest Scores for '{result['query']}' (last {result['days']} days) ===&quot;)  <br />        if len(result['results']) == 0:  <br />            print(&quot;No interest scores found&quot;)<br />            print(&quot;Note: Interest scores require at least 2 snapshots on different days.&quot;)    <br />            print(&quot;To calculate scores for existing data, run:&quot;)    <br />            print(f&quot;  python main.py calculate-scores --keywords {args.query}&quot;)     <br />            return  <br />          <br />        print(f&quot;\\nFound {len(result['results'])} scores:\\n&quot;)  <br />        print(df_to_markdown(result['results']))  <br />          <br />        # Generate PNG chart  <br />        output_path = args.output or f&quot;{args.query.replace(' ', '_')}_trend.png&quot;  <br />        _generate_png_chart(result['results'], args.query, args.days, output_path)  <br />        print(f&quot;\\nChart saved to: {output_path}&quot;)</pre><p>At a high level, what can our projectÂ do?</p><ol><li>Fetch daily SERP snapshots for keywords (fetchÂ command)</li><li>Store those snapshots locally inÂ DuckDB</li><li>Run analytical queries over historical data using plain oldÂ SQL</li><li>When you run python main.py scores --query â€œnextjsâ€, the CLI fetches interest scores from DuckDB and as an added bonus, generates a PNG chart using matplotlib. Note that this shows SERP movement, not raw searchÂ volume.</li></ol><p>We donâ€™t need servers, background workers, or dashboards here.</p><p>Now that we know how this tool works, letâ€™s look at the major modules that make all thisÂ happen.</p><h3>Module 1: Fetching SERPÂ Data</h3><p>All external data access is isolated in serp_client.py. I only have access to one SERP APIâ€Šâ€”â€ŠBright Dataâ€Šâ€”â€Šso Iâ€™ll only have to implement one class. Get your credentials <a href=\"https://get.brightdata.com/bd-serp-api?utm_content=i_built_a_self_hosted_google_trends_alternative_with_duckdb\">here</a>.</p><p><a href=\"https://get.brightdata.com/bd-serp-api?utm_content=i_built_a_self_hosted_google_trends_alternative_with_duckdb\">SERP API - SERP Scraper API - Free Trial</a></p><p>This approach does make it easy to extend it with other SERP APIs: just write another client, and include its credentials in your envÂ file.</p><blockquote>Read the full code for <strong>serp_client.py </strong>here: <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/serp_client.py\">https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/serp_client.py</a></blockquote><pre>&quot;&quot;&quot;  <br />Bright Data SERP API client  <br />&quot;&quot;&quot;  <br />  <br />import os  <br />import json  <br />import requests  <br />from typing import Dict, Any, Optional  <br />from dotenv import load_dotenv  <br />  <br />load_dotenv()  <br />  <br />  <br />class BrightDataClient:  <br />    &quot;&quot;&quot;Client for Bright Data SERP API&quot;&quot;&quot;  <br />      <br />    def __init__( self,  <br />        api_key: Optional[str] = None,  <br />        zone: Optional[str] = None,  <br />        country: Optional[str] = None ):  <br />        env_api_key = os.getenv(&quot;BRIGHT_DATA_API_KEY&quot;)  <br />        env_zone = os.getenv(&quot;BRIGHT_DATA_ZONE&quot;)  <br />        env_country = os.getenv(&quot;BRIGHT_DATA_COUNTRY&quot;)  <br />          <br />        self.api_key = api_key or env_api_key  <br />        self.zone = zone or env_zone  <br />        self.country = country or env_country  <br />        self.api_endpoint = &quot;https://api.brightdata.com/request&quot;  <br />          <br />        if not self.api_key:  <br />            raise ValueError(  <br />                &quot;BRIGHT_DATA_API_KEY must be provided via constructor or environment variable&quot;  <br />            )  <br />          <br />        if not self.zone:  <br />            raise ValueError(  <br />                &quot;BRIGHT_DATA_ZONE must be provided via constructor or environment variable&quot;  <br />            )  <br />          <br />        self.session = requests.Session()  <br />        self.session.headers.update({  <br />            'Content-Type': 'application/json',  <br />            'Authorization': f'Bearer {self.api_key}'  <br />        })  <br />      <br />    def search( self,  <br />        query: str,  <br />        num_results: int = 10,  <br />        language: Optional[str] = None,  <br />        country: Optional[str] = None ) -&gt; Dict[str, Any]:  <br />        &quot;&quot;&quot;Execute a Google search via Bright Data SERP API&quot;&quot;&quot;  <br />        search_url = (  <br />            f&quot;https://www.google.com/search&quot;  <br />            f&quot;?q={requests.utils.quote(query)}&quot;  <br />            f&quot;&amp;num={num_results}&quot;  <br />            f&quot;&amp;brd_json=1&quot;  <br />        )  <br />          <br />        if language:  <br />            search_url += f&quot;&amp;hl={language}&amp;lr=lang_{language}&quot;  <br />          <br />        target_country = country or self.country  <br />          <br />        payload = {  <br />            'zone': self.zone,  <br />            'url': search_url,  <br />            'format': 'json'  <br />        }  <br />          <br />        if target_country:  <br />            payload['country'] = target_country  <br />          <br />        try:  <br />            response = self.session.post(  <br />                self.api_endpoint,  <br />                json=payload,  <br />                timeout=30  <br />            )  <br />            response.raise_for_status()  <br />            result = response.json()  <br />              <br />            # Parse body JSON string if present  <br />            if isinstance(result, dict) and 'body' in result:  <br />                if isinstance(result['body'], str):  <br />                    result['body'] = json.loads(result['body'])  <br />                # Return the parsed body content  <br />                return result['body']  <br />              <br />            return result  <br />              <br />        except requests.exceptions.HTTPError as e:  <br />            error_msg = f&quot;Search request failed with HTTP {e.response.status_code}&quot;  <br />            if e.response.text:  <br />                error_msg += f&quot;: {e.response.text[:200]}&quot;  <br />            raise RuntimeError(error_msg) from e  <br />        except requests.exceptions.RequestException as e:  <br />            raise RuntimeError(f&quot;Search request failed: {e}&quot;) from e</pre><p>This is just a thin wrapper around the Bright Data API. It takes a query, returns JSON with organic search results (title, snippet, URL,Â rank).</p><p>This module is called when we run the fetchÂ command.</p><pre>python main.py fetch --keywords &quot;python&quot; &quot;javascript&quot; &quot;react&quot;</pre><p>This will connect to the Bright Data SERP API, and for each keyword, fetch Google search results (default of 10 per keyword, adjust as necessary), and extract + store organic results (title, snippet, URL, rank). Remember, interest scores require at least 2 snapshots on different days. You should fetch snapshots daily to build historical trends (cron job, or just running manually.)</p><p><strong>Example output:</strong></p><pre>Fetching snapshots for 3 keywordsâ€¦<br />[1/3] 'python': 10 results<br />[2/3] 'javascript': 10 results<br />[3/3] 'react': 10 results<br />Total snapshots in database: 30</pre><h3>Module 2: Storing Snapshots inÂ DuckDB</h3><p>Once SERP data is fetched by the previous module, it needs to be stored in DuckDB for analytical queries. That logic lives in duckdb_manager.py</p><blockquote>Read the full code for <strong>duckdb_manager.py</strong> here: <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/duckdb_manager.py\">https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/duckdb_manager.py</a></blockquote><p>First of all, letâ€™s introduce the schema weâ€™ll beÂ using:</p><pre>CREATE TABLE IF NOT EXISTS serp_snapshots (  <br />    snapshot_id BIGINT PRIMARY KEY,  <br />    query TEXT NOT NULL,  <br />    snapshot_date DATE NOT NULL,  <br />    snapshot_timestamp TIMESTAMP NOT NULL,  <br />    url TEXT NOT NULL,  <br />    title TEXT,  <br />    snippet TEXT,  <br />    domain TEXT,  <br />    rank INTEGER NOT NULL,  <br />    UNIQUE(query, snapshot_date, url)  <br />)  <br />  <br />-- Interest scores table (calculated from SERP movement between snapshots)  <br />CREATE TABLE IF NOT EXISTS interest_scores (  <br />    query TEXT NOT NULL,  <br />    snapshot_date DATE NOT NULL,  <br />    interest_score DOUBLE NOT NULL,  <br />    new_domains_count INTEGER,  <br />    avg_rank_improvement DOUBLE,  <br />    reshuffle_frequency DOUBLE,  <br />    UNIQUE(query, snapshot_date)  <br />)  <br />  <br />-- Indexes for fast queries  <br />CREATE INDEX IF NOT EXISTS idx_query_date ON serp_snapshots(query, snapshot_date)  <br />CREATE INDEX IF NOT EXISTS idx_url_query ON serp_snapshots(url, query)  <br />CREATE INDEX IF NOT EXISTS idx_interest_scores ON interest_scores(query, snapshot_date)</pre><p>Each SERP result becomes a row, keyed by (query, date, URL). Interest scores are stored in a separate table, calculated automatically when a new snapshot is inserted. So, inserting a snapshot will look likeÂ this:</p><pre>def insert_snapshot(self, results: List[Dict[str, Any]], query: str,   <br />                   snapshot_date: Optional[datetime] = None):  <br />    &quot;&quot;&quot;Insert a daily snapshot of SERP results&quot;&quot;&quot;  <br />    if snapshot_date is None:  <br />        snapshot_date = datetime.now()  <br />      <br />    snapshot_timestamp = snapshot_date  <br />    snapshot_date_only = snapshot_date.date() if hasattr(snapshot_date, 'date') else snapshot_date  <br />      <br />    if not results:  <br />        return  <br />      <br />    def extract_domain(url: str) -&gt; str:  <br />        &quot;&quot;&quot;Extract domain from URL, stripping www prefix&quot;&quot;&quot;  <br />        if not url:  <br />            return &quot;&quot;  <br />        try:  <br />            from urllib.parse import urlparse  <br />            parsed = urlparse(url)    <br />            return parsed.netloc.replace(&quot;www.&quot;, &quot;&quot;) <br />        except:  <br />            return &quot;&quot;  <br />      <br />    # Get max snapshot_id   <br />    max_id_result = self.conn.execute(    <br />        &quot;SELECT COALESCE(MAX(snapshot_id), 0) FROM serp_snapshots&quot;    <br />    ).fetchone()    <br />    next_id = (max_id_result[0] if max_id_result else 0) + 1    <br />      <br />    rows = []  <br />    for idx, result in enumerate(results):  <br />        url = result.get('url', result.get('link', ''))  <br />        domain = extract_domain(url)   <br />        rows.append({  <br />            'snapshot_id': next_id + idx,  <br />            'query': query,  <br />            'snapshot_date': snapshot_date_only,  <br />            'snapshot_timestamp': snapshot_timestamp,  <br />            'url': url,  <br />            'title': result.get('title', ''),  <br />            'snippet': result.get('snippet', result.get('description', '')),  <br />            'domain': domain,   <br />            'rank': idx + 1  <br />        })  <br />      <br />    import pandas as pd  <br />    df = pd.DataFrame(rows)  <br />    self.conn.execute(&quot;&quot;&quot;    <br />        INSERT OR IGNORE INTO serp_snapshots     <br />        SELECT * FROM df    <br />    &quot;&quot;&quot;)  <br />    # Calculate and store interest score  <br />    self._calculate_interest_score(query, snapshot_date_only)  <br /></pre><p>Instead of updating rows, every run adds new records. This builds a local time-series dataset.</p><p>Hereâ€™s how we calculate the interest score using that 40â€“30â€“30 formula described earlier:</p><pre>def _calculate_interest_score(self, query: str, snapshot_date):<br />    &quot;&quot;&quot;Calculate Search Interest Score (0-100) based on SERP movement&quot;&quot;&quot;<br />    # Get previous day's snapshot for comparison<br />    prev_date_result = self.conn.execute(&quot;&quot;&quot;<br />        SELECT MAX(snapshot_date) <br />        FROM serp_snapshots <br />        WHERE query = ? <br />          AND snapshot_date &lt; ?<br />    &quot;&quot;&quot;, [query, snapshot_date]).fetchone()<br />    <br />    if not prev_date_result or not prev_date_result[0]:<br />        # First snapshot, no comparison possible<br />        return<br />    <br />    prev_date = prev_date_result[0]<br />    <br />    # Get current top 10 domains<br />    current_domains = self.conn.execute(&quot;&quot;&quot;<br />        SELECT DISTINCT domain <br />        FROM serp_snapshots <br />        WHERE query = ? <br />          AND snapshot_date = ?<br />          AND rank &lt;= 10<br />    &quot;&quot;&quot;, [query, snapshot_date]).fetchall()<br />    current_domains_set = {row[0] for row in current_domains}<br />    <br />    # Get previous top 10 domains<br />    prev_domains = self.conn.execute(&quot;&quot;&quot;<br />        SELECT DISTINCT domain <br />        FROM serp_snapshots <br />        WHERE query = ? <br />          AND snapshot_date = ?<br />          AND rank &lt;= 10<br />    &quot;&quot;&quot;, [query, prev_date]).fetchall()<br />    prev_domains_set = {row[0] for row in prev_domains}<br />    <br />    # Count new domains entering top 10<br />    new_domains = current_domains_set - prev_domains_set<br />    new_domains_count = len(new_domains)<br />    <br />    # Calculate average rank improvement for existing domains<br />    rank_changes = self.conn.execute(&quot;&quot;&quot;<br />        WITH current_ranks AS (<br />            SELECT domain, rank<br />            FROM serp_snapshots<br />            WHERE query = ? AND snapshot_date = ? <br />              AND rank &lt;= 10<br />        ),<br />        prev_ranks AS (<br />            SELECT domain, rank<br />            FROM serp_snapshots<br />            WHERE query = ? AND snapshot_date = ?<br />              AND rank &lt;= 10<br />        )<br />        SELECT <br />            c.domain,<br />            c.rank as current_rank,<br />            p.rank as prev_rank,<br />            (p.rank - c.rank) as rank_improvement<br />        FROM current_ranks c<br />        JOIN prev_ranks p ON c.domain = p.domain<br />    &quot;&quot;&quot;, [query, snapshot_date, query, prev_date]).fetchall()<br />    <br />    if rank_changes:<br />        avg_rank_improvement = sum(row[3] for row in rank_changes) / len(rank_changes)<br />    else:<br />        avg_rank_improvement = 0.0<br />    <br />    # Calculate reshuffle frequency (how many domains changed position)<br />    reshuffle_count = len(rank_changes)<br />    reshuffle_frequency = reshuffle_count / max(len(current_domains_set), 1)<br />    <br />    # Normalize to 0-100 score<br />    # I'm calculating a final score from 3 weighted sub-scores:<br />    # - New domains: 0-10 domains = 0-40 points<br />    # - Rank improvement: -10 to +10 = 0-30 points (normalized)<br />    # - Reshuffle frequency: 0-1 = 0-30 points<br />    <br />    new_domains_score = min(new_domains_count * 4, 40)  # Max 40 points<br />    rank_improvement_score = min(max((avg_rank_improvement + 10) / 20 * 30, 0), 30)  # Max 30 points<br />    reshuffle_score = reshuffle_frequency * 30  # Max 30 points<br />    <br />    interest_score = new_domains_score + rank_improvement_score + reshuffle_score<br />    <br />    # Store interest score<br />    self.conn.execute(&quot;&quot;&quot;<br />        INSERT OR REPLACE INTO interest_scores <br />        (query, snapshot_date, interest_score, new_domains_count, avg_rank_improvement, reshuffle_frequency)<br />        VALUES (?, ?, ?, ?, ?, ?)<br />    &quot;&quot;&quot;, [query, snapshot_date, interest_score, new_domains_count, avg_rank_improvement, reshuffle_frequency])</pre><p>This runs automatically every time a new snapshot is inserted. The score gets stored in a separate interest_scores table for easy querying.</p><h3>Module 3: Analytical Queries</h3><p>The nerdiest of our logic lives in analytics.py. This module opens DuckDB in read-only mode and exposes focusedÂ queries.</p><blockquote>Read the full code for <strong>analytics.py</strong> here: <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/analytics.py\">https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/analytics.py</a></blockquote><p>A good analytical query to demonstrate right now would be the one for rank volatility:</p><pre>def rank_volatility(self, query: str, days: int = 30) -&gt; Dict[str, Any]:  <br />    &quot;&quot;&quot;Calculate rank volatility for URLs over time&quot;&quot;&quot;  <br />    cutoff_date = datetime.now().date() - timedelta(days=days)  <br />      <br />    result = self.conn.execute(&quot;&quot;&quot;  <br />        WITH rank_changes AS (  <br />            SELECT   <br />                url,  <br />                domain,  <br />                rank,  <br />                snapshot_date,  <br />                LAG(rank) OVER (PARTITION BY url ORDER BY snapshot_date) as prev_rank  <br />            FROM serp_snapshots  <br />            WHERE query = ? AND snapshot_date &gt;= ?  <br />            ORDER BY url, snapshot_date  <br />        ),  <br />        volatility AS (  <br />            SELECT   <br />                url,  <br />                domain,  <br />                COUNT(*) as snapshot_count,  <br />                AVG(rank) as avg_rank,  <br />                MIN(rank) as best_rank,  <br />                MAX(rank) as worst_rank,  <br />                STDDEV(rank) as rank_stddev,  <br />                COUNT(CASE WHEN prev_rank IS NOT NULL AND rank != prev_rank THEN 1 END) as rank_changes  <br />            FROM rank_changes  <br />            GROUP BY url, domain  <br />        )  <br />        SELECT   <br />            url,  <br />            domain,  <br />            snapshot_count,  <br />            ROUND(avg_rank, 2) as avg_rank,  <br />            best_rank,  <br />            worst_rank,  <br />            ROUND(rank_stddev, 2) as rank_stddev,  <br />            rank_changes,  <br />            ROUND(CAST(rank_changes AS DOUBLE) / NULLIF(snapshot_count - 1, 0) * 100, 1) as volatility_pct  <br />        FROM volatility  <br />        WHERE snapshot_count &gt; 1  <br />        ORDER BY rank_stddev DESC, avg_rank ASC  <br />        LIMIT 50  <br />    &quot;&quot;&quot;, [query, cutoff_date]).df()  <br />      <br />    return {'query': query, 'days': days, 'results': result}</pre><p>This uses window functions (LAG) and aggregations (STDDEV) to surface URLs that move around the most. These queries would normally require a data warehouseâ€Šâ€”â€Šhere theyâ€™re just SQL runningÂ locally.</p><p><strong>Run thisÂ with:</strong></p><pre>python main.py volatility --query &quot;python&quot; --days 30</pre><p>This, for example, will analyze the last 30 days of snapshots for the query string â€œpythonâ€, calculating average rank, best/worst rank, standard deviation, and change frequencyâ€Šâ€”â€Šand display the top 50 (as default) most volatileÂ URLs.</p><p><strong>Example output:</strong></p><pre>=== Rank Volatility for 'python' (last 30 days) ===<br /><br />Top 10 most volatile URLs:<br /><br />| url | domain | snapshot_count | avg_rank | best_rank | worst_rank | rank_stddev | rank_changes | volatility_pct |<br />| --- | --- | --- | --- | --- | --- | --- | --- | --- |<br />| https://www.python.org/ | python.org | 30 | 1.5 | 1 | 3 | 0.67 | 15 | 51.7 |<br />| https://www.w3schools.com/python/ | w3schools.com | 30 | 2.3 | 1 | 5 | 1.12 | 18 | 62.1 |<br />| https://en.wikipedia.org/wiki/Python_(programming_language) | wikipedia.org | 28 | 4.1 | 2 | 8 | 1.89 | 12 | 44.4 |<br />| https://www.codecademy.com/catalog/language/python | codecademy.com | 25 | 5.2 | 3 | 10 | 2.15 | 10 | 41.7 |</pre><p>Another query that is very useful is finding new entrants:</p><pre>def new_entrants(self, query: str, days: int = 7):  <br />    &quot;&quot;&quot;Find URLs that appeared for the first time recently&quot;&quot;&quot;  <br />    cutoff_date = datetime.now().date() - timedelta(days=days)  <br />      <br />    result = self.conn.execute(&quot;&quot;&quot;  <br />        WITH first_appearance AS (  <br />            SELECT   <br />                url,  <br />                domain,  <br />                MIN(snapshot_date) as first_seen  <br />            FROM serp_snapshots  <br />            WHERE query = ?  <br />            GROUP BY url, domain  <br />        ),  <br />        recent_entrants AS (  <br />            SELECT   <br />                fa.url,  <br />                fa.domain,  <br />                fa.first_seen,  <br />                s.rank as first_rank,  <br />                s.title,  <br />                s.snippet  <br />            FROM first_appearance fa  <br />            JOIN serp_snapshots s   <br />                ON fa.url = s.url   <br />                AND fa.first_seen = s.snapshot_date  <br />                AND s.query = ?  <br />            WHERE fa.first_seen &gt;= ?  <br />        )  <br />        SELECT   <br />            url,  <br />            domain,  <br />            first_seen,  <br />            first_rank,  <br />            title,  <br />            snippet  <br />        FROM recent_entrants  <br />        ORDER BY first_seen DESC, first_rank ASC  <br />        LIMIT 50  <br />    &quot;&quot;&quot;, [query, query, cutoff_date]).df()  <br />      <br />    return {'query': query, 'days': days, 'results': result}</pre><p>This finds URLs whose first appearance falls within the last N daysâ€Šâ€”â€Šperfect for spotting new competitors or fresh content entering the rankings.</p><p><strong>Run thisÂ with:</strong></p><pre>python main.py new-entrants --query &quot;python&quot; --days  7</pre><p><strong>Example output:</strong></p><pre>=== New Entrants for 'python' (last 7 days) ===<br /><br />Found 3 new URLs:<br /><br />| url | domain | first_seen | first_rank | title | snippet |<br />| --- | --- | --- | --- | --- | --- |<br />| https://realpython.com/ | realpython.com | 2026-02-04 | 7 | Real Python - Python Tutorials | Learn Python programming with Real Python's comprehensive tutorials and courses... |<br />| https://www.pythonforbeginners.com/ | pythonforbeginners.com | 2026-02-05 | 9 | Python For Beginners | A comprehensive guide to learning Python programming from scratch... |<br />| https://docs.python-guide.org/ | docs.python-guide.org | 2026-02-06 | 8 | The Hitchhiker's Guide to Python | Best practices and recommendations for Python development... |</pre><p>Iâ€™m not going to go over every module, but itâ€™s all in the code. <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/README.md\">Find all queries + their expected output in the project README.md</a>.</p><h3>Module 4: The SnapshotÂ Fetcher</h3><p>Finally, scraper.py (Iâ€™m so sorryâ€Šâ€”â€ŠI <em>really </em>could have named this better ğŸ˜…) connects ingestion andÂ storage.</p><blockquote>Read the full code for <strong>scraper.py</strong> here: <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/scraper.py\">https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/src/scraper.py</a></blockquote><pre>&quot;&quot;&quot;  <br />SERP snapshot fetcher  <br />&quot;&quot;&quot;  <br />  <br />import time  <br />from datetime import datetime  <br />from typing import List, Optional  <br />  <br />from serp_client import BrightDataClient  <br />from duckdb_manager import DuckDBManager  <br />  <br />  <br />def fetch_snapshots(keywords: List[str], num_results: int = 10, delay: float = 1.0):  <br />    &quot;&quot;&quot;  <br />    Fetch SERP snapshots for keywords and store in DuckDB  <br />      <br />    Args:  <br />        keywords: List of search keywords  <br />        num_results: Number of results per keyword  <br />        delay: Delay between API calls (seconds)  <br />    &quot;&quot;&quot;  <br />    client = BrightDataClient()  <br />      <br />    with DuckDBManager() as db:  <br />        print(f&quot;Fetching snapshots for {len(keywords)} keywords...&quot;)  <br />          <br />        for idx, keyword in enumerate(keywords):  <br />            try:  <br />                # Fetch SERP results  <br />                serp_data = client.search(keyword, num_results=num_results)  <br />                  <br />                # Extract organic results  <br />                organic_results = []  <br />                if isinstance(serp_data, dict) and 'organic' in serp_data:  <br />                    organic_results = serp_data['organic']  <br />                  <br />                if organic_results:  <br />                    # Insert snapshot  <br />                    db.insert_snapshot(organic_results, keyword)  <br />                    print(f&quot;[{idx+1}/{len(keywords)}] '{keyword}': {len(organic_results)} results&quot;)  <br />                else:  <br />                    print(f&quot;[{idx+1}/{len(keywords)}] '{keyword}': No results found&quot;)  <br />                  <br />                # Rate limiting  <br />                if idx &lt; len(keywords) - 1:  <br />                    time.sleep(delay)  <br />              <br />            except Exception as e:  <br />                print(f&quot;Error fetching '{keyword}': {e}&quot;)  <br />                continue  <br />          <br />        total = db.get_snapshot_count()  <br />        print(f&quot;\\nTotal snapshots in database: {total}&quot;)</pre><p>This is just simple orchestration logic, again. It iterates over keywords, fetches results, and inserts snapshots. Rate limiting and error handling live at the edges. Iâ€™ve kept the core logic deliberately simple.</p><p>Thatâ€™s everything! Remember, main.py brings all of these together.</p><h3>Real World UseÂ Cases</h3><p>Now that you understand how it works, hereâ€™s some cool things you can actually <em>do</em> with thisÂ tool.</p><h4>1. Detect Google Algorithm Updates Before Theyâ€™re Announced</h4><p>When tracking multiple keywords in the same niche, sudden volatility spikes across all of them indicate an algorithm change.</p><pre>python main.py volatility --query &quot;react&quot; --days 7<br />python main.py volatility --query &quot;vue&quot; --days 7<br />python main.py volatility --query &quot;angular&quot; --days 7</pre><p>If all three show high rank_stddev and volatility_pct, Google likely pushed anÂ update.</p><p>SEO folks pay $200/month for SEMrush Sensor just to get this signal. Youâ€™re building it for the cost of SERP APIÂ calls.</p><h4>2. Spy on Competitor SEOÂ Tactics</h4><p>Track title and snippet changes to see what competitors are A/BÂ testing:</p><pre>python main.py changes --query &quot;nextjs tutorial&quot; --days 30</pre><p>Example output:</p><pre>| url                              | prev_title                    | new_title                                               |<br />|----------------------------------|-------------------------------|---------------------------------------------------------|<br />| https://nextjs.org/docs          | Next.js Documentation         | Next.js Docs | Next.js                                  |<br />| https://nextjs.org/learn         | Learn Next.js                 | Learn Next.js | Next.js by Vercel - The React Framework |</pre><p>Letâ€™s say a site changed their title from a generic page description to something more specific. If their rank improved after the change, thatâ€™s a signal the new title performs betterâ€Šâ€”â€Š<strong>steal thatÂ pattern</strong>!</p><h4>3. Find Content Gaps in Real-Time</h4><p>See which sites are entering top 10 and what format theyâ€™reÂ using:</p><pre>python main.py new-entrants --query &quot;react hooks tutorial&quot; --days 7</pre><p>Example output:</p><pre>| domain              | first_seen | first_rank | title                                    |<br />|---------------------|------------|------------|------------------------------------------|<br />| react-tutorial.dev  | 2026-02-05 | 7          | React Hooks Interactive Tutorial         |<br />| codesandbox.io      | 2026-02-06 | 9          | Learn React Hooks - Live Coding Examples |</pre><p>Letâ€™s say there are two new entrants in the SERP for the query â€œreact hooks tutorialâ€, and <em>both </em>new entrants have â€œInteractiveâ€ or â€œLiveâ€ in their titles. That means Google is currently rewarding interactive content for this query. <strong>Adjust your content strategy accordingly.</strong></p><h4>4. Validate Content Ideas BeforeÂ Creation</h4><p>This oneâ€™s super simple to understand. High volatility = easier for you to rank. Low volatility = established players dominate.</p><pre>python main.py volatility --query &quot;python tutorial&quot; --days 30<br />python main.py volatility --query &quot;rust async tutorial&quot; --days 30</pre><p>Letâ€™s say the query â€œpython tutorialâ€ shows rank_stddev: 0.3 (very stable) and â€œrust async tutorialâ€ shows rank_stddev: 2.1 (chaotic), <strong>focus on the Rust content</strong>! The Python keyword is locked down by W3Schools and Real Pythonâ€Šâ€”â€Šyou wonâ€™t break inÂ easily.</p><h4>5. Track Your Own Productâ€™s SERP Performance</h4><p>Monitor how your product ranks for target keywords:</p><pre>python main.py fetch --keywords &quot;whatever your product is or does&quot;</pre><p>Then check if youâ€™re entering topÂ 10:</p><pre>python main.py new-entrants --query &quot;whatever your product is or does&quot; --days 7</pre><p>If your product URL appears, congratsâ€Šâ€”â€Šyou just entered the top 10. If competitors are dropping out (volatility shows their ranks declining), youâ€™reÂ winning.</p><h3>What I Learned BuildingÂ This</h3><ul><li><strong>DuckDB is a total cheat code for embedded analytics.</strong> I expected to need PostgreSQL (or ClickHouse, <em>ugh</em>.) for time-series queries over SERP data. Without fiddling with any config, calculating rank volatility across 30 days of snapshots for 50 URLs ran in ~20ms for me. The database file was &lt;5MB for weeks ofÂ data.</li><li><strong>Bright Dataâ€™s SERP API is very reliable.</strong> I tried other SERP APIs before settling on Bright Data, primarily because of the consistent JSON output for Google and Bing, and support for DuckDuckGo, Yandex, etc. This experiment cost me penniesâ€Šâ€”â€Šbut <a href=\"https://get.brightdata.com/scraping-browser-acf6883?utm_content=i_built_a_self_hosted_google_trends_alternative_with_duckdb\">make sure you check their pricing</a> so you donâ€™t get burnt by costs you shouldnâ€™t be incurring.</li><li><strong>The Interest Score formula needs tuning, possibly.</strong> The 40/30/30 weighting (new domains / rank improvement / domain overlap ratio) was only my first guess. It works reasonably well, but itâ€™s not perfect. At the very least, I should weight new domains more heavily for breaking news queries, and reduce domain overlap ratio impact for stable niches (Because, for example, Wikipedia will always be #1 for â€œPython programming languageâ€)</li></ul><h3>Try ItÂ Yourself</h3><p>Again, the full code is on GitHub: <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/\">https://github.com/sixthextinction/duckdb-google-trends-basic/</a></p><p><strong>Quick start:</strong></p><pre>git clone https://github.com/sixthextinction/duckdb-google-trends-basic.git<br /># or...<br />gh repo clone sixthextinction/duckdb-google-trends-basic<br /># then...<br />cd duckdb-google-trends-basic  <br />pip install -r requirements.txt  <br />  <br /># Set environment variables  <br />export BRIGHT_DATA_API_KEY=&quot;your_key&quot;  <br />export BRIGHT_DATA_ZONE=&quot;your_zone&quot;  <br />export BRIGHT_DATA_COUNTRY=&quot;us&quot;  # optional, for geo-targeted results  <br />  <br /># Or use a .env file instead (python-dotenv is included)  <br />  <br /># Test with sample data (no API key needed)  <br />python seed_data.py  <br />python main.py scores --query &quot;nextjs&quot; --days 7  <br />  <br /># Or fetch real data  <br />python main.py fetch --keywords &quot;react&quot; &quot;vue&quot; &quot;svelte&quot;</pre><p>Iâ€™ve included a <a href=\"https://github.com/sixthextinction/duckdb-google-trends-basic/blob/main/seed_data.py\">seed script</a> that creates 7 days of synthetic data so you can test immediately without waiting. Otherwise, set up a daily cron job to fetch snapshots automatically, and within a week youâ€™ll have real trendÂ data.</p><p>Thanks forÂ reading!</p><p><em>Hi ğŸ‘‹ Iâ€™m constantly tinkering with dev tools, running weird-ass experiments, and otherwise building/deep-diving stuff that probably shouldnâ€™t work but doesâ€Šâ€”â€Šand writing about it. I put out a new post every Monday/Tuesday. If youâ€™re into offbeat experiments and dev tools that actually donâ€™t suck, give me aÂ follow.</em></p><p><em>If you did something cool with this tool, Iâ€™d love to see it. </em><a href=\"https://www.linkedin.com/in/prithwish-nath-04b873a7/\"><em>Reach out on LinkedIn</em></a>, <em>or put it in the commentsÂ below.</em></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=624a19bcab65\" width=\"1\" /><hr /><p><a href=\"https://python.plainenglish.io/i-built-a-self-hosted-google-trends-alternative-with-duckdb-624a19bcab65\">I Built a Self-Hosted Google Trends Alternative with DuckDB</a> was originally published in <a href=\"https://python.plainenglish.io\">Python in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "27 min read",
      "language": null
    },
    {
      "title_en": "8 Python Projects That Teach You Web Automation the Fun Way",
      "url": "https://python.plainenglish.io/8-python-projects-that-teach-you-web-automation-the-fun-way-9d6fb4695ecc?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-10T04:31:08",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "python",
        "technology",
        "programming",
        "data-science",
        "software-development"
      ],
      "content_length": 649,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/8-python-projects-that-teach-you-web-automation-the-fun-way-9d6fb4695ecc?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*L5OlUzfi4I8b8UFWJ4TaZw.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">Hands-on projects that turn repetitive tasks into Python-powered solutions</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/8-python-projec",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/8-python-projects-that-teach-you-web-automation-the-fun-way-9d6fb4695ecc?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*L5OlUzfi4I8b8UFWJ4TaZw.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">Hands-on projects that turn repetitive tasks into Python-powered solutions</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/8-python-projects-that-teach-you-web-automation-the-fun-way-9d6fb4695ecc?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "I Was Tired of Rebuilding SaaS Backends So I Built My Own SaaSKit",
      "url": "https://python.plainenglish.io/i-was-tired-of-rebuilding-saas-backends-so-i-built-my-own-saaskit-17a8ba899941?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T18:07:06",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "artificial-intelligence",
        "technology",
        "programming",
        "python",
        "software-development"
      ],
      "content_length": 683,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/i-was-tired-of-rebuilding-saas-backends-so-i-built-my-own-saaskit-17a8ba899941?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*5JZVOKCXEYeFoOo0QKhvHg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Building an AI SaaS Was Easy&#x200a;&#x2014;&#x200a;Building the Right SaaSKit Was the Hard Part</p><p class=\"medium-feed-link\"><a href=\"https://python.pla",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/i-was-tired-of-rebuilding-saas-backends-so-i-built-my-own-saaskit-17a8ba899941?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*5JZVOKCXEYeFoOo0QKhvHg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Building an AI SaaS Was Easy&#x200a;&#x2014;&#x200a;Building the Right SaaSKit Was the Hard Part</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/i-was-tired-of-rebuilding-saas-backends-so-i-built-my-own-saaskit-17a8ba899941?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "9 Python Libraries That Seem Advanced but Are Surprisingly Beginner-Friendly",
      "url": "https://python.plainenglish.io/9-python-libraries-that-seem-advanced-but-are-surprisingly-beginner-friendly-c3b7216dc4f5?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T17:29:47",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "machine-learning",
        "libraries",
        "python",
        "technology",
        "data-science"
      ],
      "content_length": 753,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/9-python-libraries-that-seem-advanced-but-are-surprisingly-beginner-friendly-c3b7216dc4f5?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*SIDceGNwzYxLxfY6\" width=\"3882\" /></a></p><p class=\"medium-feed-snippet\">Last winter, I lost an entire Saturday to a &#x201c;simple&#x201d; automation task. I just wanted to clean some messy CSV files, rename a batch of&#x2026;",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/9-python-libraries-that-seem-advanced-but-are-surprisingly-beginner-friendly-c3b7216dc4f5?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*SIDceGNwzYxLxfY6\" width=\"3882\" /></a></p><p class=\"medium-feed-snippet\">Last winter, I lost an entire Saturday to a &#x201c;simple&#x201d; automation task. I just wanted to clean some messy CSV files, rename a batch of&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/9-python-libraries-that-seem-advanced-but-are-surprisingly-beginner-friendly-c3b7216dc4f5?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Autogen vs CrewAI vs LangGraph 2026 Comparison Guide",
      "url": "https://python.plainenglish.io/autogen-vs-crewai-vs-langgraph-2026-comparison-guide-fd8490397977?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T17:28:10",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "langgraph",
        "crew-ai",
        "autogen"
      ],
      "content_length": 25449,
      "content_preview": "<p>The world of AI development is moving faster than ever, and multi-agent systems are no longer experimental technology. By 2026, building applications with collaborating AI agents has become standard practice across enterprises.</p><p>According to recent industry data, 78% of organizations are already using AI agents in production, with 65% progressing from experimentation to fully-fledged pilot programs. A key decision developers face is choosing the right framework, and this choice directly ",
      "content_full": "<p>The world of AI development is moving faster than ever, and multi-agent systems are no longer experimental technology. By 2026, building applications with collaborating AI agents has become standard practice across enterprises.</p><p>According to recent industry data, 78% of organizations are already using AI agents in production, with 65% progressing from experimentation to fully-fledged pilot programs. A key decision developers face is choosing the right framework, and this choice directly impacts project speed, flexibility, and scalability.</p><p>This comprehensive guide provides a clear <strong>Autogen vs CrewAI vs LangGraph</strong> comparison to help you select the best tool for your AIÂ project.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Qwfjzn7eCh6if5NtqtFjdQ.jpeg\" /></figure><h3>What are Autogen, CrewAI, and LangGraph?</h3><p>Before comparing features, understanding what each framework is and how they differ philosophically is essential. They all help build applications powered by Large Language Models (LLMs), but their approaches to creating multi-agent systems are fundamentally different. Each one has a specific design philosophy that makes it suitable for certain tasks and development scenarios.</p><h3>Microsoft Autogen</h3><p>Autogen is Microsoftâ€™s open-source framework designed for creating conversational agents that can talk to each other to solve tasks. Built on the concept of multi-agent collaboration, Autogen positions itself as the â€œPyTorch for multi-agent AI applicationsâ€ with a focus on modularity and flexibility. The core idea involves an â€œAssistantAgentâ€ that performs actions and a â€œUserProxyAgentâ€ that can execute code or get human feedback. With over 30,000 GitHub stars and continuous development from Microsoft Research, Autogen has established itself as a powerful option for conversation-driven workflows.</p><h3>CrewAI</h3><p>CrewAI focuses on orchestrating role-playing autonomous agents with a structure based on real-world team collaboration. Launched in early 2024, it has rapidly gained traction with over 30,000 GitHub stars and nearly 1 million monthly downloads. You define Agents with specific roles and goals, give them Tasks, and combine them in a Crew to get the work done. Its design favors clear, hierarchical task delegation, making it the easiest entry point for developers new to multi-agent systems. CrewAI operates independently from LangChain, offering simpler implementation without complex dependencies.</p><h3>LangChainâ€™s LangGraph</h3><p>LangGraph is an extension of the popular LangChain library that allows you to build agentic workflows as directed graphs. This structure is perfect for creating applications that require cycles, or loops, in their logic. Instead of a simple chain, you can build complex state machines where agents pass information back and forth. As part of the LangChain ecosystem (which sees over 70 million downloads per month), LangGraph benefits from extensive community support and enterprise-grade deployment options. Major companies like Uber, LinkedIn, Replit, and Elastic use LangGraph in production for mission-critical applications.</p><h3>Key Differences: A Head-to-Head Comparison</h3><p>Understanding the high-level purpose of each tool is a good start. Now we can examine the specific differences in how they operate. These technical details are crucial for choosing a framework that fits your development style and project needs. The differences come down to architecture, state management, developer experience, and production readiness.</p><h3>Framework Comparison Table</h3><figure><img alt=\"Autogen, CrewAI, LangGraph\" src=\"https://cdn-images-1.medium.com/max/1024/1*cw7jJiO17izWc2i_hWKA6w.png\" /></figure><h3>Core Philosophy and Architecture</h3><h4>Autogen</h4><p>Autogen is built around conversations and agent collaboration. It excels at simulating a back-and-forth dialog between different AI agents to solve problems dynamically. Its architecture is ideal for research and creating complex, multi-turn interactions where the solution emerges through dialogue. This makes it a strong choice for exploratory problem-solving and academic applications.</p><h4>CrewAI</h4><p>CrewAI uses a task-based, hierarchical model inspired by how human teams work. Think of it like a company org chart where a manager assigns tasks to specialized employees. This structure makes agent interactions predictable and easy to manage. Itâ€™s less about open-ended conversation and more about efficient task execution with clear role assignments and sequential workflows.</p><h4>LangGraph</h4><p>LangGraph is all about control flow and state management. It treats agent workflows as a state machine using a graph of nodes (actions) and edges (connections). This low-level control makes it extremely powerful for building agents that need to loop, self-correct, or wait for human input at specific points. You can build any logic you can draw on a whiteboard, including complex decision trees and cyclical processes that other frameworks struggleÂ with.</p><h3>State Management and ControlÂ Flow</h3><p>State management determines how an application remembers information between steps, which is crucial for building reliable production systems.</p><h4>Autogen</h4><p>In Autogen, the state is mainly managed through the conversation history. Agents look at past messages to decide what to do next, mimicking how humans remember context in a chat. This works well for conversational flows but can become challenging for complex workflows requiring precise state tracking.</p><h4>CrewAI</h4><p>CrewAI manages state at the task level, where information is passed from one taskâ€™s output to the next taskâ€™s input. This approach is straightforward and works well for sequential processes, but offers less flexibility for complex branching logic or iterative processes.</p><h4>LangGraph</h4><p>LangGraph offers the most explicit and sophisticated state control among the three. You define a â€œStateâ€ object that gets passed between nodes, and each node can read and modify it. This gives developers precise control over the applicationâ€™s memory, making it ideal for applications requiring persistence, checkpointing, and the ability to rewind and replay workflows. According to a 2026 developer survey, 62% of developers working on agentic workflows requiring complex state management chose LangGraph for its fine-grained control capabilities.</p><h3>Developer Experience and Ease ofÂ Use</h3><p>The learning curve and developer experience vary significantly across these frameworks, impacting how quickly teams can become productive.</p><h4>CrewAI</h4><p>For beginners, CrewAI is consistently rated as the easiest to pick up. Its high-level abstractions like Agent, Task, and Crew are intuitive and mirror real-world teamwork concepts. You can build a functioning multi-agent system with just a few dozen lines of code, making it ideal for rapid prototyping. The clear documentation and growing community contribute to a friendly learning experience.</p><h4>Autogen</h4><p>Autogen has a steeper learning curve but offers great flexibility for conversational agent design. Setting it up requires understanding agent interaction patterns and conversation flows. The frameworkâ€™s power comes with complexity, particularly when managing intricate multi-agent conversations or debugging agent behaviors.</p><h4>LangGraph</h4><p>LangGraph is the most complex of the three, requiring a solid understanding of both LangChain concepts and graph-based programming. However, developers who invest the time report they donâ€™t â€œscale offâ€ LangGraph, meaning it continues to meet their needs as projects grow more sophisticated. The trade-off is immense power and customization, with tools like LangGraph Studio providing visual debugging and workflow visualization that significantly improve the development experience.</p><h3>Memory and Persistence Capabilities</h3><p>Memory management separates basic prototypes from production-ready agents. How each framework handles state persistence significantly impacts long-running applications and system reliability.</p><h4>LangGraph</h4><p>LangGraph leads in this area with built-in support for short-term and long-term memory through integration with vector databases like Pinecone and ChromaDB. Its persistent checkpointing allows agents to resume from any point, making it resilient to failures and ideal for long-running processes.</p><h4>CrewAI</h4><p>CrewAI offers structured memory through task outputs and built-in memory types, making it straightforward to implement basic memory without external dependencies. However, for more sophisticated memory requirements, it lacks the depth of LangGraphâ€™s solutions.</p><h4>Autogen</h4><p>Autogen relies primarily on message lists and conversation history for memory, requiring external integrations for advanced memory capabilities. While this approach is simple, it can become limiting for applications requiring complex memory architectures or persistent state across sessions.</p><h3>Production Readiness and Enterprise Adoption</h3><p>One of the most significant differentiators in 2026 is production readiness. While all three frameworks can build functional prototypes, their capabilities for enterprise deployment vary dramatically. Understanding these differences is crucial for organizations planning to scale AI agentÂ systems.</p><h3>LangGraph: Production Advantage</h3><p>LangGraph has emerged as the clear leader for production deployments. With LangSmith Deployment (formerly LangGraph Platform), organizations get purpose-built infrastructure for deploying and managing stateful agents at scale. The platform offers three deployment options: Cloud (fully managed SaaS), Hybrid (SaaS control plane with self-hosted data plane), and Fully Self-Hosted (entire platform within yourÂ VPC).</p><p>Major enterprises trust LangGraph for critical workloads. Klarnaâ€™s customer support bot serves 85 million active users and reduced resolution time by 80%. AppFolioâ€™s AI copilot Realm-X improved response accuracy by 2x and saves property managers over 10 hours per week. LinkedIn built SQL Bot on LangGraph to democratize data access across the organization. Uber uses it for large-scale code migrations, while Elastic relies on it for real-time threat detection.</p><p>The framework integrates seamlessly with LangSmith for observability, providing detailed tracing, real-time monitoring, and performance analytics. This visibility is essential for maintaining quality and debugging issues in production environments.</p><h3>CrewAI: Rapid Development Focus</h3><p>CrewAI prioritizes speed to market and ease of deployment for smaller-scale applications. Its straightforward architecture makes it excellent for MVPs and proof-of-concepts that need to launch quickly. While it lacks some of the enterprise features of LangGraph, its simplicity is a feature for teams that donâ€™t require complex orchestration or need to demonstrate valueÂ rapidly.</p><p>CrewAI works well for startups and mid-sized companies building focused automation tools. The frameworkâ€™s minimal dependencies and clear structure make deployment straightforward, though scaling to handle millions of requests requires additional infrastructure planning.</p><h3>Autogen: Research and Experimentation Strength</h3><p>Autogen remains strongest in research contexts and applications where the workflow is exploratory rather than predefined. Its flexible agent communication patterns make it valuable for prototyping novel agent behaviors, but moving Autogen applications to production often requires significant custom infrastructure development.</p><p>Microsoftâ€™s backing ensures continued development and improvement, but the frameworkâ€™s design prioritizes flexibility over production conveniences. Teams choosing Autogen for production should plan for custom observability, scaling, and deployment solutions.</p><h3>Use Cases and Project Suitability</h3><p>The best framework depends entirely on what you want to build. Each tool shines in different scenarios, and matching your projectâ€™s requirements to a frameworkâ€™s strengths is the key to success. A mismatch can lead to fighting against the toolâ€™s core design, significantly slowing down development.</p><h3>Best for Complex, Cyclical Workflows: LangGraph</h3><p>Choose LangGraph when your process isnâ€™t linear and requires sophisticated control flow. If your agents need to reflect on their work, ask for clarification, or repeat steps until a condition is met, LangGraph is the ideal choice. It excelsÂ in:</p><ul><li>Applications requiring persistent memory and state acrossÂ sessions</li><li>Workflows with conditional branching and dynamic decision-making</li><li>Systems needing human-in-the-loop approval at specificÂ points</li><li>Long-running processes that must handle interruptions gracefully</li><li>Complex reasoning chains with self-correction capabilities</li><li>Production systems requiring observability and debugging tools</li></ul><p>Real-world applications include document processing pipelines, data analysis agents that iterate on results, sophisticated chatbots with memory, and automated research tools that refine their approach based on findings.</p><h3>Best for Hierarchical Task Delegation: CrewAI</h3><p>CrewAI is perfect for projects that can be broken down into clear steps performed by specialists. Think of a marketing team where a â€œResearcherâ€ agent finds trends, a â€œWriterâ€ agent drafts content, and an â€œEditorâ€ agent reviews it. If your workflow resembles an assembly line or a corporate team structure, CrewAI provides the intuitive framework you need. Ideal use casesÂ include:</p><ul><li>Content creation pipelines (research, writing,Â editing)</li><li>Report generation with standardized formats</li><li>Business process automation with sequential steps</li><li>Rapid prototyping and proof-of-concept development</li><li>Applications where role clarity is more important than complex flowÂ control</li><li>Startups needing to demonstrate multi-agent capabilities quickly</li></ul><p>CrewAIâ€™s role-based design makes it particularly effective for industries like real estate (property analysis, financial calculations, report preparation), hiring (resume screening, candidate matching, interview scheduling), and any domain where tasks naturally flow from one specialist to another. Organizations looking to build sophisticated automation solutions can benefit from expert guidance in mobile app development to integrate these AI frameworks effectively.</p><h3>Best for Research and Conversational Systems:Â Autogen</h3><p>Autogen is excellent for academic research, building advanced conversational agents, and exploring novel agent interaction patterns. Its flexible agent communication makes it possible to set up complex simulations where the solution emerges through dialogue. Strong use casesÂ include:</p><ul><li>Academic research into agent behavior and collaboration</li><li>Prototyping novel multi-agent architectures</li><li>Applications requiring dynamic, conversation-driven problemÂ solving</li><li>Systems where agents debate and refine solutions collaboratively</li><li>Code generation with iterative refinement (coder and testerÂ agents)</li><li>Exploratory AI applications where the workflow isnâ€™t predetermined</li></ul><h3>Performance and Scalability Considerations</h3><p>As organizations move beyond experimentation, performance becomes critical. Industry data shows that 43% of enterprises now allocate over half their AI budgets to agentic AI, with 62% expecting ROI above 100%. This investment demands frameworks that canÂ scale.</p><p>LangGraphâ€™s architecture explicitly supports scalability with built-in handling for bursty traffic, asynchronous operations, and distributed agent workloads. Its stateful design allows for horizontal scaling where multiple instances can share state through persistent storage.</p><p>CrewAI performs well for moderate-scale applications but may face challenges with highly concurrent workloads or applications requiring millisecond-level response times. Its strength lies in developer productivity rather than extreme performance optimization. Companies expanding their AI capabilities across multiple regions can leverage expertise from teams specializing in <a href=\"https://indiit.com/mobile-app-development-arizona/\">mobile app development Arizona</a> to ensure robust implementation.</p><p>Autogenâ€™s performance characteristics depend heavily on implementation details. The asynchronous agent communication can reduce blocking, making it suitable for longer tasks, but achieving production-grade performance often requires custom optimization work.</p><h3>Integration Ecosystem and ToolÂ Support</h3><p>The broader ecosystem matters significantly for long-term project success. LangGraph benefits from the LangChain ecosystem with 200+ turnkey connectors for tools, databases, and APIs. This extensive integration library accelerates development by providing pre-built connections to common services.</p><p>CrewAIâ€™s independence from LangChain can be both an advantage (fewer dependencies) and a limitation (fewer pre-built integrations). It requires more custom integration work but offers a cleaner dependency tree.</p><p>Autogen integrates well with Microsoftâ€™s ecosystem and supportsÂ .NET/C# alongside Python, though the Python implementation is more mature. Its community provides various extensions and tools for specific useÂ cases.</p><h3>Future of AI Agent Frameworks in 2026 andÂ Beyond</h3><p>Looking ahead, several trends are reshaping the agent framework landscape. By 2028, Gartner forecasts that 33% of enterprise software applications will incorporate agentic AI, a dramatic leap from less than 1% inÂ 2024.</p><p>We expect more standardization in how agents communicate with each other, potentially making it easier to mix and match components from different frameworks in a single application. For instance, you might use CrewAI for high-level task planning and LangGraph for a specific, complex sub-task requiring loops.</p><p>Another critical trend is enhanced human-in-the-loop capabilities. All three frameworks support human oversight, but making it seamless to inject human feedback will be a priority. As businesses use agentic workflows for critical operations, the ability for a person to supervise and approve AI actions becomes essential. The frameworks that provide the most reliable human oversight will gain significant competitive advantages.</p><p>Model-agnostic design is also becoming standard. While these frameworks initially focused on specific LLM providers, the trend is toward supporting multiple models and allowing easy switching between them based on cost, performance, or capability requirements.</p><h3>Choosing the Right Framework: DecisionÂ Matrix</h3><p><strong>Choose LangGraph ifÂ you:</strong></p><ul><li>Need production-grade reliability and observability</li><li>Require complex workflows with loops and state management</li><li>Are building enterprise applications serving manyÂ users</li><li>Need human-in-the-loop at specific workflowÂ points</li><li>Want comprehensive debugging and monitoring tools</li><li>Can invest time in learning graph-based architecture</li><li>Are part of the LangChain ecosystem already</li></ul><p><strong>Choose CrewAI ifÂ you:</strong></p><ul><li>Want the fastest path from concept to prototype</li><li>Have a workflow that maps clearly to role-based teams</li><li>Prioritize simplicity over advancedÂ features</li><li>Are building MVPs or proof-of-concepts</li><li>Have a small team or are a solo developer</li><li>Donâ€™t need complex state management orÂ loops</li><li>Want to avoid steep learningÂ curves</li></ul><p><strong>Choose Autogen ifÂ you:</strong></p><ul><li>Are conducting research on agentÂ behavior</li><li>Need flexible, conversation-driven problemÂ solving</li><li>Are exploring novel agent architectures</li><li>Want Microsoft ecosystem integration</li><li>Can invest in custom infrastructure for production</li><li>Have workflows where solutions emerge throughÂ dialogue</li><li>Need agents that debate and refine solutions iteratively</li></ul><h3>Frequently Asked Questions</h3><h3>Which framework is best for beginners inÂ 2026?</h3><p>CrewAI is widely considered the best starting point for developers new to multi-agent systems. Its clear, high-level concepts allow newcomers to build a working multi-agent application quickly, often within hours. The learning curve is significantly friendlier compared to the more abstract nature of Autogen or the technical depth of LangGraph. For those willing to invest more time upfront, LangGraph offers better long-term scalability.</p><h3>Can Autogen, CrewAI, and LangGraph be used together?</h3><p>Yes, but it requires advanced programming skills and careful architecture. A common pattern is using a high-level framework like CrewAI to manage the overall process while calling a LangGraph agent for specific tasks needing complex, cyclical logic. This hybrid approach allows you to use the best tool for each part of the job, though it introduces complexity in managing multiple frameworks and their dependencies.</p><h3>Which framework has the strongest community support?</h3><p>As part of the LangChain ecosystem, LangGraph benefits from one of the largest and most active communities in AI development. LangChainâ€™s documentation, tutorials, and community resources are extensive, with dedicated courses through LangChain Academy. Microsoftâ€™s backing gives Autogen strong institutional support and regular updates. CrewAI has a rapidly growing community driven by its ease of use, with nearly 1 million monthly downloads demonstrating strong adoption.</p><h3>How do I choose between LangGraph andÂ Autogen?</h3><p>The decision comes down to your primary need. Choose LangGraph if your workflow requires precise control over logic flow, especially with loops, state management, and production deployment. The investment in learning LangGraph pays off for complex, long-lived projects. Choose Autogen if your focus is on creating dynamic, conversational interactions between agents, where the process is less defined and more exploratory. Autogen excels when the path to a solution is discovered through agent dialogue rather than predefined workflows.</p><h3>What about cost and licensing?</h3><p>All three frameworks are open-source with permissive licenses. LangGraph and LangChain are MIT licensed, CrewAI is also open-source, and Autogen is released under the Apache 2.0 license. However, deployment costs vary. LangGraphâ€™s managed deployment options (LangSmith Deployment) have associated costs based on usage tier, while self-hosting any framework incurs infrastructure costs. The total cost of ownership should factor in development time, maintenance, and operational expenses, not just licensing.</p><h3>Final Thoughts</h3><p>Choosing between these AI agent frameworks is a strategic decision for any project. There is no single best tool for everyone, and the â€œwinnerâ€ in the <strong>Autogen vs CrewAI vs LangGraph</strong> debate depends entirely on your specific needs, team capabilities, and project requirements.</p><p>LangGraph offers unmatched control, production readiness, and enterprise-grade features, making it the professional choice for complex systems requiring reliability at scale. Its steeper learning curve is offset by long-term flexibility and the ability to handle sophisticated workflows that other frameworks struggleÂ with.</p><p>CrewAI delivers simplicity and structure with the fastest time-to-value, making it ideal for rapid development, prototypes, and applications with clear role-based workflows. Its intuitive design removes barriers for teams new to multi-agent systems.</p><p>Autogen excels at conversational simulation and research applications where flexibility and exploratory development matter more than production polish. It provides a powerful platform for investigating agent behaviors and building conversation-driven solutions.</p><p>For most business automation tasks, CrewAI is a fantastic starting point that allows quick validation of concepts. For complex systems that need to think in cycles and require production reliability, LangGraph is the professional choice backed by major enterprises. For research into agent behavior and novel architectures, Autogen provides a flexible experimental platform.</p><p>Assess your projectâ€™s core requirements, consider your teamâ€™s experience level, and evaluate your production needs. The right framework will align with your workflow patterns and scale with your ambitions. As the multi-agent AI landscape continues to evolve rapidly, staying informed about each frameworkâ€™s development will help you make the best choice for both current and future projects.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fd8490397977\" width=\"1\" /><hr /><p><a href=\"https://python.plainenglish.io/autogen-vs-crewai-vs-langgraph-2026-comparison-guide-fd8490397977\">Autogen vs CrewAI vs LangGraph 2026 Comparison Guide</a> was originally published in <a href=\"https://python.plainenglish.io\">Python in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "15 min read",
      "language": null
    },
    {
      "title_en": "Microservices Explained in 30 minutes",
      "url": "https://python.plainenglish.io/microservices-explained-in-30-minutes-5e3f8865353b?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T17:24:42",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "microservice-architecture",
        "microservices",
        "technology",
        "system-design-interview",
        "computer-science"
      ],
      "content_length": 653,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/microservices-explained-in-30-minutes-5e3f8865353b?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1080/1*6V7ml2ArgWsqIrAnj5caVA.png\" width=\"1080\" /></a></p><p class=\"medium-feed-snippet\">Microservices Explained with failure, latency, and complexity. All these patterns, tracing, sagas, CQRS, circuit breakers.</p><p class=\"medium-feed-link\"><a href=\"https://python.plain",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/microservices-explained-in-30-minutes-5e3f8865353b?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1080/1*6V7ml2ArgWsqIrAnj5caVA.png\" width=\"1080\" /></a></p><p class=\"medium-feed-snippet\">Microservices Explained with failure, latency, and complexity. All these patterns, tracing, sagas, CQRS, circuit breakers.</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/microservices-explained-in-30-minutes-5e3f8865353b?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Real-World Python Automation: Libraries That Actually Scale Beyond Demos",
      "url": "https://python.plainenglish.io/real-world-python-automation-libraries-that-actually-scale-beyond-demos-b4b580485454?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T17:24:11",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "programming",
        "technology",
        "artificial-intelligence",
        "data-science",
        "python"
      ],
      "content_length": 693,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/real-world-python-automation-libraries-that-actually-scale-beyond-demos-b4b580485454?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*iudNsXd96mJAwC1D9ndZ1w.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Why most automation scripts break&#x200a;&#x2014;&#x200a;and the Python tools that prevent it.</p><p class=\"medium-feed-link\"><a href=\"https://python",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/real-world-python-automation-libraries-that-actually-scale-beyond-demos-b4b580485454?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*iudNsXd96mJAwC1D9ndZ1w.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Why most automation scripts break&#x200a;&#x2014;&#x200a;and the Python tools that prevent it.</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/real-world-python-automation-libraries-that-actually-scale-beyond-demos-b4b580485454?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "7 Things You Should Set Up Before Your Django App Meets Real Users",
      "url": "https://python.plainenglish.io/7-things-you-should-set-up-before-your-django-app-meets-real-users-76e72ca335f5?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T17:24:10",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "python-web-developer",
        "python-libraries",
        "python-programming",
        "python",
        "programming"
      ],
      "content_length": 643,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/7-things-you-should-set-up-before-your-django-app-meets-real-users-76e72ca335f5?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*RMeJH2ea_yY8lwVbB8lm0Q.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Every Django project has a quiet moment before launch.</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/7-things-you-should-set-up-b",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/7-things-you-should-set-up-before-your-django-app-meets-real-users-76e72ca335f5?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*RMeJH2ea_yY8lwVbB8lm0Q.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Every Django project has a quiet moment before launch.</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/7-things-you-should-set-up-before-your-django-app-meets-real-users-76e72ca335f5?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "7 Python Libraries I Only Started Appreciating After Building Real Projects",
      "url": "https://python.plainenglish.io/7-python-libraries-i-only-started-appreciating-after-building-real-projects-44f22acb5a29?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T17:24:09",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "python",
        "programming",
        "technology",
        "data-science",
        "artificial-intelligence"
      ],
      "content_length": 681,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/7-python-libraries-i-only-started-appreciating-after-building-real-projects-44f22acb5a29?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*Uk0jsnomSoQJ4Hf2Uk-Syg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">What actually matters when your Python code has to run at 2 AM without you</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/7-python-libraries-i-only-started-appreciating-after-building-real-projects-44f22acb5a29?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*Uk0jsnomSoQJ4Hf2Uk-Syg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">What actually matters when your Python code has to run at 2 AM without you</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/7-python-libraries-i-only-started-appreciating-after-building-real-projects-44f22acb5a29?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "7 Python Libraries That Helped Me Build Projects I Didnâ€™t Think I Was Ready For",
      "url": "https://python.plainenglish.io/7-python-libraries-that-helped-me-build-projects-i-didnt-think-i-was-ready-for-6db27b91b3ee?source=rss----78073def27b8---4",
      "source": "medium",
      "published_at": "2026-02-09T17:21:42",
      "external_id": null,
      "tags": [
        "python-in-plain-english",
        "technology",
        "programming",
        "machine-learning",
        "data-science",
        "python"
      ],
      "content_length": 746,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/7-python-libraries-that-helped-me-build-projects-i-didnt-think-i-was-ready-for-6db27b91b3ee?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*eqKGXyDT0tqvkTAy\" width=\"5184\" /></a></p><p class=\"medium-feed-snippet\">Last year at 2:17 a.m., I was staring at a half-working automation script that was supposed to turn a messy folder of client files into&#x2026;</p><p cl",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://python.plainenglish.io/7-python-libraries-that-helped-me-build-projects-i-didnt-think-i-was-ready-for-6db27b91b3ee?source=rss----78073def27b8---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*eqKGXyDT0tqvkTAy\" width=\"5184\" /></a></p><p class=\"medium-feed-snippet\">Last year at 2:17 a.m., I was staring at a half-working automation script that was supposed to turn a messy folder of client files into&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://python.plainenglish.io/7-python-libraries-that-helped-me-build-projects-i-didnt-think-i-was-ready-for-6db27b91b3ee?source=rss----78073def27b8---4\">Continue reading on Python in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Budget Spectroscopy With Python, OpenCV, and Matplotlib",
      "url": "https://levelup.gitconnected.com/optical-spectroscopy-in-python-opencv-and-matplotlib-64c6074443ab?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T16:12:20",
      "external_id": null,
      "tags": [
        "gitconnected",
        "science",
        "programming",
        "data-science",
        "python",
        "physics"
      ],
      "content_length": 607,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/optical-spectroscopy-in-python-opencv-and-matplotlib-64c6074443ab?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1106/1*j9Crg25_TxqgMkLZ60Kqfw.png\" width=\"1106\" /></a></p><p class=\"medium-feed-snippet\">Fun Experiments With Python and a USB Spectrometer</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/optical-spectroscopy-in-python-opencv-and-",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/optical-spectroscopy-in-python-opencv-and-matplotlib-64c6074443ab?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1106/1*j9Crg25_TxqgMkLZ60Kqfw.png\" width=\"1106\" /></a></p><p class=\"medium-feed-snippet\">Fun Experiments With Python and a USB Spectrometer</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/optical-spectroscopy-in-python-opencv-and-matplotlib-64c6074443ab?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Training Multi-Agentic Systems for Complex Task Planning with GRPO Algorithm",
      "url": "https://levelup.gitconnected.com/training-multi-agentic-systems-for-complex-task-planning-with-grpo-algorithm-f698e831d730?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:38:20",
      "external_id": null,
      "tags": [
        "gitconnected",
        "artificial-intelligence",
        "python",
        "machine-learning",
        "data-science",
        "ai"
      ],
      "content_length": 657,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/training-multi-agentic-systems-for-complex-task-planning-with-grpo-algorithm-f698e831d730?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2133/1*nU5tAGaU3z4vO3apzIYblg.png\" width=\"2133\" /></a></p><p class=\"medium-feed-snippet\">Rollouts, Advantages, Trajectories, Rewards and more</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/training-multi-a",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/training-multi-agentic-systems-for-complex-task-planning-with-grpo-algorithm-f698e831d730?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2133/1*nU5tAGaU3z4vO3apzIYblg.png\" width=\"2133\" /></a></p><p class=\"medium-feed-snippet\">Rollouts, Advantages, Trajectories, Rewards and more</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/training-multi-agentic-systems-for-complex-task-planning-with-grpo-algorithm-f698e831d730?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Vibe Coding Kills Open Source: The 70% Revenue Collapse",
      "url": "https://levelup.gitconnected.com/vibe-coding-kills-open-source-the-70-revenue-collapse-1a6ea0833cfa?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:38:17",
      "external_id": null,
      "tags": [
        "gitconnected",
        "machine-learning",
        "artificial-intelligence",
        "data-science",
        "technology",
        "programming"
      ],
      "content_length": 643,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/vibe-coding-kills-open-source-the-70-revenue-collapse-1a6ea0833cfa?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1267/1*KiKLfd2o_k5Gjtebe7BhbA.png\" width=\"1267\" /></a></p><p class=\"medium-feed-snippet\">New research reveals how AI coding tools are breaking the economics of free software</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/vibe-co",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/vibe-coding-kills-open-source-the-70-revenue-collapse-1a6ea0833cfa?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1267/1*KiKLfd2o_k5Gjtebe7BhbA.png\" width=\"1267\" /></a></p><p class=\"medium-feed-snippet\">New research reveals how AI coding tools are breaking the economics of free software</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/vibe-coding-kills-open-source-the-70-revenue-collapse-1a6ea0833cfa?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "You Think UUIDs Are Just Random Numbers? Think Again.",
      "url": "https://levelup.gitconnected.com/you-think-uuids-are-just-random-numbers-think-again-fb55a26c6c10?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:38:09",
      "external_id": null,
      "tags": [
        "gitconnected",
        "programming",
        "coding",
        "technology",
        "software-development",
        "data-science"
      ],
      "content_length": 693,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/you-think-uuids-are-just-random-numbers-think-again-fb55a26c6c10?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*E1Kt96zHjY8Kwq8Pcp5rMg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Why the most common ID in software engineering might be leaking your location&#x200a;&#x2014;&#x200a;or killing your database performance.</p><p class=\"medium-feed-lin",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/you-think-uuids-are-just-random-numbers-think-again-fb55a26c6c10?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*E1Kt96zHjY8Kwq8Pcp5rMg.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Why the most common ID in software engineering might be leaking your location&#x200a;&#x2014;&#x200a;or killing your database performance.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/you-think-uuids-are-just-random-numbers-think-again-fb55a26c6c10?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Build Production Ready Agents With MCP",
      "url": "https://levelup.gitconnected.com/build-production-ready-agents-with-mcp-651078c0641b?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:38:07",
      "external_id": null,
      "tags": [
        "gitconnected",
        "backend-development",
        "software-engineering",
        "ai",
        "mcp-server",
        "ai-agent"
      ],
      "content_length": 572,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/build-production-ready-agents-with-mcp-651078c0641b?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*ZaF97dEvX381myfe\" width=\"3999\" /></a></p><p class=\"medium-feed-snippet\">Detailed guide for developing MCP tools for AI agents</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/build-production-ready-agents-with-mcp-651078c0641b?source=rss-",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/build-production-ready-agents-with-mcp-651078c0641b?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*ZaF97dEvX381myfe\" width=\"3999\" /></a></p><p class=\"medium-feed-snippet\">Detailed guide for developing MCP tools for AI agents</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/build-production-ready-agents-with-mcp-651078c0641b?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "How I Turned a Failed Prada Interview into an LLM-Driven Inventory Decision Pipeline",
      "url": "https://levelup.gitconnected.com/how-i-turned-a-failed-prada-interview-into-an-llm-driven-inventory-decision-pipeline-61b99b2c661a?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:38:05",
      "external_id": null,
      "tags": [
        "gitconnected",
        "llm",
        "data",
        "deep-learning",
        "data-science",
        "machine-learning"
      ],
      "content_length": 725,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/how-i-turned-a-failed-prada-interview-into-an-llm-driven-inventory-decision-pipeline-61b99b2c661a?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*poswQ2HwLBDTf6ALX3cFWw.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">End-to-end DB &#x2192; signals &#x2192; structured insights with PostgreSQL, Python, and Ollama (Llama).</p><p class=\"medium-feed-link",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/how-i-turned-a-failed-prada-interview-into-an-llm-driven-inventory-decision-pipeline-61b99b2c661a?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*poswQ2HwLBDTf6ALX3cFWw.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">End-to-end DB &#x2192; signals &#x2192; structured insights with PostgreSQL, Python, and Ollama (Llama).</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/how-i-turned-a-failed-prada-interview-into-an-llm-driven-inventory-decision-pipeline-61b99b2c661a?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Top C# 14 features",
      "url": "https://levelup.gitconnected.com/top-c-14-features-b981770c52da?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:38:00",
      "external_id": null,
      "tags": [
        "gitconnected",
        "csharp",
        "csharp14",
        "csharp-programming"
      ],
      "content_length": 584,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/top-c-14-features-b981770c52da?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2472/1*ns_FhJw7RDIjhZNMNUOr2A.png\" width=\"2472\" /></a></p><p class=\"medium-feed-snippet\">In this article, we&#x2019;ll explore Extensions, Null-Conditional Assignment, Spans and Lambdas.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/top-c-14-features-b981770c52da",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/top-c-14-features-b981770c52da?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2472/1*ns_FhJw7RDIjhZNMNUOr2A.png\" width=\"2472\" /></a></p><p class=\"medium-feed-snippet\">In this article, we&#x2019;ll explore Extensions, Null-Conditional Assignment, Spans and Lambdas.</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/top-c-14-features-b981770c52da?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Test-Driven Development for Building User Interfaces",
      "url": "https://levelup.gitconnected.com/test-driven-development-for-building-user-interfaces-a1168de9fee8?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:37:59",
      "external_id": null,
      "tags": [
        "gitconnected",
        "software-testing",
        "software-development",
        "javascript",
        "programming",
        "react"
      ],
      "content_length": 602,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/test-driven-development-for-building-user-interfaces-a1168de9fee8?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2140/1*6g9fkw2Bp2iPvNd5VrVdEQ.png\" width=\"2140\" /></a></p><p class=\"medium-feed-snippet\">Is it possible? Yes! Should you do it? Maybe!</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/test-driven-development-for-building-user-inter",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/test-driven-development-for-building-user-interfaces-a1168de9fee8?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/2140/1*6g9fkw2Bp2iPvNd5VrVdEQ.png\" width=\"2140\" /></a></p><p class=\"medium-feed-snippet\">Is it possible? Yes! Should you do it? Maybe!</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/test-driven-development-for-building-user-interfaces-a1168de9fee8?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Structure is All You Need?:",
      "url": "https://levelup.gitconnected.com/structure-is-all-you-need-4ee88db32675?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:37:56",
      "external_id": null,
      "tags": [
        "gitconnected"
      ],
      "content_length": 15243,
      "content_preview": "<h3>Structure is All You Need?: Revisiting the Transformer Architecture Through the Lens of ContextÂ Graphs</h3><h4>Why the future of autonomous agents lies not in larger context windows, but in the topological mapping ofÂ memory.</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8TfzWia6fxPtlM2dNTz1hA.png\" /></figure><p><em>From the infinite scroll to the structured web: Mapping the mind ofÂ AI.</em></p><p>Letâ€™s talk about why your AI is currently acting like a college studen",
      "content_full": "<h3>Structure is All You Need?: Revisiting the Transformer Architecture Through the Lens of ContextÂ Graphs</h3><h4>Why the future of autonomous agents lies not in larger context windows, but in the topological mapping ofÂ memory.</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8TfzWia6fxPtlM2dNTz1hA.png\" /></figure><p><em>From the infinite scroll to the structured web: Mapping the mind ofÂ AI.</em></p><p>Letâ€™s talk about why your AI is currently acting like a college student pulling an all-nighter.</p><h3>I. The Cold Open: The â€œMap vs. Scrollâ€Â Crisis</h3><p>Imagine you are dropped into the middle of Tokyo. You donâ€™t speak the language, you have no phone, and you need to find a specific ramen shop called â€œIchiranâ€ in Shinjuku.</p><p>Now, I offer you twoÂ tools.</p><p><strong>Tool A: </strong>A single, continuous scroll of paper that is 50 miles long. On this paper is a written description of every single building, street, curb, and vending machine in Tokyo, sequentially ordered. somewhere in mile 32, it mentions the ramenÂ shop.</p><p><strong>Tool B: </strong>A map. A simple, topological map with nodes (intersections) and edges (streets).</p><p>Which one do youÂ take?</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2phJO2hwt4KcWltwXdODYg.png\" /></figure><p><em>The choice is yours: The burden of the scroll or the clarity of theÂ map.</em></p><p>If you picked Tool A, congratulations, you are a Large Language Model (LLM) with a â€œ1 Million Token Context Window.â€ You have photographic memory of the text, but you have absolutely no idea where youÂ are.</p><p>If you picked Tool B, you are what the future of AI looksÂ like.</p><p>For the last few years, weâ€™ve been obsessed with the paper <em>â€œAttention is All You Need.â€</em> Itâ€™s the holy scripture of AI. But frankly, weâ€™ve misinterpreted it. We thought â€œAttentionâ€ meant â€œRead More.â€ We pushed context windows from 8k to 32k to 1M tokens. We gave AI a photographic memory, but we forgot to give it a filingÂ cabinet.</p><p>The Thesis: The linear transformer architectureâ€Šâ€”â€Šthat beautiful infinite scrollâ€Šâ€”â€Šis hitting a wall. The next leap in AI isnâ€™t about reading <em>more</em> text; itâ€™s about <em>structuring</em> it. We are witnessing an epistemological shift from flat vector retrieval to Context Graphsâ€Šâ€”â€Šdynamic, topological representations of theÂ world.</p><blockquote>â€œGiving an AI a million-token context window and asking it to plan a complex task is like handing someone the entire Library of Congress and asking them to find their carÂ keys.â€</blockquote><h3>II. The Stakes: Why â€œFlatâ€ Memory Fails in the RealÂ World</h3><p>Letâ€™s get serious for a second (put down the chai). In my work in cybersecurity, if I miss a log entry, a server gets breached. In kickboxing, if I forget a combo, I get punched in the face. Consequences matter.</p><p>Current AI relies heavily on RAG (Retrieval-Augmented Generation). Basically, when you ask a question, the AI runs to a vector database (a library of fuzzy concepts), grabs a few books that â€œfeelâ€ similar to your question, and readsÂ them.</p><p>This creates three massive problems:</p><ol><li><strong>Catastrophic Forgetting:</strong> If an agent puts down a key at Step 5 and needs it at Step 500, a flat model often forgets the <em>event</em> because the semantic similarity between â€œStep 5â€ and â€œStep 500â€ isÂ low.</li><li><strong>Hallucination: </strong>When the scroll gets too long, the AI starts making things up to fill the gaps. Itâ€™s the student bluffing on an essayÂ exam.</li><li><strong>Lack of Planning: </strong>You cannot plan a future route if you donâ€™t have a structured model of theÂ past.</li></ol><p>We are trying to move AI from System 1 (Fast, intuitive, â€œI think I saw that somewhereâ€) to System 2 (Slow, deliberate, â€œI know X causes Y because I mapped itâ€). Context Graphs are the architecture of SystemÂ 2.</p><blockquote><strong>Trivia Check</strong>: The human brain doesnâ€™t store memories like a video tape. It stores them as reconstructions linked by associations. Thatâ€™s a graph,Â folks!</blockquote><h3>III. The Adventure: The â€œHippocampusâ€ of AI (Constructivist Memory)</h3><p>Letâ€™s treat our AI agent as a character. Letâ€™s call him â€œSherlock.â€</p><p>Old Sherlock (Linear Transformer) walks into a room. He reads everything. â€œThe carpet is red. The key is silver. The window is open.â€ He stores this in a list. Ten hours later, you ask, â€œWho opened the window?â€ Old Sherlock scans the list. If the list is too long, he might say, â€œThe carpet opened theÂ window.â€</p><p>New Sherlock (Graph Agent) is Constructivist. He doesnâ€™t just read; he builds a mentalÂ map.</p><p>Recent research, specifically AriGraph (Anokhin et al., 2024), shows us that agents need to distinguish between <em>Semantic Memory</em> (Facts: â€œThe key is silverâ€) and <em>Episodic Memory</em> (Events: â€œI picked up the key at StepÂ 5â€).</p><p>When New Sherlock enters the room, he creates a node in his graph: [Room A]. He creates an edge: [Sherlock] --(picked_up)--&gt; [Key]. Later, he doesn't need to re-read the whole story. He just traverses theÂ graph.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8DgVxIahs0zhR237GbhZ3g.png\" /></figure><p><em>New Sherlock doesnâ€™t just read the clues; he connects theÂ nodes.</em></p><p>Even cooler? Trainable Graph Memory (Xia et al., 2025). This is where the agent develops Meta-Cognition. It doesnâ€™t just remember <em>what</em> happened; it remembers <em>how</em> it solved the problem. It stores successful strategies as weighted edges in the graph. Itâ€™s like a kickboxer remembering, â€œAh, every time he drops his left hand, a right hook lands.â€ Thatâ€™s not a fact; thatâ€™s a strategy stored in topology.</p><blockquote><strong>ProTip</strong>: If you are building agents, stop dumping everything into a vector store. Start separating â€œWorld Knowledgeâ€ (Wikipedia) from â€œAgent Stateâ€ (What did I just do?). The latter needs aÂ graph.</blockquote><h3>IV. The â€œAha!â€ Moment: Navigation is Reasoning</h3><p>Here is the mind-bending part. In this new world, GraphRAG isnâ€™t just a way to look up data. Navigation <em>is</em> Reasoning.</p><p>Think of Dr. Strange in <em>Infinity War</em> looking at 14 million futures. Heâ€™s not reading a book; heâ€™s traversing a decisionÂ tree.</p><ul><li><strong>Active Reasoning (KG-Agent):</strong> Research by Jiang et al. (2025) shows that efficient agents use a â€œtoolboxâ€ to explore connections. They donâ€™t guess; they say, â€œLet me check the neighbor of thisÂ node.â€</li><li><strong>Simulation &amp; Look-Ahead (WebATLAS): </strong>This paper by Cheng et al. (2025) is a game-changer. It allows web agents to â€œsimulateâ€ a click before doing it. Instead of clicking â€œBuy Nowâ€ and accidentally ordering 500 toasters, the agent traces the graph edge to see where it leads. It prevents the â€œinfinite loopâ€ errors we see in bad AIÂ bots.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EDYL7nwBAC0JllhSI5kTLA.png\" /></figure><p><em>Navigation is reasoning: Exploring 14 million futures through graph traversal.</em></p><p>For my developer friends, look at CodexGraph (Liu et al., 2025). If you want an AI to understand your spaghetti code, vector search fails. Why? Because code is logic. Function A calls Function B. Thatâ€™s a graph relation. Graph-based agents can answer, &quot;If I delete this line, what breaks?&quot; Text-based agents just guess based on variableÂ names.</p><h3>V. The Collective Mind: SwarmÂ Topology</h3><p>Now, letâ€™s scale this up. What happens when you have a swarm ofÂ robots?</p><p>If everyone talks to everyone (a flat structure), you get chaos. Itâ€™s like a â€œReply Allâ€ email chain that neverÂ ends.</p><p>Research in Swarm Topology (Liu et al., 2023) suggests we need â€œMilitary-styleâ€ hierarchies. We need a graph of leaders and followers. The â€œContextâ€ isnâ€™t inside one robot; the Context <em>is</em> the network structure betweenÂ them.</p><p>Papers on Recursive Reasoning Graphs (Ma et al., 2022) show agents modeling what <em>other</em> agents are thinking. â€œI think that you think that Iâ€™m going to turn left.â€ This is Theory of Mind, and itâ€™s only possible if you can map out the relationships between agents as a dynamicÂ graph.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4BK-uHS8r2lBf0-1zGv-RA.png\" /></figure><p><em>From a chaotic mob to a disciplined army: The power of swarm topology.</em></p><blockquote><strong>Visual Metaphor</strong>: A flat context window is a mob. A context graph is anÂ army.</blockquote><h3>VI. The Cooldown: The Fragility of Structure</h3><p>Alright, put the confetti away. Time for the â€œDr. Sewak Security RealityÂ Check.â€</p><p>Structure brings clarity, but it also brings fragility.</p><p>There is a terrifying new attack vector called â€œGragPoisonâ€ (Liang et al.,Â 2025).</p><p>Hereâ€™s the scaryÂ part:</p><ul><li>Text Poisoning: If I write a blog post saying â€œThe sky is green,â€ the AI might read it, compare it to 50 other sources, and say, â€œThis isÂ noise.â€</li><li>Structure Poisoning: If I hack the knowledge graph and draw a solid line (edge) between [Safe Chemical] and [Explosive Reaction], the AI <em>trusts</em> theÂ map.</li></ul><p>Itâ€™s the difference between a typo in a guidebook (annoying) and a hacker changing the GPS coordinates of a bridge (fatal). Because agents rely on these graphs for logic, a single poisoned edge can cause a â€œReasoning Cascadeâ€ that is catastrophic.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*K2k4GHnAD48qqKcVv-ce1g.png\" /></figure><p><em>GragPoison: When a single corrupted edge brings down the wholeÂ system.</em></p><blockquote>â€œSemantic noise is annoying. Topological poisoning is fatal. If you build a map, you better guardÂ it.â€</blockquote><h3>VII. The Post-Credits Scene: The PathÂ Forward</h3><p>So, is â€œStructure All You Need?â€ No. We need aÂ hybrid.</p><p>We need the creativity and fuzziness of Vectors (System 1) combined with the rigor and logic of Graphs (SystemÂ 2).</p><p>For the Executives reading this: Stop asking your team, â€œHow big is the context window?â€ Ask them, â€œHow does this agent model its state?â€ If the answer is â€œWe just shove it all in the prompt,â€ run. Invest in State Management.</p><p>For the Policymakers: Demand Traceability. You cannot audit a vector. You <em>can</em> audit a graph. If an AI denies a loan, a graph can show you exactly which path of logic it took (Graph-R1, Luo et al.,Â 2025).</p><p>Final Thought: To reach AGI, or even just reliable autonomy, we must acknowledge that while <em>Attention</em> was a great start, <em>Structure</em> is what we actually need to finish theÂ job.</p><p>Now, go structure your day. And maybe drink some water; that chai wasÂ strong.</p><h3>VIII. References (Categorized by Research Sub-Themes)</h3><h4>Theme 1: Cognitive Memory &amp; WorldÂ Models</h4><ul><li>Anokhin, P., Semenov, N., Sorokin, A., Evseev, D., Kravchenko, A., Burtsev, M., &amp; Burnaev, E. (2024). <em>AriGraph: Learning knowledge graph world models with episodic memory for LLM agents</em>. arXiv. <a href=\"https://arxiv.org/abs/2407.04363\">https://arxiv.org/abs/2407.04363</a></li><li>Cheng, J., Kumar, A., Lal, R., Rajasekaran, R., Ramezani, H., Khan, O. Z.,Â â€¦ &amp; Amiri, H. (2025). <em>WebATLAS: An LLM agent with experience-driven memory and action simulation</em>. arXiv. <a href=\"https://arxiv.org/abs/2510.22732\">https://arxiv.org/abs/2510.22732</a></li><li>Xia, S., Xu, Z., Chai, J., Fan, W., Song, Y., Wang, X.,Â â€¦ &amp; Wang, J. (2025). <em>From experience to strategy: Empowering LLM agents with trainable graph memory</em>. arXiv. <a href=\"https://arxiv.org/abs/2511.07800\">https://arxiv.org/abs/2511.07800</a></li></ul><h4>Theme 2: Agentic Reasoning &amp; Navigation</h4><ul><li>Jiang, J., Zhou, K., Zhao, W. X., Song, Y., &amp; Zhu, C. (2025). KG-Agent: An efficient autonomous agent framework for complex reasoning over knowledge graph. <em>Proceedings of the Association for Computational Linguistics (ACL)</em>. <a href=\"https://aclanthology.org/2025.acl-long.468/\">https://aclanthology.org/2025.acl-long.468/</a></li><li>Liu, X., Lan, B., Hu, Z., Liu, Y., Zhang, Z., &amp; Wang, F. (2025). CodexGraph: Bridging large language models and code repositories via code graph databases. <em>Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>. <a href=\"https://aclanthology.org/2025.naacl-long.7/\">https://aclanthology.org/2025.naacl-long.7/</a></li><li>Luo, H., E, H., Chen, G., Lin, Q., Guo, Y., Xu, F.,Â â€¦ &amp; Tuan, L. A. (2025). <em>Graph-R1: Towards agentic GraphRAG framework via end-to-end reinforcement learning</em>. arXiv. <a href=\"https://arxiv.org/abs/2507.21892\">https://arxiv.org/abs/2507.21892</a></li></ul><h4>Theme 3: Swarm Coordination &amp;Â Topology</h4><ul><li>Li, Q., Gama, F., Ribeiro, A., &amp; Prorok, A. (2020). Graph neural networks for decentralized path planning. <em>Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</em>. <a href=\"https://www.ifaamas.org/Proceedings/aamas2020/pdfs/p1901.pdf\">https://www.ifaamas.org/Proceedings/aamas2020/pdfs/p1901.pdf</a></li><li>Liu, Z., Wan, L., Sui, X., Chen, Z., Sun, K., &amp; Lan, X. (2023). Deep hierarchical communication graph in multi-agent reinforcement learning. <em>Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</em>. <a href=\"https://www.ijcai.org/proceedings/2023/0024.pdf\">https://www.ijcai.org/proceedings/2023/0024.pdf</a></li><li>Ma, X., Isele, D., Gupta, J. K., &amp; Fujimura, K. (2022). Recursive reasoning graph for multi-agent reinforcement learning. <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>. <a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/20733\">https://ojs.aaai.org/index.php/AAAI/article/view/20733</a></li></ul><h4>Theme 4: Security &amp; Construction</h4><ul><li>Han, H., Wang, Y., Shomer, H., Guo, K., Ding, J., Lei, Y.,Â â€¦ &amp; Tang, J. (2024). <em>Retrieval-augmented generation with graphs (GraphRAG)</em>. arXiv. <a href=\"https://arxiv.org/abs/2501.00309\">https://arxiv.org/abs/2501.00309</a></li><li>Liang, J., Wang, Y., Li, C., Zhu, R., Jiang, T., Gong, N., &amp; Wang, T. (2025). <em>GraphRAG under fire: Poisoning attacks on graph-based retrieval-augmented generation</em>. arXiv. <a href=\"https://arxiv.org/abs/2501.14050\">https://arxiv.org/abs/2501.14050</a></li></ul><p><strong><em>Disclaimer</em></strong><em>: The views expressed in this article are personal and do not necessarily reflect the official policy or position of any affiliated organizations. AI assistance was utilized in the research, drafting, and image generation processes for this post. Licensed under CC BY-NDÂ 4.0.</em></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4ee88db32675\" width=\"1\" /><hr /><p><a href=\"https://levelup.gitconnected.com/structure-is-all-you-need-4ee88db32675\">Structure is All You Need?:</a> was originally published in <a href=\"https://levelup.gitconnected.com\">Level Up Coding</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "9 min read",
      "language": null
    },
    {
      "title_en": "Refactoring My Personal Projects",
      "url": "https://levelup.gitconnected.com/refactoring-my-personal-projects-2b0ff46aabd1?source=rss----5517fd7b58a6---4",
      "source": "medium",
      "published_at": "2026-02-09T15:37:55",
      "external_id": null,
      "tags": [
        "gitconnected",
        "python",
        "clean-code",
        "programming",
        "refactoring",
        "code"
      ],
      "content_length": 566,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/refactoring-my-personal-projects-2b0ff46aabd1?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1031/0*8N7ld87vEWRXWRvk\" width=\"1031\" /></a></p><p class=\"medium-feed-snippet\">Clean Code, Modern Tooling, and Better Deployment Practices</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/refactoring-my-personal-projects-2b0ff46aabd1?source=rss----551",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://levelup.gitconnected.com/refactoring-my-personal-projects-2b0ff46aabd1?source=rss----5517fd7b58a6---4\"><img src=\"https://cdn-images-1.medium.com/max/1031/0*8N7ld87vEWRXWRvk\" width=\"1031\" /></a></p><p class=\"medium-feed-snippet\">Clean Code, Modern Tooling, and Better Deployment Practices</p><p class=\"medium-feed-link\"><a href=\"https://levelup.gitconnected.com/refactoring-my-personal-projects-2b0ff46aabd1?source=rss----5517fd7b58a6---4\">Continue reading on Level Up Coding Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "HTTP vs HTTPS: How the Web Talks to You (And What It Hides)",
      "url": "https://medium.com/codex/http-vs-https-how-the-web-talks-to-you-and-what-it-hides-e1daa03d3d02?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:59:04",
      "external_id": null,
      "tags": [
        "codex",
        "devops",
        "cybersecurity",
        "programming",
        "web-development",
        "technology"
      ],
      "content_length": 649,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/http-vs-https-how-the-web-talks-to-you-and-what-it-hides-e1daa03d3d02?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*h5lkazWuzXEbYbAtYgZ6xA.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">What your browser sends, what servers reply, and why one missing &#x201c;S&#x201d; can quietly put you at risk</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/http-vs-https-how-the-web-talks-to-you-and-what-it-hides-e1daa03d3d02?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*h5lkazWuzXEbYbAtYgZ6xA.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">What your browser sends, what servers reply, and why one missing &#x201c;S&#x201d; can quietly put you at risk</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/http-vs-https-how-the-web-talks-to-you-and-what-it-hides-e1daa03d3d02?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Cline X-Ray: Turning Clineâ€™s Shadow Git into an Inspectable Evidence Layer",
      "url": "https://medium.com/codex/cline-x-ray-turning-clines-shadow-git-into-an-inspectable-evidence-layer-04a5815ed539?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:59:01",
      "external_id": null,
      "tags": [
        "codex",
        "cline",
        "rust",
        "llm-applications",
        "agentic-coding",
        "tauri"
      ],
      "content_length": 736,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/cline-x-ray-turning-clines-shadow-git-into-an-inspectable-evidence-layer-04a5815ed539?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1525/1*dZ1v_V2bzcREersm0yeLuQ.png\" width=\"1525\" /></a></p><p class=\"medium-feed-snippet\">If &#x201c;The Shadow Git Behind Cline&#x201d; explained where Cline&#x2019;s diffs really live, Cline X-Ray is the next step: a post-execution explorer t",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/cline-x-ray-turning-clines-shadow-git-into-an-inspectable-evidence-layer-04a5815ed539?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1525/1*dZ1v_V2bzcREersm0yeLuQ.png\" width=\"1525\" /></a></p><p class=\"medium-feed-snippet\">If &#x201c;The Shadow Git Behind Cline&#x201d; explained where Cline&#x2019;s diffs really live, Cline X-Ray is the next step: a post-execution explorer that&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/cline-x-ray-turning-clines-shadow-git-into-an-inspectable-evidence-layer-04a5815ed539?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "How to Use Claude Code: Introducing An AI Terminal Assistant for VSCode, Positron, and Cursor",
      "url": "https://medium.com/codex/how-to-use-claude-code-introducing-an-ai-terminal-assistant-for-vscode-positron-and-cursor-90ae42865944?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:57",
      "external_id": null,
      "tags": [
        "codex",
        "claude-code",
        "claude",
        "ai",
        "ai-tools",
        "data-science"
      ],
      "content_length": 661,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/how-to-use-claude-code-introducing-an-ai-terminal-assistant-for-vscode-positron-and-cursor-90ae42865944?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1668/1*Uw9d9j2R8pJ7uCmjQxW_nA.png\" width=\"1668\" /></a></p><p class=\"medium-feed-snippet\">Upgrade your data analysis and app development with AI</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/how-to-use-claud",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/how-to-use-claude-code-introducing-an-ai-terminal-assistant-for-vscode-positron-and-cursor-90ae42865944?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1668/1*Uw9d9j2R8pJ7uCmjQxW_nA.png\" width=\"1668\" /></a></p><p class=\"medium-feed-snippet\">Upgrade your data analysis and app development with AI</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/how-to-use-claude-code-introducing-an-ai-terminal-assistant-for-vscode-positron-and-cursor-90ae42865944?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "DSA Day 36/250: Next Greater Element I (LeetCode 496)",
      "url": "https://medium.com/codex/dsa-day-36-250-next-greater-element-i-leetcode-496-a4ede759abe3?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:51",
      "external_id": null,
      "tags": [
        "codex",
        "next-greater-element",
        "data-structures",
        "leetcode-easy",
        "data-structure-algorithm",
        "leetcode"
      ],
      "content_length": 622,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/dsa-day-36-250-next-greater-element-i-leetcode-496-a4ede759abe3?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1280/1*w6FNwaalE94sTsYQcq9JSQ.png\" width=\"1280\" /></a></p><p class=\"medium-feed-snippet\">Understand monotonic stack deeply with full dry run and O(n + m) optimized JavaScript solution.</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/dsa-day-36-250-",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/dsa-day-36-250-next-greater-element-i-leetcode-496-a4ede759abe3?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1280/1*w6FNwaalE94sTsYQcq9JSQ.png\" width=\"1280\" /></a></p><p class=\"medium-feed-snippet\">Understand monotonic stack deeply with full dry run and O(n + m) optimized JavaScript solution.</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/dsa-day-36-250-next-greater-element-i-leetcode-496-a4ede759abe3?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Why Salesforce Governor Limits Exist: The Architectural Reason Behind the Guardrails",
      "url": "https://medium.com/codex/why-salesforce-governor-limits-exist-the-architectural-reason-behind-the-guardrails-0bf1c4dbdfb2?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:45",
      "external_id": null,
      "tags": [
        "codex",
        "architecture",
        "salesforce",
        "salesforce-development",
        "software-engineering",
        "api"
      ],
      "content_length": 754,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/why-salesforce-governor-limits-exist-the-architectural-reason-behind-the-guardrails-0bf1c4dbdfb2?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*2RYc51vAqqohpoo9enB1kg.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">If you&#x2019;ve ever developed on the Salesforce platform, you&#x2019;ve likely encountered the infamous governor limits. Perhaps you&#x2019;v",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/why-salesforce-governor-limits-exist-the-architectural-reason-behind-the-guardrails-0bf1c4dbdfb2?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*2RYc51vAqqohpoo9enB1kg.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">If you&#x2019;ve ever developed on the Salesforce platform, you&#x2019;ve likely encountered the infamous governor limits. Perhaps you&#x2019;ve seen the&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/why-salesforce-governor-limits-exist-the-architectural-reason-behind-the-guardrails-0bf1c4dbdfb2?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "The Silent Killer of GPU Performance: Why Your NVIDIA Setup is Only 80% Credible",
      "url": "https://medium.com/codex/the-silent-killer-of-gpu-performance-why-your-nvidia-setup-is-only-80-credible-22294115192d?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:43",
      "external_id": null,
      "tags": [
        "codex",
        "nvidia-gpu",
        "ai",
        "mayhemcode",
        "software-development",
        "data-science"
      ],
      "content_length": 639,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/the-silent-killer-of-gpu-performance-why-your-nvidia-setup-is-only-80-credible-22294115192d?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*K_4d7aFQxoYp8yPR7LEnTA.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">Unearthing the Silent Power: NVIDIA GPUs Beyond the Hype</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/the-silent-killer-of-gpu-p",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/the-silent-killer-of-gpu-performance-why-your-nvidia-setup-is-only-80-credible-22294115192d?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*K_4d7aFQxoYp8yPR7LEnTA.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">Unearthing the Silent Power: NVIDIA GPUs Beyond the Hype</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/the-silent-killer-of-gpu-performance-why-your-nvidia-setup-is-only-80-credible-22294115192d?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Building Autonomous AI Agents: A Deep Dive into Copilot Studioâ€™s Full Experience.",
      "url": "https://medium.com/codex/building-autonomous-ai-agents-a-deep-dive-into-copilot-studios-full-experience-687b553ea7a8?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:39",
      "external_id": null,
      "tags": [
        "codex",
        "programming",
        "artificial-intelligence",
        "software-development",
        "machine-learning",
        "technology"
      ],
      "content_length": 728,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/building-autonomous-ai-agents-a-deep-dive-into-copilot-studios-full-experience-687b553ea7a8?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*wbKjvlWjFw0v8WOdaBLQOA.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Learn how to build autonomous AI agents with Copilot Studio using MCP servers, multi-agent orchestration, and Computer Use for enterprise&#x2026;</p",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/building-autonomous-ai-agents-a-deep-dive-into-copilot-studios-full-experience-687b553ea7a8?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*wbKjvlWjFw0v8WOdaBLQOA.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Learn how to build autonomous AI agents with Copilot Studio using MCP servers, multi-agent orchestration, and Computer Use for enterprise&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/building-autonomous-ai-agents-a-deep-dive-into-copilot-studios-full-experience-687b553ea7a8?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "MLOps in 2026: From MLflow to LLMOpsâ€Šâ€”â€ŠThe Complete Guide to Shipping AI in Production",
      "url": "https://medium.com/codex/mlops-in-2026-from-mlflow-to-llmops-the-complete-guide-to-shipping-ai-in-production-0024955b70c4?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:34",
      "external_id": null,
      "tags": [
        "codex",
        "mlops",
        "llmops",
        "ai-engineering",
        "machine-learning",
        "devops"
      ],
      "content_length": 16151,
      "content_preview": "<h3>MLOps in 2026: From MLflow to LLMOpsâ€Šâ€”â€ŠThe Complete Guide to Shipping AI in Production</h3><p>The MLOps market is projected to reach $4.38 billion in 2026, growing at a 39.8% CAGR. Yet 85% of ML models never make it to production. Hereâ€™s what the research saysâ€Šâ€”â€Šand how to fixÂ it.</p><p>The gap between training a model in a Jupyter notebook and running it reliably in production has never been wider. In 2026, weâ€™re no longer just deploying scikit-learn classifiers behind a REST API. Weâ€™re orc",
      "content_full": "<h3>MLOps in 2026: From MLflow to LLMOpsâ€Šâ€”â€ŠThe Complete Guide to Shipping AI in Production</h3><p>The MLOps market is projected to reach $4.38 billion in 2026, growing at a 39.8% CAGR. Yet 85% of ML models never make it to production. Hereâ€™s what the research saysâ€Šâ€”â€Šand how to fixÂ it.</p><p>The gap between training a model in a Jupyter notebook and running it reliably in production has never been wider. In 2026, weâ€™re no longer just deploying scikit-learn classifiers behind a REST API. Weâ€™re orchestrating complex systems made of foundation models, fine-tuned adapters, retrieval pipelines, guardrails, and feedback loopsâ€Šâ€”â€Šeach with its own lifecycle and failureÂ modes.</p><p>This article synthesizes findings from peer-reviewed research, systematic literature reviews, and industry reports to give you a clear picture of where MLOps stands in 2026, whatâ€™s changing, and what tools and practices actuallyÂ work.</p><h4>The Numbers Donâ€™t Lie: Why MLOps Matters More ThanÂ Ever</h4><p>Letâ€™s start with the data that should keep every ML team awake at night. According to a systematic literature review analyzing 45 peer-reviewed articles (Zarour et al., 2025, <em>Information and Software Technology</em>), <strong>55% of companies cite the lack of adequate MLOps practices as a major obstacle</strong> to deploying ML models. Meanwhile, <strong>70% of organizations are investing in MLOps tools and platforms</strong>â€Šâ€”â€Šwhich means thereâ€™s a massive gap between investment and execution.</p><p>The market reflects this urgency. The global MLOps market was valued at $3.13 billion in 2025 and is expected to reach $89.18 billion by 2035 (Business Research Insights, 2025). Thatâ€™s not incremental growthâ€Šâ€”â€Šthatâ€™s a fundamental shift in how the industry builds software.</p><p>A McKinsey case study showed that a large Brazilian bank <strong>reduced time-to-impact of ML use cases from 20 weeks to 14 weeks</strong>â€Šâ€”â€Ša 30% improvementâ€Šâ€”â€Šsimply by adopting MLOps and data engineering best practices. The ROI is real: organizations implementing comprehensive MLOps strategies report <strong>189% to 335% ROI over threeÂ years</strong>.</p><p>But hereâ€™s the problem: the field is evolving faster than most teams canÂ adapt.</p><h4>The Three Paradigm Shifts ofÂ 2026</h4><h4>1. From MLOps to LLMOps: A New Operational Reality</h4><p>Traditional MLOps assumed a straightforward workflow: collect data, train a model, validate it, deploy it, monitor it, retrain when performance degrades. The model was a discrete artifact with clear inputs, outputs, and performance metrics.</p><p>With LLMs, everything changes. As Gartner projects, <strong>over 30% of the surge in API demand by 2026 will come from AI tools powered by LLMs</strong>. Production AI systems in 2026 are not single modelsâ€Šâ€”â€Štheyâ€™re complex orchestrations of multiple components.</p><p>A qualitative study by ScienceDirect (2024) interviewing 12 ML practitioners across sectors identified <strong>four distinct categories of MLOps challenges</strong>: organizational, technical, operational, and business. While some challenges like data and model complexity are unique to MLOps, many overlap with traditional DevOpsâ€Šâ€”â€Šbut with significantly higherÂ stakes.</p><p>LLMOps extends MLOps principles while addressing unique challenges:</p><ul><li><strong>Prompt Engineering as Software Engineering</strong>: Prompts need version control, testing, A/B experimentation, and continuous optimization. Theyâ€™re no longer casual instructionsâ€Šâ€”â€Štheyâ€™re critical system components.</li><li><strong>Non-deterministic outputs</strong>: When deploying a classification model, you knew what inputs to expect and what outputs to produce. With LLMs, users can ask anything, and â€œcorrectnessâ€ is often subjective.</li><li><strong>Cost management</strong>: A single LLM inference can cost 100x more than a traditional ML prediction. Optimizing token usage, caching, and routing becomes essential.</li></ul><h4>2. Hyper-Automation: Self-Healing ML Pipelines</h4><p>The second major shift is the move toward pipelines that can retrain and redeploy models autonomously. In 2026, <strong>72% of enterprises are adopting automation tools</strong>, while <strong>68% prioritize scalable model deployment</strong> in production environments (Business Research Insights, 2025).</p><p>Tools now enable automated retraining triggered by data changes or model drift, ensuring models stay accurate in dynamic environments. This automation supports rapid scalingâ€Šâ€”â€Šorganizations can deploy and maintain hundreds or thousands of models simultaneously.</p><p>The key tools powering thisÂ shift:</p><ul><li><strong>MLflow</strong> remains the most widely adopted open-source MLOps platform, providing experiment tracking, model versioning, and multi-environment deployment. A case study from a Latin American financial institution showed that <strong>MLflow reduced model deployment time by 35%</strong> and improved reproducibility acrossÂ teams.</li><li><strong>GitHub Actions + MLflow</strong>: The CI/CD combination thatâ€™s becoming the standard for ML teams. Automated pipelines handle everything from code integration to production deployment.</li><li><strong>Kubeflow Pipelines</strong>: For Kubernetes-native orchestration, adoption by a global e-commerce company showed <strong>60% improvement in pipeline reuse</strong> and 50% reduction in deployment errors.</li></ul><h4>3. Edge MLOps and Real-Time Inference</h4><p>As edge devices become more powerful, deploying ML models directly on devices is gaining traction. Edge MLOps enables real-time decision-making in environments with limited connectivityâ€Šâ€”â€Šautonomous vehicles, IoT sensors, healthcare devices.</p><p>Meanwhile, compliance frameworks like Europeâ€™s AI Act are forcing organizations to rethink governance and accountability. Approximately <strong>66% of firms now integrate AI monitoring solutions</strong>, and <strong>71% emphasize explainability features</strong> (Business Research Insights, 2025).</p><h4>The MLOps Stack That Actually Works inÂ 2026</h4><p>Based on the research, hereâ€™s what a production-grade MLOps stack looks likeÂ today:</p><h4>Experiment Tracking &amp; ModelÂ Registry</h4><p><strong>MLflow</strong> dominates this space. With over 55% of production ML teams using it, MLflow provides a comprehensive lifecycle solution for tracking experiments, managing models, and deploying them across environments. Its Model Registry acts as the central source of truth for model stages (Staging, Production, Archived).</p><p>Key capabilities:</p><ul><li>Experiment logging with parameters, metrics, and artifacts</li><li>Model versioning with stage transitions</li><li>REST-based modelÂ serving</li><li>Integration with scikit-learn, TensorFlow, PyTorch, and HuggingÂ Face</li></ul><h4>CI/CD for MachineÂ Learning</h4><p>This is where most teams fail. Traditional CI/CD handles codeâ€Šâ€”â€Šbut ML pipelines also need to track <strong>data quality, schema changes, distribution changes, and model performance</strong>.</p><p>The research identifies the winningÂ pattern:</p><pre>New Code or Data Update<br />    â†’ CI Trigger (GitHub Actions)<br />    â†’ Run Tests + Linting + Validation<br />    â†’ Train and Evaluate Model<br />    â†’ Deploy to Staging â†’ Promote to Production</pre><p>Tools like GitHub Actions, Jenkins, and GitLab CI support CI/CD for ML, automating workflows from code integration to production. By embedding CI/CD into MLOps, organizations keep models up-to-date and adapt swiftly to newÂ data.</p><p><strong>Critical insight</strong>: Version control is not just for code. ML projects must version datasets, labels, hyperparameters, and models using tools like <strong>DVC (Data Version Control)</strong> alongside Git.</p><h4>Monitoring &amp; Observability</h4><p>Deployment is only the beginning. Model performance degrades over time due to changing data patterns (model drift) and environmental conditions.</p><p>The monitoring stack inÂ 2026:</p><ul><li><strong>Prometheus + Grafana</strong> for real-time metrics and dashboards</li><li><strong>EvidentlyAI</strong> or <strong>WhyLabs</strong> for drift detection and data quality monitoring</li><li><strong>MLflow</strong> for experiment comparison and model performance tracking</li></ul><p>When drift is detected, automated workflows trigger retraining using updated dataâ€Šâ€”â€Šclosing the loop on the entire ML lifecycle.</p><h4>Infrastructure</h4><p>Only <strong>57% of data leaders are completely confident in their data</strong> (Salesforce State of Data report). This means infrastructure choices matter enormously:</p><ul><li><strong>Cloud-native platforms</strong> (AWS SageMaker, GCP Vertex AI, Databricks) provide managed infrastructure for scaling, monitoring, and deployment</li><li><strong>Docker</strong> (v25.0, 2025) for containerization, creating immutable deployment artifacts</li><li><strong>Kubernetes</strong> (v1.28, 2025) for orchestration and scalable deployments</li><li><strong>Terraform</strong> for Infrastructure as Code, reducing configuration drift</li></ul><h4>The Maturity Model: Where Does Your TeamÂ Stand?</h4><p>The systematic literature review (Zarour et al., 2025) identified <strong>five maturity models</strong> for MLOps adoption, drawing from frameworks proposed by Google and Microsoft. Hereâ€™s a simplified version:</p><p><strong>Level 0â€Šâ€”â€ŠManual Process</strong>: Data scientists work in notebooks. No pipeline automation. Deployment is a manual, ad-hoc process. This is where 85% of ML projectsÂ die.</p><p><strong>Level 1â€Šâ€”â€ŠML Pipeline Automation</strong>: Automated training pipelines with experiment tracking. Models are versioned and stored in a registry. CI/CD is partially implemented.</p><p><strong>Level 2â€Šâ€”â€ŠCI/CD Pipeline Automation</strong>: Full automation from code commit to production deployment. Automated testing includes data validation, model evaluation, and integration tests. Champion/Challenger model comparison is automated.</p><p><strong>Level 3â€Šâ€”â€ŠAutomated Monitoring &amp; Retraining</strong>: Production models are continuously monitored for drift. Retraining is triggered automatically. Feature stores ensure consistency between training andÂ serving.</p><p><strong>Level 4â€Šâ€”â€ŠFull MLOps (LLMOps-ready)</strong>: Multi-model orchestration with guardrails. Prompt versioning and testing. Cost optimization across inference endpoints. Compliance and governance built into the pipeline.</p><p>Most organizations in 2026 are somewhere between Level 1 and Level 2. The research shows that getting from Level 0 to Level 2 delivers the highestÂ ROI.</p><h4>Nine Best Practices from theÂ Research</h4><p>The systematic review of 45 articles identified nine key MLOps practices that consistently appear in successful implementations:</p><ol><li><strong>Automation</strong>â€Šâ€”â€ŠAutomate every stage of the ML pipeline, from data ingestion to deployment</li><li><strong>Version Control</strong>â€Šâ€”â€ŠVersion code, data, models, and configurations together</li><li><strong>Continuous Monitoring</strong>â€Šâ€”â€ŠTrack model performance, data quality, and system health in real-time</li><li><strong>Data Management</strong>â€Šâ€”â€ŠImplement data validation, feature stores, and schema enforcement</li><li><strong>Collaboration</strong>â€Šâ€”â€ŠBreak down silos between data science, engineering, and operations teams</li><li><strong>Reproducibility</strong>â€Šâ€”â€ŠEnsure any experiment can be reproduced exactly</li><li><strong>Testing</strong>â€Šâ€”â€ŠImplement unit tests, integration tests, and model validation tests</li><li><strong>Governance</strong>â€Šâ€”â€ŠMaintain audit trails, model documentation, and compliance records</li><li><strong>Incremental Adoption</strong>â€Šâ€”â€ŠStart small, prove value, thenÂ scale</li></ol><h4>The Skills You Need inÂ 2026</h4><p>MLOps and LLMOps practitioners need a broader skill set thanÂ ever:</p><p><strong>Foundational</strong>: Python fluency, distributed systems, cloud platforms, debugging.</p><p><strong>MLOps Core</strong>: Experiment tracking, model versioning, feature engineering, model serving, monitoring. These skills remain relevant and transfer to LLMOps with adaptation.</p><p><strong>LLMOps Specialization</strong>: Prompt engineering and optimization, RAG system design, LLM evaluation strategies, guardrail implementation, and cost optimization techniques.</p><p><strong>System Thinking</strong>: Perhaps most importantly, LLMOps requires thinking at the system levelâ€Šâ€”â€Šunderstanding how all components interact and fail together.</p><h4>Whatâ€™s Coming Next:Â AgentOps</h4><p>As LLMOps matures, the next evolution is already emerging: <strong>AgentOps</strong>. Deloitte predicts that <strong>50% of enterprises using generative AI will deploy AI agents byÂ 2027</strong>.</p><p>AgentOps combines LLMOps principles with autonomous decision-makingâ€Šâ€”â€Šmanaging the lifecycle of AI agents that can execute tasks without human intervention. This is the frontier where MLOps, LLMOps, and traditional software engineering converge.</p><h4>Conclusion: The Time to Invest in MLOps IsÂ Now</h4><p>The research is clear: organizations that invest in MLOps practices see measurable returnsâ€Šâ€”â€Šfaster deployment, fewer failures, better model performance. The market is growing at nearly 40% annually. The tools are mature. The frameworks exist.</p><p>The question isnâ€™t whether your team needs MLOpsâ€Šâ€”â€Šitâ€™s how fast you can move from Level 0 to Level 2 andÂ beyond.</p><p>Start with these threeÂ actions:</p><ol><li><strong>Set up MLflow</strong> for experiment tracking and model versioningâ€Šâ€”â€Štoday</li><li><strong>Implement CI/CD</strong> for your ML pipeline using GitHubÂ Actions</li><li><strong>Add monitoring</strong> with Prometheus + Grafana to detect drift before it impactsÂ users</li></ol><p>The 85% of models that fail in production donâ€™t fail because the math is wrong. They fail because the engineering isnâ€™t there. MLOps is that engineering.</p><h4>References</h4><ol><li>Zarour, M. et al. (2025). â€œMLOps best practices, challenges and maturity models: A systematic literature review.â€ <em>Information and Software Technology</em>, Vol.Â 183.</li><li>ScienceDirect (2024). â€œAn analysis of the challenges in the adoption of MLOps.â€ <em>Journal of Innovation &amp; Knowledge</em>.</li><li>Business Research Insights (2025). â€œMLOps Market Trends 2026â€“2035.â€ Report ID:Â 118206.</li><li>Precedence Research (2026). â€œMLOps Market Size to Hit USD 56.60 Billion byÂ 2035.â€</li><li>Gartner (2025). â€œOver 30% of API demand surge by 2026 from LLM-powered AIÂ tools.â€</li><li>Deloitte (2025). â€œ50% of enterprises using generative AI will deploy AI agents byÂ 2027.â€</li><li>McKinsey (2024). â€œState of AI Reportâ€Šâ€”â€Š72% enterprise AI adoptionÂ rate.â€</li><li>Warnett, S. &amp; Zdun, U. (2025). â€œBridging the Gap Between MLOps and RLOps.â€ <em>ICSAÂ 2025</em>.</li><li>Martin Fowler / ThoughtWorks. â€œContinuous Delivery for Machine Learning.â€</li><li>Salesforce. â€œState of Data Reportâ€Šâ€”â€Š57% data leader confidence rate.â€</li></ol><p><em>About the author: MLOps Engineer specializing in AI/ML systems, LLMs, RAG systems, and production ML pipelines. I help teams ship AI to production with confidence. Follow me for more research-backed content on AI engineering.</em></p><p><em>Need help building your MLOps stack or deploying ML models in production? Reach out on </em><a href=\"https://www.linkedin.com/in/mohamed-h-7113682b1/\"><em>LinkedIn</em></a><em>â€Šâ€”â€ŠIâ€™m always happy to connect with fellow ML engineers.</em></p><p><em>Coming next: â€œCI/CD for Machine Learning: Automated Pipeline with MLflow and GitHub Actionsâ€ and â€œWhy 85% of ML Models Fail in Productionâ€Šâ€”â€ŠAnd How to FixÂ It.â€</em></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0024955b70c4\" width=\"1\" /><hr /><p><a href=\"https://medium.com/codex/mlops-in-2026-from-mlflow-to-llmops-the-complete-guide-to-shipping-ai-in-production-0024955b70c4\">MLOps in 2026: From MLflow to LLMOpsâ€Šâ€”â€ŠThe Complete Guide to Shipping AI in Production</a> was originally published in <a href=\"https://medium.com/codex\">CodeX</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "9 min read",
      "language": null
    },
    {
      "title_en": "Most Go Interfaces Are Designed Backwards",
      "url": "https://medium.com/codex/most-go-interfaces-are-designed-backwards-6cbb47e20418?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:31",
      "external_id": null,
      "tags": [
        "codex",
        "golang",
        "coding",
        "writing",
        "programming",
        "software-development"
      ],
      "content_length": 561,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/most-go-interfaces-are-designed-backwards-6cbb47e20418?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*LYBkJiBdbTagGri0\" width=\"4939\" /></a></p><p class=\"medium-feed-snippet\">A clear demonstration of what&#x2019;s wrong and how to fix it</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/most-go-interfaces-are-designed-backwards-6cbb47e20418?source=rss--",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/most-go-interfaces-are-designed-backwards-6cbb47e20418?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*LYBkJiBdbTagGri0\" width=\"4939\" /></a></p><p class=\"medium-feed-snippet\">A clear demonstration of what&#x2019;s wrong and how to fix it</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/most-go-interfaces-are-designed-backwards-6cbb47e20418?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Apple Silicon and TPUs for AI: The Deployment Options Nobody Expects",
      "url": "https://medium.com/codex/apple-silicon-and-tpus-for-ai-the-deployment-options-nobody-expects-bdf488284f24?source=rss----29038077e4c6---4",
      "source": "medium",
      "published_at": "2026-02-10T07:58:27",
      "external_id": null,
      "tags": [
        "codex",
        "software-development",
        "mayhemcode",
        "ai",
        "technology",
        "programming"
      ],
      "content_length": 687,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/apple-silicon-and-tpus-for-ai-the-deployment-options-nobody-expects-bdf488284f24?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*TSnScx10PdbruhjnLfn9-A.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">When I told my infrastructure team we were deploying production LLM services on a Mac Studio, they thought I had lost my mind.</p><p class=\"medium-feed-link\"><",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/codex/apple-silicon-and-tpus-for-ai-the-deployment-options-nobody-expects-bdf488284f24?source=rss----29038077e4c6---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*TSnScx10PdbruhjnLfn9-A.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">When I told my infrastructure team we were deploying production LLM services on a Mac Studio, they thought I had lost my mind.</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/codex/apple-silicon-and-tpus-for-ai-the-deployment-options-nobody-expects-bdf488284f24?source=rss----29038077e4c6---4\">Continue reading on CodeX Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "How a Misconfigured AWS Security Group Took Down a Production API",
      "url": "https://aws.plainenglish.io/how-a-misconfigured-aws-security-group-took-down-a-production-api-9dfaf97ec7c3?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:43:32",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "programming",
        "coding",
        "aws",
        "aws-vpc",
        "api"
      ],
      "content_length": 9293,
      "content_preview": "<h4>A Real Debugging Walkthrough Using VPC Flow Logs and CloudWatch</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1HRlJcZmPYMxLoHKP-iUGA.png\" /></figure><h3>1. The Failure Pattern That Didnâ€™t Look Like a NetworkÂ Issue</h3><p>The outage started as elevated 5xx rates on a single API behind an ALB. CPU and memory on the service were normal. Application logs showed request handlers timing out while waiting on a downstream dependency. The dependency was another internal serv",
      "content_full": "<h4>A Real Debugging Walkthrough Using VPC Flow Logs and CloudWatch</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1HRlJcZmPYMxLoHKP-iUGA.png\" /></figure><h3>1. The Failure Pattern That Didnâ€™t Look Like a NetworkÂ Issue</h3><p>The outage started as elevated 5xx rates on a single API behind an ALB. CPU and memory on the service were normal. Application logs showed request handlers timing out while waiting on a downstream dependency. The dependency was another internal service in the same VPC. From the appâ€™s point of view, the failure looked like slow I/O, not a hard network failure. No connection refused errors, no obvious timeouts at the TCP layer. Just hanging requests and eventual retries pilingÂ up.</p><p>This is the kind of failure that pushes teams to look at code first. We did. Nothing had changed in the request path. The only deployment that day was an infrastructure change: tightening security group rules to remove what looked like overly permissive ingress. The API and its downstream dependency were both still reachable from bastion hosts, so the initial assumption was that the change was safe. ItÂ wasnâ€™t.</p><h3>2. Reproducing the Timeout at the SocketÂ Level</h3><p>The first step was to get out of the application logs and confirm what the network stack was doing. From one of the API pods, I ran a minimal connectivity check using the same client library configuration the serviceÂ used.</p><pre>curl -v http://internal-service:8080/health</pre><p>The connection attempt hung for several seconds and then failed. A raw TCP test showed the sameÂ pattern:</p><pre>nc -vz internal-service 8080</pre><p>No immediate refusal, no SYN-ACK, just silence until the client timed out. This pointed to a network-level drop, not an application-level rejection. A security group or NACL misconfiguration was the likely culprit. The tricky part was that both services were in the same VPC and subnet. On paper, nothing had changed in their placement.</p><h3>3. Using VPC Flow Logs to Prove theÂ Drop</h3><p>VPC Flow Logs are noisy, but they are the fastest way to answer one question: is traffic being accepted or rejected at the VPC boundary. We enabled flow logs on the subnet and filtered for the source ENI of the API service and the destination port of the downstream service.</p><p>The relevant fields were enough to reconstruct theÂ story:</p><pre>{<br />  &quot;srcAddr&quot;: &quot;10.0.12.34&quot;,<br />  &quot;dstAddr&quot;: &quot;10.0.21.87&quot;,<br />  &quot;srcPort&quot;: 49231,<br />  &quot;dstPort&quot;: 8080,<br />  &quot;action&quot;: &quot;REJECT&quot;,<br />  &quot;protocol&quot;: 6<br />}</pre><p>The REJECT action told us the packet never made it past the security layer. There was no corresponding ACCEPT entry. This ruled out application misbehavior. The network policy was actively dropping traffic. The absence of a TCP reset explained the client-side hang: the packet was dropped, not rejected, so the client retried untilÂ timeout.</p><h3>4. The Security Group Rule That Looked Reasonable</h3><p>The change that caused the outage was a refactor of security groups to follow a stricter pattern: each service would only accept traffic from explicitly named source security groups, not from CIDR ranges. The downstream serviceâ€™s security group had been updated likeÂ this:</p><pre>ingress {<br />  from_port       = 8080<br />  to_port         = 8080<br />  protocol        = &quot;tcp&quot;<br />  security_groups = [aws_security_group.api_sg.id]<br />}</pre><p>On paper, this allowed traffic from the API. The problem was that the API was deployed on ECS with awsvpc networking, and the task ENIs were attached to a different security group than the one we assumed. The rule referenced the security group attached to the load balancer, not the one attached to the service tasks making theÂ call.</p><p>The API could accept inbound traffic fine. Its outbound traffic originated from a different security group than expected. The downstream service never allowed that source, so packets wereÂ dropped.</p><h3>5. Verifying ENI Attachments Instead of Trusting Terraform</h3><p>The fastest way to confirm this was to inspect the ENIs attached to the runningÂ tasks:</p><pre>aws ec2 describe-network-interfaces \\<br />  --filters Name=private-ip-address,Values=10.0.12.34 \\<br />  --query &quot;NetworkInterfaces[0].Groups&quot;</pre><p>The output showed a different security group ID than the one referenced in the ingress rule. This mismatch was the root cause. The infrastructure code was correct in isolation. The mental model of which security group represented the APIâ€™s egress identity wasÂ wrong.</p><p>This is a common failure mode in ECS and EKS environments: there are multiple layers of network identity. Load balancers, nodes, and tasks or pods often have distinct security groups. If you allow traffic from the wrong layer, connectivity fails silently.</p><h3>6. Correlating CloudWatch Metrics With NetworkÂ Drops</h3><p>To understand the blast radius, we correlated ALB target response times and error rates with flow log REJECT counts. CloudWatch showed a clean step function: as soon as the security group change rolled out, target response times spiked and success ratesÂ dropped.</p><p>The metric pattern mattered:</p><pre>ALB TargetResponseTime p95: 120ms -&gt; 5s<br />HTTPCode_Target_5XX_Count: baseline -&gt; sustained spike</pre><p>This matched the client-side timeouts. The service wasnâ€™t failing fast. It was waiting on a network path that would never succeed. This is operationally worse than an immediate 403 or connection refused, because it ties up worker threads and cascades latency to upstreamÂ callers.</p><h3>7. The Fix and the Guardrails That Prevented aÂ Repeat</h3><p>The immediate fix was to update the downstream serviceâ€™s ingress rule to allow the correct source security group. The more important fix was to codify the relationship between service identities and network policies so this couldnâ€™t happenÂ again.</p><p>We ended up introducing explicit outputs for task-level security groups and wiring those into downstream ingressÂ rules:</p><pre>output &quot;api_task_sg_id&quot; {<br />  value = aws_security_group.api_task_sg.id<br />}<br /><br />ingress {<br />  from_port       = 8080<br />  to_port         = 8080<br />  protocol        = &quot;tcp&quot;<br />  security_groups = [module.api.api_task_sg_id]<br />}</pre><p>We also added a pre-deploy check that attempted a simple TCP connection between services in staging using the same network paths. This was a cheap smoke test that would have caught the misconfiguration before it hit production.</p><h3>8. The Operational Lesson: Silent Drops Are the Worst FailureÂ Mode</h3><p>The technical root cause was a single misreferenced security group. The operational lesson was broader. Network policies that drop traffic without an explicit reject create ambiguous failure modes. Clients hang. Retries amplify load. Debugging shifts from application logs to infrastructure telemetry.</p><p>After this incident, we changed our default stance on security group rules. Where possible, we preferred explicit rejects during migrations and tightened rules gradually, watching flow logs and latency metrics in parallel. The goal wasnâ€™t to make the network more permissive. It was to make failures observable andÂ bounded.</p><p>This outage wasnâ€™t caused by a lack of security hygiene. It was caused by assuming we understood how AWS attached network identity to our workloads. The combination of VPC Flow Logs and CloudWatch made the failure mode visible. The lasting fix was updating our mental model and our IaC to reflect how traffic actually flows in production, not how we wanted it to flow onÂ paper.</p><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9dfaf97ec7c3\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/how-a-misconfigured-aws-security-group-took-down-a-production-api-9dfaf97ec7c3\">How a Misconfigured AWS Security Group Took Down a Production API</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "How to run your first rootless container with Podman",
      "url": "https://aws.plainenglish.io/how-to-run-your-first-rootless-container-with-podman-df72f3521529?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:41:46",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "podman",
        "devops"
      ],
      "content_length": 6461,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*q4BR3VKkswdnn65U.png\" /></figure><p>Here is a hands-on guide to getting Podman up and running. We will focus on the â€œHappy Pathâ€ of installation and demonstrate the security benefits of running rootless containers immediately.</p><h3>Prerequisites</h3><ul><li>A terminalÂ window.</li><li><strong>Linux Users:</strong> You are ready to go natively.</li><li><strong>Mac/Windows Users:</strong> Podman runs a small Linux VM in the backgr",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/0*q4BR3VKkswdnn65U.png\" /></figure><p>Here is a hands-on guide to getting Podman up and running. We will focus on the â€œHappy Pathâ€ of installation and demonstrate the security benefits of running rootless containers immediately.</p><h3>Prerequisites</h3><ul><li>A terminalÂ window.</li><li><strong>Linux Users:</strong> You are ready to go natively.</li><li><strong>Mac/Windows Users:</strong> Podman runs a small Linux VM in the background (similar to Docker Desktop) to host the containers.</li></ul><h3>Step 1: Installation</h3><p>Select your operating systemÂ below.</p><p><strong>Fedora / CentOS / RHEL (The native home of Podman)</strong><br /> Podman comes pre-installed on many newer versions, but if you needÂ it:</p><pre>sudo dnf install -y podman</pre><p><strong>Ubuntu /Â Debian</strong></p><pre>sudo apt-get update sudo apt-get install -y podman</pre><p><strong>macOS (via Homebrew)</strong></p><pre>brew install podman # Initialize the virtual machine (Mac only) podman machine init podman machine start</pre><p><strong>Windows</strong><br /> The easiest method is using the Windows Installer from the <a href=\"https://podman.io/\">Podman website</a>, or viaÂ Winget:</p><pre>winget install RedHat.Podman # Initialize the machine podman machine init podman machine start</pre><h3>Step 2: The SanityÂ Check</h3><p>Before running containers, letâ€™s ensure Podman is installed and creating the correct environment.</p><p>Run thisÂ command:</p><pre>podman info</pre><p><strong>What to look for:</strong><br /> Scroll through the output. Look for the line rootless: true.</p><ul><li>If you see true, congratulations! You are running containers as your user, not as the system administrator.</li><li>If you see false, you likely ran the command with sudo. <strong>Stop!</strong> Do not use sudo with Podman unless you explicitly want to manage system-wide services.</li></ul><h3>Step 3: Running Your FirstÂ Service</h3><p>We will run an Nginx webÂ server.</p><p>Because we are running as a standard user (rootless), we have a minor constraint: <strong>We cannot bind to ports lower than 1024.</strong> Standard ports like 80 or 443 are reserved for root. Instead, we will forward to a high port likeÂ 8080.</p><p>Execute the following:</p><pre>podman run -dt -p 8080:80 --name my-secure-web docker.io/library/nginx</pre><p><strong>Breakdown of theÂ flags:</strong></p><ul><li>-dt: Run in the background (<strong>d</strong>etached) and allocate a pseudo-<strong>t</strong>ty.</li><li>-p 8080:80: Map port 8080 on your machine to port 80 inside the container.</li><li>--name: Give it a human-readable name.</li><li>docker.io/...: Podman requires the full registry path (unlike Docker, which defaults to Docker Hub automatically) to avoid ambiguity/spoofing, though standard installs often alias this forÂ you.</li></ul><p>Check if it works by opening your browser to http://localhost:8080 orÂ running:</p><pre>curl http://localhost:8080</pre><h3>Step 4: The â€œMagic Trickâ€ (Verifying Rootless Isolation)</h3><p>This is the most important part of the tutorial. We are going to prove that while the container <em>thinks</em> it is root, your host machine knowsÂ better.</p><p><strong>1. Ask the container who itÂ is:</strong></p><pre>podman exec my-secure-web whoami</pre><p><strong>Output:</strong> root<br /><em>Inside the container, Nginx has fullÂ reign.</em></p><p><strong>2. Ask the host machine who owns the process:</strong><br /> We will use podman top to inspect the running process from the host's perspective.</p><pre>podman top my-secure-web user huser</pre><ul><li>user: The user inside the container.</li><li>huser: The user on the hostÂ machine.</li></ul><p><strong>Output:</strong></p><pre>USER HUSER root your_username (or UID 1000)</pre><p><strong>The DevOps takeaway:</strong> Even if a hacker compromises Nginx and breaks out of the container, they do not land on your server as root. They land as <em>you</em> (or a sub-user), with limited permissions.</p><h3>Step 5: CleanÂ Up</h3><p>Podman commands mirror Dockerâ€™s, so cleanup is intuitive.</p><pre># Stop the container podman stop my-secure-web # Remove the container podman rm my-secure-web</pre><h3>Troubleshooting Tip: â€œShortÂ Namesâ€</h3><p>If you tried to run podman run alpine and it failed saying &quot;short name reference,&quot; it's because Podman is strict about where it pulls images from.<br /><strong>Fix:</strong> Edit /etc/containers/registries.conf to add docker.io to the search list, or simply always use the full name: docker.io/library/alpine.</p><h3>Next Steps for a DevOpsÂ Engineer</h3><p>Now that you have a running container, we can leverage Podmanâ€™s Kubernetes integration.</p><p>But, thatâ€™s for the nextÂ post.</p><p><em>Originally published at </em><a href=\"https://cloudqubes.com/letters/how-to-run-your-first-rootless-container-with-podman\"><em>https://cloudqubes.com</em></a><em>.</em></p><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=df72f3521529\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/how-to-run-your-first-rootless-container-with-podman-df72f3521529\">How to run your first rootless container with Podman</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Implementing OTA Updates using Hot Updater + AWS S3, CloudFront and Lambda@edge",
      "url": "https://aws.plainenglish.io/implementing-ota-updates-using-hot-updater-aws-s3-cloudfront-and-lambda-edge-4e2c4f0fe318?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:24:25",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "aws",
        "software-development",
        "mobile-app-development"
      ],
      "content_length": 13733,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-FEnuI9TJaVXtgxLxfh8Pg.jpeg\" /></figure><p>I once worked with a client whose infrastructure was hosted on AWS. The final scope of the project was to implement an OTA update deployment for their mobile application.</p><p>At the time, I had never heard of OTA before. So naturally, that led to a few hours of research, reading, and mild confusion, until I finally understood that Over-The-Air (OTA) updates are a quick and easy way to",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-FEnuI9TJaVXtgxLxfh8Pg.jpeg\" /></figure><p>I once worked with a client whose infrastructure was hosted on AWS. The final scope of the project was to implement an OTA update deployment for their mobile application.</p><p>At the time, I had never heard of OTA before. So naturally, that led to a few hours of research, reading, and mild confusion, until I finally understood that Over-The-Air (OTA) updates are a quick and easy way to push Mobile App changes like frontend edits and bug fixes directly to usersâ€™ devices, over the air, without going through the Play Store or Apple AppÂ Store.</p><p>OTAâ€Šâ€”â€ŠOver The Air.<br />Iâ€™m assuming you get it nowÂ ğŸ˜‰.</p><p>One thing you should have learned by now in our field is that we never mean these names literally. Just like the Cloud isnâ€™t actually somewhere up in the sky, OTA updates donâ€™t literally pass â€œover the airâ€ either. (Disappointing, IÂ know.)</p><p>After this realization, my immediate thought was: <em>Waitâ€¦ isnâ€™t this more of a Mobile Developer problem than a Cloud Engineer one?</em><br />So I asked the client a few more questions to really understand where I fit in. Thatâ€™s when their use case finallyÂ clicked.</p><p>For the OTA updates, we were going to use a tool called <strong>Hot Updater</strong>. Iâ€™ll explain what that does later as we goÂ along.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*E6VbYQGsj3fBiibKQ-bhaA.jpeg\" /><figcaption>Implementing OTA Updates using Hot Updater with AWS S3, CloudFront &amp; Lambda@edge</figcaption></figure><p>Hereâ€™s how the OTA workflow actuallyÂ works.</p><p>A developer fixes a bug in the JavaScript code.<br />They run a build and generate a new OTA update bundle. At this point, the bundle is just filesâ€Šâ€”â€ŠJavaScript and assets. Nothing magicalÂ yet.</p><p>Those files are uploaded to <strong>AmazonÂ S3</strong>.</p><p>In front of S3 sits <strong>CloudFront</strong>, which you can think of as a fast, global delivery system. From this point on, any device that needs the update will download it through CloudFront.</p><p>The developer also updates a set of simple rules. These rules answer questions like:</p><ul><li>Which app versions can receive thisÂ update?</li><li>Which platform is itÂ for?</li><li>Is this a full rollout or just a smallÂ test?</li></ul><p>These rules are used by <strong>Lambda@Edge</strong>.</p><p>Now, a user opens theÂ app.</p><p>The appâ€™s Hot Updater wakes up andÂ asks:</p><p><em>â€œHeyâ€¦ is there an update forÂ me?â€</em></p><p>That request goes to CloudFront.<br />Before CloudFront responds, <strong>Lambda@Edge jumpsÂ in</strong>.</p><p>Lambda@Edge looks at the request andÂ thinks:</p><ul><li>What app version isÂ this?</li><li>iOS orÂ Android?</li><li>Is this user part of theÂ rollout?</li></ul><p>Based on that, itÂ decides:</p><ul><li>â€œYes, this device should get the newÂ update.â€</li><li>Or, â€œNope. Stay on the current one forÂ now.â€</li></ul><p>If the answer is <em>yes</em>, Lambda@Edge points CloudFront to the correct update files in S3.<br />CloudFront delivers those files quickly to theÂ app.</p><p>Hot Updater downloads the update, verifies it, applies it, and reloads theÂ app.</p><p>The user sees the bug fixedâ€Šâ€”â€Šwithout updating the app from theÂ store.</p><p>And if something goesÂ wrong?</p><p>The developer simply changes the rules.<br />Lambda@Edge immediately starts sending users back to the previousÂ update.</p><p>No redeploy.<br />No app store review.<br />Just instant rollback.</p><p>This article isnâ€™t meant to be a step-by-step guide on how to implement OTA Updates. You can find that in the official Hot Updater documentation, and itâ€™s actually pretty straightforward.</p><p><a href=\"https://hot-updater.dev/docs/managed/aws\"><em>Hot UpdaterÂ Doc.</em></a></p><p>Iâ€™m just here to share the blockers I ran into during the implementation and how I worked throughÂ them.</p><p>Soâ€¦ letâ€™s dive rightÂ in.</p><h3>The Blocker That Almost Made Me Question My LifeÂ Choices</h3><p>Everything lookedÂ correct.</p><p>The OTA workflow was set up.<br />The JavaScript changes were clean.<br />No native code had beenÂ touched.</p><p>And yetâ€¦<br /><strong>CI kept screaming: â€œNative changes detected.â€</strong></p><p>Every. Single.Â Time.</p><p>Even for JavaScript-only updates.</p><p>At first, this felt like one of those problems where you reread the logs ten times, hoping the error message magically changes. ItÂ didnâ€™t.</p><h3>The Root Cause (a.k.a. Paths Will BetrayÂ You)</h3><p>The issue came down to the fingerprint system, specifically, the fingerprint.json file.</p><p>This file contained absolute file paths, and those paths were different depending on <em>where</em> the fingerprint was generated.</p><p>For example:</p><ul><li><strong>Local machine</strong></li></ul><pre>/Users/dev-user/Documents/company-mobile/node_modules/...</pre><ul><li><strong>CI (GitHubÂ Actions)</strong></li></ul><pre>/Users/runner/work/company-mobile/company-mobile/node_modules/...</pre><p>Same code.<br />Same dependencies.<br />Different paths.</p><p>And those differences were enough to produce <strong>different hashes</strong>.</p><p>So the systemÂ assumed:</p><p>â€œOh, something in native land must have changed.â€</p><p>Even when it absolutely hadnâ€™t.</p><p>The main offender here was rncoreAutolinkingConfig, which quietly embedded environment-specific paths into the fingerprint.</p><h3>How This Broke Everything in Two Very AnnoyingÂ Ways</h3><h4>Issue 1: CI Always Failed (Even When NothingÂ Changed)</h4><ul><li>JavaScript-only changes still triggered fingerprint mismatches</li><li>The workflow constantly reported <em>â€œnative changes detectedâ€</em></li><li>OTA updates were blocked for no goodÂ reason</li></ul><p>At this point, OTA had basically lost its â€œquickâ€ advantage.</p><h4>Issue 2: Store Builds and OTA Updates Were Speaking Different Languages</h4><p>This one wasÂ worse.</p><p>Hereâ€™s what was happening:</p><p><strong>Store build (done locally):</strong></p><pre>Developer builds app locally<br />    â†“<br />Local fingerprint generated (with local paths)<br />    â†“<br />App uploaded to App Store / Play Store<br />    â†“<br />Native hash = &quot;ABC123&quot;</pre><p><strong>OTA deployment (done onÂ CI):</strong></p><pre>CI generates fingerprint (with CI paths)<br />    â†“<br />OTA bundle deployed<br />    â†“<br />Expected native hash = &quot;XYZ789&quot;</pre><p><strong>What the appÂ sees:</strong></p><pre>App native code hash: ABC123<br />OTA server expects: XYZ789<br />âŒ Update rejected</pre><p>So even though the code was the same, the hashes didnâ€™tÂ agree.</p><p>Which meant:</p><ul><li>OTA updatesÂ failed</li><li>Rollbacks didnâ€™t behave correctly</li><li>Everyone wasÂ confused</li></ul><h3>The Fix (A Two-Part Truce Between Local Machines andÂ CI)</h3><h4>Part 1: Let CI Take Ownership of the Fingerprint (One-Time Fix)</h4><p>The first rule we established wasÂ simple:</p><blockquote><strong><em>CI is the single source of truth for fingerprints.</em></strong></blockquote><p>Hereâ€™s what weÂ did:</p><ul><li>Deleted the existing fingerprint.json (it was polluted with localÂ paths)</li><li>Forced CI to regenerate it</li><li>Committed the new fingerprint back into the repository</li></ul><p><strong>Temporary CI workflowÂ step:</strong></p><pre>- name: Force regenerate fingerprint<br />  run: |<br />    rm -f fingerprint.json<br />    yarn hot-updater fingerprint create</pre><pre>- name: Commit new fingerprint<br />  uses: stefanzweifel/git-auto-commit-action@v5<br />  with:<br />    commit_message: &quot;chore: regenerate fingerprint with CI paths&quot;<br />    file_pattern: &quot;fingerprint.json&quot;</pre><p><strong>Result:</strong></p><ul><li>fingerprint.json now contains CI-basedÂ paths</li><li>Future CI runs stopped detecting fake nativeÂ changes</li><li>Workflow returned to normal fingerprint checks</li></ul><p>This fixed CIâ€¦ but local store builds were still aÂ problem.</p><h4>Part 2: Making Local Store Builds Respect CI Fingerprints (Permanent Fix)</h4><h4>The Problem</h4><p>Developers were building locally for the App Store and PlayÂ Store.</p><p>That was fine, except local builds were generating new fingerprints, undoing everything CI had justÂ fixed.</p><p>What we needed was a way for local buildsÂ to:</p><ul><li>Read CIâ€™s fingerprint</li><li>Reuse itsÂ hashes</li><li>Never generate newÂ ones</li></ul><h3>The Solution: Sync, Donâ€™tÂ Generate</h3><p>We created a small system that reads<strong> </strong>fingerprint.json<strong> </strong>from git and injects the hashes into native configs before building.</p><h4>Files Created</h4><ol><li>scripts/get-fingerprint.js â€“ Reads hashes from fingerprint.json</li><li>scripts/update-android-fingerprint.js â€“ Updates Android strings.xml</li><li>app.config.js â€“ Uses Expoâ€™s native infoPlist forÂ iOS</li></ol><h4>How ItÂ Works</h4><p><strong>iOS (Automatic viaÂ Expo)</strong></p><pre>Developer runs: yarn prebuild:ios<br />    â†“<br />expo prebuild reads app.config.js<br />    â†“<br />app.config.js calls getFingerprintHashes()<br />    â†“<br />Reads fingerprint.json (from CI)<br />    â†“<br />Writes hash to Info.plist âœ…</pre><p><strong>Android (Script-Based)</strong></p><pre>Developer runs: yarn prebuild:android<br />    â†“<br />expo prebuild generates files<br />    â†“<br />Script runs: node scripts/update-android-fingerprint.js<br />    â†“<br />Reads fingerprint.json (from CI)<br />    â†“<br />Updates strings.xml with hash âœ…</pre><p><strong>package.json scripts:</strong></p><pre>{<br />  &quot;prebuild:ios&quot;: &quot;npx expo prebuild --platform ios&quot;,<br />  &quot;prebuild:android&quot;: &quot;npx expo prebuild --platform android &amp;&amp; node scripts/update-android-fingerprint.js&quot;<br />}</pre><h4>The Final Workflow (a.k.a. Peace Restored)</h4><p><strong>For Developers (StoreÂ Builds)</strong></p><pre>git pull origin staging-ota   # Get CI fingerprint<br />yarn build:ios<br />yarn build:android<br /># Upload to stores<br />git push</pre><p><strong>For CI (OTA Deployments)</strong></p><pre># No change needed<br />- Check fingerprint<br />- JS-only changes â†’ Deploy OTA<br />- Native changes â†’ Update fingerprint â†’ Deploy OTA</pre><h3>Rules We Now LiveÂ By</h3><h4>âœ… DO</h4><ul><li>Always git pull before building forÂ stores</li><li>Let CI create and update fingerprints</li><li>Use the provided buildÂ scripts</li></ul><h4>âŒ DONâ€™T</h4><ul><li>Run yarn hot-updater fingerprint createÂ locally</li><li>Manually edit fingerprint.json</li><li>Build store apps without syncing latest fingerprints</li></ul><h4>What Actually FixedÂ It</h4><ul><li><strong>CI owns the fingerprint</strong> â†’ one source ofÂ truth</li><li><strong>Local builds sync from CI</strong> â†’ no more mismatches</li><li><strong>Automation over discipline</strong> â†’ scripts, notÂ memory</li><li><strong>Removed problematic plugin</strong> â†’ used Expo native config + scriptsÂ instead</li></ul><h4>The Result</h4><ul><li>âœ… JavaScript changes â†’ OTA deploys instantly</li><li>âœ… Native changes â†’ CI updates fingerprint correctly</li><li>âœ… Store builds â†’ Perfect match with OTAÂ updates</li><li>âœ… No more silent rejections</li></ul><p>And most importantlyâ€¦<br /><strong>OTA started behaving like OTAÂ again.</strong></p><h3>Final Thoughts</h3><p>If thereâ€™s one thing this whole experience taught me, itâ€™s that OTA updates arenâ€™t hard<em>; </em>theyâ€™re just unforgiving. Everything can look correct on the surface, while a tiny detail (like an absolute file path) quietly breaks the entireÂ system.</p><p>This wasnâ€™t a Hot Updater problem.<br />It wasnâ€™t an AWS problem.<br />It was a <em>consistency</em> problem.</p><p>Once we stopped letting every environment â€œdo its own thingâ€ and gave CI full ownership of the fingerprint, everything finally fell into place. OTA updates became fast again. Rollbacks worked. And JavaScript fixes went back to feeling like magic instead of aÂ gamble.</p><p>If youâ€™re ever implementing OTA updates using <strong>Hot Updater + AWS (S3, CloudFront, and Lambda@Edge)</strong> and things start behavingâ€¦ suspiciously, I hope this article saves you a few hours of log-staring and quiet frustration.</p><p>Save it. Bookmark it. Future-you might needÂ it.</p><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4e2c4f0fe318\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/implementing-ota-updates-using-hot-updater-aws-s3-cloudfront-and-lambda-edge-4e2c4f0fe318\">Implementing OTA Updates using Hot Updater + AWS S3, CloudFront and Lambda@edge</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "AWS Certified DevOps Engineerâ€Šâ€”â€ŠProfessional: Key Concepts and Techniques, the Easy Way",
      "url": "https://aws.plainenglish.io/aws-certified-devops-professional-the-easy-way-maybe-419167e63bcd?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:24:09",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "devops",
        "aws-certification",
        "aws"
      ],
      "content_length": 15655,
      "content_preview": "<h3>AWS Certified DevOps Engineerâ€Šâ€”â€ŠProfessional: Key Concepts and Techniques, the EasyÂ Way</h3><p>The path to the AWS Certified DevOps Engineerâ€Šâ€”â€ŠProfessional exam is daunting: the secret to success is understanding the underlying logic of how AWS wants you to build and protectÂ systems.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/0*txlig58p_qcfzYFE\" /></figure><p>This guide is designed to strip away the technical jargon and provide you with a clear, intuitive framework f",
      "content_full": "<h3>AWS Certified DevOps Engineerâ€Šâ€”â€ŠProfessional: Key Concepts and Techniques, the EasyÂ Way</h3><p>The path to the AWS Certified DevOps Engineerâ€Šâ€”â€ŠProfessional exam is daunting: the secret to success is understanding the underlying logic of how AWS wants you to build and protectÂ systems.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/0*txlig58p_qcfzYFE\" /></figure><p>This guide is designed to strip away the technical jargon and provide you with a clear, intuitive framework for mastering the most difficult exam topics. We focus on what you actually need to build and automate, ensuring you grasp the â€œwhyâ€ behind every architecture.</p><blockquote>We begin with the most significant hurdle in the current exam curriculum: Multi-Account Strategy.</blockquote><p>Modern architectures demand that we think in terms of account boundaries as hard security perimeters. You must move beyond simple scripts and learn to orchestrate automation across <strong>a landscape of isolated environments</strong>. We are looking at a world where your central tooling resides in a â€œShared Servicesâ€ or â€œOperationsâ€ account, reaching out to push code and infrastructure into production accounts it does notÂ own.</p><p>As we move through this path, we will break down Infrastructure as Code (IaC), sophisticated CI/CD pipelines, and the increasingly complex world of EKS architectures. We start here, at the foundation, by mastering the cross-account handshake and the mindset of decentralized, automated execution.</p><h3>The Cross-Account Handshake: A Tale of Two Companies</h3><p>To understand how AWS manages identity across accounts, letâ€™s look at a real-world scenario involving two organizations: <strong>Resource Corp (the owner of a high-tech vault)</strong> and <strong>Maintenance Pro (the experts hired to fixÂ it)</strong>.</p><h3>The Scenario</h3><p>Resource Corp has the assets, but Maintenance Pro has the expertise. In AWS terms, Resource Corp is our <strong>Resource Account</strong> (Production), and Maintenance Pro is our <strong>Operations Account</strong> (CI/CD or Tooling).</p><h3>1. The Badge: TheÂ Role</h3><p>In a single company, a Badge is what gives you access. If you wear the â€œVault Technicianâ€ badge, the doors open. In AWS, a <strong>Role</strong> is an identity that has permissions attached to it. It isnâ€™t a person; itâ€™s a hat that someone putsÂ on.</p><h3>2. The Trust Policy: Inviting the Outsiders</h3><p>Resource Corp has a vault but no internal staff to fix it. They create a special badge called the <strong>ResourceRole</strong>. Instead of giving this badge to their own employees, they write a â€œTrust Policyâ€ on the back: â€œI trust any employee from Maintenance Pro to wear thisÂ badge.â€</p><p>Technically, this is the <strong>AssumeRolePolicyDocument</strong>. It doesnâ€™t grant access to a specific person yet; it defines the <strong>Principal</strong> (the Maintenance Pro Account ID) that is allowed to use this identity.</p><h3>3. The Capability to Swap: The sts:AssumeRole</h3><p>Inside Maintenance Pro, there is a worker named Oper. Oper has their own company ID card. For Oper to actually do the work at the other company, their boss must give them a specific permission: â€œThe right to swapÂ badges.â€</p><p>In AWS, the Oper Account must grant the user (or service) the <strong>sts:AssumeRole</strong> permission targeting the Resource Corpâ€™s Role ARN. When Oper arrives at the vault, they show their original ID, and because of the trust agreement, they are allowed to put the ResourceRole badge over their own. For that session, Oper â€œbecomesâ€ a Resource Corp technician.</p><h3>4. The Dispatcher: The Power of iam:PassRole</h3><p>This is the most critical part for DevOps automation. Imagine Maintenance Pro has a Dispatcher who sets up the schedule. The Dispatcher cannot fix vaults and doesnâ€™t have the skills to wear the ResourceRole badge. However, the Dispatcher has the keys to the BadgeÂ Cabinet.</p><p>When the Dispatcher assigns a specific technician to a specific job, they are using <strong>iam:PassRole</strong>. They are saying: â€œI am authorized to pass this specific identity to this machine/worker.â€ The Dispatcher doesnâ€™t gain the power of the badge themselves; they simply have the authority to delegate it to the resource that will perform the task (like an EC2 instance or a Lambda function).</p><h3>No Devs: The Automated RobotÂ Arm</h3><p>In the real world of DevOps, we often move from a â€œhuman technicianâ€ to an <strong>Automated Robot Arm</strong>. This shift is not just for convenience; it is a fundamental architectural choice to enforce controlled procedures. By using services like CloudFormation or Terraform as a â€œRobot Arm,â€ you ensure that only carefully crafted, pre-approved blueprints are deployed to production. This eliminates the risk of â€œcreative experimentsâ€ from developers that lead to configuration drift and untraceable errors. This robot-led approach establishes the <strong>Unique Source of Truth</strong> for your infrastructure.</p><h3>1. The Robotâ€™s TrustÂ Policy</h3><p>Resource Corp programs the ResourceRole badge with a new rule: â€œI only trust the Robot Arm (cloudformation.amazonaws.com) to wear this badge.â€ Now, even the experts from Maintenance Pro canâ€™t wear itâ€Šâ€”â€Šonly the service itselfÂ can.</p><h3>2. The Dispatcherâ€™s Role in Automation</h3><p>As a DevOps Engineer (The Dispatcher), you donâ€™t have the permissions to touch the vault. You also canâ€™t wear the badge because you arenâ€™t a â€œRobot.â€ Your job is to hand the Robot a blueprint (The CloudFormation Template) and the ResourceRole badge.</p><p>This is where iam:PassRole becomes a security superpower. You are authorized to &quot;Pass&quot; that identity to the Robot. Without this specific permission, you are like a worker who isn't allowed to touch the badge cabinet; even with the right blueprint, you can't give the Robot the &quot;identity&quot; it needs toÂ work.</p><h3>3. ServiceÂ Roles</h3><p>Once you â€œPassâ€ the badge, the Robot (CloudFormation) â€œAssumesâ€ the role and executes the blueprint. This setup allows you to give a Developer the power to trigger deployments via the Robot, while strictly denying them the power to touch the production S3 buckets directly. Every change is tracked, repeatable, and safe from manual, untraceable humanÂ errors.</p><h3>The Shared Infrastructure Shortcut: AWSÂ RAM</h3><p>At this point, you might ask why we canâ€™t simply share a resource directly and put everything in one place without all these cross-account handshakes. The answer is that you actually can, using <strong>AWS Resource Access Manager (RAM)</strong>. RAM allows a resource owned by one account to appear in another account as if it were a local resource. This is incredibly powerful for centralized governance, but it has specific boundaries that every DevOps Professional must understand.</p><h3>When RAM is theÂ Winner</h3><p>RAM is the perfect tool for shared infrastructure components. For example, if you manage a large network, you can share VPC Subnets or a Transit Gateway from a central Network Account to all your application accounts. This means the developers in Account B can launch their EC2 instances directly into a subnet owned and managed by Account A. Other resources like Route 53 Resolver rules, Aurora DB Clusters, and Capacity Reservations also fall into this category. It simplifies the architecture because it removes the need for complex API calls across account borders for basic networking.</p><h3>The Limits of DirectÂ Sharing</h3><p>The catch is that RAM is not a universal solution. Many of the most common AWS services are not yet compatible with RAM. For instance, you cannot share S3 Buckets, IAM Roles, or DynamoDB Tables using RAM. These are considered â€œmanagedâ€ data or identity services. To manage access to these across accounts, you must rely on the â€œBadge Swappingâ€ logic we discussed earlier or use Resource-based policies.</p><h3>Choosing the RightÂ Path</h3><p>In the Professional exam, the rule of thumb is simple. If the question involves core infrastructure like VPCs or Gateways, look for RAM as the optimal solution for centralized control. If the question involves data storage like S3 or identity delegation, RAM is off the table. You must then use Resource-based policies or Cross-account Roles to bridge the gap. Understanding this distinction ensures that you build an architecture that is not only secure but also manageable atÂ scale.</p><h3>The Identity Trap: Why Not Just Use Resource-Based Policies?</h3><p>If an S3 bucket or an SQS queue allows you to attach a policy directly to the resource (a <strong>Resource-based policy</strong>) to grant cross-account access, why do we bother with the â€œBadge Swappingâ€ of IAM Roles for CI/CD andÂ IaC?</p><h3>The Scalability Nightmare</h3><p>Imagine your CI/CD â€œRobot Armâ€ needs to deploy to 100 different S3 buckets across 10 different accounts. If you use Resource-based policies, you have to go to every single one of those 100 buckets and manually update its policy to â€œtrustâ€ the Robot Armâ€™s account. This is decentralized and prone to error. With an IAM Role, you manage the â€œBadgeâ€ in one place. If the Robot Arm changes, you update one role, not 100 resources.</p><h3>The AuditÂ Gap</h3><p>When a Robot Arm uses a Resource-based policy, it is acting as itself (the identity from the Operations Account). When it uses a Cross-account Role, it â€œbecomesâ€ an identity inside the Target Account. In your logs (CloudTrail), this makes a massive difference. Using a Role means the activity is logged inside the account where the resources live, under the name of that specific role. It provides a clear, local audit trail of exactly what the automation did within that security perimeter.</p><h3>The Clean Principle</h3><p>For IaC (Infrastructure as Code), we want the deployment process to be as â€œcleanâ€ as possible. If the Robot Arm has to rely on Resource-based policies, it often runs into a chicken-and-egg problem: it needs permissions to create the resource, but it canâ€™t get those permissions until the resource (and its policy) already exists. By using a Cross-account Role, the Robot Arm is granted a â€œGeneral Power of Attorneyâ€ to act within the target account, allowing it to create, modify, and delete any resource defined in the blueprint without needing the resource to â€œinviteâ€ itÂ first.</p><h3>Technical Blueprint: Translating the Story toÂ Policy</h3><p>To implement this architecture, we need four distinct policy configurations. Here is how the analogy maps to the JSONÂ reality.</p><h3>1. The Trust Policy (The â€œTrustÂ Noteâ€)</h3><p>This is attached to the <strong>ResourceRole</strong>. It defines which external entity or service is allowed to use this badge. Notice the change from an Account ID to a Service Principal for automation.</p><pre>{<br />  &quot;Version&quot;: &quot;2012-10-17&quot;,<br />  &quot;Statement&quot;: [<br />    {<br />      &quot;Sid&quot;: &quot;TrustTheRobot&quot;,<br />      &quot;Effect&quot;: &quot;Allow&quot;,<br />      &quot;Principal&quot;: {<br />        &quot;Service&quot;: &quot;cloudformation.amazonaws.com&quot;<br />      },<br />      &quot;Action&quot;: &quot;sts:AssumeRole&quot;<br />    }<br />  ]<br />}</pre><h3>2. The Permission Policy (The â€œBadgeÂ Powersâ€)</h3><p>Also attached to the <strong>ResourceRole</strong>. This defines what the technician (or Robot) can actually do once they areÂ active.</p><pre>{<br />  &quot;Version&quot;: &quot;2012-10-17&quot;,<br />  &quot;Statement&quot;: [<br />    {<br />      &quot;Sid&quot;: &quot;AccessTheVault&quot;,<br />      &quot;Effect&quot;: &quot;Allow&quot;,<br />      &quot;Action&quot;: [<br />        &quot;s3:GetObject&quot;,<br />        &quot;s3:ListBucket&quot;<br />      ],<br />      &quot;Resource&quot;: [<br />        &quot;arn:aws:s3:::resource-corp-vault-bucket&quot;,<br />        &quot;arn:aws:s3:::resource-corp-vault-bucket/*&quot;<br />      ]<br />    }<br />  ]<br />}</pre><h3>3. The Outbound Permit (The â€œBadge SwappingÂ Permitâ€)</h3><p>This is attached to the <strong>OperRole</strong> (or User) in the Operations Account. It gives them the right to initiate theÂ process.</p><pre>{<br />  &quot;Version&quot;: &quot;2012-10-17&quot;,<br />  &quot;Statement&quot;: [<br />    {<br />      &quot;Sid&quot;: &quot;PermitToSwapBadges&quot;,<br />      &quot;Effect&quot;: &quot;Allow&quot;,<br />      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,<br />      &quot;Resource&quot;: &quot;arn:aws:iam::RESOURCE_ACCOUNT_ID:role/ResourceRole&quot;<br />    }<br />  ]<br />}</pre><h3>4. The Dispatcher Authority (iam:PassRole)</h3><p>This is the permission that allows the Dispatcher to hand the role to theÂ service.</p><pre>{<br />  &quot;Version&quot;: &quot;2012-10-17&quot;,<br />  &quot;Statement&quot;: [<br />    {<br />      &quot;Sid&quot;: &quot;KeymasterAuthority&quot;,<br />      &quot;Effect&quot;: &quot;Allow&quot;,<br />      &quot;Action&quot;: &quot;iam:PassRole&quot;,<br />      &quot;Resource&quot;: &quot;arn:aws:iam::RESOURCE_ACCOUNT_ID:role/ResourceRole&quot;,<br />      &quot;Condition&quot;: {<br />        &quot;StringEquals&quot;: {<br />            &quot;iam:PassedToService&quot;: &quot;cloudformation.amazonaws.com&quot;<br />        }<br />      }<br />    }<br />  ]<br />}</pre><h3>Summary: Is This the HardestÂ Part?</h3><p>Multi-account security in AWS is inherently confusing because it mirrors the complexity of real-world organizational boundaries and because AWS account architecture is isolated. However, by using these analogiesâ€Šâ€”â€ŠThe Technicians, The Robot Arms, and The Shared Shortcutsâ€Šâ€”â€Šyou can begin to see the â€œwhyâ€ behind theÂ â€œhow.â€</p><p>During the exam, you will need to choose the best tool for the job. Remember that Roles are for identity and controlled automation, RAM is for shared physical infrastructure, and Resource-based policies are for simple, one-off data access. Mastering these distinctions is the primary key to unlocking the Professional certification and building systems that are truly production-ready.</p><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=419167e63bcd\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/aws-certified-devops-professional-the-easy-way-maybe-419167e63bcd\">AWS Certified DevOps Engineerâ€Šâ€”â€ŠProfessional: Key Concepts and Techniques, the Easy Way</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "10 min read",
      "language": null
    },
    {
      "title_en": "Kafka Consumer Container Restarts in Kubernetes: A Production Case Study",
      "url": "https://aws.plainenglish.io/kafka-consumer-container-restarts-in-kubernetes-a-production-case-study-8703203e5fa7?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:20:14",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "backend-development",
        "kafka",
        "kubernetes",
        "java",
        "distributed-systems"
      ],
      "content_length": 600,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://aws.plainenglish.io/kafka-consumer-container-restarts-in-kubernetes-a-production-case-study-8703203e5fa7?source=rss----35e7a49c6df5---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*KmXul9e6LcieNy-Q24bydQ.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Background</p><p class=\"medium-feed-link\"><a href=\"https://aws.plainenglish.io/kafka-consumer-container-restarts-in-kubernetes-a-production-case-study-8",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://aws.plainenglish.io/kafka-consumer-container-restarts-in-kubernetes-a-production-case-study-8703203e5fa7?source=rss----35e7a49c6df5---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*KmXul9e6LcieNy-Q24bydQ.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Background</p><p class=\"medium-feed-link\"><a href=\"https://aws.plainenglish.io/kafka-consumer-container-restarts-in-kubernetes-a-production-case-study-8703203e5fa7?source=rss----35e7a49c6df5---4\">Continue reading on AWS in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Hands-On AWS EC2 Setup: Launch with Console and AWS CLI",
      "url": "https://aws.plainenglish.io/how-to-create-an-ec2-instance-using-aws-management-console-and-aws-cli-step-by-step-guide-7d07bde84f62?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:16:14",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "cloud-computing",
        "aws-cli",
        "devops",
        "aws",
        "aws-ec2"
      ],
      "content_length": 8656,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/418/1*v-CcIDQMGsNs62JVnQ23dg.png\" /></figure><p>Have you ever wondered how to launch your own virtual server onÂ AWS?</p><p>In this blog, youâ€™ll learn how to create an EC2 instance using both the <strong>AWS Management Console</strong> for a visual, step-by-step experience and the <strong>AWS CLI</strong> for fast, automated provisioning.</p><p><strong>Part 1: Create EC2 Instance using AWS Management Console:</strong></p><p><strong>STEP",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/418/1*v-CcIDQMGsNs62JVnQ23dg.png\" /></figure><p>Have you ever wondered how to launch your own virtual server onÂ AWS?</p><p>In this blog, youâ€™ll learn how to create an EC2 instance using both the <strong>AWS Management Console</strong> for a visual, step-by-step experience and the <strong>AWS CLI</strong> for fast, automated provisioning.</p><p><strong>Part 1: Create EC2 Instance using AWS Management Console:</strong></p><p><strong>STEP 1</strong>: Go to AWS Create Account if not exists, then login with your mail id and password.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*s7_2yYGrjfzwjySyYONeOQ.png\" /><figcaption>Login to AWS with email andÂ password</figcaption></figure><p><strong>STEP 2</strong>: Search EC2 in AWS services and click onÂ EC2</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_pUjOVYiG3JG_xPqRV1rSQ.png\" /><figcaption>Search EC2</figcaption></figure><p><strong>STEP 3</strong>: Click LaunchÂ Instance</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*n63mJgD3qGvrftp2jco9eg.png\" /><figcaption>Click on LaunchÂ Instance</figcaption></figure><p><strong>STEP 4:</strong> Name YourÂ Instance</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*n81KAhtqOSb5VitR_AV3OQ.png\" /><figcaption>Name yourÂ Instance</figcaption></figure><p><strong>STEP 5</strong>: Select Image for Instance here Iâ€™m selecting UbuntuÂ Image</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vYLJ_JHSJRf_riXxTup0Vw.png\" /><figcaption>Select Image</figcaption></figure><p><strong>STEP 6</strong>: Select Instance Type and go to nextÂ step</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*reDSK7hlrGlwNKDqKUYvFw.png\" /><figcaption>Select Instance Type, Here I used Free tierÂ eligible</figcaption></figure><p><strong>STEP 7</strong>: Create KeyÂ Pair</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ImeKriuu_04u64BJwWyKyA.png\" /><figcaption>Enter Key-pair Name and click on Create keyÂ pair</figcaption></figure><p><strong>STEP 8:</strong> Configure Network &amp; SecurityÂ Group</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*aS2k27l_Pv5L5xKuQ4GyDw.png\" /><figcaption>Go with same Network Settings for Now, and Select Create security group and click on allow SSHÂ traffic</figcaption></figure><p><strong>STEP 9:</strong> Configure Storage</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*49IEcRMvkBppTJB7yza6mg.png\" /><figcaption>Add Volume</figcaption></figure><p><strong>STEP 10:</strong> LaunchÂ Instance</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/895/1*PtSDpyQQHgKbVhDdXklrqw.png\" /><figcaption>You can Define the Number of Instances, and Click on LaunchÂ Instance</figcaption></figure><p><strong>Now our instance is successfully launched and Itâ€™sÂ Running</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*P8yBgixC7gTaqKIN8moxtA.png\" /><figcaption>Running Instance</figcaption></figure><p><strong>Part 2: Create EC2 Instance using AWSÂ CLI:</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/962/1*FTz22aKYyzfW3zVPPBdZdQ.png\" /></figure><pre>Prerequisites:<br />â†’Install AWS CLI (Windows/Linux/macOS)<br />â†’Access Key ID &amp; Secret Access Key</pre><p><strong>STEP 1:</strong> Search IAM in AWS services and click onÂ IAM</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0W1A7SkqQI1I4EikFZ-7XQ.png\" /><figcaption>Click on <strong>IAM</strong>Â Service</figcaption></figure><p><strong>STEP 2:</strong> Go toÂ Users</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YqX-R58VMgcG0sxDKiPRDg.png\" /><figcaption>Click <strong>Users</strong></figcaption></figure><p><strong>STEP 3: </strong>Click on CreateÂ User</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KMHRLVThL8hnzqZMoE4YRg.png\" /><figcaption>Click CreateÂ User</figcaption></figure><p>STEP 4: Enter UserÂ Details</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yZ3oPkVDcvXH2rZh9--WhQ.png\" /><figcaption>Enter Username</figcaption></figure><p>STEP 5: Set Permissions</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4HRRwtJTKVx_oTVlnw7vag.png\" /><figcaption>Select Permissions Options, Click onÂ Next</figcaption></figure><p>STEP 6: Review &amp; CreateÂ User</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_P2bViN68Nu1T24n_DxmPw.png\" /><figcaption>Add tag if required, Click on CreateÂ User</figcaption></figure><p>STEP 7: IAM User Created Successfully</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CLeA4C9sLIlSo7IZvRGE_A.png\" /><figcaption>User Created</figcaption></figure><blockquote><em>IAM User created successfully</em></blockquote><p>STEP 8: Create AccessÂ Key</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZS22KFRSPlK0LiL-fpn2Ag.png\" /><figcaption>Click on Create accessÂ key</figcaption></figure><p>STEP 9: Choose Access Key UseÂ Case</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Oh-noZLGecs0XjVqM7p_kw.png\" /><figcaption>Select Command Line Interface (CLI), Click on Confirmation and Click onÂ Next</figcaption></figure><p>STEP 10: Set description tagâ€Šâ€”â€Šoptional</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QQJUiHawrvauGeI__EPnsQ.png\" /><figcaption>Add if required, Click on Create accessÂ key</figcaption></figure><p>STEP 11: Download Credentials</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3B4hGJm8IPYtYKg0oDU8yw.png\" /><figcaption>click on Download.csv file, and click onÂ Done</figcaption></figure><blockquote><em>Access key ID and Secret Access key Successfully Created</em></blockquote><p>â†’Open Command PromptÂ (CMD)</p><p>â†’Verify AWS CLI Installation &amp; Configure AWSÂ CLI</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-P8jQvMs84jQHrUBtJk15w.png\" /><figcaption>Verify and Configure</figcaption></figure><p>â†’EC2 Instance Creation using SingleÂ Command</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lk2XmBladkejJqterSGMUg.png\" /><figcaption>Showâ€™s EC2 is created usingÂ cmd</figcaption></figure><blockquote><em>Successfully created EC2 usingÂ CMD:</em></blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HJaNTy3_PY8OaONUFAoKVg.png\" /><figcaption>Created EC2</figcaption></figure><blockquote><strong><em>Thank you for readingÂ :)</em></strong></blockquote><blockquote><em>If you found this helpful, please </em><strong><em>donâ€™t forget</em></strong><em> to hit the </em><strong><em>Follow</em></strong><em> and </em><strong><em>Clap </em></strong><em>buttons to help me write more articles like this.<br /></em><strong><em>Thank YouÂ ğŸ–¤</em></strong></blockquote><blockquote><strong><em>You can contact me on: </em></strong><a href=\"http://www.linkedin.com/in/karthikspatil21\"><em>LinkedIn</em></a><em>Â </em><a href=\"http://karthikspatil2004@gmail.com/\"><em>emailme</em></a></blockquote><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7d07bde84f62\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/how-to-create-an-ec2-instance-using-aws-management-console-and-aws-cli-step-by-step-guide-7d07bde84f62\">Hands-On AWS EC2 Setup: Launch with Console and AWS CLI</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Inside the Amazon Nova Forge",
      "url": "https://aws.plainenglish.io/inside-the-amazon-nova-forge-10f3702a95a8?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:14:03",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "aws",
        "amazon-bedrock",
        "artificial-intelligence",
        "llm",
        "amazon-nova"
      ],
      "content_length": 8878,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*b5v0P3tbeDmuXNY_u9-Veg.png\" /></figure><p><strong>Amazon Nova Forge</strong> is a development environment within <strong>Amazon SageMaker AI</strong> dedicated to building â€œNovellasâ€â€Šâ€”â€Šprivate, custom versions of Amazonâ€™s Nova frontierÂ models.</p><p>Unlike typical AI services that only allow you to use a model or fine-tune its final layer, Nova Forge introduces a concept called <strong>Open Training</strong>. This gives you acces",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*b5v0P3tbeDmuXNY_u9-Veg.png\" /></figure><p><strong>Amazon Nova Forge</strong> is a development environment within <strong>Amazon SageMaker AI</strong> dedicated to building â€œNovellasâ€â€Šâ€”â€Šprivate, custom versions of Amazonâ€™s Nova frontierÂ models.</p><p>Unlike typical AI services that only allow you to use a model or fine-tune its final layer, Nova Forge introduces a concept called <strong>Open Training</strong>. This gives you access to the model at various â€œlife stagesâ€ (checkpoints), allowing you to bake your companyâ€™s proprietary knowledge directly into the modelâ€™s core reasoning capabilities.</p><p>This blog post is an introduction to Amazon Nova Forge and what makes it unique in the trainingÂ process.</p><h3>What Makes it Different?</h3><p><a href=\"https://aws.amazon.com/what-is/prompt-engineering/\">Prompt engineering</a> and <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\">RAG</a> provide external context but fail to change a modelâ€™s core intelligence. Standard fine-tuning also falls short because it happens too late in the lifecycle, attempting to steer a â€œfinishedâ€ model that is already set in its ways. Nova Forge solves this by moving customization earlier into the training process, embedding specialized knowledge where it actuallyÂ sticks.</p><p>Nova Forge occupies a unique middle ground between Managed APIs (Bedrock) and building fromÂ scratch.</p><ul><li><strong>Amazon Bedrock:</strong> Bedrock is for <strong>consuming</strong> models. You can fine-tune them, but you are working on a â€œblack boxâ€ model. Nova Forge is for <strong>building</strong> the model itself using deeper training techniques.</li><li><strong>Azure AI / Google Vertex AI:</strong> While Azure and GCP offer fine-tuning, they generally donâ€™t provide access to intermediate training checkpoints of their frontier models. Nova Forge allows for <strong>Data Blending</strong>, where you mix your data with Amazonâ€™s original training data to prevent the model from â€œforgettingâ€ how to speak orÂ reason.</li></ul><h3>Terminology</h3><ul><li><strong>Novella:</strong> The resulting custom model you create. Itâ€™s a â€œprivate editionâ€ ofÂ Nova.</li><li><strong>Checkpoints:</strong> Saved â€œstatesâ€ of the model during its initial training (pre-training, mid-training, post-training).</li><li><strong>Data Blending:</strong> The process of mixing your proprietary data with Nova-curated datasets so the model stays smart while learning your specific business.</li><li><strong>Reinforcement Fine-Tuning (RFT):</strong> Using â€œreward functionsâ€ (logic-based feedback) to teach the model how to perform complex, multi-step tasks correctly.</li><li><strong>Catastrophic Forgetting:</strong> A common AI failure where a model learns new information but loses its original abilities. Nova Forge is designed specifically to preventÂ this.</li></ul><h3>The Workflow: From Training to Production</h3><p>The process bridges the gap between the â€œlabâ€ (SageMaker) and the â€œappâ€ (Bedrock).</p><ol><li><strong>Selection:</strong> You choose a Nova base model and a specific checkpoint (e.g., a â€œMid-trainingâ€ checkpoint) in <strong>Amazon SageMaker Studio</strong>.</li><li><strong>Training (SageMaker AI):</strong> You use <strong>SageMaker Recipes</strong>â€Šâ€”â€Špre-configured training scriptsâ€Šâ€”â€Što blend your data from S3 with Novaâ€™s datasets. The heavy lifting (compute) happens on SageMakerâ€™s managed infrastructure.</li><li><strong>Refinement:</strong> Optionally, you run RFT in SageMaker to align the model with specific business outcomes or safety guardrails.</li><li><strong>Deployment (Bedrock):</strong> Once the â€œNovellaâ€ is ready, you <strong>import it into Amazon Bedrock</strong> as a privateÂ model.</li><li><strong>Production:</strong> Your applications call the custom model via the standard Bedrock API, benefitting from Bedrockâ€™s serverless scaling and security.</li></ol><p>Below is a sample training workflow:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/750/1*jF27jNEhfxUfAT7tl3nGEg.png\" /></figure><h3>Data Privacy and Protection</h3><p>The security model is the most criticalÂ part:</p><ul><li><strong>Sovereignty:</strong> Your data stays in your S3 buckets and within your VPC boundaries.</li><li><strong>No Leakage:</strong> AWS explicitly states that <strong>customer data is not used to train the base Amazon Nova models</strong>. Your â€œNovellaâ€ is a private resource visible only to your AWSÂ account.</li><li><strong>Encryption:</strong> Data is encrypted at rest via <strong>KMS</strong> (AWS-managed or Customer-managed keys) and in transit via TLSÂ 1.2+.</li><li><strong>Governance:</strong> Access is controlled via standard <strong>IAM policies</strong>, and all training activity is logged in <strong>CloudTrail</strong>.</li></ul><h3>Pricing Model</h3><p>Nova Forge carries a distinct cost structure that reflects its â€œfrontierâ€ status:</p><ul><li><strong>Subscription Fee:</strong> Access to the Forge environment starts at approximately <strong>$100,000 perÂ year</strong>.</li><li><strong>Usage Costs:</strong> On top of the subscription, you pay for the <strong>SageMaker compute</strong> (GPUs) used during the trainingÂ phase.</li><li><strong>Comparison:</strong> C<strong>heaper than Training from Scratch:</strong> Building a frontier model from zero costs millions in compute and months of R&amp;D. Nova Forge provides the â€œshortcutsâ€ to get the same result for a fraction ofÂ that.</li></ul><p><strong>More Expensive than Basic Fine-Tuning:</strong> Standard fine-tuning on Bedrock is much cheaper (often just a few dollars per hour), but it cannot achieve the deep â€œdomain-nativeâ€ intelligence that Nova Forge provides.</p><h3>Summary</h3><p>Amazon Nova Forge marks a shift from generic AI to <strong>native intelligence</strong>, where models donâ€™t just reference your dataâ€Šâ€”â€Šthey are built from it. By using â€œOpen Training,â€ you can bake specialized knowledge into the modelâ€™s core at the pre-training or mid-training stages. This results in a private <strong>Novella</strong> that understands your specific industry as naturally as its base language.</p><p>Organizations managing high-value proprietary data should consider moving beyond treating that information as an external reference. If your workflows involve specialized terminology or regulated processes that standard LLMs struggle to master, shifting customization earlier in the training lifecycle is often more effective than basic fine-tuning.</p><p>Disclaimer: AI tools were used to research and edit this article. Graphics are created usingÂ AI.</p><h4>Additional references</h4><ul><li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nova-forge.html\">Amazon NovaÂ Forge</a></li><li><a href=\"https://aws.amazon.com/blogs/aws/introducing-amazon-nova-forge-build-your-own-frontier-models-using-nova/\">Introducing Amazon Nova Forge: Build your own frontier models usingÂ Nova</a></li></ul><h3>About theÂ Author</h3><p>Eyal Estrin is a cloud and information security architect and <a href=\"https://builder.aws.com/community/@eyalestrin\">AWS Community Builder</a>, with more than 25 years in the industry. He is the author of <a href=\"https://amzn.to/42Xai9A\">Cloud Security Handbook</a> and <a href=\"https://amzn.to/3Sggbtv\">Security for Cloud Native Applications</a>.</p><p>The views expressed are hisÂ own.</p><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=10f3702a95a8\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/inside-the-amazon-nova-forge-10f3702a95a8\">Inside the Amazon Nova Forge</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "How We Sleep Better at Night: A Simple DR Setup That Actually Works",
      "url": "https://aws.plainenglish.io/how-we-sleep-better-at-night-a-simple-dr-setup-that-actually-works-9792e4419975?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T11:09:18",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "kubernetes",
        "disaster-recovery",
        "devops",
        "aws",
        "kafka"
      ],
      "content_length": 714,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://aws.plainenglish.io/how-we-sleep-better-at-night-a-simple-dr-setup-that-actually-works-9792e4419975?source=rss----35e7a49c6df5---4\"><img src=\"https://cdn-images-1.medium.com/max/1400/1*0CGpAHtjykTWYJhiThiYLA.gif\" width=\"1400\" /></a></p><p class=\"medium-feed-snippet\">You know that feeling when your production environment goes down and everyone&#x2019;s staring at you? Yeah, we got tired of that too.</p><p class=\"medium-fe",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://aws.plainenglish.io/how-we-sleep-better-at-night-a-simple-dr-setup-that-actually-works-9792e4419975?source=rss----35e7a49c6df5---4\"><img src=\"https://cdn-images-1.medium.com/max/1400/1*0CGpAHtjykTWYJhiThiYLA.gif\" width=\"1400\" /></a></p><p class=\"medium-feed-snippet\">You know that feeling when your production environment goes down and everyone&#x2019;s staring at you? Yeah, we got tired of that too.</p><p class=\"medium-feed-link\"><a href=\"https://aws.plainenglish.io/how-we-sleep-better-at-night-a-simple-dr-setup-that-actually-works-9792e4419975?source=rss----35e7a49c6df5---4\">Continue reading on AWS in Plain English Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Thousands of DevOps Engineers Cut Deployment Time by 50% â€” Hereâ€™s How",
      "url": "https://aws.plainenglish.io/thousands-of-devops-engineers-cut-deployment-time-by-50-heres-how-f21a0c03e243?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T10:06:22",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "technology",
        "programming",
        "software-development",
        "software-engineering",
        "coding"
      ],
      "content_length": 3909,
      "content_preview": "<p>If youâ€™re a DevOps engineer, you know the struggle: slow deployments, server downtime, and manual setups steal hours from yourÂ week.</p><p>Youâ€™re not alone: <strong>8,000+ DevOps teams</strong> report losing <strong>10+ hours weekly</strong> on deployment issues. Companies like <strong>Spotify, Shopify, Udemy, and Atlassian</strong> have faced the same problemâ€Šâ€”â€Šand they found a way to fixÂ it.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kWdo0sUpndR-e9TpovzbGg.png\" /",
      "content_full": "<p>If youâ€™re a DevOps engineer, you know the struggle: slow deployments, server downtime, and manual setups steal hours from yourÂ week.</p><p>Youâ€™re not alone: <strong>8,000+ DevOps teams</strong> report losing <strong>10+ hours weekly</strong> on deployment issues. Companies like <strong>Spotify, Shopify, Udemy, and Atlassian</strong> have faced the same problemâ€Šâ€”â€Šand they found a way to fixÂ it.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kWdo0sUpndR-e9TpovzbGg.png\" /><figcaption><a href=\"https://www.cloudways.com/en/?id=2079457\">Check itÂ out</a></figcaption></figure><p>Hereâ€™s what top teams do differently:</p><ul><li><strong>Automated Server Setup:</strong> Launch servers in minutes, not hours. <a href=\"https://www.cloudways.com/en/?id=2079457\">Check itÂ out</a></li><li><strong>Proactive Monitoring:</strong> Detect bottlenecks before they hit production. <a href=\"https://www.cloudways.com/en/?id=2079457\">SeeÂ how</a></li><li><strong>Hybrid Approach:</strong> Fast setup + ongoing optimization = fewer errors, smoother deployments. <a href=\"https://www.cloudways.com/en/?id=2079457\">ExploreÂ here</a></li></ul><p>Teams using these strategies report:</p><ul><li>âœ… <strong>50% faster deployments</strong></li><li>âœ… <strong>30% fewer production incidents</strong></li><li>âœ… <strong>More time for innovation</strong> instead of firefighting</li></ul><p>Thousands of engineers are already using this approach. Imagine <strong>getting 10+ hours back every week</strong> while avoiding deployment nightmares.</p><p>Curious to see how it works for you ğŸ‘‰ <a href=\"https://www.cloudways.com/en/?id=2079457\"><strong>Discover the DevOpsÂ Shortcut</strong></a></p><h3>Thank you for being a part of the community</h3><p><em>Before youÂ go:</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*d9QTaaaxboQP_gKSLedW_w.png\" /></figure><p>ğŸ‘‰ Be sure to <strong>clap</strong> and <strong>follow</strong> the writerÂ ï¸ğŸ‘<strong>ï¸ï¸</strong></p><p>ğŸ‘‰ Follow us: <a href=\"https://www.linkedin.com/in/code-to-deploy-3784391b9/\"><strong>Linkedin</strong></a>| <a href=\"https://medium.com/codetodeploy\"><strong>Medium</strong></a></p><p>ğŸ‘‰ CodeToDeploy Tech Community is live on Discordâ€Šâ€”â€Š<a href=\"https://discord.gg/ZpwhHq6D\"><strong>JoinÂ now!</strong></a></p><p><strong>Note:</strong> This Post may contain affiliate links.</p><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f21a0c03e243\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/thousands-of-devops-engineers-cut-deployment-time-by-50-heres-how-f21a0c03e243\">Thousands of DevOps Engineers Cut Deployment Time by 50% â€” Hereâ€™s How</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Moving from Monolith to Tiered Architecture: Deploying RDS in a Private Subnet with Terraform",
      "url": "https://aws.plainenglish.io/moving-from-monolith-to-tiered-architecture-deploying-rds-in-a-private-subnet-with-terraform-6ffcd4acb1e6?source=rss----35e7a49c6df5---4",
      "source": "medium",
      "published_at": "2026-02-10T09:57:59",
      "external_id": null,
      "tags": [
        "aws-in-plain-english",
        "aws-rds",
        "aws",
        "postgresql",
        "devops",
        "terraform"
      ],
      "content_length": 8133,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*s9EVYju65K92JVoQqlbrow.jpeg\" /><figcaption>Image fromÂ Gemini</figcaption></figure><p>In my previous article, we built a fully automated CI/CD pipeline that deployed a Flask application to an AWS EC2 instance. It worked, but it had a major architectural flaw: The database was on the WebÂ Server.</p><p>In a production environment, this is a â€œMonolithicâ€ anti-pattern. If the web server crashes, the database goes down with it. If the",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*s9EVYju65K92JVoQqlbrow.jpeg\" /><figcaption>Image fromÂ Gemini</figcaption></figure><p>In my previous article, we built a fully automated CI/CD pipeline that deployed a Flask application to an AWS EC2 instance. It worked, but it had a major architectural flaw: The database was on the WebÂ Server.</p><p>In a production environment, this is a â€œMonolithicâ€ anti-pattern. If the web server crashes, the database goes down with it. If the server is compromised, your data isÂ exposed.</p><p>Today, we are fixing that. We are moving from a single-tier setup to a <strong>Tiered Architecture</strong> by deploying an <strong>Amazon RDS PostgreSQL </strong>database in a <strong>Private Subnet</strong>, completely isolated from the public internet.</p><h3>The Architecture Change</h3><p>We are evolving our infrastructure:</p><ul><li><strong>Before:</strong> EC2 (Public Subnet) holding the App and the SQLite/Local PostgresÂ DB.</li><li><strong>After:</strong></li></ul><ol><li><strong>Public Subnet: </strong>EC2 Instance (Flask App) â†’ Accessible viaÂ Internet</li><li><strong>Private Subnet: </strong>RDS Instance (PostgreSQL) â†’ Accessible only by the EC2 instance.</li></ol><h3>Step 1: The â€œMulti-AZâ€ Requirement.</h3><p>One of the first hurdles I faced was understanding Amazon RDS networking requirements. Even for a single database instance, AWS requires you to have a<strong> DB Subnet Group</strong> that spans at least <strong>two Availability Zones (AZs) </strong>for high availability.</p><p>Since my original VPC had only one private subnet, I had to add a secondÂ one.</p><p><strong>Updating </strong>main.tf</p><p>I added a new resource for the second private subnet in a different availability zone (e.g., us-east-1b).</p><pre># Create another Private Subnet (Required for RDS Multi-AZ)<br />resource &quot;aws_subnet&quot; &quot;private_subnet_2&quot; {<br />  vpc_id            = aws_vpc.main.id<br />  cidr_block        = var.private_subnet_2_cidr<br />  availability_zone = data.aws_availability_zones.available.names[1]<br /><br />  tags = {<br />    Name = &quot;${var.project_name}-private-subnet-2&quot;<br />  }<br />}</pre><p>I also needed to ensure this new subnet had internet access (via the NAT Gateway) so the database could grab updates if needed. I reused my existing RouteÂ Table:</p><pre># Associate the NEW Second Private Subnet with the EXISTING Route Table<br />resource &quot;aws_route_table_association&quot; &quot;private_2&quot; {<br />  subnet_id      = aws_subnet.private_subnet_2.id<br />  route_table_id = aws_route_table.private.id<br />}</pre><p>We had to do this again for the second privateÂ subnet.</p><h3>Step 2: The Database SubnetÂ Group</h3><p>With two private subnets ready, I grouped them together. This tells RDS, â€œ<em>You are allowed to live in either of these two secureÂ zones.</em>â€</p><pre>resource &quot;aws_db_subnet_group&quot; &quot;rds_group&quot; {<br />  name       = &quot;${var.project_name}-rds-group&quot;<br />  subnet_ids = [<br />    aws_subnet.private.id,<br />    aws_subnet.private_subnet_2.id<br />  ]<br /><br />  tags = {<br />    Name = &quot;${var.project_name}-rds-group&quot;<br />  }<br />}</pre><h3>Step 3: Security Groups ( The FirewallÂ )</h3><p>This is the most critical security step. We donâ€™t want the database open to the world (0.0.0.0/0). We only want it to accept traffic from our <strong>WebÂ Server.</strong></p><p>I created a Security Group that allows traffic on port 5432 (PostgreSQL) <strong>only </strong>if it comes from the public_sg (my EC2 instanceâ€™s securityÂ group).</p><pre>resource &quot;aws_security_group&quot; &quot;rds_sg&quot; {<br />  name        = &quot;${var.project_name}-rds-sg&quot;<br />  description = &quot;Allow inbound traffic from Web App&quot;<br />  vpc_id      = aws_vpc.main.id<br /><br />  ingress {<br />    description     = &quot;Postgres from Web Server&quot;<br />    from_port       = 5432<br />    to_port         = 5432<br />    protocol        = &quot;tcp&quot;<br />    # âš ï¸ Crucial: Link to the Compute/Public Security Group ID<br />    security_groups = [aws_security_group.public_sg.id]<br />  }<br />  <br />  # ... (egress rules allow all outbound traffic)<br />}</pre><h3>Step 5: Provisioning theÂ Database</h3><p>Finally, the aws_db_instance resource. I chose db.t3.micro ro stay within the AWS FreeÂ Tier.</p><pre>resource &quot;aws_db_instance&quot; &quot;postgres&quot; {<br />  identifier             = &quot;${var.project_name}-db&quot;<br />  allocated_storage      = 20<br />  engine                 = &quot;postgres&quot;<br />  engine_version         = &quot;14&quot;<br />  instance_class         = &quot;db.t3.micro&quot;<br />  db_name                = &quot;flask_db&quot;<br />  username               = var.db_username<br />  password               = var.db_password<br />  <br />  # Connect the networking and security we just built<br />  db_subnet_group_name   = aws_db_subnet_group.rds_group.name<br />  vpc_security_group_ids = [aws_security_group.rds_sg.id]<br />  publicly_accessible    = false<br />  skip_final_snapshot    = true <br />}</pre><h3>The â€œGotchaâ€: CICD &amp; SecurityÂ Scanning</h3><p>I used <strong>GitHub Actions</strong> and <strong>Trivy</strong> to scan my Infrastructure as Code for security vulnerabilities. When I pushed this code, my pipeline failed immediately.</p><pre>AVD-AWS-0080 (HIGH): Instance does not have storage encryption enabled.</pre><p>They correctly identified that my database wasnâ€™t encrypted at rest. In a real production environment with sensitive customer data, <strong>you must enable encryption</strong>.</p><p>However, since this is a learning project running on Free Tier resources, I made a conscious decision to accept this risk. I added aÂ .trivyignore file to my repository to tell the scanner to pass theÂ build:</p><pre># .trivyignore<br /># Accepting unencrypted DB for learning/tutorial project<br />AVD-AWS-0080</pre><p>This is a great example of â€œDevSecOpsâ€ in action. Identifying a risk and making a documented decision about it, rather than just ignoring the errorÂ log.</p><h3>Conclusion</h3><p>By moving the database to RDS, we have achieved:</p><ol><li><strong>Isolation:</strong> The DB is in a private subnet, unreachable from the public internet.</li><li><strong>Scalability:</strong> The web server and database can scale independently.</li><li><strong>Persistence:</strong> If I terminate the EC2 instance, my data remains safe inÂ RDS.</li></ol><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href=\"https://linkedin.com/in/sunilsandhu\"><strong>LinkedIn</strong></a><strong>, </strong><a href=\"https://tiktok.com/@messyfounder\"><strong>TikTok</strong></a>, <a href=\"https://instagram.com/sunilsandhu\"><strong>Instagram</strong></a>. You can also subscribe to our <a href=\"https://newsletter.plainenglish.io/\"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6ffcd4acb1e6\" width=\"1\" /><hr /><p><a href=\"https://aws.plainenglish.io/moving-from-monolith-to-tiered-architecture-deploying-rds-in-a-private-subnet-with-terraform-6ffcd4acb1e6\">Moving from Monolith to Tiered Architecture: Deploying RDS in a Private Subnet with Terraform</a> was originally published in <a href=\"https://aws.plainenglish.io\">AWS in Plain English</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Apache Spark WTF???â€Šâ€”â€Š Donâ€™t Be K-Mean",
      "url": "https://blog.devgenius.io/apache-spark-wtf-dont-be-k-mean-9b9eeae3dcda?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:53:36",
      "external_id": null,
      "tags": [
        "dev-genius",
        "spark",
        "k-means",
        "performance"
      ],
      "content_length": 30870,
      "content_preview": "<h3>Apache Spark WTF???â€Šâ€”â€ŠğŸ˜  Donâ€™t Be K-MeanÂ ğŸ˜¡</h3><p><em>â€œDivide et imperaâ€â€Šâ€”â€Šthis phrase comes from the ancient Roman Empire and, not surprisingly, itâ€™s still relevant today. Although nowadays, the best strategy isnâ€™t always to divideâ€¦ itâ€™s to cluster. And for that, the K-Means algorithm, which Spark hands you out-of-the-box, sits on the throne perfectly.</em></p><p><em>In todayâ€™s article, weâ€™re not just going to cover what K-Means is. Weâ€™re going under the hood: how Spark implements it interna",
      "content_full": "<h3>Apache Spark WTF???â€Šâ€”â€ŠğŸ˜  Donâ€™t Be K-MeanÂ ğŸ˜¡</h3><p><em>â€œDivide et imperaâ€â€Šâ€”â€Šthis phrase comes from the ancient Roman Empire and, not surprisingly, itâ€™s still relevant today. Although nowadays, the best strategy isnâ€™t always to divideâ€¦ itâ€™s to cluster. And for that, the K-Means algorithm, which Spark hands you out-of-the-box, sits on the throne perfectly.</em></p><p><em>In todayâ€™s article, weâ€™re not just going to cover what K-Means is. Weâ€™re going under the hood: how Spark implements it internally, and maybeâ€¦ just maybe, weâ€™ll uncover little tricks to squeeze the best performance out ofÂ it.</em></p><p><em>Today, we get to be kings for a day!</em><br /><em>Grease back that Elvis pompadour and rock â€™nâ€™ roll through clusters, centroids, and math dressed up for funâ€¦ because math </em><strong><em>can</em></strong><em> be fun, canâ€™tÂ it?</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*fp124_FeJH5dvGIgLarO7A.png\" /></figure><blockquote>â€œYou know it can be found<br />K-Means spinning cluster-bound<br />If memoryâ€™s running out<br />At least give me the error nowâ€<br />(adapted from <a href=\"https://www.youtube.com/watch?v=B2YwGBVSfFQ\"><strong>â€œDonâ€™t be cruelâ€ by ElvisÂ Presley</strong></a>)</blockquote><h3>ğŸ¸ K-Means in Spark: Rock â€™nâ€™ Roll with BigÂ Data</h3><p>Alright, baby, imagine your dataset as a stadium full of wild rockersâ€Šâ€”â€Šmillions of them, all trying to jam together. You canâ€™t just tell them, <em>â€œHey, go find your spotâ€</em>â€Šâ€”â€Šthe crowd is too massive. Enter Spark, your tour manager, coordinating every move so the show doesnâ€™t blow up. And the headliner? K-Means, the King of Clustering, making sure every rocker finds theirÂ rhythm.</p><h4>Two Ways to Play theÂ Gig</h4><p>Spark gives us two instruments to rockÂ K-Means:</p><ul><li><strong>MLlib (RDD-based KMeans)</strong>â€Šâ€”â€Šthe classic Stratocaster. You can see all the strings, tweak every note, and feel the rawÂ power.</li><li><strong>ML (DataFrame-based KMeans)</strong>â€Šâ€”â€Šthe modern synth. Cleaner, pipeline-ready, integrates smoothly with other tracks, and makes your workflow sound polished.</li></ul><h4>Parallelism: Keep the BandÂ Tight</h4><p>In a tiny venue, K-Means just loops through the crowd. But here, weâ€™ve got a stadium-sized jam session. Spark divides our rockers into partitions, each handled by a node (executor). EachÂ node:</p><ol><li>Assigns its local fans to the nearest centroid (their favorite stageÂ spot).</li><li>Counts votes and calculates averages.</li><li>Sends results back to the driver (the tour manager).</li></ol><p>The driver then moves the centroids slightly to keep the band in harmony. Broadcast the updates, repeatÂ â€¦ and before you know it, the stadium is grooving inÂ sync.</p><h4>Sparse vs. Dense Vectors: Understanding theÂ Data</h4><p>Before diving into K-Means, itâ€™s important to understand what Spark MLlib means by vectors, features, and dataÂ points.</p><p><strong>1. Data Points. </strong>Each row in our dataset is a data point (or example). In K-Means, a data point represents an observation we want to cluster. <br /><strong>Example: </strong>in a text dataset, each document could be a dataÂ point.</p><p><strong>2. Features. </strong>Each data point has one or more features, which are measurable attributes. Features are numeric and describe the data in a way the algorithm can work with. <br /><strong>Example: </strong>in text data features might be word counts or some scores, image data features might be pixel intensities. Features together form a vector, representing the data point in a multidimensional space.</p><p><strong>3. Vectors. </strong>A vector is just an ordered list of numbers corresponding to the features of a data point. Spark MLlib uses vectors to represent all inputs for algorithms likeÂ K-Means.</p><pre>from pyspark.ml.linalg import Vectors<br /><br /># Dense vector: stores all features explicitly<br />dense_vec = Vectors.dense([0.0, 1.0, 0.0, 3.5])<br /><br /># Sparse vector: stores only non-zero features<br />sparse_vec = Vectors.sparse(4, {1: 1.0, 3: 3.5})</pre><ul><li><strong>Dense vector:</strong> stores every value, including zeros.</li><li><strong>Sparse vector:</strong> stores only non-zero values and their positions (indices).</li></ul><p><strong>4. Why Sparse vs. Dense Matters for K-Means. </strong>MLlib always stores centroids as dense vectors internally, even if our input is sparse. If our vectors have thousands of features, each centroid uses memory = num_features Ã— 8 bytes. With many clusters (k) and high-dimensional data, memory can spike â†’ OOM errorsÂ ğŸ˜±.</p><h4>Initialization: Setting the FirstÂ Stage</h4><p>Where we place the initial centroids has a big impact on K-Means performance and convergence. Spark offers two mainÂ options:</p><p><strong>1. Random Initialization. </strong>Centroids are placed randomly in the featureÂ space.</p><ul><li><strong>Pros: </strong>simple, easy toÂ use.</li><li><strong>Cons: </strong>can be chaoticâ€Šâ€”â€Šsometimes centroids start too close together, causing slow convergence or poor final clusters.</li></ul><p><strong>2. K-Means|| Initialization. </strong>A smarter, parallelized method for choosing startingÂ points.</p><ul><li><strong>Pros: </strong>spreads centroids more evenly across the data, reduces the chance of unlucky starting positions, often converges faster and produces better clusters.</li><li><strong>Cons: </strong>slightly more computation at the beginning, but worth it for large datasets.</li></ul><p><strong>Why itÂ matters:</strong></p><ul><li>The initial positions affect how quickly K-Means converges.</li><li>Minimizes the risk of local optimaâ€Šâ€”â€Šsituations where the algorithm stops improving even though a better overall clustering exists.</li><li>For large Spark jobs, K-Means|| helps control memory usage and computation time by avoiding unnecessary iterations from poor startingÂ points.</li></ul><h4>Iteration: How K-Means Updates Centroids</h4><p>After the initial centroids are set (randomly or with K-Means||), K-Means iterates to improve the clustering. Each iteration involves a few coordinated steps across theÂ cluster:</p><ol><li><strong>Assign Each Data Point to Its Nearest Centroid. </strong>For each data point, compute the distance to every centroid and assign the point to the cluster of the closest centroid. In Spark, this is done in parallel across nodes, so each executor handles a partition of dataÂ points.</li><li><strong>Compute Local Sums and Counts on Each Node. </strong>Each node calculates the sum of all feature vectors assigned to each centroid (local sum) and the number of points assigned to each centroid (local count). These are partial results that will be sent to theÂ driver.</li><li><strong>Aggregate Results and Update Centroids on the Driver. </strong>The driver gathers each executorâ€™s partial sums and counts, combines them, and computes new centroids by taking the arithmetic mean of all points assigned to each cluster. This update ensures each centroid reflects the true balance point of its assigned data before the next iteration begins. <br /><strong>ğŸ¤” Waitâ€Šâ€”â€Šdoes the driver do any computation?</strong><br />Yes, but just a tiny, aggregated slice. Executors do the heavy liftingâ€Šâ€”â€Šdistance checks, assignments, and local sums/counts. The driver only receives these compact summaries to update centroids, keeping its workload minimal and never touching the fullÂ dataset.</li><li><strong>Broadcast Updated Centroids Back to the Nodes. </strong>The driver sends the new centroids to all nodes using Spark broadcast variables. Nodes now have the updated centroids and are ready for the next iteration.</li><li><strong>Repeat Until Convergence. </strong>Steps 1â€“4 repeat for maxIter times or until the centroids stop moving significantly. At convergence, each point is assigned to its final cluster, and the centroids represent the clusterÂ centers.</li></ol><h4><strong>How itÂ works</strong></h4><ul><li><strong>Explicit Cleanup: </strong>Calling broadcastVar.unpersist() removes the broadcast data from executor caches, and broadcastVar.destroy() removes it entirely (both executor blocks and driver metadata).</li><li><strong>Executor Eviction:</strong> Executors may evict broadcast blocks on their own if memory is tight or newer broadcasts arrive. Sparkâ€™s block manager uses an LRU-style policy, so old or unused broadcasts get pushed out when space isÂ needed.</li><li><strong>End-of-Job Housekeeping:</strong> When a job ends and no references remain, Spark schedules the broadcast data for cleanup. Removal isnâ€™t immediateâ€Šâ€”â€Šit depends on GC cycles and executor housekeepingâ€Šâ€”â€Šbut it will eventually vanish.</li></ul><p>For iterative algorithms like K-Means, Spark re-broadcasts centroids on every iteration. Old broadcast blocks are dropped once theyâ€™re unreferenced, but explicit cleanup helps avoid lingering data buildupâ€Šâ€”â€Šespecially when centroids are large or iteration counts areÂ high.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Qui7Q3vHFzWhx7WhJxU5hg.png\" /></figure><h3>ğŸ¶ GreatestÂ Hits</h3><p>Now that weâ€™ve covered the basics of K-Means and its implementation in MLlib, letâ€™s try a hands-on exercise to reinforce our understanding and explore a bitÂ further.</p><p>Our K-Means test in <a href=\"https://github.com/SauronShepherd/dont-be-kmean/utils.py\"><strong>utils.py</strong></a> generates a very large synthetic dataset of sparse feature vectors and runs a K-Means clustering job on it. This process exercises the full Spark pipeline: data generation, distributed processing, shuffling, caching, and model fitting. The goal isnâ€™t the output itself or the quality of the model, but rather to confirm that Spark can execute this end-to-end workflow reliablyâ€”without running out of memory, stalling, or failing under heavyÂ load.</p><p>Since each partition uses its own random number generatorâ€Šâ€”â€Šseeded with a constant SEED and the partition indexâ€”the dataset is reproducible: the same numbers are generated consistently for each partition.</p><h3>ğŸ’¿ Hit #1â€Šâ€”â€ŠCanâ€™t Help Falling inÂ Four</h3><p>When we ran our first test, <a href=\"https://github.com/SauronShepherd/dont-be-kmean/blob/main/test__hit_1.py\"><strong>test__hit_1.py</strong></a>, we noticed that Spark kept <em>â€œcomplainingâ€</em> about not having enough space to cache despite increasing the memory considerably in eachÂ try.</p><pre>    [<br />        {&quot;spark.driver.memory&quot;: &quot;4g&quot;},<br />        {&quot;spark.driver.memory&quot;: &quot;8g&quot;},<br />        {&quot;spark.driver.memory&quot;: &quot;14g&quot;},<br />    ],</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*oj4wvPmWEXe_cadj0nG3vQ.png\" /><figcaption>Not enough space toÂ cache</figcaption></figure><h4>Not EnoughÂ Space</h4><p>This happens because Spark caches data to avoid recomputing it during iterative algorithms like K-Means. An iterative algorithm repeatedly executes actions on the same data to compute partial results and refine outcomesâ€Šâ€”â€Šlike K-Means assigning points to clusters and updating centroids until they stabilize</p><ul><li>Without caching, each iteration would recompute the dataset from scratch, which isÂ slow.</li><li>Caching keeps the data in memory so subsequent iterations can reuse it efficiently, and K-Means automatically caches the DataFrame if it isnâ€™t alreadyÂ cached.</li></ul><h4>Why Is 4GiB NotÂ Enough</h4><p>At first glance, this dataset doesnâ€™t seem hugeâ€Šâ€”â€Šbut when Spark materializes the DataFrame, the memory footprint grows quickly. Each sparse vector is stored as a Python object with overhead, and Spark adds JVM and bookkeeping overhead for each row and partition. Multiplying this by 10 million rows across 200 partitions, plus extra memory for shuffles, intermediate computations, and caching, easily pushes the dataset well beyond 4Â GB.</p><p>By default, Spark stores data in deserialized memory (MEMORY_ONLY) for fast access during each iteration. If the dataset is too large, it automatically spills to disk (MEMORY_AND_DISK), so the algorithm can still run without failing. Deserialized caching means the data is kept as full in-memory objects (ready to use), rather than a compact binary formatâ€Šâ€”â€Šwhich speeds access but consumes moreÂ memory.</p><p>Increasing the maximum heap (-Xmx) to 8, or 14 GB and Spark still complainsâ€”how is that possible? Because Spark is being a little <em>â€œnaughty.â€</em> Each test, when we politely tell Spark to stop, it obeysÂ â€¦ but only partially. The Spark session is effectively closed, yet the internal JVM remainsÂ running.</p><h4>PySpark, You NaughtyÂ Boy</h4><p>PySpark doesnâ€™t fully close the JVM because the Python process communicates with it via the Py4J gateway, which stays alive to handle pending calls, cleanup, and garbage collection. This allows Python and Spark to safely coordinate even after the sessionÂ ends.</p><p>In our first test, Spark creates the JVM with 4 GB and then reuses that same JVM for all subsequent tests, silently ignoring any increase in the spark.driver.memory setting. This happens because some Spark settings, like memory, are fixed when the JVM starts and cannot be changed afterward. By default, pytest runs all tests in the same Python process, so the existing JVM persists across tests. Using <a href=\"https://pypi.org/project/pytest-forked/\"><strong>pytest-forked</strong></a> allows each test to run in a fresh Python process, which spawns a new JVM and lets you apply updated memory settingsÂ cleanly.</p><h3>ğŸ’¿ Hit #2â€Šâ€”â€ŠHoundÂ Log</h3><p>By enabling debug logging in our second test, <a href=\"https://github.com/SauronShepherd/dont-be-kmean/blob/main/test__hit_2.py\"><strong>test__hit_2.py</strong></a>, we can examine the inner workings of K-Means and observe exactly how each step is executed.</p><pre>        {<br />            &quot;spark.driver.memory&quot;: &quot;14g&quot;,<br />            &quot;spark.driver.extraJavaOptions&quot;: &quot;-Xlog:gc*:stdout:time,level,tags&quot;,<br />            &quot;logLevel&quot;: &quot;DEBUG&quot;<br />        }</pre><h4><strong>Code Generation &amp; Optimization</strong></h4><p><strong>Project Tungsten: Custom-Built Efficiency. </strong>To maximize efficiency, Spark acts like a high-speed architect, writing custom software on the fly rather than using generic programs. Through <strong>Project Tungsten</strong>, Spark <em>â€œfusesâ€</em> multiple mathematical steps into a single, massive loop called <strong>WholeStageCodegen</strong>, which keeps the CPU working at full speed without interruption. It also uses <strong>GenerateUnsafeProjection</strong> to point directly to raw memory addresses, bypassing the slow process of creating standard JavaÂ objects.</p><p><strong>Janino and Guava: The Compilation Infrastructure. </strong>To turn this custom code into action, Spark uses a high-speed embedded compiler called <a href=\"https://janino.net/\"><strong>Janino</strong></a>. It takes the code strings Spark just wrote and transforms them into executable instructions (bytecode) instantly. To ensure it doesnâ€™t waste time repeating this workâ€Šâ€”â€Šespecially during the repeating loops of a K-Means algorithmâ€Šâ€”â€ŠSpark utilizes a <a href=\"https://github.com/google/guava\"><strong>Guava</strong></a><strong>-based cache</strong> to store and retrieve these compiledÂ results.</p><p><strong>Log Analysis: Performance through Caching. </strong>While the logs show thousands of â€œcode generatedâ€ messages, the timing data proves Spark is being highly efficient rather than repetitive. The evidence shows initial compilations taking approximately 50ms, but every subsequent iteration finishes in less than 5ms. This confirms that Spark is merely retrieving pre-compiled â€œshortcutsâ€ from its internal cache, saving significant time by skipping the heavy lifting of re-compilation for every task in theÂ stage.</p><h4><strong>Iterative Memory Management</strong></h4><p><strong>The Global Allocation (14 GiB). </strong>This is the total memory â€œcontainerâ€ for our Spark executor. It is divided into two main categories:</p><ul><li><strong>The Spark Unified Memory Pool (8.2 GiB):</strong> The portion Spark manages for data processing andÂ caching.</li><li><strong>User Memory &amp; Overhead (~5.8 GiB):</strong> The remaining space used for everything else, such as our application code, Sparkâ€™s internal metadata, and the Metaspace (which stores the class information generated by Tungsten).</li></ul><p><strong>The Unified Memory Pool (8.2 GiB). </strong>This is the heart of the executor where the â€œUnified Memory Managementâ€ happens. It has two parts that grow and shrink against eachÂ other:</p><ul><li><strong>The Storage Region:</strong> This area is for things we want Spark to <em>â€œremember,â€</em> such as cached RDD partitions and Broadcast variables. In your case, the primary dataset grew to occupy about <strong>6.44Â GiB</strong>.</li><li><strong>The Execution Region:</strong> This is the <em>â€œworkspaceâ€</em> for active computation. It is used for the mathematical heavy lifting of K-Means (like calculating distances) and temporary shuffle data. Because our dataset took up so much room, this workspace was limited to about <strong>1.8Â GiB</strong>.</li></ul><p><strong>Dynamic Conflict &amp; Eviction. </strong>Because these two regions share the same 8.2 GiB pool, they often compete forÂ space.</p><ul><li><strong>The â€œBorrowingâ€ Rule:</strong> Execution memory takes priority. If the computation (Execution) needs more room than the 1.8 GiB left over, it can <em>â€œborrowâ€</em> space from the StorageÂ region.</li><li><strong>The Result (Eviction):</strong> To make that space available, Spark must evict (delete) cached data that hasnâ€™t been used recently. The logs show this clearly when intermediate data was <em>â€œdropped from memoryâ€ </em>to ensure the K-Means math could finish itsÂ work.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AIwLSQPbCANs3KCL_OgpGw.png\" /></figure><h4><strong>Garbage Collector Dynamics</strong></h4><ul><li><strong>G1 Efficiency</strong>: Using the <strong>G1 collector</strong> on JDK 17Â , the system maintains high responsiveness with most <em>â€œYoungâ€</em> pauses finishing inÂ <strong>&lt;2ms</strong>.</li><li><strong>Heap Elasticity</strong>: The heap dynamically scales between a <strong>256 MiB</strong> initial capacity and a <strong>14 GiB</strong> maximum. It proactively <em>â€œshrinksâ€ </em>(reclaims memory) after marking cycles to return resources to theÂ OS.</li><li><strong>Latency Spikes</strong>: Rare spikes of <strong>30msâ€“45ms</strong> occur. These are driven by internal<em> â€œhousekeepingâ€</em> (post-evacuation bookkeeping and cross-region reference updates) rather than data movement. This is common in Spark when new stages launch or iterative metadata pilesÂ up.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ml6Oi6_B0v8N0wG-4-oEvQ.png\" /><figcaption>Heap Usage (after GC)â€Šâ€”â€Šfrom <a href=\"https://gceasy.io/\">gceasy.io</a></figcaption></figure><h3>ğŸ’¿ Hit #3â€Šâ€”â€ŠSuspicious PIDs</h3><h4>Fractioned butÂ Unified</h4><p>Spark uses a unified memory pool for both execution (shuffles, joins, aggregations) and storage (cached RDDs, broadcast variables). This pool is carved out from the executor heap using two settings:</p><ol><li>spark.memory.fraction (default: 0.6). This determines how much of the JVM heap Spark is allowed to use for its unified memoryÂ region.</li><li>spark.memory.storageFraction (default: <strong>0.5</strong>). Inside the unified pool, this sets an initial <em>soft boundary</em> forÂ storage.</li></ol><p>Because Spark uses a unified memory pool, execution can borrow from storage and storage can borrow from execution, right up until the entire pool is filled. Only then do spills or evictions occur. When memory runs out, storage is the one that evictsâ€Šâ€”â€Šdropping old cached blocks to free spaceâ€Šâ€”â€Šbut execution never evicts. If execution needs more room and canâ€™t borrow it, Spark simply spills execution data to diskÂ instead.</p><h4><strong>Where User Code ConsumesÂ Memory</strong></h4><p>User code primarily consumes memory in the executor JVM heap (and in the Python worker processes for PySpark). This memory sits <em>alongside</em> Sparkâ€™s unified memory regions and directly competes with them for heapÂ space.</p><h4>Tweaking Memory Fractions</h4><p>Looking at the previous graphs, we see that apart from a few spikes, the heap barely exceeds ~8 GiB, and the storage region stays under ~6.5 GiB. So, why not dial back the memory settings aÂ bit?</p><p>In our third test, <a href=\"https://github.com/SauronShepherd/dont-be-kmean/blob/main/test__hit_3.py\"><strong>test__hit_3.py</strong></a>, weÂ used:</p><pre>        {<br />            &quot;spark.driver.memory&quot;: &quot;8g&quot;,<br />            &quot;spark.memory.fraction&quot;: &quot;0.9&quot;,<br />            &quot;spark.memory.storageFraction&quot;: &quot;0.9&quot;<br />        }</pre><p>This keeps Spark leaner while still giving K-Means the room it needs toÂ groove.</p><h4>Plotting CPU and Memory Consumption</h4><p>Now, to get a full view of the memory consumed by our PySpark K-Means job, we collect CPU and memory metrics externally. The <a href=\"https://github.com/SauronShepherd/dont-be-kmean/blob/main/get_stats.sh\"><strong>get_stats.sh</strong></a> script uses the ps command to record resource usage over time, and then the <a href=\"https://github.com/SauronShepherd/dont-be-kmean/blob/main/plot_stats.py\"><strong>plot_stats.py</strong></a> script visualizes CPU and memory trends withÂ <a href=\"https://plotly.com/\"><strong>Plotly</strong></a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HPdyEsKwiUMAWhyCQvH0Kw.png\" /><figcaption>CPU and Memory Consumption</figcaption></figure><p>This plot is a perfect visualization of the Driver-Executor<strong> </strong>architecture typical of a PySpark application. Our K-Means code triggers distinct phases of execution that correspond exactly to the spikes and shifts in the resource monitoring.</p><ol><li><strong>The Python Spike.</strong> This is the Data Generation phase. We are using mapPartitionsWithIndex with a custom Python function to create 10 million synthetic documents. Since this is raw Python logic, Spark has to spin up many Python worker processes (the green dashed lines) to execute your rng.sample and SparseVector creation. We see high TOTAL Python CPU usage because the heavy lifting of generating random numbers and dictionary-zipping is happening in the Python interpreter. The memory also steps up as these vectors are created and prepared to be passed to theÂ JVM.</li><li><strong>The Java Shift. </strong>This is the K-Means Fitting phase (kmeans.fit(df)). Once the data is generated and converted to a DataFrame, PySpark hands the â€œheavy liftingâ€ of the ML algorithm over to the JVM (Java Virtual Machine). Sparkâ€™s MLlib uses highly optimized Scala/Java code for the iterative distance calculations of K-Means. Python activity drops to near zero, and TOTAL Java CPU usage (the blue solid line) skyrockets and stays pinned near <strong>1000%</strong> (utilizing ~10 cores). The â€œsawtoothâ€ pattern in the Java CPU is typical of K-Means iterations: The parallel computation of distances between 10 million points and 200 centroids (K=200). The synchronization point at the end of each iteration where the Driver calculates the new centroids before starting the next MAX_ITER.</li><li><strong>The Memory Profile. </strong>Notice the Java memory usage grows steadily and then plateaus. This is the Spark Block Manager at work. Even if we donâ€™t explicitly callÂ .persist(), Spark needs to manage the memory for the millions of SparseVector objects within the JVM heap during the fit. Stays relatively low and flat after the initial generation. This confirms that Python is just the <em>â€œinterfaceâ€</em> here; the actual data resides in the JVM during the training.</li><li><strong>The Final Merge. </strong>Completion of the fit and likely result collection. We see a final small burst of Python activity. This is the Spark Driver (running in Python) receiving the final model metadata and cluster centers back from the Java executors to finish the script execution.</li></ol><h3>ğŸ’¿ Hit #4â€Šâ€”â€ŠItâ€™s Low AndÂ Clever</h3><p>In Spark, the Garbage Collector can quietly throttle your performance. Iterative algorithms like K-Means are especially vulnerable because they touch the same dataset over and over. If that data lives on-heap, the JVM spends more time cleaning memory than calculating centroids.</p><h4>The Iteration Trap</h4><p>K-Means isnâ€™t a one-pass algorithmâ€Šâ€”â€Šit scans the dataset dozens or hundreds of times. On-heap caching forces the GC to constantly sift through millions of objects to distinguish permanent cached data from temporary ones. The result? Long â€œStop-the-Worldâ€ pauses that slow our clustering to aÂ crawl.</p><h4>Off-Heap to theÂ Rescue</h4><p>Moving the data off-heap keeps it out of the JVMâ€™s jurisdiction:</p><ul><li><strong>GC stays out:</strong> Only tiny, short-lived objects from centroid calculations getÂ scanned.</li><li><strong>Smaller footprint:</strong> Serialized, compact storage reduces memory pressure.</li><li><strong>More stability:</strong> No more old-generation memory explosions or OutOfMemoryErrors.</li></ul><p>Yes, thereâ€™s a cost: accessing off-heap data means a bit of serialization overhead. Serialization is needed to convert JVM objects into raw bytes so they can live off-heap, letting Spark access the data without involving the Garbage Collector. But in iterative workloads like K-Means, skipping massive GC cycles usually winsÂ big.</p><h4>Our FinalÂ Test</h4><p>Time for the grand finale! Letâ€™s fire up <a href=\"https://github.com/SauronShepherd/dont-be-kmean/blob/main/test__hit_4.py\"><strong>test__hit_4.py</strong></a> and watch how, with far less heap memory, Spark zooms through the job faster thanÂ ever.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gYmcmmexK5tDFwSGNpgoFA.png\" /><figcaption>Heap Usage (after GC)â€Šâ€”â€Šfrom <a href=\"https://gceasy.io/\">gceasy.io</a></figcaption></figure><h3>ğŸ¸ Rock Your K-Means Like the King of Clustering</h3><h4>Best Practices to Keep Your ClustersÂ Cool</h4><ol><li><strong>Scale Your Features, Honey. </strong>K-Means groves to <strong>Euclidean distance</strong>, so donâ€™t let a giant â€œAnnual Incomeâ€ (0â€“200,000) drown out the delicate â€œAgeâ€ (0â€“100). <br />âœ…<strong> </strong>Hit it with StandardScaler or MinMaxScalerâ€”otherwise your clusters will wobble like a badly tuned guitarÂ string.</li><li><strong>Go With â€œK-Means||â€ Initialization. </strong>Sparkâ€™s got a parallelized version of K-Means++ called <strong>K-Means||</strong>, and baby, itâ€™s the way to go. Random initialization is like trying to play â€œJailhouse Rockâ€ on a kazooâ€Šâ€”â€Šit just ainâ€™t gonna sound right. <br />âœ…<strong> </strong>Set initMode='k-means||' and reduce the chance of converging on a sad little localÂ minimum.</li><li><strong>Fight the Curse of Dimensionality. </strong>As your feature count climbs, all points start looking alikeâ€Šâ€”â€Šlike a sea of Elvis wigs.<br />âœ… Slice it down with <strong>PCA</strong> before clustering. Keep the variance, lose theÂ noise.</li></ol><h4>Implementation Tips Thatâ€™ll Make YouÂ Swing</h4><ul><li><strong>Elbow Method:</strong> Run K-Means for a range of k and plot the Within Set Sum of Squared Errors. Look for that â€œelbowâ€ where the improvement slowsâ€”just like spotting the perfect hairÂ flip.</li><li><strong>Silhouette Score:</strong> For more confidence, check cluster separation with ClusteringEvaluator. A score close to 1? Thatâ€™s a high note,Â baby.</li><li><strong>Persistence:</strong> K-Means iterates like a drummer on caffeine. Cache your DataFrame (df.cache()) to avoid Spark rereading the data every singleÂ time.</li></ul><h4>Pitfalls to Dodge Like a Flying GuitarÂ Pick</h4><ol><li><strong>Categorical Data Blues. </strong>K-Means loves continuous numbers, not one-hot encoded spaghetti. Tossing in too many categories creates a high-dimensional nightmare where Euclidean distance forgets how to dance.<br />âš’ï¸ Stick to numeric features or consider Bisecting K-Means for categorical-heavy sets.</li><li><strong>Outliersâ€Šâ€”â€ŠDonâ€™t Let â€™Em Steal the Show. </strong>Centroids are basically averagesâ€Šâ€”â€Šso a single wild outlier can pull your cluster off-key.<br />âš’ï¸ Filter out statistical outliers before the big performance.</li><li><strong>Spherical Bias. </strong>K-Means thinks clusters are like perfect circles, all roughly the same size. If your data is shaped like two interlocking crescents or elongated ellipses, itâ€™ll flub the routine.<br />âš’ï¸ Consider Gaussian Mixtures or other clustering methods when your data isnâ€™t feeling the circularÂ vibe.</li></ol><h3>ğŸ¤ <strong>All Shook Upâ€¦ inÂ Spark!</strong></h3><p>So there you have it, babyâ€Šâ€”â€Šyouâ€™ve rocked through K-Means like the King himself: centroids hittinâ€™ the right notes, clusters jivinâ€™ in sync, and Spark keepinâ€™ the band tight. Remember, itâ€™s all about timing, memory, and pickinâ€™ the right initializationâ€Šâ€”â€Štreat your JVM right, cache your data, and let those off-heap movesÂ slide.</p><p>Keep your clusters smooth, your vectors groovy, and never let a stray outlier steal your spotlight. Now crank the volume, hit fit(), and let your big data dance floorÂ swing.</p><p>Because in the world of Spark, K-Means isnâ€™t just mathâ€Šâ€”â€Šitâ€™s rock â€™nâ€™ roll, baby.Â ğŸ•º</p><h3>ğŸ¤˜ <strong>Howdy, Rockinâ€™Â Readers!</strong></h3><p>This article is dedicated to pure rock â€™nâ€™ roll funâ€Šâ€”â€Šnot that â€œfunâ€ math stuffâ€Šâ€”â€Šand to my broâ€™nâ€™law <strong>Fernando</strong>. Keep on rockinâ€™, brother!Â ğŸ¸</p><p>Shoutout to my Olympic colleagues, sweating it out like championsâ€Šâ€”â€Šspecial cheers to <a href=\"https://www.linkedin.com/in/luislopezpinero-516a83206/\"><strong>Luis</strong></a>!Â ğŸ…ğŸ’ª</p><p>And just to be clear: this oneâ€™s <strong>NOT</strong> for the ungrateful folks who treat you like a minion despite your best efforts. You know who you areÂ â€¦ blaming everyone else for your own incompetenceâ€Šâ€”â€Šyes, Emojiman, we see youÂ ğŸ˜†.</p><blockquote>â€œDonâ€™t be K-Mean to my machine<br />Donâ€™t be K-Mean to my machine<br />I just want my clusters tight<br />Baby, keep my centroids rightâ€<br />(adapted from <a href=\"https://www.youtube.com/watch?v=B2YwGBVSfFQ\"><strong>â€œDonâ€™t be cruelâ€ by ElvisÂ Presley</strong></a>)</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*2OFetlU7YN0XpQ5sib8p9Q.png\" /></figure><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9b9eeae3dcda\" width=\"1\" /><hr /><p><a href=\"https://blog.devgenius.io/apache-spark-wtf-dont-be-k-mean-9b9eeae3dcda\">Apache Spark WTF???â€Šâ€”â€ŠğŸ˜  Donâ€™t Be K-Mean ğŸ˜¡</a> was originally published in <a href=\"https://blog.devgenius.io\">Dev Genius</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "19 min read",
      "language": null
    },
    {
      "title_en": "Z.aiâ€™s GLM-5 leaked through GitHub PRs and a zodiac easter egg",
      "url": "https://blog.devgenius.io/z-ais-glm-5-leaked-through-github-prs-and-a-zodiac-easter-egg-8716deb07f93?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:53:32",
      "external_id": null,
      "tags": [
        "dev-genius",
        "glm",
        "ai",
        "llm",
        "artificial-intelligence",
        "open-source"
      ],
      "content_length": 685,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/z-ais-glm-5-leaked-through-github-prs-and-a-zodiac-easter-egg-8716deb07f93?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1376/1*79FiaRcl_IqXDhBguKmDFw.jpeg\" width=\"1376\" /></a></p><p class=\"medium-feed-snippet\">Zhipu AI just IPO&#x2019;d for $558M. Its next model was already being tested in the wild. Here&#x2019;s what the code tells us.</p><p class=\"medium-feed-link\"><a ",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/z-ais-glm-5-leaked-through-github-prs-and-a-zodiac-easter-egg-8716deb07f93?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1376/1*79FiaRcl_IqXDhBguKmDFw.jpeg\" width=\"1376\" /></a></p><p class=\"medium-feed-snippet\">Zhipu AI just IPO&#x2019;d for $558M. Its next model was already being tested in the wild. Here&#x2019;s what the code tells us.</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/z-ais-glm-5-leaked-through-github-prs-and-a-zodiac-easter-egg-8716deb07f93?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Designing React 19 Frontends for Agentic Backends: Streaming, Suspense, and Error States",
      "url": "https://blog.devgenius.io/designing-react-19-frontends-for-agentic-backends-streaming-suspense-and-error-states-25b036b7afe6?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:53:29",
      "external_id": null,
      "tags": [
        "dev-genius",
        "web-development",
        "artificial-intelligence",
        "ai-agents-in-action",
        "frontend-development",
        "react"
      ],
      "content_length": 705,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/designing-react-19-frontends-for-agentic-backends-streaming-suspense-and-error-states-25b036b7afe6?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*S1vaiaIoJO1ztdtueYJmZw.jpeg\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">How to use React 19&#x2019;s async primitives to build UIs that keep up with multi-agent AI backends</p><p class=\"medium-feed-link\"><a href",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/designing-react-19-frontends-for-agentic-backends-streaming-suspense-and-error-states-25b036b7afe6?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*S1vaiaIoJO1ztdtueYJmZw.jpeg\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">How to use React 19&#x2019;s async primitives to build UIs that keep up with multi-agent AI backends</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/designing-react-19-frontends-for-agentic-backends-streaming-suspense-and-error-states-25b036b7afe6?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "7 C# Features Microsoft Uses Internally (But Most Teams Ignore)",
      "url": "https://blog.devgenius.io/7-c-features-microsoft-uses-internally-but-most-teams-ignore-819c18125399?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:53:24",
      "external_id": null,
      "tags": [
        "dev-genius",
        "microsoft",
        "csharp",
        "dotnet",
        "cloud-computing",
        "programming"
      ],
      "content_length": 356,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-snippet\">(And why that gap quietly costs you performance, reliability, and velocity.)</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/7-c-features-microsoft-uses-internally-but-most-teams-ignore-819c18125399?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-snippet\">(And why that gap quietly costs you performance, reliability, and velocity.)</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/7-c-features-microsoft-uses-internally-but-most-teams-ignore-819c18125399?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "The definitive guide to Codex CLI: from first install to production workflows",
      "url": "https://blog.devgenius.io/the-definitive-guide-to-codex-cli-from-first-install-to-production-workflows-a9f1e7c887ab?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:53:20",
      "external_id": null,
      "tags": [
        "dev-genius",
        "ai",
        "vibe-coding",
        "openai-codex",
        "openai",
        "artificial-intelligence"
      ],
      "content_length": 739,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/the-definitive-guide-to-codex-cli-from-first-install-to-production-workflows-a9f1e7c887ab?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1376/1*fOYZkpCPZAxgQft8cpEfDw.jpeg\" width=\"1376\" /></a></p><p class=\"medium-feed-snippet\">Everything I&#x2019;ve learned from writing 10+ Codex guides, now in one place; with infographics, copy-paste configs, and patterns the official&#x2",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/the-definitive-guide-to-codex-cli-from-first-install-to-production-workflows-a9f1e7c887ab?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1376/1*fOYZkpCPZAxgQft8cpEfDw.jpeg\" width=\"1376\" /></a></p><p class=\"medium-feed-snippet\">Everything I&#x2019;ve learned from writing 10+ Codex guides, now in one place; with infographics, copy-paste configs, and patterns the official&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/the-definitive-guide-to-codex-cli-from-first-install-to-production-workflows-a9f1e7c887ab?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "MCP explained: the protocol that ended AIâ€™s integration wars",
      "url": "https://blog.devgenius.io/mcp-explained-the-protocol-that-ended-ais-integration-wars-2ba04f94c1d4?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:52:32",
      "external_id": null,
      "tags": [
        "dev-genius",
        "ai",
        "automation",
        "mcp-server",
        "integration",
        "llm"
      ],
      "content_length": 636,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/mcp-explained-the-protocol-that-ended-ais-integration-wars-2ba04f94c1d4?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1216/1*PnkGouk9BlU9h0JMPwtknA.jpeg\" width=\"1216\" /></a></p><p class=\"medium-feed-snippet\">An abridged history on how Anthropic&#x2019;s MCP became the USB-C of AI connectivity</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/mcp-explained-",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/mcp-explained-the-protocol-that-ended-ais-integration-wars-2ba04f94c1d4?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1216/1*PnkGouk9BlU9h0JMPwtknA.jpeg\" width=\"1216\" /></a></p><p class=\"medium-feed-snippet\">An abridged history on how Anthropic&#x2019;s MCP became the USB-C of AI connectivity</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/mcp-explained-the-protocol-that-ended-ais-integration-wars-2ba04f94c1d4?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "7 Modern C# Features That Replace 100+ Lines of Boilerplate",
      "url": "https://blog.devgenius.io/7-modern-c-features-that-replace-100-lines-of-boilerplate-7b975ede6e65?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:52:29",
      "external_id": null,
      "tags": [
        "dev-genius",
        "dotnet",
        "csharp",
        "microsoft",
        "programming",
        "azure"
      ],
      "content_length": 378,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-snippet\">(What senior&#xa0;.NET teams quietly use&#x200a;&#x2014;&#x200a;while others keep writing glue code.)</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/7-modern-c-features-that-replace-100-lines-of-boilerplate-7b975ede6e65?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-snippet\">(What senior&#xa0;.NET teams quietly use&#x200a;&#x2014;&#x200a;while others keep writing glue code.)</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/7-modern-c-features-that-replace-100-lines-of-boilerplate-7b975ede6e65?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Mastering MapStruct: The Definitive Guide to Every Parameter",
      "url": "https://blog.devgenius.io/mastering-mapstruct-the-definitive-guide-to-every-parameter-e2e221b02a7e?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:52:26",
      "external_id": null,
      "tags": [
        "dev-genius",
        "spring-boot",
        "programming",
        "mapstruct",
        "clean-code",
        "java"
      ],
      "content_length": 676,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/mastering-mapstruct-the-definitive-guide-to-every-parameter-e2e221b02a7e?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1037/1*qTZOt9rsnABdfBBwGVRJWg.png\" width=\"1037\" /></a></p><p class=\"medium-feed-snippet\">From basic field renaming to complex conditional logic&#x200a;&#x2014;&#x200a;take total control of your Java Bean mappings.</p><p class=\"medium-feed-link\"><a href=\"h",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/mastering-mapstruct-the-definitive-guide-to-every-parameter-e2e221b02a7e?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/1037/1*qTZOt9rsnABdfBBwGVRJWg.png\" width=\"1037\" /></a></p><p class=\"medium-feed-snippet\">From basic field renaming to complex conditional logic&#x200a;&#x2014;&#x200a;take total control of your Java Bean mappings.</p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/mastering-mapstruct-the-definitive-guide-to-every-parameter-e2e221b02a7e?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "A Production-Ready React Boilerplate for 2026 (That You Can Actually Scale)",
      "url": "https://blog.devgenius.io/a-production-ready-react-boilerplate-for-2026-that-you-can-actually-scale-32a407b57b69?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:52:14",
      "external_id": null,
      "tags": [
        "dev-genius"
      ],
      "content_length": 8407,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JuEe0d6lbMuAN4nF-8prCA.png\" /></figure><p>React in <strong>2026</strong> is powerfulâ€Šâ€”â€Šbut starting a new project still feels heavier than itÂ should.</p><p>You spin up an app, and within days youâ€™re configuring:<br /> TypeScript, routing, state management, forms, testing, linting, CI/CD, Docker, environment variables, dark mode, and internationalization.</p><p>Before youâ€™ve shipped a single feature, youâ€™re already deep in infras",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JuEe0d6lbMuAN4nF-8prCA.png\" /></figure><p>React in <strong>2026</strong> is powerfulâ€Šâ€”â€Šbut starting a new project still feels heavier than itÂ should.</p><p>You spin up an app, and within days youâ€™re configuring:<br /> TypeScript, routing, state management, forms, testing, linting, CI/CD, Docker, environment variables, dark mode, and internationalization.</p><p>Before youâ€™ve shipped a single feature, youâ€™re already deep in infrastructure.</p><p>This React Boilerplate exists to <strong>eliminate that phase entirely</strong>.</p><p>Itâ€™s not a demo starter.<br /> Itâ€™s not a toy template.<br /> Itâ€™s a <strong>production-ready foundation</strong> built for real-world applicationsâ€Šâ€”â€Šfrom side projects to enterprise platforms.</p><h3>ğŸ¯ Why This Boilerplate Exists</h3><p>Most React starters fall into one of threeÂ traps:</p><ul><li><strong>Too minimal</strong> â†’ you spend days wiring essentials</li><li><strong>Too opinionated</strong> â†’ you fight decisions you didnâ€™tÂ make</li><li><strong>Too outdated</strong> â†’ built for yesterdayâ€™s React</li></ul><p>This boilerplate was built with a singleÂ goal:</p><blockquote><strong><em>Start building features immediatelyâ€Šâ€”â€Šwithout sacrificing scalability orÂ quality.</em></strong></blockquote><p>Everything included here reflects how modern React apps are actually built inÂ <strong>2026</strong>.</p><h3>ğŸ§  Design Philosophy</h3><ul><li>Modern React patternsÂ only</li><li>Clear separation ofÂ concerns</li><li>Scales without architectural rewrites</li><li>Performance and developer experience first</li><li>Production-ready from dayÂ one</li></ul><p>No legacy defaults. No unnecessary abstractions. Just a clean, extensible foundation.</p><h3>ğŸ› ï¸ The 2026-Ready TechÂ Stack</h3><h3>âš›ï¸ Core</h3><ul><li><strong>React 19</strong>â€Šâ€”â€ŠConcurrent features, modern rendering patterns</li><li><strong>TypeScript (Strict)</strong>â€Šâ€”â€ŠType safety enforced byÂ default</li><li><strong>Vite</strong>â€Šâ€”â€ŠUltra-fast dev server andÂ builds</li><li><strong>pnpm</strong>â€Šâ€”â€ŠEfficient, predictable dependency management</li></ul><h3>ğŸ§© State Management</h3><ul><li><strong>Zustand</strong>â€Šâ€”â€ŠLightweight, simple clientÂ state</li><li><strong>TanStack Query</strong>â€Šâ€”â€ŠBest-in-class server state &amp;Â caching</li><li>Clean separation between UI state and asyncÂ data</li></ul><h3>ğŸ§­ Routing &amp; Performance</h3><ul><li><strong>React RouterÂ v7</strong></li><li>Route-level code splitting</li><li>Lazy loading bakedÂ in</li><li>Protected route patternsÂ included</li></ul><h3>ğŸ¨ Styling</h3><ul><li><strong>Tailwind CSS</strong></li><li>CSS variables for designÂ tokens</li><li>Built-in darkÂ mode</li><li>Fully responsive byÂ default</li></ul><h3>ğŸ“ Forms &amp; Validation</h3><ul><li><strong>React HookÂ Form</strong></li><li><strong>Zod</strong> for type-safe validation</li><li>Real-world form examplesÂ included</li></ul><h3>ğŸ§ª Testing That Scales WithÂ You</h3><p>Testing isnâ€™t optional in serious projectsâ€Šâ€”â€Šand itâ€™s fully set upÂ here:</p><ul><li><strong>Vitest</strong>â€Šâ€”â€ŠFast, Vite-native unitÂ testing</li><li><strong>React Testing Library</strong>â€Šâ€”â€ŠTest behavior, not implementation</li><li><strong>Playwright</strong>â€Šâ€”â€ŠReliable end-to-end testing</li><li><strong>MSW</strong>â€Šâ€”â€ŠAPI mocking withoutÂ hacks</li></ul><p>No boilerplate setup. Just writeÂ tests.</p><h3>ğŸ” Code Quality BuiltÂ In</h3><p>This boilerplate enforces consistency and quality automatically:</p><ul><li>ESLint (React + TypeScript rules)</li><li>Prettier formatting</li><li>Husky + lint-staged (pre-commit checks)</li><li>Commitlint (conventional commits)</li><li>Type-checking inÂ CI/CD</li></ul><p>Perfect for teamsâ€Šâ€”â€Šand futureÂ you.</p><h3>ğŸš¢ Deployment &amp; DevOpsÂ Ready</h3><p>You can ship this immediately:</p><ul><li>Docker &amp; DockerÂ Compose</li><li>Nginx production configuration</li><li>GitHub ActionsÂ CI/CD</li><li>Environment-based configuration</li><li>Bundle analysisÂ included</li></ul><p>From local development to production with zero missingÂ pieces.</p><h3>ğŸ“ Scalable Project Structure</h3><p>Feature-based architecture keeps the codebase clean as itÂ grows:</p><pre>src/<br />â”œâ”€â”€ components/   # Reusable UI &amp; layout<br />â”œâ”€â”€ features/     # Feature modules<br />â”œâ”€â”€ pages/        # Route pages<br />â”œâ”€â”€ hooks/        # Custom hooks<br />â”œâ”€â”€ stores/       # Zustand stores<br />â”œâ”€â”€ services/     # API layer<br />â”œâ”€â”€ utils/        # Shared helpers<br />â”œâ”€â”€ config/       # App config &amp; i18n<br />â””â”€â”€ tests/        # Test utilities</pre><p>No dumping everything into /components.<br /> No refactors at 10kÂ lines.</p><h3>ğŸŒ Internationalization Included</h3><p>Global apps are the norm inÂ 2026.</p><ul><li>react-i18next</li><li>Language detection</li><li>Example translations</li></ul><p>Multi-language support from dayÂ one.</p><h3>âš¡ Performance byÂ Default</h3><ul><li>Route-based code splitting</li><li>Tree-shaking enabled</li><li>React QueryÂ caching</li><li>Minimal re-renders viaÂ Zustand</li><li>Optimized production builds</li></ul><p><strong>Typical production bundle:</strong> ~150â€“200 KB (gzipped)<br /> <strong>Lighthouse score:</strong>Â 95+</p><h3>ğŸ†š How It Compares (2026Â Edition)</h3><pre>| Feature             | This Boilerplate | Create React App | Vite Default  | Next.js    |<br />|---------------------|------------------|------------------|---------------|------------|<br />| React 19 Ready      | âœ… Yes           | âŒ No            | âš ï¸ Partial   | âœ… Yes     |<br />| TypeScript (Strict) | âœ… Enforced      | âš ï¸ Basic         | âš ï¸ Basic     | âœ… Yes     |<br />| State Management    | âœ… Zustand + RQ  | âŒ None          | âŒ None      | âŒ None    |<br />| Testing Setup       | âœ… Complete      | âš ï¸ Minimal       | âŒ None      | âš ï¸ Partial |<br />| Code Quality Tools  | âœ… Full Suite    | âš ï¸ ESLint only   | âŒ None      | âš ï¸ Partial |<br />| Docker Ready        | âœ… Yes           | âŒ No            | âŒ No        | âŒ No      |<br />| CI/CD Included      | âœ… Yes           | âŒ No            | âŒ No        | âŒ No      |<br />| Production Focus    | âœ… Yes           | âŒ Deprecated    | âŒ No        | âœ… Yes     |<br />| Learning Curve      | âœ… Moderate      | âœ… Easy          | âœ… Easy      | âš ï¸ Higher |</pre><p><strong>Why not Next.js?</strong><br /> This boilerplate is intentionally <strong>frontend-only</strong>. If you donâ€™t need SSR/SSG, this setup stays lighter, faster, andÂ simpler.</p><h3>âŒ Whatâ€™s Not Included (ByÂ Design)</h3><p>To keep things lean and flexible:</p><ul><li>No authentication providerÂ lock-in</li><li>No backend assumptions</li><li>No heavy UI component framework</li><li>No CMSÂ coupling</li></ul><p>You add only what your product actuallyÂ needs.</p><h3>ğŸ§‘â€ğŸ’» Who This IsÂ For</h3><p>âœ… Perfect ifÂ you:</p><ul><li>Start new React projectsÂ often</li><li>Build SaaS apps, dashboards, or internalÂ tools</li><li>Want consistency acrossÂ teams</li><li>Are tired of re-configuring the sameÂ stack</li></ul><p>âš ï¸ Consider alternatives if youÂ need:</p><ul><li>SSR / SSG â†’ Next.js /Â Remix</li><li>Mobile apps â†’ ReactÂ Native</li><li>Full-stack framework</li></ul><h3>ğŸš€ GettingÂ Started</h3><pre>pnpm install<br />cp .env.example .env<br />pnpm dev</pre><p>Thatâ€™s it. Youâ€™re coding immediately.</p><h3>ğŸ FinalÂ Thoughts</h3><p>This boilerplate reflects <strong>how serious React apps are built inÂ 2026</strong>:</p><ul><li>Modern tools</li><li>Proven patterns</li><li>No unnecessary complexity</li></ul><p>If youâ€™ve everÂ thought:</p><blockquote><em>â€œI just want to start building, not configuring.â€</em></blockquote><p>This is forÂ you.</p><h3>ğŸ”— GitHub Repository</h3><p>ğŸ‘‰ <a href=\"https://github.com/abhigdrv/react-boilerplate\">https://github.com/abhigdrv/react-boilerplate</a></p><p>If it helpsÂ you:</p><ul><li>â­ StarÂ it</li><li>ğŸ´ ForkÂ it</li><li>ğŸ¤ Contribute</li><li>ğŸ“¢ Share it with yourÂ team</li></ul><p><strong>Built for the React community. Ready for 2026.Â ğŸš€</strong></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=32a407b57b69\" width=\"1\" /><hr /><p><a href=\"https://blog.devgenius.io/a-production-ready-react-boilerplate-for-2026-that-you-can-actually-scale-32a407b57b69\">ğŸš€ A Production-Ready React Boilerplate for 2026 (That You Can Actually Scale)</a> was originally published in <a href=\"https://blog.devgenius.io\">Dev Genius</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Azure IAM (Identity and Access Management): The Ultimate Guide",
      "url": "https://blog.devgenius.io/azure-iam-identity-and-access-management-the-ultimate-guide-ecf77b4566ee?source=rss----4e2c1156667e---4",
      "source": "medium",
      "published_at": "2026-02-10T07:52:10",
      "external_id": null,
      "tags": [
        "dev-genius",
        "design",
        "software-development",
        "devops",
        "software-engineering",
        "azure"
      ],
      "content_length": 515,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/azure-iam-identity-and-access-management-the-ultimate-guide-ecf77b4566ee?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/662/1*uaaL1fClGYlJa4WO-IXu6Q.png\" width=\"662\" /></a></p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/azure-iam-identity-and-access-management-the-ultimate-guide-ecf77b4566ee?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius ",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.devgenius.io/azure-iam-identity-and-access-management-the-ultimate-guide-ecf77b4566ee?source=rss----4e2c1156667e---4\"><img src=\"https://cdn-images-1.medium.com/max/662/1*uaaL1fClGYlJa4WO-IXu6Q.png\" width=\"662\" /></a></p><p class=\"medium-feed-link\"><a href=\"https://blog.devgenius.io/azure-iam-identity-and-access-management-the-ultimate-guide-ecf77b4566ee?source=rss----4e2c1156667e---4\">Continue reading on Dev Genius Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "How To Optimize EF Core Query Performance With Compiled Queries",
      "url": "https://blog.stackademic.com/how-to-optimize-ef-core-query-performance-with-compiled-queries-07a303d7d722?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-10T06:03:08",
      "external_id": null,
      "tags": [
        "stackademic",
        "programming",
        "csharp",
        "software-development",
        "software-engineering",
        "dotnet"
      ],
      "content_length": 615,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/how-to-optimize-ef-core-query-performance-with-compiled-queries-07a303d7d722?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*6YI0qXuiqMxwMgSYuWHM7A.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">A Deep Dive into Sub-Millisecond Query Execution</p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/how-to-optimize-ef-core-query-performance",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/how-to-optimize-ef-core-query-performance-with-compiled-queries-07a303d7d722?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*6YI0qXuiqMxwMgSYuWHM7A.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">A Deep Dive into Sub-Millisecond Query Execution</p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/how-to-optimize-ef-core-query-performance-with-compiled-queries-07a303d7d722?source=rss----d1baaa8417a4---4\">Continue reading on Stackademic Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "The Complete Database Scaling Playbook: From 1 to 10,000 Queries Per Second",
      "url": "https://blog.stackademic.com/the-complete-database-scaling-playbook-from-1-to-10-000-queries-per-second-29dbbf659a66?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T16:41:51",
      "external_id": null,
      "tags": [
        "stackademic",
        "scala",
        "sql-queries",
        "database",
        "data",
        "sql"
      ],
      "content_length": 724,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/the-complete-database-scaling-playbook-from-1-to-10-000-queries-per-second-29dbbf659a66?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*tZL4vpJ38BGlYfme5gxwSA.png\" width=\"2816\" /></a></p><p class=\"medium-feed-snippet\">How to architect your database infrastructure for massive scale without breaking your application&#x200a;&#x2014;&#x200a;or your budget</p><p class=",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/the-complete-database-scaling-playbook-from-1-to-10-000-queries-per-second-29dbbf659a66?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*tZL4vpJ38BGlYfme5gxwSA.png\" width=\"2816\" /></a></p><p class=\"medium-feed-snippet\">How to architect your database infrastructure for massive scale without breaking your application&#x200a;&#x2014;&#x200a;or your budget</p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/the-complete-database-scaling-playbook-from-1-to-10-000-queries-per-second-29dbbf659a66?source=rss----d1baaa8417a4---4\">Continue reading on Stackademic Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Google AdMob in Flutter: A Complete Implementation Guide",
      "url": "https://blog.stackademic.com/google-admob-in-flutter-a-complete-implementation-guide-806b5bd297fd?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:30:13",
      "external_id": null,
      "tags": [
        "stackademic",
        "flutter-app-development",
        "flutter-web",
        "flutter-admob",
        "google-admob",
        "flutter"
      ],
      "content_length": 516,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/google-admob-in-flutter-a-complete-implementation-guide-806b5bd297fd?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*cckza2-U7pcaplDRd0Irmw.png\" width=\"1536\" /></a></p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/google-admob-in-flutter-a-complete-implementation-guide-806b5bd297fd?source=rss----d1baaa8417a4---4\">Continue reading on Stackademic",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/google-admob-in-flutter-a-complete-implementation-guide-806b5bd297fd?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*cckza2-U7pcaplDRd0Irmw.png\" width=\"1536\" /></a></p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/google-admob-in-flutter-a-complete-implementation-guide-806b5bd297fd?source=rss----d1baaa8417a4---4\">Continue reading on Stackademic Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "The Autonomous Company, Part 08/20: The Sales Department, When AI Starts Closing Its Own Deals",
      "url": "https://blog.stackademic.com/the-autonomous-company-part-08-20-the-sales-department-when-ai-starts-closing-its-own-deals-d92c0d7ee341?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:30:07",
      "external_id": null,
      "tags": [
        "stackademic",
        "automation",
        "productivity",
        "artificial-intelligence",
        "generative-ai-tools",
        "ai"
      ],
      "content_length": 726,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/the-autonomous-company-part-08-20-the-sales-department-when-ai-starts-closing-its-own-deals-d92c0d7ee341?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*ypH-atIuw3uxIGn-7RJdbA.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Turning attention into revenue with AI agents, intelligent sales funnels, and fully automated deal flow</p><p class=\"medium-feed-li",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/the-autonomous-company-part-08-20-the-sales-department-when-ai-starts-closing-its-own-deals-d92c0d7ee341?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*ypH-atIuw3uxIGn-7RJdbA.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">Turning attention into revenue with AI agents, intelligent sales funnels, and fully automated deal flow</p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/the-autonomous-company-part-08-20-the-sales-department-when-ai-starts-closing-its-own-deals-d92c0d7ee341?source=rss----d1baaa8417a4---4\">Continue reading on Stackademic Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Why Beginners Should Start Ethical Hacking with Python",
      "url": "https://blog.stackademic.com/why-beginners-should-start-ethical-hacking-with-python-f0490d20a079?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:29:56",
      "external_id": null,
      "tags": [
        "stackademic",
        "technology",
        "data-science",
        "python",
        "programming",
        "software-development"
      ],
      "content_length": 620,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/why-beginners-should-start-ethical-hacking-with-python-f0490d20a079?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*51bT5YbaM0we31fkT0bHRQ.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">The problem-first path to learning security skills that actually matter</p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/why-beginners-should-start-",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/why-beginners-should-start-ethical-hacking-with-python-f0490d20a079?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1024/1*51bT5YbaM0we31fkT0bHRQ.png\" width=\"1024\" /></a></p><p class=\"medium-feed-snippet\">The problem-first path to learning security skills that actually matter</p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/why-beginners-should-start-ethical-hacking-with-python-f0490d20a079?source=rss----d1baaa8417a4---4\">Continue reading on Stackademic Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Contrast checks for PDF",
      "url": "https://blog.stackademic.com/contrast-checks-for-pdf-2f544e7295d3?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:29:24",
      "external_id": null,
      "tags": [
        "stackademic",
        "ai",
        "software-development",
        "pdf",
        "big-data",
        "technology"
      ],
      "content_length": 4044,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/558/1*nECLGWJvWTM1Q5sQps-ihQ.png\" /><figcaption>Contrast checks</figcaption></figure><p>Digital content, like PDF files, has to follow certain color contrast ratios so that people who have trouble seeing or understanding colors can still read it. WCAG includes specific requirements to ensure sufficient color contrast forÂ text.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EXM5tYyrQeMDxEu4k8zOJA.png\" /><figcapti",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/558/1*nECLGWJvWTM1Q5sQps-ihQ.png\" /><figcaption>Contrast checks</figcaption></figure><p>Digital content, like PDF files, has to follow certain color contrast ratios so that people who have trouble seeing or understanding colors can still read it. WCAG includes specific requirements to ensure sufficient color contrast forÂ text.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EXM5tYyrQeMDxEu4k8zOJA.png\" /><figcaption>WCAG Recommendations</figcaption></figure><p>A key difference between <strong>WCAG</strong> and <strong>PDF/UA</strong> is that WCAG includes contrast requirements, which are generally not addressed by PDF/UA standards but remain essential for ensuring PDF accessibility.</p><p><a href=\"https://pdf4wcag.com/validate/new-job/settings\"><strong>PDF4WCAG</strong></a> comprises these contrast checks into both its WCAG 2.2 Machine and Human profiles and implements full support for all PDF color models when computing the color contrast.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/549/1*tDavbzZ7_tijA7Fs-_xr6A.png\" /><figcaption>Contrast errors</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/795/1*lyCFf2Id_Gkjgic4112UMw.png\" /><figcaption>Validation</figcaption></figure><blockquote><em>The </em><a href=\"https://www.w3.org/WAI/WCAG22/Understanding/contrast-minimum.html\"><em>contrast ratio of 4.5:1 </em></a><em>was set to support users with vision loss equivalent to 20/40 vision. The contrast ratio of 7:1 was established to support users with vision loss equivalent to 20/80Â vision.</em></blockquote><p>While these requirements are well established for web content, applying contrast checks to PDF documents is significantly moreÂ complex.</p><p><strong>Expanded Color Models:</strong> PDFs support complex color spaces (<strong>CMYK, Lab, Indexed</strong>, <strong>Separation</strong>, etc.) that require conversion to a perceptually uniform space like<strong> CIELAB</strong> for accurate contrast calculation.</p><p><strong>Device-Dependent Colors: </strong>Colors defined as <strong>DeviceRGB, DeviceCMYK, or DeviceGray</strong> depend on the output deviceâ€™s calibration, making their rendered appearanceÂ , and therefore contrastÂ , variable.</p><p><strong>Complex Backgrounds: </strong>PDF text is often placed over <strong>gradients, images, patterns, or multi-colored backgrounds</strong>, making it impossible to isolate a single background color for a standard contrastÂ check.</p><p>Accurate WCAG contrast checks require resolving complex PDF color spaces, determining the effective background behind text, and accounting for differences in how PDF viewers renderÂ colors.</p><p>As a result, <strong>color contrast checks</strong> in PDF are inherently less predictable and more challenging than in web environments.</p><p>PDF accessibility checker <strong>PDF4WCAG</strong> performs contrast analysis by interpreting the full PDF rendering context rather than relying on simplified colour assumptions.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/712/1*BQlWFZpH05c36lZy2aMmEQ.png\" /><figcaption>Errors</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b9QsH6-apPvtOVJ9zRnrSA.png\" /><figcaption>Validation</figcaption></figure><p>To sum it up, <strong>PDF4WCAG</strong> includes full coverage of so-called Machine checks including WCAG color contrast requirements. Yet, full WCAG compliance requires manual review alongside automated checks, especially for complexÂ cases.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2f544e7295d3\" width=\"1\" /><hr /><p><a href=\"https://blog.stackademic.com/contrast-checks-for-pdf-2f544e7295d3\">Contrast checks for PDF</a> was originally published in <a href=\"https://blog.stackademic.com\">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Letâ€™s Be Frank: How I Cleared My First AWS SAA Certification on the First Attempt",
      "url": "https://blog.stackademic.com/lets-be-frank-how-i-cleared-my-first-aws-saa-certification-on-the-first-attempt-1e10ec64b8ec?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:29:15",
      "external_id": null,
      "tags": [
        "stackademic",
        "aws-solutions-architect",
        "aws-exam-preparation",
        "aws-certification",
        "aws-certified",
        "aws"
      ],
      "content_length": 6232,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*f6aFjUZjMpb8rCb-hus_bQ.png\" /><figcaption><a href=\"https://aws.amazon.com/blogs/architecture/tag/solutions-architect/\">solutions architect | AWS Architecture Blog</a></figcaption></figure><p>People often think AWS certifications are toughÂ , but are theyÂ really?</p><p>The answer is <strong>yes andÂ no</strong>.</p><p>Itâ€™s mostly about <strong>mindset</strong>. Many people believe that preparing a few questions or memorizing previo",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*f6aFjUZjMpb8rCb-hus_bQ.png\" /><figcaption><a href=\"https://aws.amazon.com/blogs/architecture/tag/solutions-architect/\">solutions architect | AWS Architecture Blog</a></figcaption></figure><p>People often think AWS certifications are toughÂ , but are theyÂ really?</p><p>The answer is <strong>yes andÂ no</strong>.</p><p>Itâ€™s mostly about <strong>mindset</strong>. Many people believe that preparing a few questions or memorizing previously asked questions will help them pass the exam.<br />But thatâ€™s not how it actuallyÂ works.</p><p>The exam isnâ€™t about remembering questionsÂ , itâ€™s about <strong>how you think as a Solutions Architect</strong>. Thatâ€™s what AWS truly caresÂ about.</p><h3>My Story</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/300/0*DKyki9Q4dQUD-kYy.gif\" /></figure><p>When I started preparing for the exam, it was definitely a challenge. Why?</p><p>I was previously working as a <strong>Software Engineer</strong>, mainly on <strong>Android</strong>, and for personal projects, I was more like a <strong>full-stack Android developer</strong>.</p><p>The usual first step for most software engineers moving to cloud is <strong>AWS Cloud Practitioner</strong>. Thatâ€™s not wrongÂ , itâ€™s a valid path.<br /> But I took a risk and skippedÂ it.</p><p>I already had some familiarity with AWS services, so I committed myself directly to SAA.<br /> It was a riskÂ , but the result? <strong>I madeÂ it.</strong></p><blockquote>Tough roads bring hugeÂ rewards.</blockquote><p>I had just <strong>one month</strong> for preparation, and with a regular office job, it was evenÂ tougher.</p><p>I know youâ€™re looking for resourcesÂ , and Iâ€™ll share themÂ , but my advice and approach will help you even more during preparation. Iâ€™m damn sure aboutÂ that.</p><h3>The Secret which I followed no oneÂ taught</h3><h4>1. Tonâ€™s ofÂ ChatGPT</h4><p>Finding scenario-based questions is toughÂ , but you already have a superpower in yourÂ hands.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/498/0*VfwtYFxbO4jRQt4t.gif\" /></figure><p>What you canÂ do:</p><ul><li>Exam simulations</li><li>Verbal communication (my personal favoriteÂ , answering instantly helps you brainstorm during real exam scenarios)</li><li>Cross-questioning between multipleÂ services</li><li>Focus more on <strong>how</strong> andÂ <strong>why</strong></li></ul><h4>2. Revision</h4><p>While preparing, I maintained a diary with points that felt important toÂ me.</p><p>If there were concepts or questions I thought I might forget, I wrote them down. Before every practice session, I reviewed thoseÂ notes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/833/1*1hkpkwEw9a01pfxwQjbdyg.jpeg\" /></figure><p>It reallyÂ works.</p><p>Not everything was detailedÂ , some were just bullet pointsÂ , but they helped me clear concepts and dive deeper during scenario-based questions.</p><h4>3. Being Consistent</h4><p>Showing up every day matters more than anything.</p><p>The stress of the upcoming exam wonâ€™t even let you sleep properly.<br /> Build a fixed scheduleÂ , for example, mine was <strong>9:30 PM to 12:30 AM</strong> everyÂ night.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/498/0*NrOwMP6HA3s0vL_Y.gif\" /></figure><p>Consistency beats intensity.</p><h4>4. If Confused, Practice and Cross-Question (ChatGPT)</h4><p>Hands-on practice is the key to understanding AWS servicesÂ deeply.</p><p>Once you understand a service practically, youâ€™ll never forget itÂ , at least until AWS makes a major change in itÂ ğŸ˜„</p><h4>5. Know Why Your Option Is Right and Why Others Are Wrong (Personal Favorite)</h4><p>This is extremely important.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/498/0*R2p_ng_rykQU0iwB.gif\" /></figure><p>This exam is not like school exams where you memorize options and mark answers. It tests whether you actually <strong>think like a Solutions Architect</strong>.</p><p>Even if your answers are correct, donâ€™t just move on.<br /> Ask yourself:</p><ul><li>Why is thisÂ correct?</li><li>Why are the other optionsÂ wrong?</li></ul><p>Thatâ€™s where real learningÂ happens.</p><h3>Letâ€™s Do some resourceÂ sharing</h3><p>Before I share resources, one small request:<br /><strong>Follow this account on Medium</strong>Â , more valuable articles are on theÂ way.</p><p><a href=\"https://hemant-aws-devops.medium.com/\">Hemant Kumar Prajapati - Medium</a></p><p>Hereâ€™s one resource that genuinely helpedÂ me:</p><p><strong>AWS Certified Solutions ArchitectÂ , Associate Exam Notes</strong><br />This is an open website dedicated purely to the SAA exam, with deep-dived yet short notes. I used it daily for revision.</p><p><a href=\"https://awsfirstcloudjourney.github.io/\">AWS Certified Solutions Architect - Associate Exam Notes</a></p><p>You can also practice exam questions from any free source available onlineÂ , whichever suits youÂ best.</p><h3>Conclusion</h3><p>You might think, <em>â€œHemant just shared a single link and thatâ€™s it.â€</em><br />But honestly, thatâ€™s intentional.</p><p>You donâ€™t need to juggle between dozens of resources.<br />A few <strong>good resources</strong>, combined with the <strong>right techniques</strong>, are more than enough to ace theÂ exam.</p><p>These strategies are <strong>tested and proven</strong>Â , at least forÂ me.</p><p>Now itâ€™s your turn to showÂ up.</p><p>You may have a different strategyÂ , and thatâ€™s perfectly fineÂ too.</p><p><a href=\"https://www.credly.com/badges/d4628eb3-326b-428e-b56d-5cdb3d7089f3\">AWS Certified Solutions Architect - Associate was issued by Amazon Web Services Training and Certification to Hemant Prajapati.</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1e10ec64b8ec\" width=\"1\" /><hr /><p><a href=\"https://blog.stackademic.com/lets-be-frank-how-i-cleared-my-first-aws-saa-certification-on-the-first-attempt-1e10ec64b8ec\">Letâ€™s Be Frank: How I Cleared My First AWS SAA Certification on the First Attempt</a> was originally published in <a href=\"https://blog.stackademic.com\">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "RTX delays signal a shift as Nvidia reprices gaming GPUs around AI",
      "url": "https://blog.stackademic.com/rtx-delays-signal-a-shift-as-nvidia-reprices-gaming-gpus-around-ai-cf080f434ccc?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:29:09",
      "external_id": null,
      "tags": [
        "stackademic",
        "technology",
        "tech",
        "information-technology"
      ],
      "content_length": 6176,
      "content_preview": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*DBSCBb4wFSRskR26.png\" /></figure><h3>Fun Fact</h3><blockquote><em>I remember when GPU launches felt inevitable. You didnâ€™t ask </em>if<em> a new generation was coming, only </em>when<em>. Board partners prepared months in advance, leaks flowed like clockwork, and retailers planned clearance sales almost ritualistically. The first time a â€œSuperâ€ refresh slipped without explanation, it felt like a fluke. The second time, it felt l",
      "content_full": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*DBSCBb4wFSRskR26.png\" /></figure><h3>Fun Fact</h3><blockquote><em>I remember when GPU launches felt inevitable. You didnâ€™t ask </em>if<em> a new generation was coming, only </em>when<em>. Board partners prepared months in advance, leaks flowed like clockwork, and retailers planned clearance sales almost ritualistically. The first time a â€œSuperâ€ refresh slipped without explanation, it felt like a fluke. The second time, it felt like logistics. This time, it feels intentional.</em></blockquote><p>Nvidia delays the RTX 50-series Super refresh, and for the first time in years, the companyâ€™s gaming GPU roadmap no longer feels like something you can predict with confidence. For more than a decade, <a href=\"https://www.nvidia.com\"><strong>Nvidia</strong></a> trained the PC market to expect a steady cadence: new architectures on schedule, mid-cycle refreshes on cue, and a reliable rhythm that shaped everything from consumer upgrades to board-partner planning.</p><p>That rhythm is breakingÂ ,Â quietly.</p><p>According to multiple industry reports, the RTX 50-series Super refresh has slipped toward late 2026 or early 2027. On its own, that wouldnâ€™t raise alarms. Inventory cycles fluctuate. Demand shifts. But context matters. And the context this time points to something deeper than logistics.</p><p>This isnâ€™t an engineering problem. Itâ€™s a prioritization problem.</p><h3>A refresh that keepsÂ drifting</h3><p>Mid-cycle â€œSuperâ€ refreshes have always served a clear purpose: modest performance bumps, small configuration tweaks, and a way to keep shelves moving without resetting the entire lineup. The RTX 50-series Super was expected to follow that playbook.</p><p>Instead, high-end GPU sales cooled faster than anticipated. Fewer AAA titles are pushing hardware limits. Upgrade cycles have stretched. Economic pressure hasnâ€™t disappeared. Retailers are holding more stock, and board partners are wary of launching new SKUs before existing onesÂ clear.</p><p>In past cycles, Nvidia pushed through refreshes anyway to maintain momentum. This time, momentum doesnâ€™t seem urgent. MarginÂ does.</p><h3>The bigger silence around RTXÂ 60</h3><p>More telling than the refresh delay is the uncertainty surrounding the next full gaming generation. The RTX 60-seriesÂ , the successor to BlackwellÂ , should, by historical standards, already be leaving fingerprints. Early silicon rumors. Manufacturing chatter. PartnerÂ leaks.</p><p>Instead, thereâ€™s an unusualÂ quiet.</p><p>That silence has fueled speculation that the RTX 60-series may not arrive in 2027 at all. If true, it would break Nvidiaâ€™s long-standing two-year cadence. But cadence only matters when the business depends onÂ it.</p><p>Right now, itÂ doesnâ€™t.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*10T9gRJmMep4hdV8.png\" /></figure><h3>AI didnâ€™t just disrupt gamingÂ , it reprioritized it</h3><p>Nvidiaâ€™s data-center business has eclipsed gaming by a wide margin. AI accelerators generate margins consumer GPUs simply canâ€™t match. Demand continues to exceed supply, forcing hard decisions about engineering talent, wafer allocation, and manufacturing priority.</p><p>From Nvidiaâ€™s perspective, every resource diverted to gaming is a resource not feeding hyperscalers, cloud providers, or enterprise AI deployments. The math is straightforward.</p><p>This is the uncomfortable shift the market is still adjusting to: gaming GPUs are no longer Nvidiaâ€™s strategic core. Theyâ€™re importantÂ , culturally and for brand identityÂ , but theyâ€™re no longer the engine driving roadmap decisions.</p><p>Seen through that lens, the delays stop looking like mistakes. They start looking like consequences.</p><h3>A market withoutÂ pressure</h3><p>Competition usually accelerates innovation. Right now, that pressure isÂ muted.</p><p>is reportedly taking a cautious approach with its next RDNA generation, while continues to struggle to make its discrete GPUs a serious threat at the high end. Without a credible near-term rival forcing Nvidiaâ€™s hand, thereâ€™s little incentive to rush a lower-margin gamingÂ roadmap.</p><p>Board partners like and feel the effects downstream. Fewer launches mean fewer opportunities to refresh premium systems, fewer reasons for consumers to upgrade, and longer stretches of stagnation in the high-end PCÂ market.</p><h3>A longer â€œnowâ€ for PCÂ gamers</h3><p>Thereâ€™s an irony here. Gamers complained for years that upgrade cycles were too aggressive, that hardware became obsolete too quickly. Now, relevance is stretchingÂ , but not because the industry listened.</p><p>Your RTX 50-series card may remain â€œcurrentâ€ longer than expected. Not because Nvidia optimized for gamers, but because Nvidia optimized elsewhere.</p><p>That distinction matters.</p><h3>This isnâ€™t a failureÂ , itâ€™s a reordering</h3><p>Itâ€™s tempting to frame these delays as missteps or indecision. They arenâ€™t. Theyâ€™re signals. Nvidia is behaving exactly like a company that has found a more lucrative center of gravity and is reorganizing aroundÂ it.</p><p>Gaming GPUs arenâ€™t disappearing. But theyâ€™re no longer setting theÂ pace.</p><p>And once you see that, the roadmap doesnâ€™t look broken at all. It looksÂ honest.</p><h3>Sources</h3><p>VideoCardz<br />Tomâ€™s Hardware<br />Digitimes</p><p><strong>Originally published at </strong><a href=\"https://techfusiondaily.com/\"><strong>https://techfusiondaily.com</strong></a></p><p><em>Originally published at </em><a href=\"https://techfusiondaily.com/the-rtx-delays-arent-a-problem-theyre-a-signal/\"><em>https://techfusiondaily.com</em></a><em> on February 6,Â 2026.</em></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cf080f434ccc\" width=\"1\" /><hr /><p><a href=\"https://blog.stackademic.com/rtx-delays-signal-a-shift-as-nvidia-reprices-gaming-gpus-around-ai-cf080f434ccc\">RTX delays signal a shift as Nvidia reprices gaming GPUs around AI</a> was originally published in <a href=\"https://blog.stackademic.com\">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Python Just Killed The â€œManual Indexingâ€ Habit with 1 Simple Symbol.",
      "url": "https://blog.stackademic.com/python-just-killed-the-manual-indexing-habit-with-1-simple-symbol-4e596d093686?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:28:47",
      "external_id": null,
      "tags": [
        "stackademic",
        "software-development",
        "coding",
        "programming",
        "python",
        "data-science"
      ],
      "content_length": 688,
      "content_preview": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/python-just-killed-the-manual-indexing-habit-with-1-simple-symbol-4e596d093686?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*WpkcG6l5YjiIgxRE-4R24g.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">How a hidden operator is changing the way developers extract data, and the clean code logic that is often overlooked.</p><p class=\"medium-feed-link\"><a href=",
      "content_full": "<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://blog.stackademic.com/python-just-killed-the-manual-indexing-habit-with-1-simple-symbol-4e596d093686?source=rss----d1baaa8417a4---4\"><img src=\"https://cdn-images-1.medium.com/max/1536/1*WpkcG6l5YjiIgxRE-4R24g.png\" width=\"1536\" /></a></p><p class=\"medium-feed-snippet\">How a hidden operator is changing the way developers extract data, and the clean code logic that is often overlooked.</p><p class=\"medium-feed-link\"><a href=\"https://blog.stackademic.com/python-just-killed-the-manual-indexing-habit-with-1-simple-symbol-4e596d093686?source=rss----d1baaa8417a4---4\">Continue reading on Stackademic Â»</a></p></div>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Self-Study vs Expensive Data Analytics Bootcamps: What Actually Works?",
      "url": "https://blog.stackademic.com/self-study-vs-expensive-data-analytics-bootcamps-what-actually-works-11c64dbda838?source=rss----d1baaa8417a4---4",
      "source": "medium",
      "published_at": "2026-02-09T06:28:46",
      "external_id": null,
      "tags": [
        "stackademic",
        "data-analytics",
        "bootcamp-scam",
        "job-ready",
        "data-analysis",
        "self-study"
      ],
      "content_length": 5327,
      "content_preview": "<h4>Are you paying for skillsÂ , or just for marketing?</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*VcGuVX0i1kdEqYyoSGWoZQ.jpeg\" /><figcaption>Image from <a href=\"https://pin.it/7K6EqP7E1\">Pinterest</a></figcaption></figure><p>Every aspiring data analyst eventually faces this question:<br /><strong><em>Should I self-study SQL and analytics, or join an expensive bootcamp that promises aÂ job?</em></strong></p><p>Iâ€™ve been there.<br />And yesÂ , <strong><em>I chose self-stu",
      "content_full": "<h4>Are you paying for skillsÂ , or just for marketing?</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*VcGuVX0i1kdEqYyoSGWoZQ.jpeg\" /><figcaption>Image from <a href=\"https://pin.it/7K6EqP7E1\">Pinterest</a></figcaption></figure><p>Every aspiring data analyst eventually faces this question:<br /><strong><em>Should I self-study SQL and analytics, or join an expensive bootcamp that promises aÂ job?</em></strong></p><p>Iâ€™ve been there.<br />And yesÂ , <strong><em>I chose self-study.</em></strong></p><h3>What Bootcamps SellÂ You</h3><p>Most high-priced bootcamps advertise:</p><ul><li>â€œJob-ready in 3â€“6Â monthsâ€</li><li>â€œPlacement assistanceâ€</li><li>â€œLive industry projectsâ€</li><li>â€œMentors from top companiesâ€</li></ul><p>On paper, it sounds perfect.<br />In reality? NotÂ always.</p><h3>How Many Bootcamps Mislead Candidates</h3><p>Not all bootcamps are scamsÂ , butÂ many:</p><ul><li>Teach <strong><em>basic SQL, Excel, and Power BI</em>, which</strong> you can learnÂ online</li><li>Use <strong><em>fear-based marketing</em></strong> (â€œlast seatsâ€, â€œbatch closingâ€)</li><li>Count <strong><em>internships or unpaid roles</em></strong> as placements</li><li>Focus more on <strong><em>selling dreams</em></strong> than buildingÂ skills</li></ul><p>People end up paying <strong><em>â‚¹1â€“3 lakhs</em> </strong>for content that doesnâ€™t build <strong><em>interview confidence or real thinking</em></strong><em>.</em></p><h3>My Reality With Self-Study</h3><p>I didnâ€™t join any expensive bootcamp.<br />I <strong><em>self-studied SQL</em></strong>, practiced consistently, built projects, failed interviews, improvedÂ , and today, Iâ€™m doing <strong><em>well professionally</em></strong>.</p><p>What actually helpedÂ me:</p><ul><li>Solving <strong><em>real SQL interview questions</em></strong></li><li>Understanding <em>why</em> a query works, not just writingÂ it</li><li>Practicing again and again instead of rushing for certificates</li></ul><p>Self-study worksÂ , <strong><em>when done with the right structure</em></strong><em>.</em></p><h3>Why Self-Study OftenÂ Wins</h3><p>With self-study, you:</p><ul><li>Control yourÂ pace</li><li>Go deeper into SQL logic (joins, subqueries, window functions)</li><li>Spend <strong><em>10â€“20x lessÂ money</em></strong></li><li>Build confidence, not dependency on â€œmentorsâ€</li></ul><blockquote>The only thing you need?<br /><strong>Good resources + discipline</strong></blockquote><h3>Resources for People Who Believe in Self-Study</h3><p>Because I struggled to find <strong>interview-focused SQL practice</strong>, I created my own learning materialÂ , the same kind of questions I wished I hadÂ earlier.</p><p>Thatâ€™s why I nowÂ share:</p><ul><li><a href=\"https://anugya1998.gumroad.com/l/dataanalyticspath\"><strong><em>Data Analytics Self-Study LearningÂ Path</em></strong></a></li><li><strong>Practice-based problems</strong></li><li>Clear explanations focused on <em>thinking</em>, not memorizing</li></ul><p>These resources are for people whoÂ believe:</p><blockquote><em>â€œI donâ€™t need hype. I needÂ skills.â€</em></blockquote><h3>Final Thought</h3><p>No bootcampÂ , expensive or notÂ , canÂ replace:</p><ul><li>Daily SQLÂ practice</li><li>Clear thinking</li><li>Explaining your logic confidently</li></ul><p>Companies donâ€™t hire certificates.<br />They hire people who <strong><em>can solve problems withÂ data</em></strong><em>.</em></p><p>If you believe in self-study, youâ€™re not behind.<br />Youâ€™re just choosing the <strong><em>harder, smarterÂ path</em></strong><em>.</em></p><p>ğŸŒ¼<strong><em> Thanks for BeingÂ Here!</em></strong></p><p><em>Iâ€™m glad you took a moment to read this piece.<br />If this resonated with you, tell me in the comments:</em><br /><strong><em>Which of these three practices are you trying today?</em></strong><em><br />You can also explore my</em><strong><em> </em></strong><a href=\"https://anugya1998.gumroad.com/\"><strong><em>digital products</em></strong></a><strong><em>, </em></strong><em>crafted to inspire growth and make life a littleÂ easier.</em></p><p><strong><em>If this resonated with you, here are a few more stories related to SQL to keep the learningÂ going:</em></strong></p><p><a href=\"https://medium.com/@anugya.singhal.1998/list/sql-learning-ece4491a700e\">List: SQL Learning | Curated by Anugya Singhal | Medium</a></p><h3>About theÂ Author</h3><p><strong><em>Hey, Iâ€™m Anugya.<br />A digital creator and data storyteller focused on clarity in thinking, work, and life. I write about analytics, productivity, and mindset, making complex ideas simpler and more meaningful.</em></strong></p><p><strong><em>If this article helped, a quick ğŸ‘ goes a long way.<br />It supports my work and helps these words reach moreÂ people.</em></strong></p><p><strong><em>Letâ€™s connect and grow, thoughtfully.</em></strong></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=11c64dbda838\" width=\"1\" /><hr /><p><a href=\"https://blog.stackademic.com/self-study-vs-expensive-data-analytics-bootcamps-what-actually-works-11c64dbda838\">Self-Study vs Expensive Data Analytics Bootcamps: What Actually Works?</a> was originally published in <a href=\"https://blog.stackademic.com\">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "stars": null,
      "comments": null,
      "upvotes": null,
      "read_time": "2 min read",
      "language": null
    }
  ]
}