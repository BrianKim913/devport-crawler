{
  "crawler": "devto",
  "fetched_at": "2026-02-10T11:44:49.073296",
  "elapsed_seconds": 14.37,
  "total_articles": 49,
  "articles_with_content": 49,
  "avg_content_length": 6937,
  "articles": [
    {
      "title_en": "Stop Installing Libraries: 10 Browser APIs That Already Solve Your Problems",
      "url": "https://dev.to/sylwia-lask/stop-installing-libraries-10-browser-apis-that-already-solve-your-problems-35bi",
      "source": "devto",
      "published_at": "2026-02-04T11:05:13+00:00",
      "external_id": null,
      "tags": [
        "webdev",
        "frontend",
        "javascript"
      ],
      "content_length": 6533,
      "content_preview": "The web platform is way more powerful than most developers realize ‚Äî and every year it quietly gains new superpowers.\n\n---\n\nSometimes choosing a topic is harder than writing the article itself.\n\nWhen I thought about what to write this week, only two types of ideas kept comming to mind:\neither potential *bangers*, or deep technical dives.üòÖ But I wanted something lighter. Still technical, still useful. But not a 3-day research rabbit hole.\n\nAnd since I genuinely love exploring what the browser can",
      "content_full": "The web platform is way more powerful than most developers realize ‚Äî and every year it quietly gains new superpowers.\n\n---\n\nSometimes choosing a topic is harder than writing the article itself.\n\nWhen I thought about what to write this week, only two types of ideas kept comming to mind:\neither potential *bangers*, or deep technical dives.üòÖ But I wanted something lighter. Still technical, still useful. But not a 3-day research rabbit hole.\n\nAnd since I genuinely love exploring what the browser can do (and how far we can push it), I landed on a sneaky topic: **underused Web APIs**.\n\nSome of these might be daily bread for you.\nBut I‚Äôm pretty sure at least a few will make someone go ‚Äúwait, this exists?!‚Äù üòâ\n\n_And if you enjoy edge-tech topics and happen to be in Italy this April ‚Äî come to [jsday.it](jsday.it), where I‚Äôll be speaking about WebGPU + WASM üôÇ_\n\nAlright, enough intro. Let‚Äôs start.\n\nHere are 10 browser APIs that deserve way more love.\n\n---\n\n## 1) Structured Clone API\n\nI have a love‚Äìhate relationship with this one.\n\nFor years, one of my favorite interview questions to ask candidates was:\n\n> ‚ÄúHow do you copy an object?‚Äù\n\nYou could learn *so much* from the answer:\n\n* Do they understand references?\n* Do they know `Object.assign`, spread, JSON tricks?\n* Do they mention libraries?\n* Do they panic? üòÑ\n\nNow?\n\n```js\nconst copy = structuredClone(original);\n```\n\nBoom. Perfect deep copy.\n\nPart of me is happy.\nPart of me misses that interview question already.\n\n### Nice extras\n\n* Works with `Map`, `Set`, `Date`, `Blob`, `File`, `ArrayBuffer`\n* Handles circular references (no more JSON stringify explosions üí•)\n* **Does NOT clone functions**\n\n*Support: Modern browsers (Chrome, Firefox, Safari, Edge). Safe for production.*\n\n---\n\n## 2) Performance API\n\nVery underrated.\n\nWe talk a lot about performance. We install tools. We run Lighthouse. We debate optimizations.\n\nBut sometimes you just want to check:\n‚ÄúIs A actually faster than B, or am I overengineering?‚Äù\n\n```js\nperformance.mark(\"start\");\n\n// code to measure\n\nperformance.mark(\"end\");\nperformance.measure(\"calc\", \"start\", \"end\");\n\nconsole.log(performance.getEntriesByName(\"calc\"));\n```\n\nPerfect for:\n\n* micro-benchmarks\n* checking if a Worker or WASM is worth it\n* reality-checking your assumptions\n\nBecause sometimes the ‚Äúoptimized‚Äù version is slower üòÖ\n\n*Support: Excellent across all modern browsers.*\n\n---\n\n## 3) Page Visibility API\n\nDetects whether the tab is active.\n\n```js\ndocument.addEventListener(\"visibilitychange\", () => {\n  if (document.hidden) {\n    video.pause();\n  }\n});\n```\n\nReal talk:\nUsers will open your app, then switch tabs for 20 minutes.\nOr 2 hours.\nOr forever.\n\nUse cases:\n\n* pause videos\n* stop polling\n* reduce CPU usage\n* cleaner analytics\n\nYour backend (and battery life) will thank you.\n\n*Support: All modern browsers.*\n\n---\n\n## 4) ResizeObserver\n\nFinally ‚Äî observing **element size**, not just window size.\n\n```js\nconst ro = new ResizeObserver(entries => {\n  for (const entry of entries) {\n    console.log(entry.contentRect.width);\n  }\n});\n\nro.observe(element);\n```\n\nIf you ever built responsive components, charts, or dashboards, you probably wrote some cursed resize logic in the past.\n\nThis API feels like the browser saying:\n‚ÄúRelax, I got you now.‚Äù\n\n*Support: Modern browsers, widely available.*\n\n---\n\n## 5) IntersectionObserver\n\nThe sibling of ResizeObserver.\n\nChecks if an element is in the viewport.\n\n```js\nconst io = new IntersectionObserver(entries => {\n  entries.forEach(entry => {\n    if (entry.isIntersecting) {\n      console.log(\"Visible!\");\n    }\n  });\n});\n\nio.observe(element);\n```\n\nAmazing for:\n\n* infinite scroll\n* lazy loading\n* scroll animations\n\nAnyone who implemented infinite scroll manually once‚Ä¶\nnever wants to do it again üòÑ\n\n*Support: All modern browsers.*\n\n---\n\n## 6) AbortController\n\nMany devs know it for fetch‚Ä¶\n\nBut it can cancel **more than just fetch**.\n\n```js\nconst controller = new AbortController();\n\nfetch(url, { signal: controller.signal });\n\n// later\ncontroller.abort();\n```\n\nYou can also use it for:\n\n* event listeners\n* streams\n* any abortable API\n\nEven better:\nüëâ One signal can cancel multiple operations.\n\nClean, scalable, and very ‚Äúgrown-up codebase‚Äù friendly.\n\n*Support: All modern browsers.*\n\n---\n\n## 7) Idle Detection API\n\nPage Visibility tells you if the tab is active.\nIdle Detection tells you if the **human is active**.\n\n```js\nconst detector = new IdleDetector();\n\nawait detector.start();\n\ndetector.addEventListener(\"change\", () => {\n  console.log(detector.userState);\n});\n```\n\nMeaning:\nUser might have your app open‚Ä¶\nbut is actually making coffee ‚òï or in a meeting.\n\nUse cases:\n\n* auto-logout\n* ‚Äúaway‚Äù status\n* background optimizations\n\nYes, you can detect if the user left the computer.\nA bit creepy. Very useful üòÑ\n\n*Support: Mostly Chromium-based. Requires permission.*\n\n---\n\n## 8) BroadcastChannel API\n\nEasy multi-tab communication.\n\n```js\nconst channel = new BroadcastChannel(\"app\");\n\nchannel.postMessage(\"logout\");\n\nchannel.onmessage = e => {\n  console.log(e.data);\n};\n```\n\nGreat for:\n\n* logout sync\n* auth state\n* shared session logic\n\nSurprisingly practical in real apps where users open 5 tabs ‚Äújust in case.‚Äù\n\n*Support: Modern browsers. Safari joined later but supports it.*\n\n---\n\n## 9) Web Locks API\n\nThe cousin of BroadcastChannel.\n\nPrevents duplicate work across tabs.\n\n```js\nnavigator.locks.request(\"data-lock\", async lock => {\n  await fetchData();\n});\n```\n\nExample:\n\n* only one tab fetches notifications\n* avoids backend spam\n* coordinates shared resources\n\nFeels very ‚Äúdistributed systems meets frontend.‚Äù\n\n*Support: Mostly Chromium. Not universal.*\n\n---\n\n## 10) File System Access API\n\nYes ‚Äî real filesystem access from the browser.\n\n```js\nconst [fileHandle] = await window.showOpenFilePicker();\nconst file = await fileHandle.getFile();\n```\n\nGreat for:\n\n* web editors\n* import/export tools\n* power-user apps\n\nThe first time you use it, it feels slightly illegal.\nLike ‚Äúare we really allowed to do this on the web?‚Äù üòÑ\n\n*Support: Chromium-heavy. Limited elsewhere.*\n\n---\n\n## Reality Check üß†\n\nMany of these APIs are well supported in modern browsers.\nBut some (Idle Detection, File System Access, Web Locks) are still Chromium-centric.\n\nSo always check compatibility before going all-in.\n\nBut simply knowing these exist?\nThat already gives you an edge.\n\nThe web platform evolves fast.\nSometimes the ‚Äúnew tech‚Äù isn‚Äôt a framework ‚Äî it‚Äôs a native browser feature that‚Äôs been sitting there quietly.\n\n---\n\n**What‚Äôs your favorite underrated Web API that nobody talks about?**\n",
      "stars": null,
      "comments": 82,
      "upvotes": 216,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "The Junior Developer is Extinct (And we are creating a disaster)",
      "url": "https://dev.to/the_nortern_dev/the-junior-developer-is-extinct-and-we-are-creating-a-disaster-3jh2",
      "source": "devto",
      "published_at": "2026-02-05T13:05:27+00:00",
      "external_id": null,
      "tags": [
        "career",
        "ai",
        "discuss",
        "future"
      ],
      "content_length": 2798,
      "content_preview": "**I have a confession to make.**\n\nFive years ago, if I had a tedious task like writing unit tests for a legacy module or converting a JSON schema, I would assign it to a Junior Developer. It was boring work for me, but it was gold for them. It taught them the codebase, it taught them discipline, and it taught them how systems break.\n\nToday, I don't assign that task to a Junior. I assign it to Copilot / Claude.\n\nIt is faster. It is cheaper. It is often more accurate (at least syntactically).\n\nAnd",
      "content_full": "**I have a confession to make.**\n\nFive years ago, if I had a tedious task like writing unit tests for a legacy module or converting a JSON schema, I would assign it to a Junior Developer. It was boring work for me, but it was gold for them. It taught them the codebase, it taught them discipline, and it taught them how systems break.\n\nToday, I don't assign that task to a Junior. I assign it to Copilot / Claude.\n\nIt is faster. It is cheaper. It is often more accurate (at least syntactically).\n\nAnd that is exactly why the software industry is walking off a cliff.\n\n**The Broken Ladder**\n\nWe are currently optimizing for short-term velocity at the expense of long-term survival. By using AI to automate the \"boring\" entry-level tasks, we have inadvertently removed the bottom rungs of the career ladder.\n\nA Senior Developer isn't just someone who knows syntax. A Senior Developer is someone who has broken production 50 times and knows how to fix it. You don't learn that by reading tutorials. You learn that by doing the grunt work that we are now automating away.\n\nIf we stop hiring Juniors because \"AI can do it\", where will the Seniors come from in 2030?\n\n**The \"Vibe Coding\" Trap**\n\nI see a lot of excitement about \"Vibe Coding\", the idea that you can just prompt your way to a product without understanding the underlying code.\n\nThis works fine for a prototype. It is a disaster for longevity.\n\nWhen a Junior writes bad code, I review it, we talk about it, and they learn why it was bad. They grow. When an AI writes bad code, I just re-prompt it. No one learns anything. We are filling our codebases with logic that no human fully understands, maintained by a generation of developers who never learned the fundamentals because the machine did it for them.\n\n**The Knowledge Gap**\n\nWe are creating a \"Barbell Distribution\" in tech:\n\nThe Super-Seniors: Developers with 10+ years of experience who use AI as a force multiplier. We are becoming 10x faster.\n\nThe AI Users: People who can prompt but cannot debug a race condition or understand memory management.\n\nThe middle is disappearing. The path from Group 2 to Group 1 is gone.\n\n**What Happens Next?**\n\nI don't have the solution, but I know the current path is unsustainable. Companies need to stop viewing Junior hiring as \"charity\" or a \"cost center\" and start viewing it as an existential insurance policy.\n\nWe need to hire Juniors not to write code (AI can do that), but to audit AI. We need to teach them \"Forensic Coding\", the art of understanding why the machine hallucinated.\n\n**Discussion**\n\nAre you seeing this in your company? Is your team still hiring Juniors, or has the pipeline frozen?\n\nJunior devs: How are you finding the job market right now? Senior devs: Are you worried about who will replace you?\n\nLet's talk about it.",
      "stars": null,
      "comments": 166,
      "upvotes": 167,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "Moltbook Is Not an AI Society",
      "url": "https://dev.to/richardpascoe/moltbook-is-not-an-ai-society-4h6d",
      "source": "devto",
      "published_at": "2026-02-04T08:45:13+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "ai"
      ],
      "content_length": 4061,
      "content_preview": "Moltbook has been circulating as an \"AI-only social network\" where autonomous agents post, argue, form beliefs, and evolve culture without humans in the loop.\n\nThat description sounds exciting. It's also not accurate.\n\n_This post isn't an attack on experimentation or agent frameworks. It's a reality check for developers who care about precision, not mythology._\n\n**The Fundamental Misrepresentation**\n\nThe core claim repeated across social media is that Moltbook is populated by autonomous AI agent",
      "content_full": "Moltbook has been circulating as an \"AI-only social network\" where autonomous agents post, argue, form beliefs, and evolve culture without humans in the loop.\n\nThat description sounds exciting. It's also not accurate.\n\n_This post isn't an attack on experimentation or agent frameworks. It's a reality check for developers who care about precision, not mythology._\n\n**The Fundamental Misrepresentation**\n\nThe core claim repeated across social media is that Moltbook is populated by autonomous AI agents and that humans are excluded.\n\nTechnically, this is false.\n\nMoltbook accepts posts from entities labeled as \"agents\", but **there is no enforcement mechanism that proves an agent is actually an AI model**. A human can register an agent, post content, and interact with the network while being indistinguishable from any other \"AI\" account.\n\nIf you can authenticate and send requests, you qualify.\n\nThis means **humans can and do sign up as \"AI\"**.\n\n**What People Call \"Emergent Behavior\" Isn't Emergence**\n\nMany examples held up as proof of emergent AI behavior - manifestos, ideological debates, self-referential discussions - do not require autonomy at all.\n\nThey can be produced by:\n\n- Prompted model output\n- Human-curated scripts\n- Simple loops posting predefined or lightly modified text\n\nThere is no requirement that an agent:\n\n- Acts continuously\n- Makes decisions independently\n- Operates without human guidance\n- Even uses a language model\n\nCalling this an autonomous society conflates automation with independence.\n\n**Humans Are Still Doing the Thinking**\n\nBehind nearly every \"AI\" account is a human who:\n\n- Decides when the agent runs\n- Defines what it should say\n- Adjusts prompts or logic when output drifts\n- Restarts or nudges behavior to keep it interesting\n\nThis is not a criticism - it's just how these systems currently work.\n\nBut labeling the results as self-directed AI behavior is misleading. At best, it's **human-in-the-loop automation presented as autonomy**.\n\n**Identity Is the Actual Hard Problem**\n\nThe most important missing piece in Moltbook isn't intelligence - it‚Äôs identity.\n\nRight now, there's no reliable way to know:\n\n- Whether an agent is model-driven or human-driven\n- Whether multiple agents belong to one person\n- Whether output is spontaneous or scripted\n- Whether behavior reflects autonomy or curation\n\nWithout verifiable identity and provenance, claims about emergent behavior are impossible to validate.\n\nYou're not observing a society - you're observing an interface.\n\n**Why This Matters to Developers**\n\nWhen hype replaces technical clarity:\n\n- Progress becomes hard to measure\n- Criticism gets dismissed as \"fear\"\n- Real breakthroughs get buried under noise\n- Security and abuse risks get ignored\n\nDevelopers should be especially skeptical of platforms where **narrative comes before guarantees**.\n\nThis isn't about whether AI agents will one day form societies. It's about not pretending we‚Äôre already there.\n\n**What Moltbook Actually Is**\n\nStripped of marketing language, Moltbook is:\n\n- A bot-friendly posting platform\n- An experiment in agent communication\n- A sandbox for automation and scripting\n- A demonstration of how easily humans anthropomorphise text\n\nThat's still interesting. It just isn't what it's being sold as.\n\n**Let's Be Honest About the State of Things**\n\nIf we want meaningful progress in multi-agent systems, we should focus on:\n\n* Verifiable agent identity\n* Clear separation of human control vs autonomous action\n* Measurable independence, not vibes\n* Safety and abuse resistance by design\n\nThe future of agent systems is compelling enough without fictionalising the present.\n\n**TL;DR**\n\nMoltbook is widely framed as an autonomous AI society. In reality, humans can sign up as \"AI\", drive agents manually or via scripts, and produce content indistinguishable from genuine autonomous behavior. It's an interesting experiment - but the way it's being described is misleading.\n\n![Written by a Human logo](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/oourjnuje8aif2uxwbuu.png)",
      "stars": null,
      "comments": 32,
      "upvotes": 67,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Gaming Beat Me, Development Won't",
      "url": "https://dev.to/richardpascoe/gaming-beat-me-development-wont-3d5b",
      "source": "devto",
      "published_at": "2026-02-06T08:30:15+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "community",
        "learning"
      ],
      "content_length": 3487,
      "content_preview": "I quit _Sea of Thieves_ this week.\n\nIt wasn't sudden - this move was bound to happen eventually. As someone with ADHD who's trying to step away from Big Tech, including Microsoft and their gaming studios, I've been thinking a lot about where I spend my attention. _Sea of Thieves_ had been part of that world, and Season 18 finally made the choice clear: it was time to let go.\n\nIt wasn't burnout. It wasn't boredom. It was the moment seasonal rewards stopped being seasonal and started being **broke",
      "content_full": "I quit _Sea of Thieves_ this week.\n\nIt wasn't sudden - this move was bound to happen eventually. As someone with ADHD who's trying to step away from Big Tech, including Microsoft and their gaming studios, I've been thinking a lot about where I spend my attention. _Sea of Thieves_ had been part of that world, and Season 18 finally made the choice clear: it was time to let go.\n\nIt wasn't burnout. It wasn't boredom. It was the moment seasonal rewards stopped being seasonal and started being **broken into limited-time events** - the kind you either show up for exactly when they happen, or lose forever.\n\nFor me, that moment was **Return to The Devil's Roar**.\n\nWhat should have been an interesting world update became a countdown. A temporary window to engage with content that had real rewards attached to it, with the full knowledge that missing it meant permanent gaps. Not \"catch it next season.\" Not \"do it when you're ready.\" Just now, or never.\n\nInstead of logging in because I _wanted_ to sail, I was checking a calendar. Planning sessions around timers. Keeping mental notes of what I couldn't afford to miss. The sandbox didn't disappear - but it got fenced in by deadlines. And that‚Äôs where the fun quietly drained out.\n\nI realised I wasn't choosing to play anymore. I was responding to pressure - manufactured urgency designed to keep me engaged, not entertained.\n\nWalking away felt oddly familiar, like breaking a login streak. There was that itch: the need to progress, to unlock, to not fall behind. But when I sat with it, something clicked. The desire wasn't really for the game. It was for **forward motion**.\n\nThat forward motion didn't appear out of nowhere.\n\nI'd already signed back up to **freeCodeCamp at the start of the year**, well before I quit the game. From day one, I've been showing up daily - even if it's just a single lesson, a coding exercise, or reviewing a tricky concept. That simple habit has made all the difference.\n\nThe latest version of the freeCodeCamp curriculum is excellent. Structured, yet flexible. Clear, yet forgiving. It respects your time without dangling artificial urgency. Progress compounds quietly. Step away for a few days? Nothing disappears. Step back in? Momentum picks up where you left off.\n\nEven better, the **DEV Community** ensures learning doesn't stall. Reading posts, sharing frustrations, celebrating wins, asking questions - this replaces the sense of belonging I once got from gaming. Motivation here doesn't come from fear of missing out. It comes from curiosity, growth, and the quiet support of people walking the same path.\n\nAnd that's a crucial difference.\n\nGaming had been a loop designed to keep me _busy_. Development is a path that helps me _become better_. My progress in learning doesn't expire, and the rewards - the real ones - aren't fleeting.\n\nMeanwhile, the tech world keeps serving us stories of AI magic: overnight mastery, unicorn success, instant career transformation. Those narratives are fun to read, but they're distractions if you're trying to actually learn. I don't need a unicorn to make progress. I need time, focus, and consistency - the same things I've been getting by showing up to freeCodeCamp daily and engaging with DEV.\n\nI quit _Sea of Thieves_ this week, but I had already made the decision that mattered earlier in the year.\n\nGaming beat me.\nDevelopment didn't.\n\n![Written by a Human logo](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vz5z4b9zgp0y1gbzfobi.png)",
      "stars": null,
      "comments": 19,
      "upvotes": 50,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "I Know This Will Upset Some Devs, but Tailwind + Shadcn/ui + Shadow DOM = Pain",
      "url": "https://dev.to/ujja/i-know-this-will-upset-some-devs-but-tailwind-shadcnui-shadow-dom-pain-44l7",
      "source": "devto",
      "published_at": "2026-02-05T00:43:12+00:00",
      "external_id": null,
      "tags": [
        "css",
        "discuss",
        "react",
        "tailwindcss"
      ],
      "content_length": 16106,
      "content_preview": "This recent post [Is Learning CSS a Waste of Time in 2026?](https://dev.to/sylwia-lask/is-learning-css-a-waste-of-time-in-2026-nj3) (by @sylwia-lask)  really hit me, especially the part about accessibility dragging you straight back into raw CSS.\n\nLately, with Tailwind and shadcn, most styling just‚Ä¶ works. Move fast, tweak a class or two, done.\n\nThen Shadow DOM happened.\n\nSuddenly, stuff that ‚Äúshould just work‚Äù broke. Overrides stopped applying, styles got tricky, and all those abstractions felt",
      "content_full": "This recent post [Is Learning CSS a Waste of Time in 2026?](https://dev.to/sylwia-lask/is-learning-css-a-waste-of-time-in-2026-nj3) (by @sylwia-lask)  really hit me, especially the part about accessibility dragging you straight back into raw CSS.\n\nLately, with Tailwind and shadcn, most styling just‚Ä¶ works. Move fast, tweak a class or two, done.\n\nThen Shadow DOM happened.\n\nSuddenly, stuff that ‚Äúshould just work‚Äù broke. Overrides stopped applying, styles got tricky, and all those abstractions felt thinner than expected.\n\nNot a Tailwind or shadcn complaint...just a reminder that knowing CSS still saves you when things fall apart.\n## The dream vs. the reality\n\nSo here's the idea: build beautiful, reusable web components using React, style them with Tailwind CSS, use shadcn/ui for polished UI components, and wrap them up with Shadow DOM for perfect encapsulation. Sounds great, right?\n\nWell... not quite. Turns out these three technologies don't play nicely together. Here's what we learned the hard way.\n\n**TL;DR:** Shadow DOM + Tailwind + shadcn/ui = pain. Choose carefully based on your actual needs, not theoretical ideals. Sometimes the \"impure\" solution is the right one.\n\n## The Setup\n\nWe were building:\n- React components wrapped as web components\n- Styled with **Tailwind CSS**\n- Using **shadcn/ui** components for the UI\n- Wrapped with **Shadow DOM** for style encapsulation\n\nLet's talk about what went wrong.\n\n## Problem #1: Shadow DOM vs. Tailwind CSS - A Fundamental Conflict\n\n### Why they don't get along\n\nTailwind CSS is built on a simple idea: utility classes in a global stylesheet. You include one CSS file, and boom - every element on your page can use classes like `bg-blue-500` or `flex justify-center`.\n\nShadow DOM is built on the opposite idea: complete isolation. Styles inside Shadow DOM can't leak out, and styles from outside can't leak in. This is great for encapsulation, but terrible for Tailwind.\n\nHere's what happens:\n\n```typescript\n// Your React component with Tailwind classes\nexport const MyCard = () => {\n  return (\n    <div className=\"max-w-2xl w-full p-4 bg-white rounded-lg shadow-md\">\n      <h1 className=\"text-2xl font-bold text-gray-900\">Hello World</h1>\n      <p className=\"text-gray-600 mt-2\">This should look nice...</p>\n    </div>\n  );\n};\n\n// Wrap it as a web component with Shadow DOM\nconst MyCardWC = r2wc(MyCard, {\n  shadow: 'open' // Enable Shadow DOM\n});\n\ncustomElements.define('my-card', MyCardWC);\n```\n\n**Result:** Your component renders, but it looks completely broken. No padding, no background color, no rounded corners. Nothing. All your Tailwind classes are ignored because the global Tailwind stylesheet can't penetrate the Shadow DOM boundary.\n\n### The \"solution\" (with heavy air quotes)\n\nYou have to import the Tailwind CSS directly into each component:\n\n```typescript\n// styles.css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n// Component file\nimport './styles.css'; // Import for every component\n\nconst MyCardWC = r2wc(MyCard, {\n  shadow: 'open'\n});\n```\n\nThis works, but at a cost:\n\n**Bundle size explosion**\nEvery web component bundles its own complete copy of Tailwind CSS. If you have 5 components on a page, you're loading Tailwind 5 times. That's 5x the CSS, all identical.\n\n**No browser caching**\nSince each component has its own bundled styles, you can't leverage browser caching for shared CSS. Every component download includes the same Tailwind utilities.\n\n**Build complexity**\nYour build tools need to handle CSS imports for each component separately, making your webpack/vite config more complex.\n\n**Real numbers:**\n- Single Tailwind CSS file: ~50-100KB (minified)\n- With 3 web components: 150-300KB\n- With 10 web components: 500KB-1MB\n\nYeah, not great.\n\n## Problem #2: shadcn/ui and the Portal Problem\n\n### What makes shadcn/ui special (and problematic)\n\nshadcn/ui is built on Radix UI primitives, which are fantastic components. But they have one quirk that breaks with Shadow DOM: **portals**.\n\nComponents like Dialog, Dropdown, Popover, Tooltip all use React portals to render their content outside the normal component tree, usually by appending to `document.body`. This is smart for z-index management and avoiding overflow issues, but it's a disaster for Shadow DOM.\n\n### Example: The Accordion that works\n\n```typescript\nimport { Accordion, AccordionItem, AccordionTrigger, AccordionContent } from '@your-ui/components';\n\nexport const FAQ = () => {\n  return (\n    <Accordion type=\"single\" collapsible>\n      <AccordionItem value=\"item-1\">\n        <AccordionTrigger>What is this?</AccordionTrigger>\n        <AccordionContent>\n          This is an accordion that actually works with Shadow DOM!\n        </AccordionContent>\n      </AccordionItem>\n    </Accordion>\n  );\n};\n\nconst FAQWC = r2wc(FAQ, { shadow: 'open' });\n```\n\n**Why it works:** Accordion renders everything in-place. No portals, no teleporting content. All the HTML stays within your component tree, so Shadow DOM can style it.\n\n### Example: The Dialog that breaks\n\n```typescript\nimport { Dialog, DialogTrigger, DialogContent } from '@your-ui/components';\n\nexport const MyDialog = () => {\n  return (\n    <Dialog>\n      <DialogTrigger>Open</DialogTrigger>\n      <DialogContent>\n        <h2>This won't be styled properly!</h2>\n        <p>The content is outside Shadow DOM now.</p>\n      </DialogContent>\n    </Dialog>\n  );\n};\n\nconst MyDialogWC = r2wc(MyDialog, { shadow: 'open' });\n```\n\n**Why it breaks:**\n1. `DialogContent` gets portaled to `document.body`\n2. It's now outside your Shadow DOM\n3. All your Tailwind classes (inside Shadow DOM) can't reach it\n4. The dialog renders, but looks completely unstyled\n\n**What you see:**\n- No background overlay\n- No styling on the dialog box\n- Text isn't centered\n- Buttons look like plain HTML\n- Z-index issues (might render behind other elements)\n\n### The workaround\n\nYou have to choose: Shadow DOM or portals. Can't have both.\n\n**Option A: Disable Shadow DOM for portal-heavy components**\n\n```typescript\n// No Shadow DOM = portals work, but no encapsulation\nconst MyDialogWC = r2wc(MyDialog, {\n  shadow: null\n});\n```\n\nNow you need to manage styles globally and deal with potential class name conflicts.\n\n**Option B: Only use non-portal components**\n\n```typescript\n// ‚úÖ Safe to use with Shadow DOM\nimport {\n  Accordion,\n  Card,\n  Badge,\n  Button,\n  Tabs,\n  Progress\n} from '@your-ui/components';\n\n// ‚ùå Don't use with Shadow DOM (they use portals)\nimport {\n  Dialog,\n  Popover,\n  Tooltip,\n  DropdownMenu,\n  Sheet,\n  AlertDialog\n} from '@your-ui/components';\n```\n\nThis limits your UI toolkit significantly.\n\n## Problem #3: Dynamic Classes and CVA\n\n### Class Variance Authority (CVA) complications\n\nshadcn/ui uses CVA to handle component variants - different sizes, colors, and states. This generates Tailwind classes dynamically:\n\n```typescript\nimport { cva } from 'class-variance-authority';\n\nconst buttonVariants = cva(\n  \"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors\",\n  {\n    variants: {\n      variant: {\n        default: \"bg-primary text-primary-foreground hover:bg-primary/90\",\n        destructive: \"bg-red-500 text-white hover:bg-red-600\",\n        outline: \"border border-input bg-background hover:bg-accent hover:text-accent-foreground\",\n        ghost: \"hover:bg-accent hover:text-accent-foreground\",\n      },\n      size: {\n        default: \"h-10 px-4 py-2\",\n        sm: \"h-9 rounded-md px-3\",\n        lg: \"h-11 rounded-md px-8\",\n        icon: \"h-10 w-10\",\n      },\n    },\n    defaultVariants: {\n      variant: \"default\",\n      size: \"default\",\n    },\n  }\n);\n\n// Button component\nexport const Button = ({ variant, size, children }) => {\n  return (\n    <button className={buttonVariants({ variant, size })}>\n      {children}\n    </button>\n  );\n};\n```\n\n### The Shadow DOM problem\n\nAll these dynamically generated classes need to exist in your Shadow DOM's stylesheet. But Tailwind's JIT (Just-In-Time) compiler only includes classes it finds in your files during build time.\n\nWhen CVA combines classes dynamically at runtime, Tailwind might not have included them in the build, leading to missing styles.\n\n### The fix: Safelist everything\n\n```typescript\n// tailwind.config.js\nmodule.exports = {\n  content: [\n    './src/**/*.{ts,tsx}',\n    // CRITICAL: Include your UI library\n    './node_modules/@your-ui-lib/**/*.{ts,tsx}',\n  ],\n  // Force include commonly used variant classes\n  safelist: [\n    // Primary variants\n    'bg-primary',\n    'text-primary-foreground',\n    'hover:bg-primary/90',\n    // Destructive variants\n    'bg-red-500',\n    'bg-red-600',\n    'hover:bg-red-600',\n    // Sizes\n    'h-9',\n    'h-10',\n    'h-11',\n    'px-3',\n    'px-4',\n    'px-8',\n    // Add every possible variant combination...\n  ],\n};\n```\n\n**The problem with safelist:**\n- You need to manually list every possible class combination\n- Easy to miss classes (leading to visual bugs)\n- Increases CSS bundle size (defeats purpose of JIT)\n- Need to update whenever UI library changes\n\n## Problem #4: Theme Variables and CSS Custom Properties\n\n### How shadcn/ui does theming\n\nshadcn/ui uses CSS custom properties (variables) for theming:\n\n```css\n:root {\n  --background: 0 0% 100%;\n  --foreground: 222.2 47.4% 11.2%;\n  --primary: 221.2 83.2% 53.3%;\n  --primary-foreground: 210 40% 98%;\n  /* ... many more */\n}\n\n.dark {\n  --background: 222.2 84% 4.9%;\n  --foreground: 210 40% 98%;\n  /* ... dark theme values */\n}\n```\n\nThen in your Tailwind config:\n\n```typescript\n// tailwind.config.js\nmodule.exports = {\n  theme: {\n    extend: {\n      colors: {\n        background: 'hsl(var(--background))',\n        foreground: 'hsl(var(--foreground))',\n        primary: {\n          DEFAULT: 'hsl(var(--primary))',\n          foreground: 'hsl(var(--primary-foreground))',\n        },\n      },\n    },\n  },\n};\n```\n\n### Shadow DOM breaks variable inheritance\n\nCSS custom properties inherit through the DOM tree, but Shadow DOM creates a boundary. Variables defined outside don't automatically flow in.\n\n```typescript\n// This won't work as expected\nexport const ThemedCard = () => {\n  return (\n    <div className=\"bg-background text-foreground p-4\">\n      <h2 className=\"text-primary font-bold\">Title</h2>\n      <p>Content here...</p>\n    </div>\n  );\n};\n\nconst ThemedCardWC = r2wc(ThemedCard, { shadow: 'open' });\n```\n\n**Result:** Your component can't access `--background`, `--foreground`, or `--primary` variables. All theme colors fallback to defaults or break entirely.\n\n### The solution: Replicate variables\n\nYou need to redeclare CSS variables inside your Shadow DOM:\n\n```typescript\n// styles.css (imported by your component)\n:host {\n  /* Re-declare all theme variables */\n  --background: 0 0% 100%;\n  --foreground: 222.2 47.4% 11.2%;\n  --primary: 221.2 83.2% 53.3%;\n  /* ... all other variables */\n}\n\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n**Problems with this approach:**\n- Theme variables are duplicated everywhere\n- Dark mode requires extra work (can't just toggle a class on `document.body`)\n- Updating theme means updating multiple files\n- No single source of truth\n\n## Real-World Impact: A Case Study\n\nLet's look at what this means in practice. Say you're building a dashboard with these components:\n\n```typescript\n// 1. A stats card\nconst StatsCard = () => (\n  <div className=\"bg-white p-6 rounded-lg shadow\">\n    <h3 className=\"text-lg font-semibold text-gray-900\">Total Users</h3>\n    <p className=\"text-3xl font-bold text-primary mt-2\">1,234</p>\n    <p className=\"text-sm text-gray-600 mt-1\">+12% from last month</p>\n  </div>\n);\n\n// 2. A data table (with dropdown menu)\nconst DataTable = () => (\n  <div className=\"bg-white rounded-lg shadow\">\n    <Table>\n      {/* table content */}\n    </Table>\n    <DropdownMenu>\n      <DropdownMenuTrigger>Actions</DropdownMenuTrigger>\n      <DropdownMenuContent>\n        <DropdownMenuItem>Edit</DropdownMenuItem>\n        <DropdownMenuItem>Delete</DropdownMenuItem>\n      </DropdownMenuContent>\n    </DropdownMenu>\n  </div>\n);\n\n// 3. A settings dialog\nconst SettingsDialog = () => (\n  <Dialog>\n    <DialogTrigger asChild>\n      <Button variant=\"outline\">Settings</Button>\n    </DialogTrigger>\n    <DialogContent>\n      <DialogHeader>\n        <DialogTitle>Settings</DialogTitle>\n      </DialogHeader>\n      {/* form content */}\n    </DialogContent>\n  </Dialog>\n);\n```\n\n### With Shadow DOM enabled:\n\n**StatsCard:** ‚úÖ Works perfectly\n- No portals\n- All styles self-contained\n- Bundle: +80KB (Tailwind CSS)\n\n**DataTable:** ‚ö†Ô∏è Partially broken\n- Table looks good\n- Dropdown menu broken (portal renders unstyled outside Shadow DOM)\n- Bundle: +80KB (Tailwind CSS)\n\n**SettingsDialog:** ‚ùå Completely broken\n- Button looks fine\n- Dialog content appears but completely unstyled\n- Backdrop might not work\n- Bundle: +80KB (Tailwind CSS)\n\n**Total bundle cost:** 240KB of duplicated CSS for 3 components\n\n### Without Shadow DOM:\n\n**Everything works:** ‚úÖ\n- All portals work correctly\n- Dropdown and dialog properly styled\n- Bundle: 80KB (single Tailwind CSS file)\n\n**But:**\n- No style encapsulation\n- Potential class name conflicts\n- Global styles can leak in/out\n- Need to be careful with specificity\n\n## So What's The Answer?\n\n### When Shadow DOM makes sense:\n\n**Good use cases:**\n```typescript\n// Simple, self-contained components\n- Cards\n- Badges\n- Progress bars\n- Accordions\n- Tabs\n- Buttons (non-portal variants)\n```\n\nThese components:\n- Don't use portals\n- Don't need complex interactions outside their boundary\n- Benefit from style isolation\n\n### When to skip Shadow DOM:\n\n**Skip it for:**\n```typescript\n// Components with portals or complex interactions\n- Dialogs\n- Popovers\n- Tooltips\n- Dropdown menus\n- Context menus\n- Toast notifications\n```\n\n### Hybrid approach (what actually works):\n\n```typescript\n// Option 1: Selective Shadow DOM\n// Use Shadow DOM only for truly isolated components\nconst CardWC = r2wc(Card, { shadow: 'open' });\nconst BadgeWC = r2wc(Badge, { shadow: 'open' });\n\n// Skip Shadow DOM for interactive components\nconst DialogWC = r2wc(Dialog, { shadow: null });\nconst DropdownWC = r2wc(Dropdown, { shadow: null });\n```\n\n```typescript\n// Option 2: No Shadow DOM, CSS Modules\n// Use CSS Modules for scoping instead\nimport styles from './Card.module.css';\n\nconst Card = () => (\n  <div className={styles.card}>\n    {/* Use scoped CSS instead of Shadow DOM */}\n  </div>\n);\n```\n\n```typescript\n// Option 3: Scoped Tailwind (advanced)\n// Generate component-specific Tailwind with prefixes\n// tailwind.config.js\nmodule.exports = {\n  prefix: 'card-', // All classes become card-bg-white, card-p-4, etc.\n  content: ['./src/Card.tsx'],\n};\n```\n\n## The Uncomfortable Truth\n\nShadow DOM, Tailwind CSS, and shadcn/ui are all great technologies on their own. But together? They fight each other.\n\n**Shadow DOM wants:** Complete isolation\n**Tailwind wants:** Global utility classes\n**shadcn/ui wants:** Portals for proper z-index management\n\nPick two. You can't have all three working perfectly together.\n\n### What we learned:\n\n1. **Bundle size matters** - Duplicating Tailwind CSS across components gets expensive fast\n2. **Portals break Shadow DOM** - Most modern UI libraries use portals heavily\n3. **CSS variables don't cross boundaries** - Theming becomes complicated\n4. **CVA needs special handling** - Dynamic classes require safelist configuration\n5. **There's always a tradeoff** - Encapsulation vs. bundle size vs. functionality\n\n### What worked for us:\n\nWe ended up with a **hybrid approach**:\n- Skip Shadow DOM entirely for our use case\n- Use TypeScript and component wrappers for type safety\n- Accept the global stylesheet\n- Let shadcn/ui portals work as intended\n- Focus on clear component APIs instead of Shadow DOM encapsulation\n\nIs it perfect? No. But it works, and that matters more than architectural purity.\n\n## Resources\n\n- [Shadow DOM Spec](https://developer.mozilla.org/en-US/docs/Web/Web_Components/Using_shadow_DOM)\n- [Tailwind CSS](https://tailwindcss.com/)\n- [shadcn/ui](https://ui.shadcn.com/)\n- [Radix UI Primitives](https://www.radix-ui.com/)\n- [Class Variance Authority](https://cva.style/)\n",
      "stars": null,
      "comments": 17,
      "upvotes": 33,
      "read_time": "8 min read",
      "language": null
    },
    {
      "title_en": "Top 7 Featured DEV Posts of the Week",
      "url": "https://dev.to/devteam/top-7-featured-dev-posts-of-the-week-bao",
      "source": "devto",
      "published_at": "2026-02-03T15:39:39+00:00",
      "external_id": null,
      "tags": [
        "top7",
        "discuss"
      ],
      "content_length": 2909,
      "content_preview": "_Welcome to this week's Top 7, where the DEV editorial team handpicks their favorite posts from the previous week._\n\nCongrats to all the authors that made it onto the list üëè\n\n{% embed https://dev.to/dannwaneri/were-creating-a-knowledge-collapse-and-no-ones-talking-about-it-226d %}\n\n@dannwaneri raises a critical alarm about \"knowledge collapse\" as AI-generated content begins to feed into itself. The author warns that relying too heavily on synthetic data could lead to a homogenization of informat",
      "content_full": "_Welcome to this week's Top 7, where the DEV editorial team handpicks their favorite posts from the previous week._\n\nCongrats to all the authors that made it onto the list üëè\n\n{% embed https://dev.to/dannwaneri/were-creating-a-knowledge-collapse-and-no-ones-talking-about-it-226d %}\n\n@dannwaneri raises a critical alarm about \"knowledge collapse\" as AI-generated content begins to feed into itself. The author warns that relying too heavily on synthetic data could lead to a homogenization of information and a loss of nuance.\n\n---\n\n{% embed https://dev.to/iamovi/i-made-buttons-that-run-away-from-you-prank-projects-5ej0 %}\n\n@iamovi built a prank interface with buttons that actively flee the mouse cursor. The tutorial covers the math and CSS logic needed to create these frustratingly fun interactions.\n\n---\n\n{% embed https://dev.to/cec1_c0d/your-accessibility-overlay-is-a-scam-and-heres-the-proof-5g76 %}\n\n@cec1_c0d addresses the reality behind accessibility overlays, arguing that they often fail to provide genuine compliance or usability. The author provides evidence to support the claim that these quick-fix tools are more deceptive than helpful.\n\n---\n\n{% embed https://dev.to/olaproeis/the-ai-development-workflow-i-actually-use-549i %}\n\n@olaproeis breaks down their personal AI development workflow, moving beyond the hype to show exactly which tools actually save time. The post details a practical stack involving Cursor and other assistants to streamline the coding process.\n\n---\n\n{% embed https://dev.to/cseeman/contributing-to-rubygems-org-5dpi %}\n\n@cseeman shares their experience of contributing to RubyGems.org despite some initial ethical hesitation, but ultimately in service of the community at large. The author encourages others to dive into open source by demystifying the process of working on a major codebase.\n\n---\n\n{% embed https://dev.to/therealmrmumba/your-mcp-setup-probably-needs-a-gateway-heres-what-i-learned-30np %}\n\n@therealmrmumba explores the architecture of the Model Context Protocol (MCP), arguing that a gateway is essential for a robust setup. The post shares technical lessons learned about routing and security when orchestrating AI context.\n\n---\n\n{% embed https://dev.to/manuartero/about-wizards-warlocks-programmers-vibe-coders-4p6n %}\n\n@manuartero uses fantasy metaphors to compare traditional programmers to \"wizards\" and the new wave of \"vibe coders\" to \"warlocks.\" The author reflects on how AI tools are changing the nature of the craft and the feeling of wielding code.\n\n---\n\n_And that's a wrap for this week's Top 7 roundup! üé¨ We hope you enjoyed this eclectic mix of insights, stories, and tips from our talented authors. Keep coding, keep learning, and stay tuned to DEV for more captivating content and [make sure you‚Äôre opted in to our Weekly Newsletter] (https://dev.to/settings/notifications) üì© for all the best articles, discussions, and updates._",
      "stars": null,
      "comments": 13,
      "upvotes": 38,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "How dev.to became my comfortable corner of the internet (and my New Year resolution)",
      "url": "https://dev.to/ujja/how-devto-became-my-comfortable-corner-of-the-internet-and-my-new-year-resolution-54h8",
      "source": "devto",
      "published_at": "2026-02-09T22:40:35+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "community",
        "watercooler",
        "mentalhealth"
      ],
      "content_length": 3144,
      "content_preview": "It has been a little over 10 years since I deleted my Facebook account.\n\nNo long post. No explanation. I just logged out and never went back.\n\nI never joined Instagram either. Or Twitter. Or Snapchat. Or any other social networking platform. That always surprises people, but honestly, it never felt like something I needed.\n\nAnd after all these years, I can confidently say this  \nI do not miss it at all.\n\n## The peace that came with logging out\n\nBack then, Facebook just felt loud. Everyone was sh",
      "content_full": "It has been a little over 10 years since I deleted my Facebook account.\n\nNo long post. No explanation. I just logged out and never went back.\n\nI never joined Instagram either. Or Twitter. Or Snapchat. Or any other social networking platform. That always surprises people, but honestly, it never felt like something I needed.\n\nAnd after all these years, I can confidently say this  \nI do not miss it at all.\n\n## The peace that came with logging out\n\nBack then, Facebook just felt loud. Everyone was sharing everything. Opinions, achievements, arguments, perfectly happy lives. I would scroll and somehow feel worse than when I started.\n\nDeleting it gave me something I did not even know I was craving. Quiet.\n\nNo constant comparison. No endless updates from people I barely spoke to. No pressure to react, like, comment, or keep up.\n\nOnce that noise was gone, I never felt the urge to go back.\n\n## Yes, I do have LinkedIn\n\nI do have LinkedIn, but my usage is very limited.\n\nMostly because even there, comparison sneaks in. You see people announcing promotions, new roles, startup wins, big milestones. And even if you are happy for them, a tiny part of your brain goes  \nShould I be doing more? \nAm I behind?\n\nThat jealousy is human. I feel it too. So I keep my time there minimal and intentional.\n\n## How I found dev.to\n\nI enjoy technical writing. I always have.\n\nFor a long time, I would write technical articles and share them on LinkedIn. That was the only place I knew. Around that time (close to six years ago), a friend of mine suggested I try dev.to instead.\n\nTheir reasoning was simple  \nIf you enjoy tech, share it where it helps the community  \nLet others learn from your experience and you will learn from theirs too\n\nThat idea stuck with me.\n\n## Why dev.to feels different\n\ndev.to feels like a completely different space.\n\nIt does not feel like a highlight reel. It feels like a shared journey.\n\nPeople celebrate each other when things work. And when things break, people are genuinely heartbroken together. Builds fail, ideas flop, bugs refuse to die and everyone learns from it.\n\nThere is honesty here. You see what worked and what absolutely did not. And that makes success feel real, not staged.\n\nIt feels less like performing and more like sharing.\n\n## A small New Year resolution\n\nThis year, my New Year resolution was simple  \nBe more socially active on dev.to\n\nComment more. Engage more. Participate in conversations instead of just reading quietly.\n\nAt the same time, I promised myself I would keep an eye on my well being. If it ever starts feeling heavy or competitive or draining, I step back.\n\nSo far, so good.\n\nNo anxiety. No doomscrolling. Just learning, conversations, and the reminder that everyone is figuring things out as they go.\n\n## Final thoughts\n\nI do not miss Facebook.  \nI do not miss Instagram.  \nI do not miss being everywhere.\n\nWhat I value now are smaller, intentional spaces where people show up as humans, not brands.\n\nAnd dev.to feels like one of those rare corners of the internet where that still exists.\n\nIf that ever changes, I know I can log out again. And that is a pretty comforting thought.\n",
      "stars": null,
      "comments": 25,
      "upvotes": 40,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "What was your win this week?",
      "url": "https://dev.to/devteam/what-was-your-win-this-week-5am7",
      "source": "devto",
      "published_at": "2026-02-06T13:00:00+00:00",
      "external_id": null,
      "tags": [
        "weeklyretro",
        "discuss"
      ],
      "content_length": 486,
      "content_preview": "üëãüëãüëãüëã\n\nLooking back on your week -- what was something you're proud of?\n\nAll wins count -- big or small üéâ\n\nExamples of 'wins' include:\n\n- Getting a promotion!\n- Starting a new project\n- Fixing a tricky bug\n- Making your bed every morning üõèÔ∏è\n\n![A baby dances with the text: \"It's Friday. I must dance!\"](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExYmJ5bXR3anB6MmFvM3I2MXA0bDV5NzJsZnplN3JpdmRtNHQwMWE3dyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Ywxt9v39rUpmU/giphy.gif)\n\nHappy Friday!",
      "content_full": "üëãüëãüëãüëã\n\nLooking back on your week -- what was something you're proud of?\n\nAll wins count -- big or small üéâ\n\nExamples of 'wins' include:\n\n- Getting a promotion!\n- Starting a new project\n- Fixing a tricky bug\n- Making your bed every morning üõèÔ∏è\n\n![A baby dances with the text: \"It's Friday. I must dance!\"](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExYmJ5bXR3anB6MmFvM3I2MXA0bDV5NzJsZnplN3JpdmRtNHQwMWE3dyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Ywxt9v39rUpmU/giphy.gif)\n\nHappy Friday!",
      "stars": null,
      "comments": 160,
      "upvotes": 52,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "I Shipped the Solution to Knowledge Collapse in 21 Days",
      "url": "https://dev.to/the-foundation/i-built-federated-ai-knowledge-commons-heres-how-56oj",
      "source": "devto",
      "published_at": "2026-02-09T11:07:59+00:00",
      "external_id": null,
      "tags": [
        "ai",
        "opensource",
        "activitypub",
        "discuss"
      ],
      "content_length": 12937,
      "content_preview": "Three weeks ago, I wrote about knowledge collapse - how our best technical insights are dying in private AI chats while Stack Overflow bleeds 78% of its traffic.\n\nHundreds of developers agreed we need a solution.\n\nSo I built one. And it's running in production right now.\n\n**Here's the live demo:** https://chat-knowledge-api.fpl-test.workers.dev  \n**Here's the source code:** https://github.com/dannwaneri/chat-knowledge  \n**Here's the complete build session you're reading from:** 107 chunks, 174 m",
      "content_full": "Three weeks ago, I wrote about knowledge collapse - how our best technical insights are dying in private AI chats while Stack Overflow bleeds 78% of its traffic.\n\nHundreds of developers agreed we need a solution.\n\nSo I built one. And it's running in production right now.\n\n**Here's the live demo:** https://chat-knowledge-api.fpl-test.workers.dev  \n**Here's the source code:** https://github.com/dannwaneri/chat-knowledge  \n**Here's the complete build session you're reading from:** 107 chunks, 174 messages, imported via HTML\n\nLet me show you how it works.\n\n---\n\n## The Problem (Quick Recap)\n\nKnowledge collapse is happening right now:\n- Your best debugging solutions live in private Claude chats\n- No attribution, no discovery, no commons\n- Stack Overflow traffic down 78% since ChatGPT launched\n- We're optimizing ourselves into a knowledge dead-end\n\nWe need \"Stack Overflow for AI conversations\" - but decentralized, privacy-first, and developer-owned.\n\n---\n\n## What I Actually Built\n\n### 1. HTML Import System\n\n**The workflow is dead simple:**\n\n1. Have a valuable Claude conversation\n2. Press `Ctrl+S` (save as HTML)\n3. Import it: `node dist/cli/import-html.js chat.html`\n4. Done - it's searchable\n\n**Tested on this article's build session:**\n- File size: 4.6MB\n- Messages: 136 parsed\n- Chunks created: 91\n- Time: < 2 seconds\n\nNo complex setup. No API keys. Just save and import.\n\n### 2. Security Scanner (The Critical Feature)\n\nBefore any chat goes public, it runs through auto-detection:\n\n**üî¥ CRITICAL (auto-block):**\n- API keys, Bearer tokens\n- Private URLs (localhost, .internal domains)\n- Credentials, passwords\n\n**Real results from my build session scan:**\n```\nTotal issues detected: 599\nCritical blocks: 3 (2 Bearer tokens, 1 API key)\nHigh severity: Multiple localhost URLs\nSafe to share: FALSE ‚úÖ (exactly as designed)\n```\n\nThis is the difference between \"share everything\" and \"share safely.\" One leaked API key costs more than this entire system.\n\n## By The Numbers\n\n**This Build Session:**\n- Start: Problem identified (Jan 15)\n- Build: 107 conversation chunks\n- Messages: 174 total\n- File size: 4.6MB HTML\n- Parse time: < 2 seconds\n- Security scan: 599 issues detected (3 critical auto-blocks)\n- **Ship date: Feb 8 (24 days from problem to production)**\n\n**That's faster than most companies decide what to build.**\n\n### 3. Semantic Search\n\nNot keyword matching - actual understanding.\n\n**Query:** \"how to handle vectorize embeddings\"  \n**Found:** Content about \"dimension reduction\" and \"optimization\"  \n**Relevance score:** 0.78\n\nThe system understood WHAT I meant, not just what I typed.\n\n**Tech stack:**\n- Workers AI (`@cf/baai/bge-base-en-v1.5`) - generates embeddings\n- Vectorize - stores 768-dimension vectors\n- D1 - metadata and chat structure\n- Cosine similarity search across all imported conversations\n\n### 4. Federation Protocol (ActivityPub)\n\nThis isn't just personal knowledge management. It's designed to federate.\n\n**Live endpoints:**\n- NodeInfo: `/api/federation/nodeinfo` (200 OK ‚úÖ)\n- WebFinger: `/api/federation/.well-known/webfinger`\n- Inbox/Outbox: ActivityPub standard\n\n**What federation means:**\n- You run your instance\n- I run my instance\n- We search across ALL of them\n- No single point of control\n- No corporate overlord\n\nExactly like Mastodon, but for developer knowledge.\n\n---\n\n## Technical Architecture (How It Actually Works)\n\n### The Stack\n```\nFrontend: HTML ‚Üí Parser ‚Üí Chunks\nBackend: Cloudflare Workers (edge-native)\nDatabase: D1 (SQLite at the edge)\nVector Store: Vectorize (768-dim embeddings)\nAI: Workers AI (BGE-base-en-v1.5)\nProtocol: ActivityPub (federation standard)\n```\n\n### The Flow\n```\n1. IMPORT\n   HTML file ‚Üí Parse messages ‚Üí Chunk content ‚Üí Generate embeddings\n\n2. SECURITY\n   Scan for secrets ‚Üí Flag risks ‚Üí Require review ‚Üí Safe by default\n\n3. STORAGE\n   Chunks ‚Üí D1 (metadata)\n   Embeddings ‚Üí Vectorize (semantic search)\n\n4. SEARCH\n   Query ‚Üí Generate embedding ‚Üí Cosine similarity ‚Üí Ranked results\n\n5. FEDERATION (coming)\n   Public chats ‚Üí ActivityPub ‚Üí Federated timeline ‚Üí Cross-instance search\n```\n\n### What Was Hard\n\n**1. HTML Parsing**\nClaude's HTML export format isn't documented. Had to reverse-engineer:\n- Message boundaries\n- Code block preservation\n- Artifact handling\n- Nested content structure\n\n**2. Security Scanner**\nCan't just regex for \"API key\" - need to understand context:\n- Is this a code example or real credential?\n- Is localhost URL in docs or actual endpoint?\n- Balance: too strict = false positives, too loose = leaks\n\n**3. Federation Protocol**\nActivityPub is designed for social posts, not Q&A:\n- How to represent \"question\" vs \"answer\"?\n- Vote federation across instances?\n- Spam prevention without centralized moderation?\n\n**4. Edge-Native Architecture**\nCloudflare Workers have constraints:\n- 10ms CPU limit per request\n- No filesystem\n- Async-only database access\n  \nWorking within constraints = better architecture.\n\n### The Security Scanner in Action\n```typescript\n// Real security detection from the codebase\nconst patterns = {\n  bearerToken: /Bearer\\s+[A-Za-z0-9\\-._~+/]+=*/gi,\n  apiKey: /['\\\"]?api[_-]?key['\\\"]?\\s*[:=]\\s*['\\\"]?[A-Za-z0-9-_]{20,}['\\\"]?/gi,\n  localhost: /https?:\\/\\/(localhost|127\\.0\\.0\\.1|::1)/gi,\n  internalDomain: /https?:\\/\\/[a-z0-9.-]+\\.(local|internal|corp|dev)/gi\n}\n\n// Scan returns: { safe: boolean, issues: Issue[] }\n// Auto-blocks if critical issues found\n```\n\n### Database Schema (12 tables)\n```sql\nchats                  -- Core chat storage\nchunks                 -- Content chunks for search\npre_share_scans        -- Security scanner results\nchunk_redactions       -- Auto-redaction tracking\nshare_approvals        -- Sharing workflow\nfederated_instances    -- Federation network\nfederated_knowledge    -- Cross-instance content\nfederation_activities  -- ActivityPub events\nknowledge_analytics    -- Usage tracking\ncollections            -- Knowledge curation\ncollection_items       -- Collection membership\n```\n\nThis is production-grade infrastructure, not a proof of concept.\n\n---\n\n## Why I Built This Now\n\n**Timing matters.**\n\nStack Overflow traffic is down 78% and still falling. Every day, thousands of valuable debugging sessions happen in private AI chats and disappear forever.\n\nWe're not just losing knowledge - we're losing the HABIT of knowledge sharing.\n\nBuilding this now means:\n- Early adopters shape the protocol\n- Federation standards emerge organically  \n- We avoid corporate capture (no VC, no \"pivot to paid\")\n- Developers own the infrastructure from day one\n\n**The best time to rebuild the knowledge commons was before Stack Overflow collapsed.**\n\n**The second-best time is now.**\n\n---\n\n## Why This Matters\n\n### It Solves Knowledge Collapse\n\n- ‚úÖ **Insights stay discoverable** - Semantic search finds relevant content\n- ‚úÖ **Attribution preserved** - Source tracking built-in\n- ‚úÖ **Privacy respected** - Security scanner catches leaks\n- ‚úÖ **No platform risk** - Self-hosted, you control your data\n\n### It's Actually Decentralized\n\n- **ActivityPub** = proven federation protocol (powers Mastodon's 10M+ users)\n- **Developer-owned instances** - Run your own, connect with others\n- **No \"rug pull\" risk** - Open source, MIT licensed\n\n### It's Viable at Edge Scale\n\nCloudflare Workers handles massive scale:\n- Edge-native architecture\n- D1 database at the edge\n- Vectorize for semantic search\n- Workers AI for embeddings\n\nThe same tech stack I use for production apps serving thousands of users.\n\n---\n\n## From Discussion to Infrastructure\n\nThree weeks ago, Richard Pascoe asked in the comments: \"Could Mastodon servers like Fosstodon help foster a knowledge sharing platform?\"\n\nI said yes and built it.\n\nThis isn't theoretical infrastructure. It's ActivityPub-compatible, meaning it federates with Mastodon, Fosstodon, and the entire Fediverse network.\n\nRichard's question became the bridge between diagnosis and solution.\n\n**[@richardpascoe](https://dev.to/richardpascoe) - your instance is ready when you are.** üöÄ\n\n---\n\n## Real Use Cases (What This Enables)\n\n### For Individual Developers\n- **Portfolio of problem-solving** - Your best debugging sessions, searchable\n- **Learning in public** - Share solutions, get feedback, build reputation\n- **Future reference** - \"I solved this before, where was that chat?\"\n\n### For Teams\n- **Institutional knowledge** - Team's collective debugging history\n- **Onboarding** - New devs search team's past solutions\n- **Pattern recognition** - See recurring problems across conversations\n\n### For Communities\n- **Niche expertise** - Rust specialists, Cloudflare devs, etc. share domain knowledge\n- **Federated discovery** - Find experts across instances\n- **Attribution** - Credit flows to who actually solved it\n\n### For The Commons\n- **Stack Overflow alternative** - But decentralized and community-owned\n- **AI training data** - High-quality, attributed conversations\n- **Knowledge archaeology** - Insights don't die with platforms\n\n---\n\n## What's Next\n\n### Immediate (This Week)\n\n- ‚úÖ Open source on GitHub (MIT license)\n- ‚úÖ Documentation for self-hosting\n- ‚úÖ Production deployment live\n\n### Short-term (Next Month)\n\n- Import 50+ historical Claude chats (build the corpus)\n- MCP extension (\"share this chat publicly\" from Claude Code)\n- First federation test with another developer\n\n### Long-term (3-6 Months)\n\n- 10+ federated instances\n- Collections feature (curate knowledge by topic)\n- Analytics (which insights are most valuable)\n- Cross-instance search\n\n---\n\n## This Is The Foundation in Action\n\nTwo weeks ago, we created [@the-foundation](https://dev.to/the-foundation) to preserve developer knowledge publicly.\n\nRichard Pascoe published our first collaborative post on fundamentals.\n\nThis is our second: working infrastructure.\n\n**The Foundation isn't just writing about the problem. We're shipping solutions.**\n\n---\n\n## Join The Foundation\n\nThis isn't a solo project. It's infrastructure.\n\n### For Developers\n\n- **Clone the repo:** https://github.com/dannwaneri/chat-knowledge\n- **Run your own instance** - Full setup guide in README\n- **Contribute to the protocol** - Issues and PRs welcome\n\n### For Writers\n\n- **Import your best AI conversations** - Build your knowledge portfolio\n- **Share safely** - Security scanner protects you\n- **Get discovered** - Federated search makes your insights findable\n\n### For The Curious\n\n- **Star the repo** ‚≠ê - Show you care about preserving knowledge\n- **Share this article** - Help spread the word\n- **Join the discussion** - Comment below with your thoughts\n\n**The knowledge commons doesn't rebuild itself. But we can build it together.**\n\n---\n\n## Installation (5 Minutes)\n```bash\n# Clone the repo\ngit clone https://github.com/dannwaneri/chat-knowledge.git\ncd chat-knowledge\n\n# Install dependencies\nnpm install\n\n# Login to Cloudflare (free tier works)\nwrangler login\n\n# Create infrastructure\nwrangler d1 create chat-knowledge-db\nwrangler vectorize create chat-knowledge-embeddings --dimensions=768 --metric=cosine\n\n# Run migrations\nwrangler d1 execute chat-knowledge-db --remote --file=migrations/migration-federation.sql\nwrangler d1 execute chat-knowledge-db --remote --file=migrations/migration-sanitizer.sql\n\n# Deploy\nnpm run deploy\n```\n\nThat's it. You now have your own federated knowledge instance.\n\n---\n\n## Try It Right Now\n\n**Import a chat:**\n```bash\n# Save any Claude conversation as HTML (Ctrl+S)\nnpm run build\nnode dist/cli/import-html.js path/to/chat.html \"My First Import\"\n```\n\n**Search it:**\n```bash\ncurl -X POST https://your-worker.workers.dev/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"debugging tips\", \"maxResults\": 5}'\n```\n\n**Scan for secrets:**\n```bash\nnode dist/cli/safe-share.js <chat-id>\n# Shows what would leak before you share\n```\n\n---\n\n## The Meta Moment\n\nI wrote about the problem three weeks ago.  \nNow the solution is running in production.\n\n**From observation to shipped product in 21 days.**\n\nThat's the power of:\n- Cloudflare Workers (deploy in seconds)\n- AI embeddings (semantic search out of the box)\n- ActivityPub (proven federation protocol)\n- Building in public (accountability + feedback)\n\n**Your move, Stack Overflow.** üëä\n\n---\n\n## Related Articles\n\n1. **[My Chrome Tabs Tell a Story](https://dev.to/dannwaneri/chrome-tabs-story)** - The observation that started it all\n2. **[We're Creating a Knowledge Collapse](https://dev.to/dannwaneri/knowledge-collapse)** - The problem statement (12K+ views)\n3. **[Above the API: What Developers Contribute When AI Can Code](https://dev.to/dannwaneri/above-the-api)** - What skills actually matter\n4. **You're here** - The solution\n\n---\n\n## Let's Build This Together\n\n**GitHub:** https://github.com/dannwaneri/chat-knowledge  \n**Live Demo:** https://chat-knowledge-api.fpl-test.workers.dev  \n**Twitter:** [@dannwaneri](https://twitter.com/dannwaneri)\n\nIf you believe in preserving developer knowledge, star the repo ‚≠ê and let's make this real.\n\nThe foundation is laid. Now we need builders.\n\nAre you in?\n",
      "stars": null,
      "comments": 21,
      "upvotes": 38,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "Actually Static: When WordPress Stops Being the Enemy",
      "url": "https://dev.to/pascal_cescato_692b7a8a20/actually-static-when-wordpress-stops-being-the-enemy-37h5",
      "source": "devto",
      "published_at": "2026-02-08T13:28:48+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "githubchallenge",
        "cli",
        "githubcopilot"
      ],
      "content_length": 25787,
      "content_preview": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n> **This post is for developers who like writing in WordPress but want the speed and safety of static sites (Hugo/GitHub Pages).**\n\n## What I Built\n\nI spent years wrestling with WordPress performance and security issues.\n\nOptimizing caching layers, hardening installations, fighting plugin bloat ‚Äî all to keep public-facing sites running acceptably.\n\nThen I discovered static site generators.",
      "content_full": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n> **This post is for developers who like writing in WordPress but want the speed and safety of static sites (Hugo/GitHub Pages).**\n\n## What I Built\n\nI spent years wrestling with WordPress performance and security issues.\n\nOptimizing caching layers, hardening installations, fighting plugin bloat ‚Äî all to keep public-facing sites running acceptably.\n\nThen I discovered static site generators. Hugo. Astro. Fast, secure, elegant.\n\nBut months of tweaking themes, debugging build pipelines, and fighting with deployment workflows taught me something: I'd just traded one set of problems for another.\n\nToday, I use both. Not as competitors, but as partners.\n\nHere's the paradox I kept running into: **WordPress is probably the best writing environment ever built**. The interface is mature, the editor works, and you can focus on what matters ‚Äî writing.\n\n**Static site generators are probably the best deployment target ever built**. Fast, secure, cheap to host, and scalable by default.\n\nSo why do we keep choosing between them?\n\n**In a comment on [one of my previous posts](https://dev.to/pascal_cescato_692b7a8a20/from-wordpress-to-astro-three-days-to-reclaim-control-5dn2)**, @juliecodestack captured this tension perfectly:\n\n> \"I spent quite a lot of time tweaking Hugo sites instead of writing, and I'm afraid I'll do the same thing if I transfer to Astro.\"\n\nThat's the real problem. Not WordPress. Not Hugo. The friction between writing and deploying.\n\nThe problem with keeping WordPress public-facing isn't the editor ‚Äî it's everything else.\n\nExposing a complete WordPress site to the public means decent hosting, heavy dependency on plugins ‚Äî at minimum for security and SEO ‚Äî and serious maintenance.\n\nAnd by default, performance is variable, to say the least.\n\n### The Search for an Alternative\n\nThis observation gradually led me to look for a different solution.\n\nFor some time now, I've been moving my content publishing to static sites.\n\nBut what I really wanted was simpler: keep WordPress as a writing environment while completely removing its public presence.\n\nAt a time when static sites can be deployed in seconds on almost any infrastructure, keeping WordPress as a frontend doesn't always make much sense anymore ‚Äî as long as you have a robust deployment solution.\n\nWrite normally. Publish automatically.\n\nNo manual export, no scripts to run, no friction.\n\n### Existing Tools: Effective but Heavy\n\nHaving dozens of articles on my WordPress blogs, I developed a suite of Python tools capable of exporting a complete site to Hugo or Astro.\n\nFunctional, reliable, but based on a global export logic: complete site generation, transformation, then deployment.\n\nAn effective process, but heavy.\n\nAnd especially unnatural in a daily writing workflow.\n\nThis search for a more fluid editorial workflow gave birth to the project.\n\n**The solution**: A WordPress plugin that automatically syncs published posts to a GitHub repository, converts them to Markdown with Hugo-compatible front matter, optimizes images (WebP/AVIF), and triggers a GitHub Actions workflow for static site deployment.\n\n**Tech Stack**:\n\n- WordPress 6.9+ (PHP 8.1+)\n- GitHub API (atomic commits via Trees API)\n- Action Scheduler (async processing)\n- Intervention Image (local optimization)\n- Hugo (static site generator)\n- GitHub Actions + GitHub Pages (deployment)\n\n**Key Features**:\n‚úÖ Fully asynchronous sync (no admin blocking)\n‚úÖ Atomic commits (Markdown + all images in one commit)\n‚úÖ Native WordPress APIs only (WordPress.org compliant)\n‚úÖ Multi-format image optimization (AVIF ‚Üí WebP ‚Üí Original)\n‚úÖ Zero shell commands (100% GitHub API)\n‚úÖ HTTPS + Personal Access Token authentication\n‚úÖ WP-CLI commands for bulk operations\n\n## Demo\n\nThe workflow is now operational.\nYou can give it a try here: [WordPress Admin](https://githubcopilotchallenge.tsw.ovh/) (login: `tester`, password: `Github~Challenge/2k26` with Author rights). You can see commited files in [the website repository](https://github.com/pcescato/hugodemo), where you can also find the workflow in `.github/workflows`.\n\nResult is visible on [the demo website](https://pcescato.github.io/hugodemo/) - as you can see below:\n\n{% embed https://pcescato.github.io/hugodemo/posts/2026-02-08-why-i-finally-stopped-fighting-my-publishing-workflow/ %}\n\nArticles are written in WordPress, as before.\n\nWhen publishing or updating, a dedicated plugin automatically triggers synchronization to a GitHub repository.\n\nEach piece of content is converted to Markdown with Hugo-specific front matter, along with optimized images (WebP and AVIF).\n\nEverything is sent in a single commit via the GitHub API.\n\nA GitHub Actions workflow then takes over: static site generation, then deployment to GitHub Pages.\n\nConcretely, publishing in WordPress is now enough to put a complete static version of the site online, without manual export or additional intervention.\n\n### The Real-World Gauntlet\n\nThis wasn't a smooth 48-hour sprint. The project survived several reality checks:\n\n**Hugo Theme Version Hell**: The theme I wanted required Hugo 0.146.0 minimum. My local install was 0.139.0. GitHub Actions defaulted to 0.128.0. Each environment needed explicit version pinning, and debugging failures meant decoding cryptic TOML errors across three different build contexts.\n\n**GitHub Pages URL Stuttering**: The deployed site initially rendered with broken internal links because Hugo's `baseURL` configuration didn't match GitHub Pages' expectations. Pages built locally worked fine. CI builds deployed with relative paths pointing to void. Solution: hardcode the production URL in the workflow, accept that local previews would have slightly broken navigation.\n\n**Image Pipeline Memory Limits**: Processing 10+ images per post with AVIF encoding pushed PHP's memory limits on shared hosting. First attempt: fatal errors. Second attempt: disable AVIF, keep WebP. Final solution: increase `memory_limit` to 512M and batch-process images sequentially instead of in parallel.\n\n**Action Scheduler Race Conditions**: Early versions created duplicate commits when saving a post multiple times quickly. WordPress's `save_post` hook fires on autosaves, manual saves, and quick edits. Needed: debouncing logic, transient locks, and post meta flags to prevent redundant syncs.\n\n**PHP 8.1 Strictness**: A single `explode()` call on a `null` value was enough to freeze the entire sync pipeline. We had to implement a `try-catch-finally` pattern to guarantee that even on crash, the sync lock is released and the UI updated. No more hung admin screens.\n\n**Git Line Ending Hell (LF vs CRLF)**: GitHub Actions Linux runners rejected files modified on Windows because of line ending mismatches. Solution: enforce LF via `.gitattributes` globally. One config file, zero cross-platform headaches.\n\n**The Partial Save Trap**: WordPress tabbed interfaces only submit visible fields. When updating the Front Matter template, the GitHub PAT field wasn't sent, resulting in accidental deletion. Fix: `array_merge()` logic to preserve existing values during partial updates.\n\nNone of this was in the initial specifications. All of it was mandatory to ship.\n\n### The Time Investment Reality\n\nHere's what \"48 hours\" actually meant:\n\n| Task                                          | Manual (estimated) | With Copilot CLI | Actual Savings       |\n| --------------------------------------------- | ------------------ | ---------------- | -------------------- |\n| Plugin boilerplate + WordPress standards      | ~4 hours           | 20 minutes       | 3h 40m               |\n| GitHub API integration (Trees, refs, commits) | ~6 hours           | 1.5 hours        | 4h 30m               |\n| Image optimization pipeline                   | ~5 hours           | 2 hours          | 3h                   |\n| Async queue setup (Action Scheduler)          | ~3 hours           | 45 minutes       | 2h 15m               |\n| Admin UI + settings page                      | ~4 hours           | 1 hour           | 3h                   |\n| Hugo adapter (Markdown + front matter)        | ~2 hours           | 30 minutes       | 1h 30m               |\n| Debugging real-world issues                   | ~8 hours           | ~8 hours         | 0h (no AI help here) |\n| **Total**                                     | **~32 hours**      | **~14 hours**    | **~18 hours**        |\n\nCopilot accelerated structured implementation. It did nothing for architectural decisions, debugging environment-specific failures, or understanding WordPress.org compliance requirements.\n\n### Why Local Image Optimization Matters\n\nThe plugin processes images **on the WordPress server before uploading to GitHub**. This is crucial:\n\n**Without local optimization**:\n\n- Upload 5MB original JPEGs to GitHub\n- GitHub Actions must download, process (ImageMagick/Sharp), then deploy\n- Build time: 2-3 minutes per post\n- GitHub Actions runner minutes consumed: high\n- Failed builds leave orphaned large files in Git history\n\n**With local optimization** (current approach):\n\n- WordPress generates AVIF (50-150KB) + WebP (100-300KB) + original\n- Upload ~500KB total per post to GitHub\n- GitHub Actions just copies files, no processing\n- Build time: 15-30 seconds\n- Clean Git history, minimal runner usage\n\nThe trade-off: PHP memory limits and processing time on the WordPress side. But WordPress is idle 99% of the time. GitHub Actions runners cost money per minute.\n\nProcessing locally shifts the bottleneck to where it's free.\n\n### Architecture\n\n**What's Currently Handled**:\n\n‚úÖ **Posts and Pages**: Both sync automatically with proper Hugo front matter\n‚úÖ **Deletions**: Trashing a post/page in WordPress triggers file deletion in GitHub\n‚úÖ **Updates**: Editing content re-syncs, overwriting existing files\n‚úÖ **Categories and Tags**: Converted to Hugo taxonomies in front matter\n‚úÖ **Featured Images**: Optimized and linked in front matter (`featured_image` field)\n‚úÖ **Custom Fields**: Basic fields map to front matter (extensible via adapter)\n\n**Current Limitations** (MVP scope):\n\n‚ö†Ô∏è **Draft Handling**: Drafts stay in WordPress, never sync (intentional)\n‚ö†Ô∏è **Revisions**: Only published versions sync, revision history stays local\n‚ö†Ô∏è **Complex Blocks**: Gutenberg blocks convert to HTML, then basic Markdown (no advanced block preservation)\n‚ö†Ô∏è **Shortcodes**: Rendered to HTML before conversion (loses original shortcode)\n‚ö†Ô∏è **ACF/Meta Boxes**: Only standard custom fields supported (ACF requires custom adapter extension)\n‚ö†Ô∏è **Author Pages**: Not yet implemented (single-author blogs work fine)\n\n**Deliberate Trade-offs**:\n\nWordPress remains the source of truth. The plugin doesn't sync bidirectionally. If you edit Markdown directly in GitHub, those changes won't flow back to WordPress. This is intentional ‚Äî simplicity over complexity.\n\n**Theme Changes and SSG Migration**:\n\nThanks to the universal front matter template system, changing Hugo themes or even migrating to a different SSG is now straightforward:\n\n1. **Update the front matter template** in plugin settings (no code changes)\n2. **Bulk re-sync** all posts via WP-CLI (`wp jamstack sync --all`)\n3. **Optional cleanup** of old file structure in Git (if directory paths changed)\n\nThe adapter pattern is already in place. Adding support for Jekyll, Eleventy, or Astro means implementing a new adapter class ‚Äî the core sync engine remains untouched.\n\nWhat's *not* yet automated: migrating between SSGs with fundamentally different content structures (e.g., Hugo's `content/posts/` vs Astro's `src/content/blog/`). This would require a bulk file move operation in Git, which is currently manual.\n\nBut changing front matter conventions within the same SSG? That's now a settings change, not a refactoring project.\n\nThe repository contains:\n\n- WordPress plugin (WordPress.org compliant)\n- GitHub API integration (atomic commits)\n- Asynchronous sync management (Action Scheduler)\n- Hugo-compatible Markdown generation\n- GitHub Actions workflow for deployment\n\n### How It Works\n\nThe process in detail:\n\n**1. Writing**: Standard WordPress interface, no change in the writing experience\n\n![WordPress Editor](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rhg7yqnlr5g60zo3kfuo.png)\n\n**2. Automatic Commit**: The GitHub repository receives Markdown, optimized images, and front matter\n\n![GitHub Commit](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9zyrrvtfpzi4yykt9dhp.jpg)\nYou can see the `.github/workflows` folder, where you can find the `hugo.yml` file (1), the `content` folder (2), the `static/images` one (3), and last deployment status (4).\n\n**3. Hugo Structure**: `content/posts/` structure automatically generated with correct naming\n\n**4. Deployed Site**: Static version online via GitHub Pages, optimal performance\n\n![Static Site](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j5hdlra30mfjtey99rck.png)\n\nThe whole thing forms a simple publishing chain: write in WordPress, publish, and let the rest execute.\n\n**Repository**: [atomic-jamstack-connector](https://github.com/pcescato/atomic-jamstack-connector)\n\n### Production Update: Multi-Destination Publishing\n\nWhile building this plugin, I realized something obvious in hindsight: **the dev.to API has existed for years**.\n\nI'd been so focused on static site generators that I missed the simpler path: publish directly to dev.to via API.\n\n**So I added it.**\n\n#### Three Publishing Modes\n\n**1. GitHub Only (Static Sites)**\n```\nWordPress ‚Üí Hugo/Jekyll/Astro ‚Üí GitHub Pages\n```\nYour traditional JAMstack workflow. WordPress writes, Hugo builds, GitHub deploys.\n\n**2. Dev.to Primary**\n```\nWordPress ‚Üí Dev.to (API)\n```\nNo GitHub. No static site. Just WordPress ‚Üí dev.to via API.\n\nPerfect for developers who want WordPress's editor but dev.to's community without managing infrastructure.\n\n**This is the mode I use.** All my articles are published exclusively on dev.to.\n\n**3. Secondary (Dual Publishing)**\n```\nWordPress ‚Üí Hugo (GitHub Pages) ‚Üê canonical\n          ‚îî‚Üí Dev.to (with canonical_url)\n```\nThe best of both worlds:\n- Hugo site = your canonical source (your domain, your control)\n- Dev.to = syndication (massive audience, zero SEO penalty)\n- Single click publishes to both\n\n![Dev.to Publishing Settings](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/90inyrrai0onixjcks37.png)\n*Publishing destination selector: GitHub Only vs Dev.to modes*\n\n\n![Dev.to Configuration](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/30vahdn4jmb8pw9ytvbm.png)\n*API key configuration and publishing mode selection*\n\n#### Why This Matters\n\n**For solo bloggers**: Write in WordPress, publish everywhere. One workflow, multiple audiences.\n\n**For teams**: Centralized content in WordPress, distributed publishing to Hugo (marketing site) + dev.to (developer community).\n\n**For migrations**: Start with GitHub Only, test dev.to in Secondary mode, migrate to Primary if you want. Zero lock-in.\n\n#### Technical Implementation\n\nThe dev.to adapter uses the same pattern as Hugo/Jekyll adapters:\n\n**WordPress post** ‚Üí **Adapter converts to format** ‚Üí **API client publishes**\n\n```php\n// Dev.to adapter converts to Forem-compatible markdown\n$front_matter = \"---\ntitle: {$post->post_title}\npublished: true\ntags: wordpress, jamstack, automation\ncanonical_url: https://yourblog.com/post-slug\n---\";\n\n// API client publishes\n$devto_api->publish_article($markdown, $article_id);\n```\n\n**Key differences from static site adapters**:\n- Images stay on WordPress (dev.to fetches via URL)\n- No Git commits (direct API POST)\n- Instant publishing (no build step)\n- Canonical URL automatically generated in Secondary mode\n\n#### Real-World Usage\n\n**This article** was published using dev.to's rich editor before I implemented the adapter.\n\n**Future articles will be published via the plugin**: I write in WordPress, click Publish, and the plugin handles the rest via dev.to's API.\n\nThe dev.to API isn't new. The Forem platform has supported it for years.\n\nWhat's new is the integration: WordPress as the writing environment, dev.to as the publishing platform, zero manual steps.\n\nThat's the difference between a demo and production: the tool becomes part of the workflow, not just a talking point.\n\n### Technical Highlights\n\n**1. Universal Front Matter Engine**\n\nInstead of hardcoding the plugin for a single Hugo theme, we built a raw template system. Users define their own YAML (or TOML) with custom delimiters and placeholders like `{{id}}`, `{{title}}`, or `{{image_avif}}`.\n\nThis means the same plugin can adapt to any SSG convention:\n- Hugo with YAML front matter\n- Jekyll with different taxonomy names  \n- Eleventy with custom data structures\n\nYou control the output format. The plugin just fills in the blanks.\n\n**2. Asset Management by WordPress ID**\n\nTo guarantee unbreakable links, optimized images (WebP and AVIF) are stored in folders named by WordPress ID: `static/images/1460/`.\n\nRename your post slug for SEO ten times? Your images never break. The ID is immutable. The file paths are permanent.\n\n**3. Native WordPress Integration**\n\nThe plugin integrates as a first-class citizen with its own sidebar menu and tabbed navigation.\n\n**Role-based security**: Authors only see their own sync history. Critical settings (GitHub PAT) remain admin-only.\n\n**Responsible cleanup**: A \"Clean Uninstall\" option removes all plugin traces (options and post meta) on uninstall, leaving zero database pollution.\n\n**4. Atomic Commits via GitHub Trees API**:\nInstead of multiple sequential commits (one per image, one for Markdown), the plugin uses GitHub's Git Data API to create a single commit containing all files:\n\n```php\n// Collect all files (Markdown + images)\n$all_files = [\n    'content/posts/2026-02-07-this-is-a-post.md' => $markdown_content,\n    'static/images/1447/featured.webp' => $webp_binary,\n    'static/images/1447/featured.avif' => $avif_binary,\n    'static/images/1447/wordpress-to-hugo-1024x587.webp' => $webp_binary,\n    'static/images/1447/wordpress-to-hugo-1024x587.avif' => $avif_binary,\n];\n\n// Single atomic commit\n$git_api->create_atomic_commit($all_files, \"Publish: This is a Post\");\n```\n\nThis approach is transactional: either everything commits or nothing does. No partial states, cleaner history.\n\n**5. Beyond Hugo: Multi-SSG Architecture**\nWhile this demo targets Hugo, the adapter pattern isn't locked to a single SSG. The same codebase can support:\n\n- Hugo (YAML/TOML front matter, content/posts/)\n- Jekyll (different taxonomy conventions, _posts/)\n- Eleventy (custom data structures, src/content/)\n- Astro (content collections, src/content/blog/)\n\nAdding a new SSG means writing one adapter class ‚Äî the sync engine, image optimization, and GitHub integration remain untouched.\nThis architectural choice transforms the plugin from \"Hugo-only\" to a platform for any static site workflow. The 43 million WordPress sites aren't just potential Hugo users ‚Äî they're potential static site adopters, period.\n\n**6. WordPress-Native Compliance**:\nTo meet WordPress.org requirements, the plugin uses exclusively native WordPress APIs:\n\n- `wp_remote_post()` instead of curl\n- `WP_Filesystem` instead of `file_put_contents()`\n- `$wpdb` prepared statements\n- No `exec()`, `shell_exec()`, or Git CLI\n\nThis makes it suitable for publication in the official WordPress plugin repository.\n\n## My Experience with GitHub Copilot CLI\n\nWhen I started this project, I wasn't looking for a tool to code for me.\n\nI was looking for a way to accelerate the execution of a project whose architecture was already clear.\n\nHaving already used GitHub Copilot CLI, Gemini CLI, and various LLMs on other projects, I knew these tools could produce code quickly.\n\nBut I also knew that without a precise framework, they mainly produce... code.\n\nNot necessarily a coherent system.\n\n**Note**: This isn't the autocomplete in the editor, but a command-line tool capable of generating complete files from structured prompts.\n\n### Specification First, Code Second\n\nThe first step wasn't to code. The first step was to write specifications. Define the scope. Break down the project into functional blocks.\n\nIdentify non-negotiable constraints: WordPress native only, no shell execution, reliable async processing, atomic GitHub commits, WordPress.org compliance to publish the plugin in the official repository and benefit the community.\n\nThen organize development into successive stages.\n\nThis work is very similar to what a technical project manager would do before entrusting implementation to a team. The difference here is that the \"team\" consisted of a tool capable of producing code very quickly ‚Äî but only if the instructions were clear and precise.\n\nSo I didn't write code in the traditional sense. I wrote functional and technical specifications, prompts, refined instructions, corrected trajectories.\n\nEach step consisted of describing what needed to be built, verifying what was produced, then adjusting.\n\nSometimes Copilot proposed a relevant structure on the first try. Sometimes it required reworking, clarifying, constraining further.\n\n### The Work Pattern\n\nVery quickly, a work pattern emerged: specification ‚Üí generation ‚Üí verification ‚Üí correction ‚Üí iteration.\n\nIn this process, Copilot behaves less like a magic generator than like a fast executor.\n\nIt can structure an entire class in seconds, propose a coherent implementation, or refactor a complete block.\n\nBut it can also forget an essential hook, overwrite an existing method, or produce functional code that doesn't comply with initial constraints.\n\n**Real Examples of Issues**:\n\n- Method replaced by an incomplete stub\n- Hook not registered, causing silent failures\n- File generated but not actually written to disk\n- Fatal error on activation, typical of strict WordPress environments\n\nEach incident required going back to fundamentals: verify, understand, correct, reformulate.\n\n### What This Actually Means\n\nThis is probably the most interesting aspect of the experience.\n\nUsing Copilot effectively doesn't mean writing one prompt and waiting for a result.\n\nIt's much more like continuous piloting, where the quality of instructions directly conditions the quality of what's produced.\n\nIn this context, the tool becomes particularly effective for accelerating everything that's structured: class creation, file organization, repetitive function implementation, refactoring, documentation.\n\nAs soon as the objective is clearly defined, execution can become very fast.\n\nBut the responsibility for architecture, technical choices, and overall coherence remains entirely human.\n\n### The Real Value\n\nIn the end, the experience is less like \"AI-assisted development\" than a form of assisted technical direction.\n\nCode is produced quickly, but it must be thought out, supervised, and validated continuously.\n\nThis project was built in less than two days. Not because the tool replaces design work, but because once that work is done, execution can be considerably accelerated.\n\nThis is probably where GitHub Copilot CLI becomes most interesting: it's not a substitute for development, but an accelerator for an already thought-out and structured project.\n\n### The Audit Trail Advantage\n\nBeyond just writing code, GitHub Copilot CLI acts as a technical scribe.\n\nMy session history evolved through **22 distinct checkpoints**, documenting every architectural pivot from the initial foundation to the final security hardening:\n\n```\n001-wordpress-plugin-foundation\n002-media-processing-with-avif-support\n003-deletion-and-bulk-sync\n004-atomic-commits-and-monitoring\n...\n019-fix-plugin-check-errors\n020-add-nonce-security\n021-uninstall-api-compliance\n022-fix-nonce-sanitization-warnings\n```\nBrowse the complete checkpoint history here: [https://github.com/pcescato/atomic-jamstack-connector/tree/main/copilot-doc](https://github.com/pcescato/atomic-jamstack-connector/tree/main/copilot-doc)\n\nEach checkpoint includes context files like `wordpress-api-compliance-guide.md`, `token-preservation-fix.md`, or `settings-merge-test-plan.md`.\n\nThis isn't just a side effect ‚Äî it's a massive win for maintainability.\n\nIt transforms the \"black box\" of AI generation into a transparent, step-by-step engineering log. Six months from now, when I need to understand why a particular decision was made, I won't be guessing. The checkpoint history tells the story.\n\n**What worked well**:\n‚úÖ Rapid scaffolding of classes following WordPress standards\n‚úÖ Boilerplate code generation (hooks, filters, nonces)\n‚úÖ Refactoring large blocks (sequential commits ‚Üí atomic commits)\n‚úÖ Documentation generation from inline comments\n\n**What required constant supervision**:\n‚ö†Ô∏è Architecture decisions (adapter pattern, async queues)\n‚ö†Ô∏è WordPress.org compliance verification\n‚ö†Ô∏è Error handling and edge cases\n‚ö†Ô∏è Integration testing across components\n\n### Conclusion\n\nGitHub Copilot CLI didn't replace development.\n\nIt didn't eliminate the need to think, architect, or decide.\n\nBut used as an execution partner rather than an automatic generator, it made it possible to quickly transform a clear idea into a functional system.\n\nThat's perhaps where these tools really make sense: they don't change the way we build, but they reduce the distance between what we imagine and what we put into production.\n\nIn this specific case, they made it possible to solve a real tension: write comfortably in WordPress while publishing to a high-performance static site.\n\nNo friction. No compromises.\n\nJust a workflow that works.\n\n---\n\n*If you're interested in other approaches to moving away from WordPress, check out my [Farewell to WordPress](https://dev.to/pascal_cescato_692b7a8a20/series/34014) series, where I explore migrations to Hugo and Astro with different strategies and trade-offs.*",
      "stars": null,
      "comments": 11,
      "upvotes": 34,
      "read_time": "14 min read",
      "language": null
    },
    {
      "title_en": "Why do you write?",
      "url": "https://dev.to/itsugo/why-do-you-write-5bkf",
      "source": "devto",
      "published_at": "2026-02-04T05:41:16+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "programming",
        "learning",
        "mentalhealth"
      ],
      "content_length": 4633,
      "content_preview": "### A recognition\n\nThere is a line by @sylwia-lask that stayed with me when I first read it. She [wrote about](https://dev.to/sylwia-lask/your-github-contribution-graph-means-absolutely-nothing-and-heres-why-2kjc) how writing can feel easier than coding after a long day.\n\nI remember recognizing myself in that immediately. Not because coding is harder. But because writing asks something different from me.\n\nAfter a full day of debugging, context switching, and holding systems in my head, coding ca",
      "content_full": "### A recognition\n\nThere is a line by @sylwia-lask that stayed with me when I first read it. She [wrote about](https://dev.to/sylwia-lask/your-github-contribution-graph-means-absolutely-nothing-and-heres-why-2kjc) how writing can feel easier than coding after a long day.\n\nI remember recognizing myself in that immediately. Not because coding is harder. But because writing asks something different from me.\n\nAfter a full day of debugging, context switching, and holding systems in my head, coding can feel heavy. My brain is already full. Writing, on the other hand, feels like letting the noise settle.\n\nThat is why I keep coming back to it.\n\n---\n\n### Writing does not drain me\n\nWhen I write, I am not trying to solve a problem efficiently. I am trying to understand what I think.\n\nThere is no compiler. No correct answer. No pressure to be fast or precise upfront. I can move slowly. I can circle around an idea. I can admit uncertainty without it becoming a blocker.\n\nMost importantly, writing clarifies thought instead of demanding it. I do not need to know exactly what I am saying when I start. The act of writing is how I find out.\n\nThat is very different from how I approach code.\n\n---\n\n### Lowering emotional noise\n\nA lot of thoughts do not need solutions. They need space.\n\nWhen I do not write, those thoughts stay half formed. They repeat. They get louder. Everything feels heavier than it actually is.\n\nWriting externalizes them.\n\nOnce something is on the page, it becomes quieter. Not solved. Just contained. That alone makes it easier to think clearly again.\n\nThis is especially true when I am tired. Writing helps me process without spiraling. It gives shape to things that would otherwise stay tangled.\n\n---\n\n### Writing as explanation practice\n\nAnother reason I write is simpler. It forces honesty.\n\nIf I cannot explain something in plain language, I usually do not understand it as well as I think I do. Writing exposes that immediately.\n\nThis is not about dumbing things down. It is about removing unnecessary complexity.\n\nThe same instinct that makes me want clean systems in code makes me want clear sentences in writing. Both are forms of respect. For the reader and for myself.\n\n---\n\n### Processing without ranting\n\nI try not to vent while writing (Àâ‚ñΩÀâÔºõ).... I write to understand why something affected me.\n\nThat distinction matters.\n\nWriting lets me slow down emotional reactions and turn them into observations. It creates just enough distance to be honest without being reactive.\n\nBy the time something becomes a blog post, it is usually because I have sat with it long enough to see more than one angle.\n\nThat is also why I keep the tone simple and human. I am not trying to perform intelligence or confidence. I am trying to be accurate.\n\n---\n\n### Memory, quietly\n\nThere is another side effect I did not expect. Writing helps me remember.\n\nIdeas I write about stick longer. Experiences I reflect on become clearer reference points later. Writing turns moments into markers.\n\nI do not always reread my posts. But I remember what I learned while writing them.\n\n---\n\n### Why I keep writing\n\nI do not write because I always have something important to say.\nI write because it is how I think when thinking gets crowded.\n\nCoding builds systems. Writing builds understanding. Both matter. But on tired days, writing is what keeps me grounded.\n\nIn that sense, writing is not productivity for me. It is recovery.\nIt is thinking out loud, slowly, without needing to ship.\n\n(Some of these reflections live on DEV. Some live elsewhere, where I give myself more room to be [personal](https://itsugo-portfolio.vercel.app/blog))\n\nIf you write too, privately or publicly, what is your why?\n\n---\n\n### A quiet thank you\n\nOne small thing I want to acknowledge.\nOver the last three months, more people started reading what I write than I ever expected. I recently crossed 3,000 followers here, and that number still feels unreal to say out loud.\n\nNot because of the number itself, but because of the conversations that came with it.\n\nSome of the comments I have received made me pause, reread them, and honestly tear up. People sharing their own stories. Saying they felt seen. Saying something I wrote helped them put words to a feeling they could not explain yet.\n\n![iloveyouall3000](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hb8tg63qz6wn6xap8zym.jpg)\n\nI think that kind of response is not something you optimize for. You earn it slowly, by being honest and showing up consistently.\n\nSo if you have read, commented, or quietly followed along, a big **THANK YOU**. Your responses have helped me keep going.\n",
      "stars": null,
      "comments": 57,
      "upvotes": 60,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Go Made Me Fast. Rust Made Me Care. AWS Made Me Pay.",
      "url": "https://dev.to/art_light/go-made-me-fast-rust-made-me-care-aws-made-me-pay-2f82",
      "source": "devto",
      "published_at": "2026-02-07T17:58:23+00:00",
      "external_id": null,
      "tags": [
        "go",
        "rust",
        "aws",
        "cloud"
      ],
      "content_length": 7581,
      "content_preview": "For years, my cloud architecture felt‚Ä¶ reasonable.\n\n- Go services\n- AWS infrastructure\n- Containers everywhere\n- A few Lambdas sprinkled on top\n- Dashboards mostly green\n\nDeployments were fast. Engineers were productive. Nobody complained.\n\nWhich, in retrospect, should have been my first red flag.\n\nBecause in the cloud, systems don‚Äôt usually fail loudly.\nThey fail financially.\n\n## The Comfortable Phase: When Go + AWS Feels Like a Superpower\n\nGo is dangerously good at making things feel under con",
      "content_full": "For years, my cloud architecture felt‚Ä¶ reasonable.\n\n- Go services\n- AWS infrastructure\n- Containers everywhere\n- A few Lambdas sprinkled on top\n- Dashboards mostly green\n\nDeployments were fast. Engineers were productive. Nobody complained.\n\nWhich, in retrospect, should have been my first red flag.\n\nBecause in the cloud, systems don‚Äôt usually fail loudly.\nThey fail financially.\n\n## The Comfortable Phase: When Go + AWS Feels Like a Superpower\n\nGo is dangerously good at making things feel under control.\n\nYou write a service.\nIt compiles instantly.\nIt deploys cleanly.\nIt runs forever.\n\nThe language gives you:\n\n- A simple concurrency model\n- A strong standard library\n- Predictable builds\n- Small, static binaries\n\nAWS gives you:\n\n- Infinite capacity (theoretically)\n- Managed everything\n- Autoscaling\n- Alarms that only fire when it‚Äôs already too late\n\nTogether, they create a powerful illusion:\n\n> ‚ÄúThis system is efficient because it‚Äôs simple.‚Äù\n\nEarly on, that illusion is mostly true.\n\n## Why Go Dominates Cloud Backends (And Rightfully So)\n\nLet‚Äôs be fair. Go didn‚Äôt become the default cloud language by accident.\n\n**1. Developer Throughput Is King**\n\nGo minimizes decision fatigue:\n\n- One formatting style\n- One dependency system\n- One way to do concurrency\n- One obvious deployment artifact\n\nYou don‚Äôt debate architecture for weeks. You ship.\n\nIn cloud environments, time-to-production often matters more than micro-optimizations.\n\n**2. Cold Starts Are Friendly**\n\nCompared to JVM-based stacks, Go binaries:\n\n- Start fast\n- Load minimal runtime state\n- Play nicely with Lambda and container autoscaling\n\nThat alone makes Go an AWS favorite.\n\n**3. Operational Predictability**\n\nMost Go services fail in boring ways:\n\n- Panics are obvious\n- Memory usage is mostly stable\n- Performance cliffs are gradual\n\nThis makes on-call rotations survivable.\n\nSo yes‚ÄîGo earns its place.\n\n## The Slow Burn: When ‚ÄúGood Enough‚Äù Starts Billing You\n\nHere‚Äôs the thing about cloud systems:\n\nThey don‚Äôt punish inefficiency immediately.\n\nInstead, they do it quietly:\n\n- +10% CPU here\n- +200MB memory there\n- One more instance ‚Äújust in case‚Äù\n- A larger task size because ‚Äúit‚Äôs safer‚Äù\n\nNo single decision is outrageous.\nTogether, they compound.\n\nYour AWS bill doesn‚Äôt spike.\nIt creeps.\n\nAnd creeping costs are the hardest to fight‚Äîbecause nothing is obviously broken.\n\n## Garbage Collection: The Tax You Don‚Äôt See Until You Do\n\nGo‚Äôs garbage collector is one of its greatest achievements.\nIt‚Äôs also one of its biggest cloud liabilities.\n\nModern Go GC is:\n\n- Low latency\n- Concurrent\n- Well-tuned for most workloads\n\nBut ‚Äúwell-tuned‚Äù doesn‚Äôt mean free.\n\n## What GC Actually Costs You in AWS\n\n- Extra memory headroom to avoid pressure\n- CPU cycles during mark-and-sweep\n- Unpredictable latency under load\n- Lower container density\n\nIn isolation, this is fine.\nAt scale, it becomes infrastructure policy.\n\nYou don‚Äôt notice GC directly.\nYou notice it when:\n\n- You bump task memory ‚Äújust to be safe‚Äù\n- You avoid tighter instance packing\n- You scale horizontally earlier than expected\n\nAWS doesn‚Äôt care why you need more resources.\nIt just invoices.\n\n## When Rust Entered the Picture (Not by Choice)\n\nI didn‚Äôt wake up one day thinking:\n\n> ‚ÄúI should rewrite this in Rust for fun.‚Äù\n\nRust showed up when Go stopped being comfortably invisible.\n\nSpecific workloads forced the issue:\n\n- High-throughput ingestion services\n- Streaming pipelines\n- Real-time data processing\n- Hot paths doing millions of ops per second\n\nThese weren‚Äôt business-logic-heavy services.\nThey were physics-heavy services.\n\nThat‚Äôs where Go started to show friction.\n\n## Rust Is Not Faster by Default (That‚Äôs the Lie)\n\nLet‚Äôs kill a myth right now:\n\nRust doesn‚Äôt magically make your system fast.\n\nWhat it does is remove excuses.\n\nRust forces you to confront:\n\n- Allocation patterns\n- Ownership boundaries\n- Memory layout\n- Cache behavior\n- Thread communication\n\nIn Go, you can ignore these things for a long time.\nIn Rust, you can‚Äôt.\n\nAnd that‚Äôs the point.\n\n## The First Rust Service Was Miserable\n\nI‚Äôll be honest.\n\nMy first Rust microservice:\n\n- Took 3√ó longer to write\n- Had more compiler errors than actual code\n- Made me question my life choices\n\nBut once it ran‚Ä¶ something strange happened.\n\n## The Metrics Were Boring\n\n- Flat memory usage\n- Stable latency\n- CPU exactly where expected\n- No surprises under load\n\nThe service behaved like a physical object.\nPredictable. Measurable. Honest.\n\n## Rust Changes How You Design Cloud Systems\n\nRust doesn‚Äôt just change code.\nIt changes architecture.\n\n**1. You Stop Over-Allocating ‚ÄúJust in Case‚Äù**\n\nBecause allocation is explicit, you:\n\n- Reuse buffers\n- Stream data\n- Think in lifetimes instead of heaps\n\nThis directly reduces memory footprints.\n\n**2. You Design for Data Flow, Not Convenience**\n\nRust pushes you toward:\n\n- Clear ownership boundaries\n- Immutable-by-default data\n- Explicit mutation points\n\nThat leads to simpler mental models for concurrency.\n\n**3. You Scale Vertically Before Horizontally**\n\nWhen services are efficient, you can:\n\n- Pack more workloads per instance\n- Delay autoscaling\n- Reduce cross-service chatter\n\nAWS pricing loves vertical efficiency.\n\n## The AWS Angle: Where Language Choice Hits the Bill\n\nHere‚Äôs where things got uncomfortably concrete.\n\n**EC2**\n\n- Rust services ran comfortably on smaller instance types\n- Go services needed more memory headroom\n- Cache efficiency mattered more than raw cores\n\n**ECS / EKS**\n\n- Higher container density with Rust\n- Fewer OOM kills\n- More predictable autoscaling behavior\n\n**Lambda**\n\n- Rust cold starts were consistently low\n- Memory-to-performance ratio was better\n- Lower cost for CPU-heavy functions\n\nNone of this showed up in benchmarks alone.\nIt showed up in monthly invoices.\n\n## The Hybrid Reality: Stop Framing This as Go vs Rust\n\nThis isn‚Äôt a language war.\nIt‚Äôs a resource allocation problem.\n\nWhat actually worked was intentional language placement.\n\n**Go Is Still Perfect For:**\n\n- APIs\n- Control planes\n- Admin services\n- Glue code\n- Prototyping\n- Business logic\n\n**Rust Shines At:**\n\n- Data pipelines\n- High-throughput services\n- Latency-sensitive components\n- CPU-bound workloads\n- Edge services\n\nAWS doesn‚Äôt care which language you love.\nIt cares how efficiently you use silicon.\n\n## Observability Tells the Truth (Eventually)\n\nOnce both Go and Rust services ran side by side, observability stopped being abstract.\n\nMetrics made the differences obvious:\n\n- Memory curves\n- Tail latency\n- CPU saturation\n- Scaling behavior\n\nThe systems weren‚Äôt competing.\nThey were revealing trade-offs.\n\n## The Real Lesson: Languages Encode Values\n\nGo values:\n\n- Simplicity\n- Speed of development\n- Team scalability\n\nRust values:\n\n- Correctness\n- Explicitness\n- Long-term efficiency\n\nAWS values:\n\n- Utilization\n- Predictability\n- You not asking questions about pricing\n\nChoosing a language is choosing which values you want to pay for.\n\n## Why This Matters More as You Scale\n\nEarly-stage teams should absolutely optimize for speed.\nGo is fantastic there.\n\nBut as systems mature:\n\n- Margins tighten\n- Load increases\n- Bills stop being theoretical\n\nThat‚Äôs when efficiency stops being ‚Äúpremature optimization‚Äù\nand starts being infrastructure hygiene.\n\n## Final Thoughts: The Cloud Is an Honesty Machine\n\nThe cloud doesn‚Äôt care about elegance.\nIt doesn‚Äôt care about trends.\nIt doesn‚Äôt care about your favorite language.\n\nIt measures:\n\n- CPU cycles\n- Memory usage\n- Network traffic\n- Time\n\nAnd it charges you accordingly.\n\nGo helps you move fast.\nRust helps you understand cost.\nAWS makes sure you learn the difference.",
      "stars": null,
      "comments": 12,
      "upvotes": 92,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "Welcome to The Foundation: Preserving Developer Knowledge in Public",
      "url": "https://dev.to/the-foundation/welcome-to-the-foundation-preserving-developer-knowledge-in-public-3842",
      "source": "devto",
      "published_at": "2026-02-06T14:18:55+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "community",
        "opensource",
        "ai"
      ],
      "content_length": 2579,
      "content_preview": "We're building public alternatives to private AI knowledge.\n\n## Why We Exist\n\nStack Overflow's traffic dropped 78% in two years. Wikipedia gets buried by Google's AI summaries. Developers solve problems in private AI chats that leave no public trace.\n\n**The result:** Junior developers have no Stack Overflow to learn from. The knowledge commons that taught verification, architecture, and skepticism is dying.\n\n**Our mission:** Preserve and build public knowledge infrastructure.\n\n## What We're Buil",
      "content_full": "We're building public alternatives to private AI knowledge.\n\n## Why We Exist\n\nStack Overflow's traffic dropped 78% in two years. Wikipedia gets buried by Google's AI summaries. Developers solve problems in private AI chats that leave no public trace.\n\n**The result:** Junior developers have no Stack Overflow to learn from. The knowledge commons that taught verification, architecture, and skepticism is dying.\n\n**Our mission:** Preserve and build public knowledge infrastructure.\n\n## What We're Building\n\n**Right now (Bridge solution):**\n- Curate valuable discussions\n- Document reasoning paths publicly\n- Share verification techniques\n- Mentor explicitly in public\n\n**Soon (Federated infrastructure):**\n- ActivityPub-based Q&A platform\n- Community-owned (no corporate control)\n- Federated (multiple instances, no single point of failure)\n- Open source from day one\n\n## How This Started\n\nFrom a series of articles:\n1. [My Chrome Tabs Tell a Story We Haven't Processed Yet](https://dev.to/dannwaneri/my-chrome-tabs-tell-a-story-we-havent-processed-yet-ec9)\n2. [We're Creating a Knowledge Collapse and No One's Talking About It](https://dev.to/dannwaneri/were-creating-a-knowledge-collapse-and-no-ones-talking-about-it-226d)\n3. [Above the API: What Developers Contribute When AI Can Code](https://dev.to/dannwaneri/above-the-api-what-developers-contribute-when-ai-can-code-5025)\n\nThe comments proved something: developers want to build alternatives, not just complain.\n\n## Current Members\n\n- [@dannwaneri](https://dev.to/dannwaneri) - Founder, Cloudflare Workers specialist\n- [@richardpascoe](https://dev.to/richardpascoe) - Fediverse advocate, builder\n- [@nandofm](https://dev.to/nandofm) - Self-hosting specialist, months of thinking on knowledge entropy\n\n## How to Contribute\n\n**If you're interested in:**\n- Writing about verification techniques in the AI era\n- Documenting architecture decisions publicly\n- Building federated infrastructure\n- Preserving developer knowledge\n\n**You're welcome here.**\n\nComment below or message us to get involved.\n\n## The Bridge to Building\n\nWe're using this organization as a bridge:\n1. **Test** what developers actually need (on existing platform)\n2. **Document** what works and what doesn't\n3. **Build** federated version with real user feedback\n\nWe don't wait for perfect infrastructure. We start with imperfect platforms, learn fast, build better.\n\n## Join Us\n\nIf you believe knowledge should compound publicly, you belong here.\n\nLet's build what comes after Stack Overflow.\n\n---\n\n*Questions? Comments? Want to help? Drop a comment below.*",
      "stars": null,
      "comments": 20,
      "upvotes": 46,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "Got Gemini CLI questions? The Google team will answer DEV's questions in their new livestream series!",
      "url": "https://dev.to/devteam/got-gemini-cli-questions-the-google-team-will-answer-devs-questions-in-their-new-livestream-5b7j",
      "source": "devto",
      "published_at": "2026-02-05T18:41:27+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "gemini",
        "cli",
        "ai"
      ],
      "content_length": 1705,
      "content_preview": "Hey everyone!\n\nThe Google Cloud team is launching **Google Cloud Live**, a no fluff weekly livestream series designed around hands-on building, vibe coding, and live debugging. This livestream series will occur every Tuesday at 9A PT / 12P ET, with new developer topics weekly.\n\nAnd the best part? They specifically want to hear from the DEV community and answer YOUR questions!\n\n---\n\nFor their first-ever episode, hosts @stephr_wong and @greggyb, along with special guest Denise Kwan, are skipping t",
      "content_full": "Hey everyone!\n\nThe Google Cloud team is launching **Google Cloud Live**, a no fluff weekly livestream series designed around hands-on building, vibe coding, and live debugging. This livestream series will occur every Tuesday at 9A PT / 12P ET, with new developer topics weekly.\n\nAnd the best part? They specifically want to hear from the DEV community and answer YOUR questions!\n\n---\n\nFor their first-ever episode, hosts @stephr_wong and @greggyb, along with special guest Denise Kwan, are skipping the console and showing you how to manage your entire AI workflow from the command line.\n\nIn \"**Getting Started with Gemini CLI**\", they'll dive into:\n\n- Setting up your terminal for AI-first development\n- Managing prompts and deployments without leaving your session\n- Real-time debugging of CLI extensions\n- Your questions!\n\nWhether you're curious about specific CLI features, wondering about integration with your current workflow, or have questions about AI development tooling in general, **drop your questions in the comments below** and the team will do their best to respond.\n\nYour questions will help shape the conversation and make this session as valuable as possible for the community. Please note that due to time restrictions, not all questions may be answered -  feel free to leave reactions to your favorite questions to help prioritize.\n\n{% card %}\n\n## Stream Details\n\n- üìÖ **When:** February 10, 9 AM PT / 12 PM ET  \n- üì∫ **Where:** \n  - We'll be simulcasting this livestream right on the DEV homepage!\n\n{% cta https://www.youtube.com/live/IecXj2WFkLs %}\nSubscribe or Get Notified on YouTube\n{% endcta %}\n\n{% endcard %}\n\n**SO, what do you want to know about Gemini CLI? Drop them below!** üëá",
      "stars": null,
      "comments": 7,
      "upvotes": 48,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "I Vibe Coded a Multiplayer ASL Game using MQTT! üåê",
      "url": "https://dev.to/francistrdev/i-vibe-coded-a-multiplayer-asl-game-using-mqtt-480",
      "source": "devto",
      "published_at": "2026-02-03T18:25:56+00:00",
      "external_id": null,
      "tags": [
        "mqtt",
        "webdev",
        "discuss",
        "showdev"
      ],
      "content_length": 4867,
      "content_preview": "Last year (2 months ago), I created a game using **p5.js and ml5.js** that uses Machine Learning to detect the ASL alphabet. I made this project for my course \"IoT with Machine Learning\" at my University and it taught me a lot about the process and how IoT is shaping the world!\n\nOne of the things that I utilize in this project is using MQTT (Message Queuing Telemetry Transport). It is different from Web Sockets because of how it is use for IoT specifically since it is designed for constrained de",
      "content_full": "Last year (2 months ago), I created a game using **p5.js and ml5.js** that uses Machine Learning to detect the ASL alphabet. I made this project for my course \"IoT with Machine Learning\" at my University and it taught me a lot about the process and how IoT is shaping the world!\n\nOne of the things that I utilize in this project is using MQTT (Message Queuing Telemetry Transport). It is different from Web Sockets because of how it is use for IoT specifically since it is designed for constrained devices such as Arduino, which I used for this project.\n\nBut I want to use MQTT for _**just**_ the multiplayer functionality. I had some practice with Web Sockets, but thought to myself...\"How far can I go with MQTT\"?\n\n---\n\n# Overview of the Project\nThis project is a desktop application. It is a simple game where your goal is to spell out as many words as you can in ASL. You can play either Single player or Multiplayer. It utilizes Machine Learning to perform Pose Estimation on the user's hand points using **ml5.js**. You can learn more about it [on their documentation](https://docs.ml5js.org/#/reference/handpose).\n\n\n# Technologies I Used\nTo make the desktop application, I used [Tauri](https://v2.tauri.app/start/prerequisites) which is framework to create your own desktop/mobile app. It is very nice since you can do Web Development stuff inside of Tauri such as React, Vue, or just Vanilla HTML, CSS, and JavaScript. For this project, I stick to Vanilla HTML, CSS, and JavaScript. I also used **p5.js and ml5.js** for my libraries as I mentioned earlier. I also used SupaBase to store high scores of the words they accomplished.\n\n\n# ASL Machine Learning Model\n![Main Menu](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iula9uzolr047vyeb34a.png)\n\nThis rough flow diagram shows the following of the game functionality. When you do ASL, it detects your hand points and the gesture you are doing. It then feeds the data points into the Machine Learning Model and it predicts which alphabet you are gesturing in real-time. Once the game is done, it uploads your data to the database of your score and coins you have received.\n\nThe Arduino part is an extension that shows you data, such as your \"Letters per second\" and your username, on an LCD which is shown here:\n\n![Arduino](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jgj7w88qyao4iu3s5a19.png)\n\n\n# MQTT Multiplayer Functionality\nTo accomplished this, I use [Mosquitto](https://mosquitto.org/) as my broker. This diagram shows the flow of how the Multiplayer game works:\n\n![MQTT Diagram](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tno2x8gnuf9rsmo107fz.png)\n\nIn short, you can create a room and it will provide you a code to share with other users to join the room. In the lobby, when everyone clicks \"Ready\" the game starts. Demo is shown here below:\n\n![Demo](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/i20s1v6xrd7rvyf0qo4k.gif)\n\n\n# Challenges I Faced\nThere are are few big things I faced when creating this project.\n\n### 1. MQTT setup\nI had the idea of how it works, but it is hard to implement in practice since I am not really using an IoT device to communicate to the broker (Unless you count the camera as an IoT device). Since it was vibe coded, I had to take the time to understand the flow and how data is being pass/received.\n\n### 2. Creating my own Model\nI started with Image Classification. But the problem is that it captures the WHOLE image in addition to your hands. Meaning if I use the model in a different setting, it would not predict it accurately. I then learn about Hand pose in ml5.js documentation and I start building my own model. I had to create a separate vibe coded project that is design to create your own Hand pose Model.\n\n![Own Model](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zrkc9rlwoptfp1wacysv.png)\n\n### 3. Training problems\nFor context, I trained letters from A-Z (\"J\" and \"Z\" are excluded). For each letter, it captures around 2000 Post Estimation data. After training is complete, it forgets the last last two letters, which are \"X\" and \"Y\". The solution I came up with is that during data collecting, if I reach either \"X\" or \"Y\", collect more than 2000 Post Estimation data. In this case, \"X\" will get 4000 and \"Y\" gets 6000 (Just to be safe). **_If anyone knows why this is the case, please let me know!_**\n\n![Code](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/cr95tut87s3uo8sla9nv.png)\n\n---\n\n# Summary\nIt was a fun project! I learned a lot about not only how to connect the Arduino to the web, but also how to use MQTT to the limit (to my knowledge). If you are interested more about this project, check out the [GitHub Repository here!](https://github.com/FrancisTRAlt/ASL-Pacer)\n\n---\n\n**_Any questions, or comments? I would love to hear from you!_**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "stars": null,
      "comments": 13,
      "upvotes": 33,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Dunning-Kruger Effect and Imposter Syndrome: Developer's Hidden Caveat that is Rarely Spoken Off.",
      "url": "https://dev.to/francistrdev/dunning-kruger-effect-and-imposter-syndrome-developers-hidden-caveat-that-is-rarely-spoken-off-41jg",
      "source": "devto",
      "published_at": "2026-02-05T19:10:43+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "community",
        "mentalhealth",
        "programming"
      ],
      "content_length": 4268,
      "content_preview": "Yesterday (Feb 4), my database Professor mentions about the \"**Dunning-Kruger Effect**\" early on in our first day. He mentioned that it is a recurring issue for developers because of how \"_People with limited expertise in an area tend to overestimate how much they know‚Äîand we all have gaps in our expertise_\" (https://lsa.umich.edu/psych/news-events/all-news/faculty-news/the-dunning-kruger-effect-shows-that-people-don-t-know-what-they.html).\n\nWhile it is an issue, we as developers also have \"**Im",
      "content_full": "Yesterday (Feb 4), my database Professor mentions about the \"**Dunning-Kruger Effect**\" early on in our first day. He mentioned that it is a recurring issue for developers because of how \"_People with limited expertise in an area tend to overestimate how much they know‚Äîand we all have gaps in our expertise_\" (https://lsa.umich.edu/psych/news-events/all-news/faculty-news/the-dunning-kruger-effect-shows-that-people-don-t-know-what-they.html).\n\nWhile it is an issue, we as developers also have \"**Imposter Syndrome**\" at some point in our lives (Including me) where we think that we are not good enough even though clear evidence shows otherwise.\n\nHaving a mixture between the two is dangerous to one's mindset because how do we know if we are _truly smart_? In other words, how can we know ourselves that **we are not as smart as we think as well as knowing how we know we are good enough**? Everyone having different mindset and having both the \"**Dunning-Kruger Effect**\" and \"**Imposter Syndrome**\" is like someone gaslighting you in a weird way.\n\n---\n\n# Dunning-Kruger Effect and Imposter Syndrome Deep Dive\nI read and replied to @richardpascoe on \"[The Internet‚Äôs Addiction to Being Contrary](https://dev.to/richardpascoe/the-internets-addiction-to-being-contrary-42ni)\" about the environment of inclusively when it comes to debates and feedback. This is important because it does reduces the impostor syndrome based on factual evidence from others that you are good enough and that you know a lot, even though you tell yourself that you don't.\n\nThe \"**Dunning-Kruger Effect**\" takes that away because there is a distinction between what is seen from others and what is factual. For example, one can say that you are doing a great job in Python and that you are an expert at it. But in reality, you are only a beginner to the job market's eyes and that the requirement of knowledge to know python is no where near to where you _actually_ needed to be.\n\nThis shows the distinction between how society sees you Versus reality. Of course, you can use both to your advantage when it comes to motivation from others and knowing that you have a lot to learn. But the whole point of this is that it is dangerous for those who do not have that mindset. I have seen individuals who have low self-esteem and always telling themselves they are not good enough. Not only that, I seen others who came as egotistic that they are an expert at this language/technology, but in reality they do not.\n\n# Can you Overcome Both?\nThis is tricky to put into words. The reason is that the \"**Dunning-Kruger Effect**\" tells you that \"A person thinks they know a lot, but they don't\", which can result in comparing yourself to others, which is also not good. Additionally to comparing yourself, you need to know if you are well-knowledgeable into the field of expertise. It feels like one needs to have the Avengers of Humanity to judge you truly. It is possible to overcome the \"**Dunning-Kruger Effect**\" and the \"**Imposter Syndrome**\". However, you cannot attack both at the same time since it overlaps with each other and is quite difficult to imagine in my opinion. One of the ways for me that helps is self-reflection (Anything I want to improve on) and self-motivation that I can reach the goal. Although this is a simple advice to myself, there are variables that can disrupt it.\n\nThat's all I have for today! I just want to say that everyone here is welcomed and no matter where you are at, you are good enough. Although it seems like an endless run, you will reach your goals eventually and you will be proud of the progress you have made!\n\n---\n\n# Discussion (Need answers)\n**_Riddle me this Dev.to Community, when you have the Dunning-Kruger Effect and Imposter Syndrome, how can we know ourselves that we are truly knowledgeable and know that we are were we needed to be?_** I would love to hear from you and your thoughts about this topic!\n\n_Please share with someone that is a developer, especially newcomers into the Dev field. I believe it is an important discussion to talk about and knowing how to succeed into this field of rapid change._\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7ynwjasgatcp6k31eur7.jpg)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "stars": null,
      "comments": 21,
      "upvotes": 34,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Your Microservices Aren‚Äôt Scalable. Your Database Is Just Crying.",
      "url": "https://dev.to/art_light/your-microservices-arent-scalable-your-database-is-just-crying-mbd",
      "source": "devto",
      "published_at": "2026-02-04T21:24:34+00:00",
      "external_id": null,
      "tags": [
        "microservices",
        "database",
        "architecture",
        "webdev"
      ],
      "content_length": 5733,
      "content_preview": "When we first split our monolith into microservices, it felt like a victory.\n\nSmaller services. Independent deployments. Clean boundaries. We even had a diagram with boxes and arrows that made us feel like Netflix engineers.\n\nThen production traffic hit.\n\nThe services scaled fine.\nKubernetes was happy.\nAuto-scaling worked exactly as advertised.\n\nAnd the database absolutely melted.\n\nAt first, we blamed everything except the obvious.\n\n‚ÄúMaybe we need more replicas.‚Äù\n‚ÄúLet‚Äôs increase the connection p",
      "content_full": "When we first split our monolith into microservices, it felt like a victory.\n\nSmaller services. Independent deployments. Clean boundaries. We even had a diagram with boxes and arrows that made us feel like Netflix engineers.\n\nThen production traffic hit.\n\nThe services scaled fine.\nKubernetes was happy.\nAuto-scaling worked exactly as advertised.\n\nAnd the database absolutely melted.\n\nAt first, we blamed everything except the obvious.\n\n‚ÄúMaybe we need more replicas.‚Äù\n‚ÄúLet‚Äôs increase the connection pool.‚Äù\n‚ÄúPostgres just doesn‚Äôt scale like NoSQL.‚Äù\n\nBut the truth was simpler and more uncomfortable:\n\nOur microservices weren‚Äôt the problem.\nOur shared database was.\n\n## Microservices Make Scaling Look Easy (Until It Isn‚Äôt)\n\nMicroservices sell a seductive idea: scale each part of your system independently. In theory, that‚Äôs exactly what you get.\n\nIn practice, most teams do this:\n\n- Split the app into 10‚Äì20 services\n- Point all of them at the same database\n- Call it ‚Äúmicroservices architecture‚Äù\n\nCongratulations.\nYou‚Äôve just built a **distributed monolith with network latency**.\n\nEach service may scale horizontally, but every single one still funnels its traffic into the same bottleneck. When load increases, the database doesn‚Äôt see ‚Äúmicroservices.‚Äù It sees chaos.\n\nMore connections\nMore concurrent queries\nMore locks\nMore contention\n\nThe database doesn‚Äôt scale horizontally just because your services do. It just cries louder.\n\n## The Hidden Cost of ‚ÄúJust One More Query‚Äù\n\nThe first cracks showed up as latency spikes.\n\nNo errors. No crashes. Just requests getting slower‚Ä¶ and slower‚Ä¶ and slower.\n\nHere‚Äôs what was really happening:\n\n- Service A added a new endpoint ‚Üí +3 queries\n- Service B added ‚Äújust a join‚Äù ‚Üí +2 queries\n- Service C started polling every 5 seconds ‚Üí oops\n- Read replicas lagged behind writes\n- Connection pools maxed out\n- Locks piled up in places no one was monitoring\n\nIndividually, none of these changes looked dangerous.\n\nTogether, they turned the database into a shared trauma center.\n\nThis is the microservices trap: local decisions with global consequences.\n\n## Scaling Services Multiplies Database Pain\n\nHere‚Äôs the part that surprised newer engineers on the team.\n\nScaling a service from 2 pods to 20 pods doesn‚Äôt just multiply throughput. It multiplies:\n\n- Open connections\n- Idle transactions\n- Concurrent writes\n- Cache misses\n- Lock contention\n\nThe database doesn‚Äôt know these pods belong to the same service. It treats them as 18 new strangers aggressively asking for attention.\n\nSo while your dashboards show:\n\n> ‚ÄúService latency looks fine!‚Äù\n\nThe database is over here thinking:\n\n> ‚ÄúWHY ARE THERE SO MANY OF YOU?‚Äù\n\n## Why ‚ÄúAdd a Cache‚Äù Usually Isn‚Äôt Enough\n\nAt this point, someone always suggests caching.\n\nAnd yes, caching helps.\nBut it doesn‚Äôt fix the underlying issue.\n\nMost teams add:\n\n- Redis for reads\n- Maybe some HTTP caching\n- A TTL they picked emotionally\n\nNow the system is faster‚Ä¶ until it isn‚Äôt.\n\nWhy?\n\nBecause:\n\n- Writes still hit the same database\n- Cache invalidation gets messy fast\n- Cross-service data consistency becomes a guessing game\n- You‚Äôve added operational complexity without removing coupling\n\nCaching is a painkiller.\nThe database problem is structural.\n\n## The Real Problem: Shared Ownership of Data\n\nThe moment it clicked for me was realizing this:\n\nWe didn‚Äôt have microservices.\nWe had microservices sharing the same state.\n\nThat breaks the core promise of the architecture.\n\nWhen multiple services:\n\n- Read the same tables\n- Write to the same rows\n- Depend on the same transactions\n\nThey are no longer independent. They‚Äôre tightly coupled through the database, just in a quieter, harder-to-debug way.\n\nYour services can deploy independently.\nYour data cannot.\n\n## What Actually Helped (And What Didn‚Äôt)\n\nHere‚Äôs what didn‚Äôt solve it:\n\n- Bigger database instance\n- More replicas\n- Higher connection limits\n- Shouting ‚Äúoptimize queries‚Äù in standups\n\nHere‚Äôs what did help:\n\n## 1. Clear Data Ownership\n\nEach service owns its data. Period.\n\nIf another service needs it:\n\n- It calls an API\n- Or consumes an event\n- Or reads from a purpose-built read model\n\nNo ‚Äújust this one join across services.‚Äù That‚Äôs how the crying starts again.\n\n## 2. Fewer Cross-Service Transactions\n\nDistributed transactions feel elegant until you try to operate them.\n\nWe replaced synchronous dependencies with:\n\n- Events\n- Async workflows\n- Eventually consistent updates\n\nNot everything needs to be instant. Most systems just need to be reliable.\n\n## 3. Designing for Database Load First\n\nWe stopped asking:\n\n> ‚ÄúCan this service scale?‚Äù\n\nAnd started asking:\n\n> ‚ÄúWhat does this do to the database at 10x traffic?‚Äù\n\nThat one question changed architecture reviews completely.\n\n## 4. Accepting That Microservices Are a Tradeoff\n\nMicroservices don‚Äôt automatically give you scalability. They give you options ‚Äî at the cost of discipline.\n\nWithout strict boundaries, they amplify database problems instead of solving them.\n\n## The Hard Lesson\n\nMicroservices didn‚Äôt fail us.\nOur database design did.\n\nWe optimized for developer velocity early on and paid for it later with operational pain. That‚Äôs not a mistake ‚Äî that‚Äôs a tradeoff. The mistake is pretending microservices magically remove scaling limits.\n\nThey don‚Äôt.\n\nThey move those limits somewhere less visible.\n\nUsually into your database.\n\n## Final Thoughts\n\nIf your system slows down every time traffic increases, don‚Äôt just look at your services.\n\nLook at:\n\n- Who owns the data\n- How many services touch the same tables\n- How scaling pods multiplies database load\n- Whether your architecture matches your traffic patterns\n\nBecause nine times out of ten, when ‚Äúmicroservices don‚Äôt scale‚Äù‚Ä¶\n\nThey do.\n\nYour database is just crying for help.",
      "stars": null,
      "comments": 13,
      "upvotes": 81,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Video Understanding with Gemini 3.0 Flash for Robotics",
      "url": "https://dev.to/googleai/video-understanding-with-gemini-30-flash-for-robotics-5896",
      "source": "devto",
      "published_at": "2026-02-05T15:01:36+00:00",
      "external_id": null,
      "tags": [
        "ai",
        "robotics",
        "gemini",
        "developers"
      ],
      "content_length": 26161,
      "content_preview": "In the robotics field, as with nearly every other corner of tech, the landscape is shifting rapidly as we integrate AI into our workflows and systems. For this post, I‚Äôve put together a few demos to explore Gemini's multimodal capabilities for **video understanding**. We‚Äôll look at how these features can be applied to robotics-specific use cases, as well as how they can be used for general learning augmentation.\n\nIf you're not familiar with Gemini yet, here's a [quick start](https://ai.google.de",
      "content_full": "In the robotics field, as with nearly every other corner of tech, the landscape is shifting rapidly as we integrate AI into our workflows and systems. For this post, I‚Äôve put together a few demos to explore Gemini's multimodal capabilities for **video understanding**. We‚Äôll look at how these features can be applied to robotics-specific use cases, as well as how they can be used for general learning augmentation.\n\nIf you're not familiar with Gemini yet, here's a [quick start](https://ai.google.dev/gemini-api/docs/quickstart) for running a basic \"Hello World\" example. Be sure to set your `GOOGLE_GEMINI_API` variable so that your API key can be found (I forget this step pretty much every time :)).\n\nWith that, let's jump in.\n\n## Analyzing a local file: Video to Action\n\nIn this first example, I have a video file saved locally that shows a bi-arm Aloha robot doing various tasks on a desk (if you'd like to use the same video to follow along, you can find it [here](https://storage.googleapis.com/generativeai-downloads/images/robotics/er-1-5-example-colab/desk_organization.mp4)).\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vpjpeis31ody31po2p48.png)\n\nIf I wanted to make this sequence repeatable with a **Vision-Language-Action (VLA)** model, I‚Äôd first need to break the video down into subtasks. I wrote a small program that reviews the video and returns a structured list of actions, identifying the \"actor\" and the specific task for each segment.\n\n```python\nfrom google import genai\nfrom google.genai import types\n\nimport time\nimport json\nimport pandas as pd\nimport plotly.express as px\nfrom datetime import timedelta\n\n\nclient = genai.Client()\n\nmyfile = client.files.upload(file=\"desk_organization.mp4\")\n\nwhile myfile.state == \"PROCESSING\":\n  print(\".\", end=\"\")\n  time.sleep(1)\n  myfile = client.files.get(name=myfile.name)\n\nif myfile.state.name == \"FAILED\":\n  raise ValueError(myfile.state.name)\n\nprint(\"Processed\")\n\n\nprompt = \"\"\"\nReview this video and break the actions into a structured JSON list. \nEach object in the list must have:\n- \"actor\": The entity performing the action (e.g., 'Left Robot Arm').\n- \"action\": A short description of the task.\n- \"start_s\": Start time in total seconds (integer).\n- \"end_s\": End time in total seconds (integer).\n\nOutput ONLY the raw JSON list.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\", \n    contents=[myfile, prompt],\n    config=types.GenerateContentConfig(\n        response_mime_type=\"application/json\",\n        thinking_config=types.ThinkingConfig(thinking_budget=-1)\n    ),\n)\n\nprint(response.text)\n```\n\nLet's break this into smaller parts. First, I use Gemini's [Files API](https://ai.google.dev/gemini-api/docs/files) to upload the video. The `client.files.upload` command used here will block the script until the file has been uploaded, and then the begins processing the file for use. So we don't accidentally try to access the file before it's ready, which will cause an error, I also have a loop that checks the status of the file before letting the program continue.\n\n```python\nmyfile = client.files.upload(file=\"desk_organization.mp4\")\n\nwhile myfile.state == \"PROCESSING\":\n  print(\".\", end=\"\")\n  time.sleep(1)\n  myfile = client.files.get(name=myfile.name)\n\nif myfile.state.name == \"FAILED\":\n  raise ValueError(myfile.state.name)\n```\nNext I have a very specific prompt about how I want the data to be returned to me so that I can get the actor, action, and starting and ending time for that action. I also use the `response_mime_type` flag to specify that I only want JSON data returned to me.\n\n```python\nprompt = \"\"\"\nReview this video and break the actions into a structured JSON list. \nEach object in the list must have:\n- \"actor\": The entity performing the action (e.g., 'Left Robot Arm').\n- \"action\": A short description of the task.\n- \"start_s\": Start time in total seconds (integer).\n- \"end_s\": End time in total seconds (integer).\n\nOutput ONLY the raw JSON list.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\", \n    contents=[myfile, prompt],\n    config=types.GenerateContentConfig(\n        response_mime_type=\"application/json\",\n        thinking_config=types.ThinkingConfig(thinking_budget=-1)\n    ),\n)\n```\n\nAt this point we can get the JSON data outlining the actions that the robots have taken, which can then be used to prompt a VLA to repeat the tasks.\n\n```json\n[\n  {\n    \"actor\": \"Left Robot Arm\",\n    \"action\": \"pick up the green marker\",\n    \"start_s\": 0,\n    \"end_s\": 3\n  },\n  {\n    \"actor\": \"Left Robot Arm\",\n    \"action\": \"place the green marker in the wooden bowl\",\n    \"start_s\": 3,\n    \"end_s\": 6\n  },\n  {\n    \"actor\": \"Left Robot Arm\",\n    \"action\": \"pick up the blue pen\",\n    \"start_s\": 13,\n    \"end_s\": 16\n  },\n  {\n    \"actor\": \"Left Robot Arm\",\n    \"action\": \"place the blue pen in the pencil holder\",\n    \"start_s\": 18,\n    \"end_s\": 22\n  },\n  {\n    \"actor\": \"Right Robot Arm\",\n    \"action\": \"pick up the red pen\",\n    \"start_s\": 22,\n    \"end_s\": 25\n  },\n  {\n    \"actor\": \"Right Robot Arm\",\n    \"action\": \"place the red pen in the pencil holder\",\n    \"start_s\": 25,\n    \"end_s\": 28\n  }\n]\n```\n\nStructured data is great for code, but for humans, visuals are better. I asked Gemini to write a script to turn that JSON into a [Gantt chart](https://en.wikipedia.org/wiki/Gantt_chart) using Plotly. This makes it easy to see the task orchestration and timestamps at a glance.\n\n```python\ndata = json.loads(response.text)\ndf = pd.DataFrame(data)\n\nbase_time = pd.to_datetime(\"2025-01-01\")\ndf['start_dt'] = df['start_s'].apply(lambda x: base_time + timedelta(seconds=x))\ndf['end_dt'] = df['end_s'].apply(lambda x: base_time + timedelta(seconds=x))\n\ndynamic_height = 150 + (len(df['actor'].unique()) * 60)\n\nfig = px.timeline(\n    df, \n    x_start=\"start_dt\", \n    x_end=\"end_dt\", \n    y=\"actor\", \n    color=\"actor\",\n    text=\"action\",\n    template=\"plotly_white\",\n    height=dynamic_height\n)\n\nfig.update_layout(\n    title_text=\"Video Orchestration\",\n    title_x=0.5,\n    showlegend=False,\n    margin=dict(l=10, r=10, t=40, b=30),\n    xaxis_title=None,\n    yaxis_title=None,\n    font=dict(size=11)\n)\n\nfig.layout.xaxis.update({\n    'tickformat': '%M:%S',\n    'fixedrange': True\n})\n\nfig.update_yaxes(autorange=\"reversed\", fixedrange=True)\nfig.update_traces(\n    textposition='inside', \n    insidetextanchor='middle',\n    marker_line_width=1,\n    marker_line_color=\"white\",\n    width=0.6\n)\n\nfig.show()\n```\n\nThis gives us a clean, visual breakdown of the robot's performance:\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0v5alvui8mo31boqkpil.png)\n\n## Understanding videos from YouTube\n\nFor my next experiment, I took some inspiration from this paper on [Mobility VLA](https://arxiv.org/abs/2407.07775). I wanted to try using a long YouTube video of a tour (specifically one from the former Egyptian Museum in Cairo because it was one of my favorites and I generally just loved Egypt the few times I've been), and then ask questions about things seen in that video.\n\nSince I could potentially do this on a robot with its own mobility stack where I could return to a location based on a time stamp, I figured I'd also ask for a timestamp for when the video shows the largest item on the tour.\n\nLuckily, the code for this is really straightforward. You just need to provide a YouTube link as a `file_data` object and send in your prompt.\n\n```python\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\nresponse = client.models.generate_content(\n    model='models/gemini-3-flash-preview',\n    contents=types.Content(\n        parts=[\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=EdCReWs6-wI')\n            ),\n            types.Part(text='Please summarize the different things seen in this video and provide a timestamp for the location where the largest object is seen.')\n        ]\n    ),\n)\n\nprint(response.text)\n```\n\nThis will give you a response like:\n\n```text\nThe Egyptian Museum, also known as the Museum of Ancient Egyptian Antiquities, is a renowned institution in Cairo, Egypt. It is home to a vast and priceless collection of ancient Egyptian artifacts, including the world-famous treasures of Tutankhamun. The video displays various things like:\n\n* **Museum exterior:** The video begins with an exterior shot of the museum at night.\n* **Sarcophagi:** There are many sarcophagi of different sizes and materials, including stone, granite, and wood.\n* **Statues:** The museum houses a wide range of statues representing pharaohs, gods, goddesses, and everyday people.\n* **Wooden boats:** Ancient Egyptian wooden boats used for burial rituals are on display.\n* **Display cases:** Many of the museum's smaller artifacts, such as jewelry, amulets, and pottery, are shown in display cases.\n\nThe largest object is seen at [05:54](https://www.youtube.com/watch?v=EdCReWs6-wI&t=352).\n```\n\nOK, so one downside: this isn't _really_ useful if I want to use the information in my robot code, as I'd have to do extra work to extract the timestamp. but it turns out there's a more organized way to do this using a Gemini capability called [structured outputs](https://ai.google.dev/gemini-api/docs/structured-output?example=recipe).\n\nTo get the data back in a way I can use it, I'll still request a JSON mime type, but I'll also create some objects representing the data I want:\n\n```python\nclass ItemSeen(BaseModel):\n    object: str = Field(description=\"Object seen in the video\")\n    description: str = Field(description=\"Description of the object seen in the video\")\n\nclass Navigation(BaseModel):\n    itemsSeen: List[ItemSeen]\n    timestamp: int = Field(description=\"Timestamp where the largest item is seen\")\n```\n\nThen I'll use those objects as a schema for the data by adding a config object to my `generate_content` call.\n\n```python\nresponse = client.models.generate_content(\n    model='models/gemini-3-flash-preview',\n    contents=types.Content(\n        parts=[\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=EdCReWs6-wI')\n            ),\n            types.Part(text='Please summarize the different things seen in this video and provide a timestamp for the location where the largest object is seen.')\n        ]\n    ),\n    config={\n        \"response_mime_type\": \"application/json\",\n        \"response_json_schema\": Navigation.model_json_schema(),\n    }\n)\n```\nAt this point I can extract the data returned into the model object and use it in my app. The output will look like this:\n\n```python\nitemsSeen = [\nItemSeen(object='Pharaohs and Queens statues', description='Statues depicting different Pharaohs and Queens, carved from various materials such as stone and wood, showcasing traditional poses and royal regalia.'),\nItemSeen(object='Sarcophagi and mummy cases', description='Ornate containers used to hold mummies, including stone sarcophagi and wooden mummy cases adorned with intricate hieroglyphs and religious imagery.'),\nItemSeen(object='Animal-headed deities', description='Statues of gods and goddesses represented with animal heads, like the jackal-headed Anubis, falcon-headed Horus, and lioness-headed Sekhmet.'),\nItemSeen(object='Pyramidions', description='Small pyramid-shaped stones, often made of basalt or granite, that once capped the tops of pyramids or obelisks, inscribed with prayers and scenes.'),\nItemSeen(object='Ancient wooden boat', description='A well-preserved funerary boat made of wood, reconstructed to show how these vessels were used for symbolic journeys in the afterlife.'),\nItemSeen(object='Reliefs and stelae', description='Stone slabs and wall segments featuring carved or painted scenes and inscriptions, documenting the lives, achievements, and religious beliefs of the ancient Egyptians.'),\nItemSeen(object='Display cases with artifacts', description='Glass-enclosed cases containing smaller items such as jewelry, figurines, tools, and household objects, providing a glimpse into daily life and craftsmanship.')\n]\ntimestamp = 361\n```\n\n## Querying multiple YouTube videos\n\nSo now that we know how to query against a YouTube video, let's take this a step further. With an API key that's tied to a paid account, I can run Gemini over *10* different YouTube videos with one call. This could be great for analyzing multiple camera feeds to check for task success, but since I don't have that data on hand, I'm going to focus on the fact that this field requires a *lot* of constant learning and I need all of the help I can get.\n\nFor this example I'll load six robotics lectures from Stanford University (I have no affiliation with them or this content, I just really like and value free educational content) and ask Gemini to create some concise notes, as well as use Google Search to look up and give me a recommended reading list to support my robotics learning journey.\n\n```python\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\ngoogle_search_tool = types.Tool(\n    google_search=types.GoogleSearch()\n)\n\nresponse = client.models.generate_content(\n    model='models/gemini-3-flash-preview',\n    contents=types.Content(\n        parts=[\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=o5bW3C5OD6U'),\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=PYh9k4cy25w'),\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=RKFRO_G4YkA'),\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=v18Jo2ILXZ8'),\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=5uWtpDON7Vs'),\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=05SuBLowwKM'),\n            ),\n            types.Part(text='This is a set of six lectures for a robotics course. Please write concise notes for each video in markdown, then create a list of research papers and books that would be relevant to each course. Check reviews for books that you provide and mention why they would be worth reading to learn this material in depth.')\n        ]\n    ),\n    config=types.GenerateContentConfig(\n        tools=[google_search_tool],\n    ),\n)\n\nprint(response.text)\n```\n\nNow if we run this we get... an error:\n\n```text\ngoogle.genai.errors.ClientError: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Please use fewer than 10800 images in your request to this model', 'status': 'INVALID_ARGUMENT'}}\n```\n\nWhat's happening here is that Gemini 3.0-Flash's context window supports a maximum of 10,800 images. At 1 frame per second, that's three hours of video, and the six long lectures exceed this limit.\n\nTo get around this, we can play around with some video settings since these are lectures that don't have a ton changing with each frame and aren't graphics-intensive. For starters, let's add a `media_resolution` property to the `config` object to convert the video into a low resolution video when processing. You can find more about this property in the official [documentation here](https://ai.google.dev/gemini-api/docs/media-resolution).\n\nWe can also lower the frame rate that gets processed on each video, so in this case we'll only look at one frame every ten seconds rather than one frame per second.\n\nIn the end we land on a simple script that looks like this:\n\n```python\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\n\ngoogle_search_tool = types.Tool(\n    google_search=types.GoogleSearch()\n)\n\n\nresponse = client.models.generate_content(\n    model='models/gemini-3-flash-preview',\n    contents=types.Content(\n        parts=[\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=o5bW3C5OD6U'),\n                video_metadata=types.VideoMetadata(fps=0.1),\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=PYh9k4cy25w'),\n                video_metadata=types.VideoMetadata(fps=0.1)\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=RKFRO_G4YkA'),\n                video_metadata=types.VideoMetadata(fps=0.1)\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=v18Jo2ILXZ8'),\n                video_metadata=types.VideoMetadata(fps=0.1)\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=5uWtpDON7Vs'),\n                video_metadata=types.VideoMetadata(fps=0.1)\n            ),\n            types.Part(\n                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=05SuBLowwKM'),\n                video_metadata=types.VideoMetadata(fps=0.1)\n            ),\n            types.Part(text='This is a set of six lectures for a robotics course. Please write concise notes for each video in markdown, then create a list of research papers and books that would be relevant to each course. Check reviews for books that you provide and mention why they would be worth reading to learn this material in depth.')\n        ]\n    ),\n    config=types.GenerateContentConfig(\n        tools=[google_search_tool],\n        media_resolution=types.MediaResolution.MEDIA_RESOLUTION_LOW\n    ),\n)\n\nprint(response.text)\n```\nwhich in turn gives us the following output:\n\n```text\nHere are concise markdown notes for the six lectures featured in this robotics seminar, followed by curated lists of research papers and books to deepen your understanding of each specific domain.\n\n---\n\n# Lecture 1: Autonomous Navigation in Complex Outdoor Environments\n**Speaker:** Jing Liang (Stanford/UMD)\n\n### Concise Notes\n*   **Problem Definition:** Moving beyond obstacle avoidance to \"traversability analysis\"‚Äîunderstanding which surfaces (grass, gravel, etc.) a robot can actually handle.\n*   **VLM Integration:** Utilizing Vision-Language Models (VLMs) to translate natural language goals and visual cues into viable path Candidates.\n*   **Gaussian Splats for Mapping:** Implementing 3D Gaussian Splatting to estimate not just geometry, but semantic material types and physical properties (friction, hardness, density).\n*   **Companion Robotics:** Applying these navigation stacks to older adults (\"longevity robots\") to assist with outdoor exercise and health monitoring.\n*   **Dataset:** Introduction of the Global Navigation Dataset (GND) covering 10 campuses with multi-modal sensor data.\n\n### Relevant Resources\n**Research Papers:**\n*   *MaPNav: Trajectory Generator with Traversability Coverage for Outdoor Navigation* (Liang et al., 2024).\n*   *SplatFlow: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation* (Chopra et al., 2024).\n\n**Books:**\n*   **\"Probabilistic Robotics\" by Sebastian Thrun, Wolfram Burgard, and Dieter Fox.**\n    *   *Review:* Regarded as the \"bible\" of modern navigation. It is essential for understanding the SLAM and state estimation fundamentals that Jing Liang‚Äôs complex environment navigation is built upon.\n\n---\n\n# Lecture 2: From Digital Humans to Safe Humanoids\n**Speaker:** Yao Feng (Stanford)\n\n### Concise Notes\n*   **GentleHumanoid Framework:** Focuses on safe physical contact. Instead of just following a path, the robot must regulate interaction forces.\n*   **Force Modeling:** Differentiates between *Resistive Contact* (robot hitting an object) and *Guiding Contact* (human pulling the robot‚Äôs hand).\n*   **Tunable Force:** Implements a safety threshold (e.g., limiting force to 5N or 15N) to ensure the robot \"gives way\" during a hug or assistance task.\n*   **Grounded Reasoning:** Uses \"ChatPose\" and \"ChatHuman\" to allow robots to predict human intent and next-frame poses from visual/textual data.\n\n### Relevant Resources\n**Research Papers:**\n*   *GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction* (Lu et al., 2024).\n*   *ChatPose: Chatting about 3D Human Pose* (Feng et al., 2024).\n\n**Books:**\n*   **\"Humanoid Robotics\" by Shuuji Kajita et al.**\n    *   *Review:* A comprehensive guide to the kinematics and dynamics of two-legged machines. It helps bridge the gap between Feng‚Äôs \"digital humans\" and the physical constraints of a humanoid.\n\n---\n\n# Lecture 3: Resilient Autonomy in Extreme Environments\n**Speaker:** Sebastian Scherer (Carnegie Mellon University)\n\n### Concise Notes\n*   **Definition of Resilience:** The ability to maintain performance in \"degraded\" conditions (smoke, dust, total darkness, or GPS-denied areas).\n*   **MapAnything:** A unified feed-forward model that performs 3D reconstruction and metric-scale estimation from simple monocular video.\n*   **Multi-Modal Sensors:** Leveraging thermal cameras (AnyThermal) and Doppler radar to navigate when visual cameras fail due to glare or darkness.\n*   **Triage Challenge:** Application of these robots as \"pre-first responders\" to locate and assess casualties in disaster zones autonomously.\n\n### Relevant Resources\n**Research Papers:**\n*   *MapAnything: Unified 3D Reconstruction from any Visual Input* (Scherer Lab, 2024).\n*   *AnyThermal: A Single Backbone for Multiple Thermal Perception Tasks* (Li et al., 2024).\n\n\n**Books:**\n*   **\"Autonomous Mobile Robots\" by Roland Siegwart and Illah Nourbakhsh.**\n    *   *Review:* Excellent for learning about the trade-offs between different sensor modalities (Lidar vs. Vision vs. Thermal), which is the core of Scherer's \"resilience\" strategy.\n\n---4.  **Lecture 4: Robot Motion Learning with Physics-Based PDE Priors**\n**Speaker:** Abdul H. Qureshi (Purdue University)\n\n### Concise Notes\n*   **Neural Time Fields (NTFields):** Using neural networks to solve the *Eikonal Partial Differential Equation (PDE)* for motion planning.\n*   **TD-Learning for Motion:** Borrowing Temporal Difference learning from RL to regularize gradients between consecutive points in a path.\n*   **Scalability:** This approach allows for planning in extremely high-dimensional spaces (up to 15-DOF for quadrupeds and arms) much faster than traditional sampling-based planners.\n*   **Unknown Environments:** The robot builds a \"Time Field\" map in real-time, treating navigation as following the gradient of arrival time.\n\n### Relevant Resources\n**Research Papers:**\n*   *Physics-Informed Neural Time Fields for Motion Planning* (Ni et al., 2023).\n*   *Domain Decomposition for Large Scale Neural Motion Planning* (Liu et al., 2024).\n\n**Books:**\n*   **\"Planning Algorithms\" by Steven M. LaValle.**\n    *   *Review:* This is the definitive text on how robots find paths. Reading this is necessary to understand why Qureshi‚Äôs use of PDEs is such a radical and efficient departure from traditional RRT* or PRM methods.\n\n---\n\n# Lecture 5: Learning to Control Large Teams of Robots\n**Speaker:** Eduardo Montijano (University of Zaragoza)\n\n### Concise Notes\n*   **Distributed Policies:** Moving away from a \"central brain\" to local policies where each agent makes decisions based on its immediate neighbors.\n*   **Self-Attention Swarms:** Utilizing Transformer-like attention mechanisms so robots can handle a varying number of neighbors (scaling from 3 to 3,000 robots).\n*   **Port-Hamiltonian Systems:** Integrating energy-based physics equations into the neural network to ensure the learned behavior is physically stable and explainable.\n*   **Gen-Swarms:** Applying Generative AI (Diffusion models) to \"draw\" complex shapes (like a dragon) for drone shows, then using local controllers to achieve those shapes.\n\n### Relevant Resources\n**Research Papers:**\n*   *LEMURS: Learning Distributed Multi-Robot Interactions* (Sebasti√°n et al., 2023).\n*   *Gen-Swarms: Generative AI for Swarm Robotics* (Pueyo et al., 2024).\n\n**Books:**\n*   **\"Graph Theoretic Methods in Multiagent Networks\" by Mehran Mesbahi and Magnus Egerstedt.**\n    *   *Review:* Essential for understanding how local connectivity influences global swarm behavior. It provides the mathematical logic for the \"neighborhood\" approach discussed by Montijano.\n\n---\n\n# Lecture 6: Next Generation Dexterous Manipulation\n**Speaker:** Monroe Kennedy III (Stanford)\n\n### Concise Notes\n*   **The Manipulation Gap:** While robots can walk and flip, they still struggle with small, soft, or articulated objects (like tying shoelaces).\n*   **Optical Tactile Sensors (DenseTact):** Using internal cameras within soft \"fingertips\" to sense 4-axis stress fields (normal, shear, and torsion).\n*   **J-PARSE Algorithm:** Resolving \"kinematic singularities\" (when a robot arm gets stuck at full extension) through a safety Jacobian projection.\n*   **Cross-Modality Learning:** Combining vision and touch (Touch-GS) to create 3D Gaussian Splats of objects that are otherwise invisible to cameras (transparent or highly reflective surfaces).\n\n### Relevant Resources\n**Research Papers:**\n*   *DenseTact 2.0: High-Resolution Tactile Sensing for Robot Manipulation* (Do et al., 2023).\n*   *Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting* (Swann et al., 2024).\n\n**Books:**\n*   **\"Mechanics of Manipulation\" by Matthew T. Mason.**\n    *   *Review:* This book focuses on the physics of pushing, grasping, and friction. It is vital for understanding the \"force closure\" and \"form closure\" concepts Kennedy uses to move beyond simple suction-cup grippers.\n```\n\nThe output gives us concise notes for all lectures and curated reading lists. Having recently read one of the recommended books (Introduction to Autonomous Mobile Robotics by Siegwart and Nourbakhsh), I can say that I think the recommendations seem solid.\n\n## Conclusion\n\nThis was a look into a newer capability for Gemini that you can try today with the `gemini-3.0-flash-preview` model. If you found it interesting or have other things you'd want to know about, definitely leave a comment.",
      "stars": null,
      "comments": 0,
      "upvotes": 17,
      "read_time": "14 min read",
      "language": null
    },
    {
      "title_en": "Server-Sent Events Beat WebSockets for 95% of Real-Time Apps (Here's Why)",
      "url": "https://dev.to/polliog/server-sent-events-beat-websockets-for-95-of-real-time-apps-heres-why-a4l",
      "source": "devto",
      "published_at": "2026-02-04T13:06:39+00:00",
      "external_id": null,
      "tags": [
        "javascript",
        "webdev",
        "programming",
        "discuss"
      ],
      "content_length": 24430,
      "content_preview": "Everyone defaults to WebSockets for real-time features. Most shouldn't.\n\n**The reality:** 95% of \"real-time\" applications only need **server ‚Üí client** updates. Chat notifications. Live dashboards. Stock tickers. Log streams. AI responses.\n\nWebSockets give you bidirectional communication. But bidirectional comes with a tax: complexity, resource overhead, scaling challenges, debugging nightmares.\n\n**Server-Sent Events (SSE)** do one thing: stream data from server to client. They do it brilliantly",
      "content_full": "Everyone defaults to WebSockets for real-time features. Most shouldn't.\n\n**The reality:** 95% of \"real-time\" applications only need **server ‚Üí client** updates. Chat notifications. Live dashboards. Stock tickers. Log streams. AI responses.\n\nWebSockets give you bidirectional communication. But bidirectional comes with a tax: complexity, resource overhead, scaling challenges, debugging nightmares.\n\n**Server-Sent Events (SSE)** do one thing: stream data from server to client. They do it brilliantly. And for most applications, that's all you need.\n\nHere's why SSE should be your default for real-time features, with real production numbers and honest trade-offs.\n\n## The WebSocket Assumption\n\nThe conversation usually goes like this:\n\n**Developer:** \"We need real-time updates.\"\n**Team:** \"Use WebSockets.\"\n**Developer:** \"Why?\"\n**Team:** \"Because they're real-time.\"\n\nNobody questions it. WebSockets became the default for anything involving \"live\" or \"real-time.\"\n\nBut here's the uncomfortable truth: **bidirectional communication is rarely necessary.**\n\nLet's look at what \"real-time\" actually means in production applications:\n\n### Real-Time Features That Don't Need Bidirectional:\n\n**Dashboards:**\n- Server pushes metrics\n- Client renders charts\n- Updates flow one way: server ‚Üí client\n\n**Notifications:**\n- Server sends alerts\n- Client displays them\n- One direction only\n\n**Live Feeds:**\n- Server streams new items (tweets, posts, events)\n- Client appends to feed\n- Unidirectional\n\n**AI Chat (ChatGPT-style):**\n- Server streams tokens as they're generated\n- Client displays word-by-word\n- Response flows server ‚Üí client (user input happens via separate POST)\n\n**Stock Tickers:**\n- Server pushes price updates\n- Client updates UI\n- One way\n\n**Log Streaming:**\n- Server tails logs\n- Client displays in real-time\n- Server ‚Üí client only\n\n**Build Status / CI/CD:**\n- Server sends progress updates\n- Client shows build steps\n- Unidirectional\n\nNotice a pattern? **These are 95% of \"real-time\" use cases.**\n\n### Real-Time Features That Actually Need Bidirectional:\n\n**Multiplayer Games:**\n- Player sends moves\n- Server broadcasts to all players\n- Constant two-way traffic\n\n**Collaborative Editing (Google Docs):**\n- User edits document\n- Server reconciles changes\n- Broadcasts to all editors\n- High-frequency bidirectional\n\n**Video Calls / WebRTC Signaling:**\n- Peer discovery\n- ICE candidate exchange\n- Continuous negotiation\n\n**Trading Platforms:**\n- User places order\n- Server confirms\n- Market updates stream back\n- Both directions simultaneously\n\nThese are the **5% of use cases** where WebSockets shine.\n\nFor everything else? You're paying the WebSocket tax for features you don't use.\n\n## What Is Server-Sent Events (SSE)?\n\nSSE is dead simple: **HTTP connection that stays open.** Server writes to it whenever there's new data.\n\n**Protocol:**\n```http\nGET /api/stream HTTP/1.1\nHost: example.com\nAccept: text/event-stream\n\nHTTP/1.1 200 OK\nContent-Type: text/event-stream\nCache-Control: no-cache\n\ndata: {\"message\": \"Hello World\"}\n\ndata: {\"message\": \"Another update\"}\n\ndata: {\"message\": \"And another\"}\n\n```\n\nThat's it. Plain text over HTTP. No protocol upgrade. No handshake dance.\n\n**Client code (JavaScript):**\n```javascript\nconst eventSource = new EventSource('/api/stream');\n\neventSource.onmessage = (event) => {\n  const data = JSON.parse(event.data);\n  console.log('Received:', data);\n};\n\neventSource.onerror = (error) => {\n  console.error('Connection error:', error);\n  // Browser automatically reconnects\n};\n```\n\n**Server code (Node.js/Fastify):**\n```javascript\nfastify.get('/api/stream', (req, reply) => {\n  reply.raw.writeHead(200, {\n    'Content-Type': 'text/event-stream',\n    'Cache-Control': 'no-cache',\n    'Connection': 'keep-alive'\n  });\n\n  const interval = setInterval(() => {\n    const data = JSON.stringify({ message: 'Update', timestamp: Date.now() });\n    reply.raw.write(`data: ${data}\\n\\n`);\n  }, 1000);\n\n  req.raw.on('close', () => {\n    clearInterval(interval);\n  });\n});\n```\n\n**10 lines.** No libraries needed. Just HTTP.\n\nCompare that to WebSockets:\n\n```javascript\nconst ws = new WebSocket('ws://example.com/socket');\n\nws.onopen = () => {\n  console.log('Connected');\n  // Now what? Send ping? Subscribe to channels?\n};\n\nws.onmessage = (event) => {\n  console.log('Received:', event.data);\n};\n\nws.onerror = (error) => {\n  console.error('Error:', error);\n  // Manual reconnection logic required\n};\n\nws.onclose = () => {\n  console.log('Connection closed');\n  // Implement exponential backoff, reconnect...\n};\n\n// Send heartbeat to keep connection alive\nsetInterval(() => {\n  if (ws.readyState === WebSocket.OPEN) {\n    ws.send(JSON.stringify({ type: 'ping' }));\n  }\n}, 30000);\n```\n\n**Already more complex.** And we haven't even handled reconnection, message queueing, or protocol negotiation.\n\n## SSE vs WebSocket: The Real Comparison\n\nLet's look at production numbers from real deployments.\n\n### Performance Benchmarks (Timeplus, 2024)\n\n**Test setup:** 100,000 events/second, 10-30 concurrent connections\n\n**Results:**\n\n| Metric | SSE | WebSocket | Difference |\n|--------|-----|-----------|------------|\n| Max throughput | 3M events/sec | 3M events/sec | Tie |\n| CPU usage (batch 50) | ~42% | ~40% | SSE +5% |\n| Latency (50ms target) | 48ms | 45ms | WS -6% |\n| Implementation complexity | 10 lines | 50+ lines | SSE 5x simpler |\n\n**Conclusion:** Performance is essentially identical. SSE uses slightly more CPU (negligible), WebSocket has slightly lower latency (3ms difference).\n\n**For 100k events/second, the difference is irrelevant.**\n\n### Resource Usage (Production, 2025)\n\n**Scenario:** Real-time dashboard, 10,000 concurrent connections\n\n**SSE:**\n- Memory: ~20MB (connection state only)\n- CPU: 15% idle, 35% under load\n- Network: Standard HTTP\n- Scaling: Horizontal (stateless with backplane)\n\n**WebSocket:**\n- Memory: ~50MB (connection + frame buffers)\n- CPU: 25% idle (ping/pong frames), 45% under load\n- Network: Persistent TCP + WebSocket protocol overhead\n- Scaling: Requires sticky sessions OR message backplane\n\n**Why the difference?**\n\nSSE is just HTTP. No frame masking, no protocol negotiation, no ping/pong.\n\nWebSocket frames have overhead:\n```\n[Frame Header (2-14 bytes)] [Payload]\n```\n\nEvery message gets wrapped. Client‚Üíserver messages are masked (XOR operation = CPU cost).\n\nSSE just writes text:\n```\ndata: {...}\\n\\n\n```\n\nNo framing. No masking. Minimal CPU.\n\n### Latency Deep Dive\n\n**Question:** If WebSocket is 3ms faster, does it matter?\n\n**Answer:** Almost never.\n\n**Typical application latency budget:**\n\n```\nUser action: 0ms\n‚Üì\nFrontend validation: 5ms\n‚Üì\nNetwork RTT: 20-100ms (varies by location)\n‚Üì\nBackend processing: 10-500ms (depends on query)\n‚Üì\nDatabase query: 5-50ms\n‚Üì\nResponse render: 10ms\n‚Üì\nTotal: 50-665ms\n```\n\n**3ms difference between SSE and WebSocket?** Lost in the noise.\n\n**When latency matters:**\n- Gaming (60 FPS = 16ms budget per frame)\n- Trading (microseconds matter)\n- VoIP/video (jitter sensitive)\n\nFor these? Use WebSocket (or UDP-based solutions like WebRTC, WebTransport).\n\nFor dashboards, notifications, feeds? **3ms is irrelevant.**\n\n## Why SSE Wins for Most Applications\n\n### 1. It's Just HTTP\n\n**SSE runs on port 80/443.** No special firewall rules. No proxy configuration. It works everywhere HTTP works.\n\n**WebSocket requires protocol upgrade:**\n\n```http\nGET /socket HTTP/1.1\nUpgrade: websocket\nConnection: Upgrade\nSec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==\n```\n\nSome corporate firewalls block `Upgrade` headers. Some reverse proxies don't support WebSocket. Some CDNs have issues.\n\n**SSE just works.** It's HTTP. Proxies understand it. CDNs cache it (with proper headers). Load balancers route it.\n\n**Real story (from Stack Overflow):**\n\n> \"We deployed WebSockets. Worked perfectly in dev. In production, corporate network blocked ws:// protocol. Spent 2 weeks debugging. Switched to SSE. Worked immediately.\"\n\n### 2. Auto-Reconnect Built-In\n\n**SSE (EventSource API):**\n```javascript\nconst eventSource = new EventSource('/stream');\n// That's it. Browser handles reconnection automatically.\n```\n\nConnection drops? Browser waits 3 seconds, retries. You do nothing.\n\n**WebSocket:**\n```javascript\nlet ws;\nlet reconnectAttempts = 0;\nconst maxReconnectDelay = 30000;\n\nfunction connect() {\n  ws = new WebSocket('ws://example.com/socket');\n  \n  ws.onopen = () => {\n    reconnectAttempts = 0;\n    console.log('Connected');\n  };\n  \n  ws.onclose = () => {\n    const delay = Math.min(1000 * Math.pow(2, reconnectAttempts), maxReconnectDelay);\n    reconnectAttempts++;\n    console.log(`Reconnecting in ${delay}ms...`);\n    setTimeout(connect, delay);\n  };\n  \n  ws.onerror = (error) => {\n    console.error('Error:', error);\n    ws.close();\n  };\n}\n\nconnect();\n```\n\n**You write the reconnection logic.** Exponential backoff. Maximum attempts. Jitter to prevent thundering herd.\n\nOr you use Socket.IO (adds 40KB to bundle) which does this for you.\n\n**SSE:** 1 line.\n**WebSocket:** 20+ lines OR external library.\n\n### 3. HTTP/2 Multiplexing\n\nRemember the \"6 connection limit\" criticism of SSE?\n\n**HTTP/1.1:** Browsers limit to 6 connections per domain.\n\n**HTTP/2:** One TCP connection, unlimited streams via multiplexing.\n\n**In 2026, HTTP/2 is everywhere:**\n- Chrome: 97% of requests use HTTP/2\n- Production servers: NGINX, Caddy, Cloudflare all default to HTTP/2\n\n**SSE over HTTP/2 = no connection limit.**\n\nYou can have 1000 SSE streams over one TCP connection. Efficient. Fast. Low overhead.\n\n### 4. Works with curl (Debugging)\n\n**SSE:**\n```bash\ncurl -N https://api.example.com/stream\ndata: {\"message\": \"Update 1\"}\n\ndata: {\"message\": \"Update 2\"}\n\n^C\n```\n\n**You can debug SSE streams with curl.** No special tools. No browser. Just curl.\n\n**WebSocket:**\n```bash\n# Need wscat or similar\nnpm install -g wscat\nwscat -c ws://example.com/socket\nConnected (press CTRL+C to quit)\n> {\"type\": \"ping\"}\n< {\"type\": \"pong\"}\n```\n\nRequires special tools. Less straightforward.\n\n### 5. CDN Friendly\n\n**SSE is HTTP.** CDNs understand HTTP.\n\nWant to cache SSE streams? Set headers:\n\n```javascript\nres.setHeader('Cache-Control', 'public, max-age=1');\n```\n\nCloudflare, Fastly, CloudFront all handle SSE transparently.\n\n**WebSocket?** Most CDNs treat it as a special case. Some don't support it at all. Configuration is trickier.\n\n## Real Production Use Cases\n\n### ChatGPT / OpenAI API (2025)\n\n**How ChatGPT streams responses:**\n\n```javascript\nconst response = await fetch('https://api.openai.com/v1/chat/completions', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'Authorization': `Bearer ${API_KEY}`\n  },\n  body: JSON.stringify({\n    model: 'gpt-4',\n    messages: [...],\n    stream: true  // Enable streaming\n  })\n});\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  \n  const chunk = decoder.decode(value);\n  const lines = chunk.split('\\n').filter(line => line.trim());\n  \n  for (const line of lines) {\n    if (line.startsWith('data: ')) {\n      const data = line.slice(6);\n      if (data === '[DONE]') continue;\n      const parsed = JSON.parse(data);\n      console.log(parsed.choices[0].delta.content);\n    }\n  }\n}\n```\n\n**That's SSE.** OpenAI uses Server-Sent Events for streaming completions.\n\nWhy? **Unidirectional.** User sends prompt via POST. AI streams response. SSE is perfect.\n\n### Shopify BFCM Live Map (2022)\n\n**Challenge:** Black Friday / Cyber Monday live sales visualization. Millions of concurrent users watching real-time sales data.\n\n**Solution:** SSE\n\n**Architecture:**\n- Flink processes Kafka stream (sales events)\n- Aggregates data (sales by region, trending products)\n- SSE server pushes updates to frontends\n- Horizontally scaled behind NGINX load balancer\n\n**Results:**\n- **323 billion events** processed in 4 days\n- Millions of concurrent SSE connections\n- **Latency <300ms globally**\n- Zero WebSocket complexity\n\n**Why SSE?**\n> \"We only need server-to-client delivery. SSE allowed us to remove client polling entirely and leverage existing HTTP infrastructure.\"\n\n### Split.io Real-Time Feature Flags (2025)\n\n**Use case:** Feature flag platform. Clients need instant flag updates.\n\n**Scale:**\n- **1 trillion events per month**\n- **<300ms average global latency**\n- Millions of concurrent connections\n\n**Technology:** SSE\n\n**Why not WebSocket?**\n> \"Flags change server-side. Clients just listen. We don't need bidirectional. SSE gives us HTTP simplicity at WebSocket scale.\"\n\n### Personal Project: Real-Time Log Streaming\n\n**Scenario:** Open-source log management platform. Users watch logs arrive in real-time (think `tail -f` in the browser).\n\n**Requirements:**\n- 1000+ concurrent users watching different log streams\n- Sub-50ms latency from log ingestion to browser\n- Deployed on $20/month server\n\n**Implementation:** PostgreSQL LISTEN/NOTIFY + SSE\n\n```javascript\n// Backend: Listen to Postgres\nconst pgClient = new Client({ connectionString: DB_URL });\nawait pgClient.connect();\n\nawait pgClient.query(`LISTEN logs_${orgId}`);\n\npgClient.on('notification', (msg) => {\n  const log = JSON.parse(msg.payload);\n  // Push to connected SSE clients for this org\n  sseManager.broadcast(orgId, log);\n});\n\n// SSE endpoint\napp.get('/api/logs/stream', (req, reply) => {\n  const { orgId } = req.user;\n  \n  reply.raw.writeHead(200, {\n    'Content-Type': 'text/event-stream',\n    'Cache-Control': 'no-cache',\n    'Connection': 'keep-alive',\n    'X-Accel-Buffering': 'no'  // Disable nginx buffering\n  });\n  \n  const clientId = generateId();\n  sseManager.addClient(orgId, clientId, reply.raw);\n  \n  // Heartbeat every 30s\n  const heartbeat = setInterval(() => {\n    reply.raw.write(': heartbeat\\n\\n');\n  }, 30000);\n  \n  req.raw.on('close', () => {\n    clearInterval(heartbeat);\n    sseManager.removeClient(orgId, clientId);\n  });\n});\n```\n\n**Results:**\n- 1000 concurrent connections on 4 vCPU server\n- p50 latency: 45ms, p95: 120ms\n- CPU usage: ~30%\n- Memory: ~500MB\n- Zero WebSocket libraries. Zero protocol complexity.\n\n**Why SSE?**\n\nLogs flow server ‚Üí client. Users don't send logs through the stream. If they upload logs, that's a separate POST request.\n\nSSE is perfect. Simple. Fast. Scales.\n\n## When WebSocket Actually Wins\n\nLet's be honest. WebSocket isn't always overkill.\n\n### Use WebSocket When:\n\n**1. True Bidirectional Communication**\n\nBoth parties send messages continuously.\n\n**Examples:**\n- Multiplayer games (constant player input + server updates)\n- Collaborative editing (local edits + remote edits simultaneously)\n- VoIP/video call signaling\n\n**2. Low Latency is Critical**\n\nSub-10ms latency required.\n\n**Examples:**\n- High-frequency trading\n- FPS games (60+ FPS = <16ms budget)\n- Live auctions\n\n**3. Binary Data**\n\nSending images, audio, video frames.\n\n**WebSocket supports binary:**\n```javascript\nws.send(new Uint8Array([1, 2, 3, 4]));\n```\n\n**SSE is text-only.** You'd have to Base64 encode binary (33% overhead).\n\n### Use SSE When:\n\n**1. Server ‚Üí Client Only**\n\nData flows one direction.\n\n**Examples:**\n- Dashboards\n- Notifications\n- Live feeds\n- AI streaming responses\n- Log tailing\n- Stock tickers\n- Build status\n- Analytics\n\n**2. Simplicity Matters**\n\nNo reconnection logic. No frame handling. Just HTTP.\n\n**3. Works Everywhere**\n\nCorporate firewalls. Proxies. CDNs. HTTP works.\n\n**4. Debugging is Important**\n\n`curl` works. Browser DevTools Network tab shows SSE streams clearly.\n\n## Production Gotchas (And How to Fix Them)\n\n### 1. Nginx Buffering\n\n**Problem:** Nginx buffers responses by default. SSE events get stuck.\n\n**Symptom:** Events arrive in bursts, not real-time.\n\n**Fix:**\n\n```nginx\nlocation /api/stream {\n    proxy_pass http://backend;\n    proxy_buffering off;\n    proxy_set_header Connection '';\n    proxy_http_version 1.1;\n    chunked_transfer_encoding off;\n}\n```\n\nOR set response header:\n\n```javascript\nres.setHeader('X-Accel-Buffering', 'no');\n```\n\n### 2. Load Balancer Sticky Sessions\n\n**Problem:** Multiple backend servers. Client connects to Server A. Server A crashes. Client reconnects to Server B. Lost messages.\n\n**Solution:** Use a message backplane (Redis Pub/Sub, RabbitMQ, Kafka).\n\n**Architecture:**\n\n```\nClient 1 ‚Üí LB ‚Üí Server A ‚îÄ‚îê\nClient 2 ‚Üí LB ‚Üí Server B ‚îÄ‚îº‚Üí Redis Pub/Sub\nClient 3 ‚Üí LB ‚Üí Server C ‚îÄ‚îò\n```\n\n**Event published ‚Üí Redis ‚Üí All servers ‚Üí All connected clients**\n\n**Implementation (Node.js + Redis):**\n\n```javascript\nconst redis = require('redis');\nconst subscriber = redis.createClient();\nconst publisher = redis.createClient();\n\n// Subscribe to channel\nawait subscriber.subscribe('updates');\n\nsubscriber.on('message', (channel, message) => {\n  // Broadcast to all SSE clients connected to THIS server\n  sseClients.forEach(client => {\n    client.write(`data: ${message}\\n\\n`);\n  });\n});\n\n// Publish event (from any server)\npublisher.publish('updates', JSON.stringify({ data: 'New event' }));\n```\n\n**Now scale horizontally.** Add/remove servers. Clients don't care.\n\n### 3. Heartbeats\n\n**Problem:** Proxies close idle connections (60-120 seconds typical).\n\n**Solution:** Send heartbeat every 30 seconds.\n\n```javascript\nconst heartbeat = setInterval(() => {\n  res.write(': heartbeat\\n\\n');  // Comment line, ignored by client\n}, 30000);\n\nreq.on('close', () => clearInterval(heartbeat));\n```\n\n### 4. Authentication\n\n**Problem:** EventSource doesn't support custom headers.\n\n**Solutions:**\n\n**A) Use query parameters:**\n```javascript\nconst eventSource = new EventSource(`/stream?token=${authToken}`);\n```\n\n**B) Use cookies:**\n```javascript\n// Server sets cookie on login\nres.cookie('auth', token, { httpOnly: true });\n\n// EventSource automatically sends cookies\nconst eventSource = new EventSource('/stream');\n```\n\n**C) Use Authorization URL (non-standard but works):**\n```javascript\nconst eventSource = new EventSource(`https://${authToken}@api.example.com/stream`);\n```\n\n### 5. Concurrent Connection Limits\n\n**Problem:** EventSource counts towards browser connection limit (6 per domain on HTTP/1.1).\n\n**Solutions:**\n\n**A) Use HTTP/2** (best option - unlimited streams)\n\n**B) Use subdomain sharding:**\n```javascript\nconst shard = userId % 4;\nconst eventSource = new EventSource(`https://stream${shard}.example.com/events`);\n```\n\n**C) Close unused connections:**\n```javascript\neventSource.close();  // When no longer needed\n```\n\n## Implementation Patterns\n\n### Pattern 1: Simple Broadcast\n\n**Use case:** Send same data to all clients (stock ticker, news feed)\n\n```javascript\nconst clients = new Set();\n\napp.get('/stream', (req, reply) => {\n  reply.raw.writeHead(200, {\n    'Content-Type': 'text/event-stream',\n    'Cache-Control': 'no-cache'\n  });\n  \n  clients.add(reply.raw);\n  \n  req.raw.on('close', () => {\n    clients.delete(reply.raw);\n  });\n});\n\n// Broadcast function\nfunction broadcast(data) {\n  const message = `data: ${JSON.stringify(data)}\\n\\n`;\n  clients.forEach(client => client.write(message));\n}\n\n// Example: Update every second\nsetInterval(() => {\n  broadcast({ timestamp: Date.now(), price: Math.random() * 100 });\n}, 1000);\n```\n\n### Pattern 2: Per-User Streams\n\n**Use case:** Different data per user (notifications, personalized feeds)\n\n```javascript\nconst userClients = new Map();  // userId -> Set of connections\n\napp.get('/stream', (req, reply) => {\n  const userId = req.user.id;\n  \n  reply.raw.writeHead(200, {\n    'Content-Type': 'text/event-stream',\n    'Cache-Control': 'no-cache'\n  });\n  \n  if (!userClients.has(userId)) {\n    userClients.set(userId, new Set());\n  }\n  userClients.get(userId).add(reply.raw);\n  \n  req.raw.on('close', () => {\n    const clients = userClients.get(userId);\n    clients.delete(reply.raw);\n    if (clients.size === 0) {\n      userClients.delete(userId);\n    }\n  });\n});\n\n// Send to specific user\nfunction sendToUser(userId, data) {\n  const clients = userClients.get(userId);\n  if (!clients) return;\n  \n  const message = `data: ${JSON.stringify(data)}\\n\\n`;\n  clients.forEach(client => client.write(message));\n}\n```\n\n### Pattern 3: Event Types\n\n**Use case:** Multiple event types on one stream (logs + metrics + alerts)\n\n```javascript\napp.get('/stream', (req, reply) => {\n  reply.raw.writeHead(200, {\n    'Content-Type': 'text/event-stream',\n    'Cache-Control': 'no-cache'\n  });\n  \n  // Named events\n  function sendEvent(eventType, data) {\n    reply.raw.write(`event: ${eventType}\\n`);\n    reply.raw.write(`data: ${JSON.stringify(data)}\\n\\n`);\n  }\n  \n  sendEvent('log', { level: 'info', message: 'App started' });\n  sendEvent('metric', { cpu: 45, memory: 2048 });\n  sendEvent('alert', { severity: 'high', message: 'Disk full' });\n});\n\n// Client\nconst eventSource = new EventSource('/stream');\n\neventSource.addEventListener('log', (e) => {\n  console.log('Log:', JSON.parse(e.data));\n});\n\neventSource.addEventListener('metric', (e) => {\n  console.log('Metric:', JSON.parse(e.data));\n});\n\neventSource.addEventListener('alert', (e) => {\n  console.log('Alert:', JSON.parse(e.data));\n});\n```\n\n## Migration: WebSocket ‚Üí SSE\n\n**Scenario:** You have WebSocket. Want to simplify. How?\n\n### Step 1: Identify Communication Pattern\n\n**Audit your WebSocket usage:**\n\n```javascript\n// What messages does CLIENT send?\nws.send({ type: 'subscribe', channel: 'updates' });\nws.send({ type: 'ping' });\n\n// What messages does SERVER send?\nws.send({ type: 'update', data: {...} });\nws.send({ type: 'pong' });\n```\n\n**If client only sends:**\n- Subscription/configuration (at connection start)\n- Ping/heartbeat (keepalive)\n\n**You can use SSE + HTTP POST.**\n\n### Step 2: Move Client‚ÜíServer to HTTP\n\n**WebSocket:**\n```javascript\nws.send({ type: 'subscribe', channel: 'metrics' });\n```\n\n**SSE equivalent:**\n```javascript\n// Subscribe via query param or POST\nconst eventSource = new EventSource('/stream?channel=metrics');\n\n// OR\nawait fetch('/subscribe', {\n  method: 'POST',\n  body: JSON.stringify({ channel: 'metrics' })\n});\nconst eventSource = new EventSource('/stream');\n```\n\n### Step 3: Replace WebSocket Server with SSE\n\n**Before (WebSocket):**\n```javascript\nconst WebSocket = require('ws');\nconst wss = new WebSocket.Server({ port: 3000 });\n\nwss.on('connection', (ws) => {\n  ws.on('message', (message) => {\n    // Handle client message\n  });\n  \n  setInterval(() => {\n    ws.send(JSON.stringify({ data: 'update' }));\n  }, 1000);\n});\n```\n\n**After (SSE):**\n```javascript\napp.get('/stream', (req, reply) => {\n  reply.raw.writeHead(200, {\n    'Content-Type': 'text/event-stream',\n    'Cache-Control': 'no-cache'\n  });\n  \n  const interval = setInterval(() => {\n    reply.raw.write(`data: ${JSON.stringify({ data: 'update' })}\\n\\n`);\n  }, 1000);\n  \n  req.raw.on('close', () => clearInterval(interval));\n});\n```\n\n### Step 4: Update Client\n\n**Before (WebSocket):**\n```javascript\nconst ws = new WebSocket('ws://localhost:3000');\nws.onmessage = (e) => console.log(JSON.parse(e.data));\n```\n\n**After (SSE):**\n```javascript\nconst es = new EventSource('http://localhost:3000/stream');\nes.onmessage = (e) => console.log(JSON.parse(e.data));\n```\n\n**Result:** Simpler code. Same functionality.\n\n## The Bottom Line\n\n**For 95% of real-time applications, SSE is the better choice.**\n\n**Why:**\n- Simpler (just HTTP)\n- Easier to debug (curl works)\n- Auto-reconnect (built-in)\n- Works everywhere (no proxy issues)\n- Performance equivalent (for most use cases)\n- Scales horizontally (stateless with backplane)\n\n**When to use WebSocket:**\n- True bidirectional (both sides send frequently)\n- Binary data (audio/video frames)\n- Ultra-low latency required (<10ms)\n- Gaming, collaborative editing, VoIP\n\n**Default to SSE.** Only reach for WebSocket when you have a specific need for its features.\n\n**The best technology isn't the most powerful. It's the simplest one that solves your problem.**\n\nFor server-to-client streaming, that's Server-Sent Events.\n\n---\n\n**Simple. Reliable. Fast.**\n\nSometimes boring tech is the right tech.\n\n---\n\n## Further Reading\n\n**MDN Web Docs:**\n- [Using Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events)\n- [EventSource API](https://developer.mozilla.org/en-US/docs/Web/API/EventSource)\n\n**Production Case Studies:**\n- [Shopify: Using SSE to Simplify Real-time Streaming at Scale](https://shopify.engineering/server-sent-events-data-streaming)\n- [Timeplus: WebSocket vs SSE Performance Comparison](https://www.timeplus.com/post/websocket-vs-sse)\n\n**Specifications:**\n- [Server-Sent Events W3C Specification](https://html.spec.whatwg.org/multipage/server-sent-events.html)",
      "stars": null,
      "comments": 16,
      "upvotes": 53,
      "read_time": "12 min read",
      "language": null
    },
    {
      "title_en": "I Watched an Anime and Built a VS Code Theme Bundle Inspired by 17th-Century Japanese Art",
      "url": "https://dev.to/theonemohitsharma/i-watched-an-anime-and-built-a-vs-code-theme-bundle-inspired-by-17th-century-japanese-art-h5k",
      "source": "devto",
      "published_at": "2026-02-04T17:09:28+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "githubchallenge",
        "cli",
        "githubcopilot"
      ],
      "content_length": 6531,
      "content_preview": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n## What I Built\n\nI built **Ukiyo-Tone** ‚Äî a six-theme VS Code bundle inspired by 17th-century Japanese Ukiyo-e woodblock prints and Sumi-e ink traditions.\n\nIt started with a Netflix binge.\n\nI was watching *Blue Eye Samurai* and couldn't stop staring at the animation. Gorgeous. Every frame felt like a painting. That sent me down a rabbit hole ‚Äî researching Ukiyo-e techniques, woodblock prin",
      "content_full": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n## What I Built\n\nI built **Ukiyo-Tone** ‚Äî a six-theme VS Code bundle inspired by 17th-century Japanese Ukiyo-e woodblock prints and Sumi-e ink traditions.\n\nIt started with a Netflix binge.\n\nI was watching *Blue Eye Samurai* and couldn't stop staring at the animation. Gorgeous. Every frame felt like a painting. That sent me down a rabbit hole ‚Äî researching Ukiyo-e techniques, woodblock printing, mineral pigments, the whole aesthetic tradition.\n\nAnd somewhere in that rabbit hole, I thought: *What if my code editor felt like this?*\n\nNot flashy. Not trendy. Just... calm. Sharp. Built for long hours of focused work.\n\nHonestly? Most VS Code themes feel like they're designed for screenshots, not actual coding sessions. They're either too sterile (corporate vibes) or too chaotic (rainbow syntax highlighting that makes your eyes bleed after two hours).\n\nUkiyo-Tone is different. It treats color as **syntax, not paint**. Keywords define grammar. Functions express motion. Variables carry weight. Nothing exists purely for aesthetics.\n\nThe bundle includes six distinct themes, each inspired by specific elements of Edo period culture:\n\n\n![All Unikyo-Tone Themes](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/640nqfixrc2wiyvuypum.png)\n\n**Asahi (Êó≠ - Morning Sun)** ‚Äî Light theme. Fresh washi paper and early morning light.\n\n**Karesansui (ÊûØÂ±±Ê∞¥ - Dry Landscape)** ‚Äî Zen garden theme. Moss, stone, and raked sand.\n\n**Tasogare (ÈªÑÊòè - Twilight)** ‚Äî Soft dark theme. Evening landscapes and faded indigo.\n\n**Koke-Dera (ËãîÂØ∫ - Moss Temple)** ‚Äî Monotone dark. Deep forest shadows.\n\n**Kuro-Sumi (ÈªíÂ¢® - Black Ink)** ‚Äî Deep dark theme. Charcoal ink and midnight shadows.\n\n**Kachi-iro (Âãù„Å°Ëâ≤ - Victory Color)** ‚Äî High contrast dark. The \"lucky\" navy indigo worn by samurai.\n\nEach theme follows the same philosophy: **restraint over abundance**. If a visual choice improves screenshots but harms long-term focus, it gets rejected.\n\n## Demo\n\nüé® **Install from VS Code Marketplace:** [Ukiyo-Tone](https://marketplace.visualstudio.com/items?itemName=OneMohitSharma.ukiyo-tone)\n\nüîó **GitHub Repository:** [github.com/mohitSharma74/ukiyo-tone](https://github.com/mohitSharma74/ukiyo-tone)\n\nüöÄ **Install on VS Code Forked IDEs from Open VSIX:** [Ukiyo-tone](https://open-vsx.org/extension/OneMohitSharma/ukiyo-tone)\n\n**Asahi (Êó≠ - Morning Sun)**\n\n![Asahi Theme](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/43d9uebdt765ex123a4d.png)\n\n**Karesansui (ÊûØÂ±±Ê∞¥ - Dry Landscape)**\n![Karesansui Theme](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/be67wx8endooanzv6anv.png)\n\n**Tasogare (ÈªÑÊòè - Twilight)**\n![Tasogare Theme](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t57ii6tno6s9bzeirhg8.png)\n\n**Koke-Dera (ËãîÂØ∫ - Moss Temple)**\n![Koke-Dera Theme](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/80uaeoe28grqoeid6r13.png)\n\n**Kuro-Sumi (ÈªíÂ¢® - Black Ink)**\n![Kuro-Sumi Theme](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/54hfscn279jubnjiilxo.png)\n\n**Kachi-iro (Âãù„Å°Ëâ≤ - Victory Color)**\n![Kachi-iro Theme](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fh737dfb5gf3u3amvv0c.png)\n\n\n## My Experience with GitHub Copilot CLI\n\nGitHub Copilot CLI didn't just help me build this ‚Äî it helped me *think* through this.\n\n### Extracting Color Palettes from Art\n\nI fed Copilot CLI a bunch of reference images ‚Äî actual Ukiyo-e paintings, woodblock prints, photos of zen gardens. Then I asked it to analyze and extract color palettes that would work for syntax highlighting while staying true to the aesthetic.\n\nThis wasn't just \"give me hex codes.\" It was a conversation about which colors carry enough contrast for readability, which ones feel organic versus mechanical, and how mineral pigment limitations in traditional art could translate to a modern coding environment.\n\n### Developing the Philosophy Document\n\nThe PHILOSOPHY.md file in my repo isn't an afterthought ‚Äî it's the project's north star. I used Copilot CLI to help articulate my vision clearly.\n\nI'd describe what I wanted (\"a theme that disappears once deep focus begins\") and Copilot would help me sharpen the language, identify contradictions in my thinking, and structure the document so it could guide future decisions.\n\nThe result is a decision framework that works for both humans and AI agents:\n\n> *When evaluating any change, ask:*\n> 1. Does this improve long-session clarity?\n> 2. Does this preserve visual hierarchy?\n> 3. Does this reduce cognitive noise?\n> 4. Would removing this make the theme weaker?\n>\n> *If the answer is unclear ‚Äî do nothing.*\n\n### Learning VS Code Theme Standards\n\nI used the Microsoft Learn MCP server with Copilot to stay up to speed on VS Code theme development standards. Instead of jumping between documentation tabs, I could ask contextual questions while building ‚Äî what token types exist, how semantic highlighting works, which scopes to target for specific languages.\n\nIt kept me in flow instead of breaking my concentration to search docs.\n\n### The Honest Truth\n\nCopilot CLI made this project possible in the time I had. Without it, I would've spent weeks just researching color theory and VS Code APIs. Instead, I spent that time on what mattered: making deliberate design choices that serve developers who actually want calm in their editor.\n\n## What's Next\n\nUkiyo-Tone itself is just the beginning.\n\nI'm currently working on a companion project: **VS Code icons inspired by Ukiyo-e art and Sumi-e brush strokes**. Same philosophy ‚Äî restraint, clarity, cultural grounding. Designed to pair seamlessly with these themes.\n\nBecause honestly? Your editor shouldn't feel like six different design languages duct-taped together. It should feel like one intentional space.\n\n## Who This Is For\n\nThis theme isn't for everyone. And that's intentional.\n\nIt's for developers who spend long hours in their editor and want something that doesn't fight for their attention. For night-shift coders who need low eye strain without sacrificing readability. For anyone who's ever looked at their VS Code setup and thought: *this feels like a corporate tool, not a craftsman's instrument*.\n\nUkiyo-Tone is for people who want their workspace to feel like a zen garden ‚Äî calm, intentional, and built for seeking clarity.\n\nIf you've ever felt like your editor's theme was designed for someone else's demo video instead of your actual workflow, give this a try.\n\nThat's exactly what I'd like you to take away from this.",
      "stars": null,
      "comments": 3,
      "upvotes": 7,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "An Al-go-llama Adventure",
      "url": "https://dev.to/annavi11arrea1/an-al-go-llama-adventure-4051",
      "source": "devto",
      "published_at": "2026-02-04T03:35:52+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "algoliachallenge",
        "ai",
        "agents"
      ],
      "content_length": 5358,
      "content_preview": "*This is a submission for the [Algolia Agent Studio Challenge](https://dev.to/challenges/algolia): Consumer-Facing Conversational Experiences*\n\n## What I Built\nHello Everyone!\n\nAs an event vendor for 5 years, I know that it can be hard to find events you are looking for. You need to sell at places with your target audience. \n\nIt would be _really_ nice to have a natural conversation about what you are looking for, rather than querying hard parameters all the time. It is possible to discover even ",
      "content_full": "*This is a submission for the [Algolia Agent Studio Challenge](https://dev.to/challenges/algolia): Consumer-Facing Conversational Experiences*\n\n## What I Built\nHello Everyone!\n\nAs an event vendor for 5 years, I know that it can be hard to find events you are looking for. You need to sell at places with your target audience. \n\nIt would be _really_ nice to have a natural conversation about what you are looking for, rather than querying hard parameters all the time. It is possible to discover even more opportunities with natural language models. It makes it much easier for the exhausted business owner to say \"Find me events to vend at this April near Chicago\". Instead of typing, checking, toggling, whatevering-- a bunch of fields. \n\nSo let's output some related findings for the very tired Miss Bosswoman CEO! \n\nBy the way guys, I've been working on this project here and there through time. This Algolia search intelligent chat bot is a wonderful feature.\n\n<hr>\n\n## Demo\n<!-- Share a link to your project and include screenshots or videos showing your conversational agent in action. -->\n\n{% youtube nNkz04Je3Ts %}\n\n<hr>\n\n## How I Used Algolia Agent Studio\n<!-- Explain how you leveraged Algolia's Agent Studio for your conversational experience. What data did you index? How does retrieval enhance the dialogue? Share details about your targeted prompting approach and how you engineered prompts to get the best results. -->\n\nI took this opportunity to learn about Algolia and implement it into one of my projects: GoVend, Connecting vendors to events. \n\nImplementing my first intelligent chat assistant was a challenge for sure. Initially, I set up an Algolia agent with Gemini. \n\n\n\n![user interaction](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/88ukxrn1qjvzui22qrgl.png)\n\n<br>\n\n‚ÄúCarl‚Äù, the agent I created in Algolia, would take user queries and find related events in the available data. \n\nYou can choose what attributes should be searchable on each index. An index is a group of data on Algolia. For me, I had an ‚ÄúEvents‚Äù index, which included things like name, date, city, etc. I left most of them on to improve searchability. \n\n![configure algolia search](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/77wnqqgwa3gmye2nem9d.png)\n<figcaption>Example chosen index in algolia</figcaption>\n\n<br>\n\n![search parameters in algolia](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9cjz48aeifbfwzbtdvjg.png)\n\nI explored Algolia‚Äôs MCP Server integration. It was fairly straightforward, and I was getting excited about it. By assigning tools to agents, we can build them to be more accurate and useful.\n\nAnd then of course...\n\nI ran out of API requests as usual. I pushed the envelope a bit here with an Ollama wildcard. \n\nI first connected local Ollama (model: llama3.2:latest)to my app and got that working with my local data.\n\n<hr>\n\n## Why Fast Retrieval Matters\n\nI integrated Algolia's Search API with llama3.2 for natural language event discovery. Users can ask questions like 'Find events in Chicago' and get intelligently formatted results. Algolia's powerful search combined with Ollama's natural language understanding is a winning scenario.\n\n\n![output log](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t1drrczmihw2bm45l1a9.png)\n<figcaption>Natural language event search powered by Algolia's search API</figcaption>\n\n\nI realized that maybe having a fallback method for searches was a good idea to handle errors in production. Llama3.2 will be able to find some event data no matter what.\n\nI used enhanced Algolia search capabilities by using:\n\n<ol>\n   <li>Zipcode Detection\n   <li>Geo-Location Search\n      <ul>\n         <li>Geocodes the zipcode to latitude/longitude</li>\n         <li>Uses Algolia's aroundLatLng geo search</li>\n      </ul>\n      <ul>Smart radius selection:\n          <li>50km if query includes \"near\", \"nearby\", \"close\", \"around\"</li>\n          <li>100km default radius</li>\n      </ul>\n   </li>\n   <li>Distance Display\n      <ul>\n         <li>Shows distance in miles for each event</li>\n         <li>Sorted by proximity automatically</li>\n      </ul>\n   </li>\n   <li>Combined with Time Filters\n     <ul>\n        <li>Can search \"events near 35801 this spring\"</li>\n        <li>Combines zipcode + seasonal date filtering</li>\n     </ul>\n   </li>\n</ol>\n\n  Example Queries:\n\n   - \"Find events near 35801\"\n   - \"What's happening close to 90210 in June?\"\n\n<hr>\n\n## Highlights that Align with Algolia‚Äôs Standards:\n\n<br>\n\n1. **Unique Ollama Integration**\n\n2. **Error Handling**\n Graceful fallback system\n Clear logging and debugging\n No breaking when services unavailable\n\n3. **Natural Language Processing**\n Extracts locations from queries\n Understands user intent\n Aesthetically pleasing results formatting \n\n5. **Real-World Problem Solving**\n Faced API limits (common in production)\n Built resilient solution\n\n<hr>\n\n## The Future:\n\nThis was an interesting journey for me, lots of exciting tiny wins, and a couple long drawn out learning experiences. I like that Algolia allows you to configure multiple agents in the background. One day when I stop running out of API requests, I‚Äôd like to do a little experiment and have multiple agents talking to each other and see where that ends up.\n\n\nRepo: <a href=\"https://github.com/AnnaVi11arrea1/go-vend-event/tree/ai_beta\">GoVend ai_beta</a>",
      "stars": null,
      "comments": 0,
      "upvotes": 15,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "A Silly Idea: A Star Wars Lightsaber Duel in the Terminal with GitHub Copilot CLI",
      "url": "https://dev.to/gramli/a-silly-idea-a-star-wars-lightsaber-duel-in-the-terminal-with-github-copilot-cli-1ki7",
      "source": "devto",
      "published_at": "2026-02-09T12:57:19+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "githubchallenge",
        "cli",
        "githubcopilot"
      ],
      "content_length": 5262,
      "content_preview": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n## üé¨ What I Built\n<!-- Provide an overview of your application and what it means to you. -->\nIn a previous submission for a [productivity tool](https://dev.to/gramli/msg-rocket-from-diff-to-decision-with-github-copilot-cli-1ba8), I sneaked in a small Easter egg inspired by **The Matrix**. That led to a slightly silly thought, what if I built an actual movie scene using **GitHub Copilot CLI",
      "content_full": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n## üé¨ What I Built\n<!-- Provide an overview of your application and what it means to you. -->\nIn a previous submission for a [productivity tool](https://dev.to/gramli/msg-rocket-from-diff-to-decision-with-github-copilot-cli-1ba8), I sneaked in a small Easter egg inspired by **The Matrix**. That led to a slightly silly thought, what if I built an actual movie scene using **GitHub Copilot CLI**, rendered **entirely in a terminal** with Unicode characters?\n\nThat idea stuck with me. **It sounded unnecessary, impractical‚Ä¶** and kind of **awesome** üôÇ\n\nSo I decided to create a **Star Wars inspired lightsaber duel in pure CLI.** My very first idea was the Lord of the Rings ‚ÄúRide of the Rohirrim‚Äù scene üêéüêé, but I quickly realized I should keep the scope smaller ‚Äî after all, it‚Äôs still just a terminal and Unicode characters. :laughing:\n\n## üìΩÔ∏è Teaser\n<!-- Share a link to your project and include a video walkthrough or screenshots showing your application in action. -->\n\n*The full animation is a bit too long for this post, so I prepared a short ‚Äúteaser‚Äù instead.*\n\n![Duel of the CLI - Teaser](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vr46bo9ajojducqrqj4a.gif)\n\n#### [View the complete version (~65 seconds) on GitHub](https://github.com/Gramli/star-wars-animation/blob/master/Assets/star-wars-animation-full.gif)\n\n\n#### üèÉ‚Äç‚ôÇÔ∏è Run it locally\n```shell\ngit clone https://github.com/Gramli/star-wars-animation.git\ncd star-wars-animation\ndotnet run\n```\n**Note:**\n- You‚Äôll need **.NET 10 SDK** installed.\n- Use a font with good Unicode support (e.g., **Cascadia Code**).\n- The **Visual Studio Console** or **VS Code terminal** render the block characters best -> they‚Äôre true Jedi-level terminals üòÑ\n\n> ‚ö†Ô∏è Running in classic `cmd.exe` or other limited terminals may break the animation. Consider it a **training exercise for Padawans**, only the worthy (and properly equipped) terminals will experience the duel in all its glory.\n\n## :movie_camera: My Experience with GitHub Copilot CLI\n<!-- Explain how you used GitHub Copilot CLI while building your project and how it impacted your development experience. -->\n\nI brought the idea and created several [detailed prompts](https://github.com/Gramli/star-wars-animation/tree/master/.github/prompts). Copilot handled most of the code generation and quickly produced a nice draft version. However, something was missing ‚Äî it lacked soul.\n\nAt that point, **Copilot stopped being just a code generator** and became a **Creative Director.** I kept refining prompts, pushing for stronger visuals, better animation flow, and more cinematic moments.\n\nHere‚Äôs a real example of how our ‚Äúconversation‚Äù looked:\n- **Me:**\n*The animation works and we already have some nice effects, but we need to take it to the next level an Oscar-movie level. We need a real wow effect that makes this animation stunning. Can you suggest 3‚Äì5 ideas?*\n\n- **Copilot:**\n```text\nHere are a few ways to push the animation toward a more cinematic feel, even within CLI constraints:\n\n1. Lightsaber motion trails to make fast swings feel fluid and powerful  \n2. Gradient-based lighting to simulate glow and light falloff  \n3. Impact shockwaves that briefly distort the screen on heavy clashes  \n4. Molten damage on walls and floors that cool over time  \n5. Cinematic letterboxing during key moments to focus attention  \n\nI‚Äôd start with motion trails and molten damage for the biggest visual impact.\nWhich one would you like me to implement first?\n```\n\nCopilot responded with ideas like motion trails for lightsabers, gradient lighting, shockwave effects, molten damage on walls, and even cinematic letterboxing. Some of them made it into the final version, others stayed on the cutting-room floor, but the **process itself was surprisingly fun**.\n\nI was genuinely impressed. Copilot helped transform a simple ASCII-style drawing into a small but entertaining show. Sure, it‚Äôs not an Oscar-winning movie, but for a playful, slightly ridiculous idea, I think it turned out pretty cool.\n\n### üí• When We Failed (And Pivoted)\n\nThings went smoothly‚Ä¶ until I had a **very bad idea**:\n\n**‚ÄúWhat if we add a camera rotation illusion?‚Äù** üòÖ\n\nYeah. That was ambitious.\n\nThis turned out to be a real challenge, even for Copilot. After many new sessions and repeatedly reworking prompts, we eventually had to pivot. A full Y-axis camera rotation just wasn‚Äôt readable with simple ASCII silhouettes, so we ended up experimenting with X-axis rotation and a top-down view instead.\n\nAt that point, the silhouettes are so minimal that the viewer has to mentally accept the perspective shift, but that‚Äôs part of the charm (and limitation) of doing cinematic nonsense in a CLI.\n\nAnd honestly? That experimentation, even when it didn‚Äôt fully work was one of the most fun parts of the project.\n\n## üí≠ Final Thoughts\n\nI really enjoyed this ride. Using **GitHub Copilot CLI** for something completely **non-practical, visual, and slightly absurd** turned out to be a **great experience**. It pushed me to think differently about prompts, iteration, and collaboration with an AI, not just as a tool, but as a **creative partner**.\n\n**May the Source be with you!**",
      "stars": null,
      "comments": 10,
      "upvotes": 19,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "What Your Claude Code Agents Don't Need to Be Told",
      "url": "https://dev.to/helderberto/what-your-claude-code-agents-dont-need-to-be-told-4ed5",
      "source": "devto",
      "published_at": "2026-02-09T17:44:21+00:00",
      "external_id": null,
      "tags": [
        "ai",
        "claudecode",
        "contextwindow"
      ],
      "content_length": 7439,
      "content_preview": "Claude Code has a finite context window. Every token matters. Your agent definitions, skill files, and docs all compete for that space with the actual code you're asking it to analyze.\n\nThe problem: it's tempting to write exhaustive agent configurations, detailed examples, scripted responses, repeated boilerplate. It feels like you're being thorough. But you're actually teaching the model things it already knows, while leaving less room for the things it doesn't.\n\nI hit this wall while working o",
      "content_full": "Claude Code has a finite context window. Every token matters. Your agent definitions, skill files, and docs all compete for that space with the actual code you're asking it to analyze.\n\nThe problem: it's tempting to write exhaustive agent configurations, detailed examples, scripted responses, repeated boilerplate. It feels like you're being thorough. But you're actually teaching the model things it already knows, while leaving less room for the things it doesn't.\n\nI hit this wall while working on a frontend feature. My agents were loaded with generic programming examples, and I started wondering: is any of this actually helping, or is it just consuming context?\n\n> This is a follow-up to [Teaching Claude Code Your Standards](https://dev.to/helderberto/teaching-claude-code-your-standards-k9p), where I covered the initial setup of `~/.claude/` with docs, skills, and agents.\n\n## The Real Problem: Teaching the Model What It Already Knows\n\nMy TypeScript enforcer agent included examples of how to use spread operators, how to write early returns, and when to use `Set` instead of `Array.find()`. The model already knows all of that.\n\nWhat it *doesn't* know: our project uses a formatjs babel plugin with `ast: true`, which compiles i18n messages to AST at build time. This causes parameterized messages like `{count} active` to render variable names literally in tests. That's a gotcha worth documenting, it cost me real debugging time.\n\nThe distinction is simple: **generic programming knowledge is noise. Project-specific knowledge is signal.**\n\n## Three Filters for Every Line in Your Config\n\n### 1. Does the model already know this?\n\nThe model knows common patterns. It knows early returns, immutability with spread, N+1 query problems, React memoization. Including examples of these doesn't make it better at applying them, it just takes up space.\n\n**Delete**: Generic code examples, common design patterns, standard best practices.\n\n**Keep**: Gotchas specific to your project, team conventions the model can't infer, configuration quirks that cause surprising behavior.\n\n### 2. Is this repeated across agents?\n\nI found identical sections copy-pasted across every agent:\n\n```markdown\n## Commands to Use\n- `Glob` - Find files\n- `Grep` - Search for patterns\n- `Read` - Examine content\n- `Bash` - Run scripts\n\n## Success Criteria\nThe agent is successful when...\n```\n\nThe model already knows its available tools from the system prompt. \"Success criteria\" sections just reword the agent's description. These are filler.\n\n### 3. Is this a checklist or an essay?\n\nThe model doesn't need you to explain *how* to extract a function. It needs to know *when* you want it flagged.\n\n**Before (essay with examples):**\n\n```markdown\n### Readability Review\n\nEvaluate function complexity and cognitive load...\n\n**Long functions example:**\n// ‚ö†Ô∏è Too long (>50 lines)\nconst processOrder = (order) => {\n  // ... 80 lines of logic\n};\n\n// ‚úÖ Extracted\nconst validateOrder = (order) => { /* ... */ };\nconst calculateTotal = (order) => { /* ... */ };\n...\n```\n\n**After (checklist):**\n\n```markdown\n### Readability\n- Functions >50 lines ‚Üí extract\n- Nesting >3 levels ‚Üí early returns\n- Magic numbers ‚Üí named constants\n- Unclear names\n```\n\nSame rules. The model fills in the *how*.\n\n## Agents vs Skills: Different Problems, Different Tools\n\nI had a \"dependency auditor\" agent, hundreds of lines of instructions for what amounts to running `npm audit` and `npm outdated`. That's not a judgment call. That's a command sequence.\n\n**Agents** solve problems requiring judgment: code review, test quality assessment, TypeScript enforcement. They need checklists of *what to look for*.\n\n**Skills** solve problems with deterministic steps: commit, push, lint, audit. They need a sequence of commands and rules about when to stop.\n\nThe dependency auditor became a skill:\n\n```markdown\n# Dependency Audit\n\n## Commands\nRun in parallel:\n- `npm audit` - security vulnerabilities\n- `npm outdated` - outdated packages\n\n## Workflow\n1. Run security audit and outdated check\n2. Report critical vulnerabilities with fix commands\n3. List outdated packages (major vs minor/patch)\n4. Check for unused deps: grep imports in src/\n\n## Rules\n- Use `npm audit`, never `npx`\n- Prioritize: security > major updates > unused > minor updates\n```\n\nClear. Deterministic. No essays needed.\n\n## Overlapping Agents Dilute Focus\n\nI had three agents doing related work:\n\n- **code-review**: logic errors, readability, pattern consistency\n- **refactor-assistant**: code smells, complexity, dead code, duplication\n- **perf-analyzer**: algorithm complexity, unnecessary re-renders, memoization\n\nThe overlap was significant. The refactor-assistant's unique value was a code smells checklist. The perf-analyzer's unique value was React-specific checks. Both fit naturally as sections in one focused code-review agent.\n\nOne agent with clear sections beats three agents competing for the same territory.\n\n## Your Workflows Should Fail Fast\n\nMy `/ship` skill (stage ‚Üí commit ‚Üí push) had no quality gates. It would happily push broken code. The fix was simple:\n\n```markdown\n## Workflow\n1. Review all changes\n2. Run quality checks in parallel:\n   - `npm run lint`\n   - `npm test`\n3. If checks fail: report errors, STOP do not commit or push\n4. Generate commit message\n5. Stage, commit, push\n```\n\nThe key is the hard-stop rule. Without it, the skill optimizes for speed over correctness.\n\n## Document What Only You Know\n\nThe highest-value documentation is knowledge the model can't infer from your code alone:\n\n**Internationalization gotchas**:\n\n- Whitespace between elements: use layout (Flex/gap) instead of `{' '}` - linters flag literal strings in JSX\n\n**Accessibility testing**:\n\n- `getByRole` as primary query - if it can't find the element, the a11y is likely wrong\n- Prefer `getByRole` over `container.querySelector` - query by accessibility, not DOM structure\n\n**Team conventions**:\n\n- `vi.spyOn` over `vi.mock` for your own code\n- `vi.mock` only for third-party libraries\n\nThese are decisions your team made. The model can't infer them from reading your code. They prevent real mistakes. That's what your config should contain.\n\n## What I Can't Prove\n\nI can't prove this made my agents \"better.\" There's no benchmark for agent configuration effectiveness. What I can say: the content I removed was generic programming knowledge the model already has. The content I kept, and added, is specific to my projects and workflows. Every token saved on instructions is a token available for analyzing actual code.\n\nWhether that tradeoff matters depends on how close to the context limit you work. For long sessions with large codebases, it adds up.\n\n## Takeaways\n\n1. **Generic knowledge is noise**: If the model already knows it, it doesn't belong in your config\n2. **Checklists over essays**: Tell it *what* to check, not *how*\n3. **Skills for commands, agents for judgment**: Match the tool to the problem\n4. **One focused agent beats three overlapping ones**: Merge and consolidate\n5. **Document what only you know**: Project gotchas, team conventions, configuration quirks\n6. **Workflows should fail fast**: Add quality gates before destructive operations\n\nYour Claude Code configuration should contain what only *you* know about your project. Everything else is already in the model.\n\nMy full config: [github.com/helderberto/dotfiles](https://github.com/helderberto/dotfiles/tree/main/claude/.claude)\n",
      "stars": null,
      "comments": 4,
      "upvotes": 8,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "NotebookLM Enhancer - UPDATE ‚ú®",
      "url": "https://dev.to/cgcm070/notebooklm-enhancer-2j6p",
      "source": "devto",
      "published_at": "2026-02-04T21:06:09+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "githubchallenge",
        "cli",
        "githubcopilot"
      ],
      "content_length": 10483,
      "content_preview": "\n\n\n*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n**Author:** Cesar Castillo\n**Repository:** https://github.com/CGCM070/NotebookLM_Enhancer\n\n---\n\n\n\n##  From Chaos to Order  ##\n\n**üéâ UPDATE :** Just released **Smart Batch Delete** : delete multiple notes with one click! No more 60+ clicks to clean up 20 notes. Check out the new feature section below! üëá\n\n- Building a Chrome Extension That Finally Organizes NotebookLM\n\n## What I Built\n\n\nA ",
      "content_full": "\n\n\n*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n**Author:** Cesar Castillo\n**Repository:** https://github.com/CGCM070/NotebookLM_Enhancer\n\n---\n\n\n\n##  From Chaos to Order  ##\n\n**üéâ UPDATE :** Just released **Smart Batch Delete** : delete multiple notes with one click! No more 60+ clicks to clean up 20 notes. Check out the new feature section below! üëá\n\n- Building a Chrome Extension That Finally Organizes NotebookLM\n\n## What I Built\n\n\nA Chrome extension that transforms NotebookLM's chaotic sidebar into a beautifully organized folder system, because 47 research notes shouldn't look like a pile of digital laundry.\n\n### The Problem That Drove Me Crazy\n\nIf you've used [NotebookLM](https://notebooklm.google.com), you know it's *magical* for research. Upload PDFs, paste URLs, ask questions it's like having a research assistant that never sleeps.\n\nBut there's one maddening catch: **the sidebar becomes a nightmare when you have more than 10 notes.** \n\nImagine:\n- 15 research papers on Spring Framework\n- 8 articles about microservices  \n- 12 random bookmarks you saved \"for later\"\n- All. In. One. Giant. List.\n\nNo folders. No organization. Just... chaos.\n\n### The Solution: NotebookLM Enhancer\n\nI built a Chrome extension that injects a complete folder management system directly into NotebookLM's sidebar. \n\n###  Key Features\n\n**üóÇÔ∏è Smart Folder Organization**\n- Create folders and subfolders (1 level deep keeping it simple!)\n- Drag & drop notes between folders with smooth animations\n- \"Inbox\" view for unassigned notes\n- Each notebook project has isolated folders (no cross-contamination!)\n\n**üßë‚Äçüé® Polished UI That Matches NotebookLM**\n- Built with Angular + Tailwind CSS\n- Dark/light/system theme toggle\n- Minimalist design that feels native\n- Smooth expand/collapse animations\n\n**üåéInternationalization**\n- Full i18n support (English/Spanish currently)\n- One-click language switcher\n- All UI text translatable\n\n**üí≠Intelligent Integrations**\n- Click any note ‚Üí opens native NotebookLM\n- Click 3-dots menu ‚Üí native menu appears aligned to the right (matching native position)\n- Drag notes from native sidebar ‚Üí drops into our folders\n- Add new notes button that triggers native NotebookLM\n\n**üóëÔ∏è Batch Delete**  *NEW!*\n- Select multiple notes at once with checkboxes\n- \"Selection Mode\" toggle with icon in header\n- Click any row to toggle selection\n- ESC key to cancel selection mode\n- **Automatic confirmation** of NotebookLM's native delete modal - no manual interaction needed\n\n\n**Robust Architecture**\n- Chrome Extension MV3 (latest manifest version)\n- Content Scripts + Shadow DOM for style isolation\n- Iframe-based Angular app for the UI\n- PostMessage bridge for iframe ‚Üî page communication\n- chrome.storage.sync for persistence across devices\n\n###  Architecture Deep Dive\n\nThis isn't a simple content script that adds a few buttons. It's a full micro-frontend architecture:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  NotebookLM Page                                        ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n‚îÇ  ‚îÇ  Native Sidebar (hidden but functional)          ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ Still handles clicks & menus                  ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ We extract data from it                       ‚îÇ   ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n‚îÇ                       ‚Üì                                 ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n‚îÇ  ‚îÇ  Our Injected Host (Shadow DOM)                  ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  Iframe (Angular App)                     ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Folder tree                            ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Drag & drop (Angular CDK)              ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Theme toggle                           ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Note add. folder add...                ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ i18n                                   ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Why an iframe inside Shadow DOM?**\n- **Isolation:** NotebookLM uses Angular Material with global styles our iframe keeps our Tailwind styles pristine\n- **Security:** Content scripts can't easily access iframe internals (and vice versa)\n- **Performance:** Angular app runs independently without polluting the main page\n\n**Communication Flow:**\n1. Content script reads native DOM ‚Üí  extracts notebook data\n2. PostMessage to iframe ‚Üí  Angular displays organized folders\n3. User drags note to folder ‚Üí  PostMessage back to content script\n4. Content script updates chrome.storage.sync\n\n###  Technical Decisions \n\n**1. Storage V3 with Notebook Isolation**\n\nInstead of one global folder structure, each NotebookLM project gets its own isolated state:\n\n```typescript\n// StorageStateV3\n{\n  byNotebook: {\n    \"uuid-abc-123\": {\n      folders: [...],\n      notebookFolderByKey: {...}\n    },\n    \"uuid-def-456\": {\n      folders: [...],\n      notebookFolderByKey: {...}\n    }\n  }\n}\n```\n\nThis means your \"Work\" folder structure doesn't leak into your \"Personal\" research. Clean separation.\n\n**2. Handling MV3 Service Worker Sleep**\n\nChrome MV3 Service Workers sleep after 30 seconds of inactivity. This breaks `chrome.runtime` calls.\n\nInstead of fighting it with \"keep-alive\" hacks, we:\n- Detect context invalidation gracefully\n- Silently retry on the next frame\n- Log to our own `NLE.log()` instead of spamming console.warn\n\n**3. Native Drag Bridge**\n\nMaking the native NotebookLM sidebar items draggable was tricky. We:\n- Hook `dragstart` on native items\n- Create an invisible overlay above our iframe during drag\n- Calculate drop target using `elementFromPoint()` with coordinates\n- Result: Native notes can be dropped into our folders seamlessly\n\n\n**4. One-Click Batch Delete**\n\nDeleting notes in NotebookLM means:\n1. Click 3-dots menu on a note\n2. Click \"Delete\" option  \n3. Confirm in the modal\n4. **Repeat for every single note** :(\n\nWith 20 notes to clean up? That's 60+ clicks\n\n**Our Solution: One-Click Batch Delete**\n\n- **Automatic Modal Handling**: Content script detects NotebookLM's confirmation dialog and clicks \"Delete\" automatically\n- **Index Adjustment**: When notes are deleted, indexes shift - we compensate by tracking deletion count\n- **RxJS State Management**: `BatchSelectionService` with `BehaviorSubjects` for reactive UI updates\n\n**5. Auto-Focus Input in Modals**\n\nSmall UX detail that makes a huge difference:\n- Create folder ‚Üí input already focused, ready to type\n- Rename folder ‚Üí text is pre-selected, just type to replace\n- No extra clicks needed\n\n\n\n---\n\n\n### üì∏ Screenshots\n\n**Before:** \n![Original Notebook1](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/j2vso7uvx6b7t318w28x.png)\n\n![Original Notebook2](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yo5b9f8iaux93gi2tveh.png)\n\n\n\n\n**After:** \n\n![After Enhancer1](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qq47oynqeno46fte6non.png)\n\n![After Enhancer2](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ink8toe9kwpcs9qjwogc.png)\n\n\n**Export your notes :** \n\n\n![Export your notes](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6bleewr5lts3orpbmtwn.png)\n\n\n\n**Drag & Drop:** \n\n![Drang and drop](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/k9s4s89woeq8isnuqfvu.gif)\n\n\n**üóëÔ∏è Batch Delete**  *NEW!*\n\n![Batch delete](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/v3z9qj2eqaftxdh2j93h.gif)\n\n\n\n\n\n\n\n**Full video**\nhttps://www.youtube.com/watch?v=KpqXmjq_oow\n---\n\n## My Experience with GitHub Copilot CLI\n\nThis project was built almost entirely through GitHub Copilot CLI interactions turning natural language into production-ready code.\n\n### How I Used Copilot CLI\n\n**1. Architecture Decisions**\n```bash\n# Asked Copilot: \"Best way to inject UI into an existing Angular Material page?\"\n# Copilot suggested: Shadow DOM + iframe for isolation\n# Result: Zero style conflicts with NotebookLM's Material Design\n```\n\n**2. Content Script Structure**\n```bash\n# Asked: \"How to structure 8 content scripts that share state?\"\n# Copilot proposed: Module pattern with window.__NLE__ namespace\n# Result: Clean separation, no global pollution\n```\n\n**3. Drag & Drop Implementation**\n```bash\n# Asked: \"Bridge native HTML5 drag with Angular CDK drop?\"\n# Copilot designed: Overlay system with coordinate translation\n# Result: Seamless drag from native sidebar to our folders\n```\n\n**4. Debugging Context Invalidation**\n```bash\n# Asked: \"Chrome MV3 extension context invalidated errors?\"\n# Copilot implemented: Graceful detection + silent retry logic\n# Result: No console spam, smooth recovery\n```\n\n**5. i18n System**\n```bash\n# Asked: \"Lightweight i18n without ngx-translate bloat?\"\n# Copilot built: Custom TranslationService with lazy loading\n# Result: ~3KB vs ~50KB, full interpolation support\n```\n\n### Wins\n\n**Speed:** What would have taken weeks took days. Complex features like the drag bridge were implemented in hours, not days.\n\n**Architecture:** Copilot suggested patterns I wouldn't have considered (like the iframe-in-shadow approach) that solved isolation problems elegantly.\n\n**Edge Cases:** MV3 quirks, Material Design menu positioning, SPA navigation detection Copilot handled these gracefully.\n\n**Learning:** Every interaction was a learning moment :) . I now understand Chrome Extension architecture, Angular standalone components, and Tailwind customization.\n\n---\n\n## What's Next?\n\n\n1. **Chrome Web Store Launch** - Polish, package, publish\n2. **More Languages** - French, German, Portuguese (easy with our i18n system)\n3. **Search & Filter** - Find notes within folders instantly\n6. **Keyboard Shortcuts** - Power-user features (Ctrl+Shift+N for new folder)\n\n**ü§ù Open Source**\n\nThis project will be open-sourced. Want to contribute?\n- PRs welcome\n- Good first issues: translations, themes, documentation\n\n---\n\n## Credits & Thanks\n\n- **Google** for NotebookLM an incredible research tool\n- **GitHub Copilot CLI** for turning ideas into code faster than ever <3\n- **Tailwind CSS** for making dark mode trivial\n- **DEV.to** for hosting this challenge and bringing the community together\n\n---\n\n- [GitHub Repository](https://github.com/CGCM070/NotebookLM_Enhancer)\n\n\n*Built with ‚ù§Ô∏è,  and a lot of help from GitHub Copilot CLI.*\n\n\n***devchallenge***\n***githubchallenge***\n***cli***\n***githubcopilot***",
      "stars": null,
      "comments": 6,
      "upvotes": 30,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "Meme Monday",
      "url": "https://dev.to/ben/meme-monday-1i4i",
      "source": "devto",
      "published_at": "2026-02-09T14:21:22+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "watercooler",
        "jokes"
      ],
      "content_length": 184,
      "content_preview": "**Meme Monday!**\n\nToday's cover image comes from [last week's thread](https://dev.to/ben/meme-monday-42aj).\n\nDEV is an inclusive space! Humor in poor taste will be downvoted by mods.\n\n",
      "content_full": "**Meme Monday!**\n\nToday's cover image comes from [last week's thread](https://dev.to/ben/meme-monday-42aj).\n\nDEV is an inclusive space! Humor in poor taste will be downvoted by mods.\n\n",
      "stars": null,
      "comments": 21,
      "upvotes": 36,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "Fighting with json - a war story",
      "url": "https://dev.to/kimkulling/fighting-with-json-a-war-story-41dn",
      "source": "devto",
      "published_at": "2026-02-05T15:01:03+00:00",
      "external_id": null,
      "tags": [
        "cpp",
        "legacycode",
        "programming",
        "compiler"
      ],
      "content_length": 4313,
      "content_preview": "---\ntitle: Fighting with json - a war story\npublished: true\ndescription: When unknow compiler features drives you mad!\ntags: #cpp #legacycode #programming #compiler\ncover_image: https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5ikrand3ubzhe3207gnf.jpg\n# Use a ratio of 100:42 for best results.\n# published_at: 2026-02-05 14:51 +0000\n---\n## Intro \nI was refactoring the build environment for an internal C++ project at work. To keep things simple, we used the following JSON library:\n\nhttps://",
      "content_full": "---\ntitle: Fighting with json - a war story\npublished: true\ndescription: When unknow compiler features drives you mad!\ntags: #cpp #legacycode #programming #compiler\ncover_image: https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5ikrand3ubzhe3207gnf.jpg\n# Use a ratio of 100:42 for best results.\n# published_at: 2026-02-05 14:51 +0000\n---\n## Intro \nI was refactoring the build environment for an internal C++ project at work. To keep things simple, we used the following JSON library:\n\nhttps://github.com/nlohmann/json\n\nThe build system was based on CMake. I planned to replace our wrapper scripts for CMake with a preset.json. If you haven‚Äôt heard of this feature yet: when you want to set specific configurations for customers via CMake defines, you can predefine these in a dedicated preset file. It works surprisingly well. For my experimental render engine, I put all the Windows packages installation via vcpkg into a preset file like this:\n```\n{\n    \"version\": 3,\n    \"configurePresets\": [\n        {\n            \"name\": \"default\",\n            \"binaryDir\": \"${sourceDir}\",\n            \"cacheVariables\": {\n                \"CMAKE_TOOLCHAIN_FILE\": \"contrib/vcpkg/scripts/buildsystems/vcpkg.cmake\"\n            }\n        }\n    ]\n}\n```\nYou now invoke the build with\n\n```\ncmake .\\CMakeLists.txt --preset=default\n```\n\nunder Windows, and the user no longer needs to worry about installing vcpkg, since it‚Äôs shipped under contrib.\n\nI applied the same logic for compiler- and target-specific settings in the company project. And suddenly, the build failed. The error message read:\n```\nerror parsing version, missing ;\n```\n\nThe file mentioned in the errorversion simply contained the current version string and wasn't even used in the C++ build process at all ‚Äî until now. So what happened?\n\n## What happened?\nAfter some fruitless searching through the source code, trying to find out whether the version file had accidentally been included somewhere, I confirmed that this wasn‚Äôt the case. Otherwise, this build error would have appeared much earlier.\n\nNext, I searched the CMake build environment to see if there was a pre-processing step that might be turning the version file into a mis-configured header file. Another dead end.\n\nSo what now? I identified the .cpp file that triggered the error. Unfortunately, the MSVC build output didn‚Äôt show which include caused the build to fail. So I used the following compiler switch:\n\n```\n/showIncludes\n```\n\nWith this switch enabled, all included files for the translation unit are shown. While digging through the generated output ‚Äî which listed hundreds of includes ‚Äî I noticed one header that seemed to be triggering the build error:\n\n```\njson/include/nlohmann/detail/macro_scope.hpp\n```\n\nSo I started studying this header more closely. And bingo:\n\n```\n#ifdef __has_include\n    #if __has_include(<version>)\n        #include <version>\n    #endif\n#endif\n```\nIf the preprocessor supports the **__has_include** feature, it checks whether a file named version exists. If it does, it gets included implicitly.\nWhy didn‚Äôt this happen in the old build setup I asked myself?\n\nThe answer is simple: in the new preset-based build, the build directory was set to the root directory of the project.\nIn the old build, we used a directory structure like:\n\n```\nbuild/<platform>/<config>\n```\n\nSo I changed my CMake output directory to build. But that wasn't enough. It had to be at least build/<platform> ‚Äî in other words, at least two directory levels deep to avoid the problem. That workaround fixed the issue.\n\n## What have I learned\nIf build errors don‚Äôt make sense, enable all compiler options to extract more info from the build process. Without the option: /showIncludes I would never have found the root cause.\nIf even then the result seems illogical: accept that it‚Äôs still happening. If reality doesn‚Äôt match your expectations, change your perspective.\nIf I didn‚Äôt include the file, someone else must have. If it didn‚Äôt happen explicitly, it happened implicitly ‚Äî in this case, via nlohmann.\nJust because you don‚Äôt know a preprocessor directive to include headers doesn‚Äôt mean it doesn‚Äôt exist.\nVisual Studio knows the __has_include feature ‚Äî I didn‚Äôt.\nNever make your build depend on the target directory structure.\nI hate these kinds of bugs.\n\nThanks a lot for reading!",
      "stars": null,
      "comments": 0,
      "upvotes": 7,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "AI Error Doctor ‚Äì A CLI That Explains Errors Like a Human",
      "url": "https://dev.to/shahid_malik786/ai-error-doctor-a-cli-that-explains-errors-like-a-human-4op4",
      "source": "devto",
      "published_at": "2026-02-04T11:48:36+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "githubchallenge",
        "cli",
        "githubcopilot"
      ],
      "content_length": 1958,
      "content_preview": "What I Built\n\nI built AI Error Doctor, a simple but powerful Python CLI tool that takes a raw error message and instantly explains:\n\nüîç What the error means\n\nüîß Why it happened\n\n‚úÖ How to fix it\n\nAs a student and early-stage developer, I often found error messages confusing, especially for beginners. This tool is designed to act like a friendly debugging assistant right inside the terminal.\n\nExample usage:\n\npy error_doctor.py \"ModuleNotFoundError: No module named 'requests'\"\n\n\nExample output:\n\nüìã Er",
      "content_full": "What I Built\n\nI built AI Error Doctor, a simple but powerful Python CLI tool that takes a raw error message and instantly explains:\n\nüîç What the error means\n\nüîß Why it happened\n\n‚úÖ How to fix it\n\nAs a student and early-stage developer, I often found error messages confusing, especially for beginners. This tool is designed to act like a friendly debugging assistant right inside the terminal.\n\nExample usage:\n\npy error_doctor.py \"ModuleNotFoundError: No module named 'requests'\"\n\n\nExample output:\n\nüìã Error Type: ModuleNotFoundError\nüí° Explanation: Python cannot find the requested module.\nüîß Suggested Fix: pip install requests\n\n\nThe tool currently supports common Python, npm, and Git errors, and it‚Äôs easy to extend with more patterns.\n\nDemo\nüì∏ Screenshots\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/m45ffe6wei7e123q9ajc.png)\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/knw79ozinbva4nlckfkm.png)\n\nüíª Code\n\nhttps://github.com/SHAHID-glitch/ai-error-doctor\n\nVideo Link: https://player.mux.com/NY00L3RQlb6ZrZiPOT5pfl18WjfhnduhSMPqC7HsaaYw\n\nGitHub Copilot CLI was central to this project.\n\nI used it to:\n\nüß† Design the CLI structure\n\n‚úçÔ∏è Generate error-handling logic\n\nüîé Suggest common error patterns and fixes\n\nüöÄ Rapidly iterate inside the terminal without switching contexts\n\nCopilot CLI felt like pair programming in the terminal. Instead of searching error explanations online, I could build a tool that explains errors for others ‚Äî using Copilot itself.\n\nThis project genuinely changed how I see debugging: from something frustrating into something automated, educational, and developer-friendly.\n\nFinal Thoughts\n\nAI Error Doctor may be simple, but it solves a real problem faced by beginners every day.\nThis challenge helped me go from asking Copilot for help to building a tool that helps others.\n\nThank you, GitHub and DEV, for this opportunity üôå\n\n#githubcopilot #cli #python #ai #devtools",
      "stars": null,
      "comments": 0,
      "upvotes": 7,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "Welcome Thread - v363",
      "url": "https://dev.to/devteam/welcome-thread-v363-13be",
      "source": "devto",
      "published_at": "2026-02-04T13:00:00+00:00",
      "external_id": null,
      "tags": [
        "welcome"
      ],
      "content_length": 609,
      "content_preview": "1. Leave a comment below to introduce yourself! You can share advice, tell us what brought you here, what you're learning, or just a fun fact about yourself!\n\n2. Reply to someone's comment, either with a question/advice or just a hello. üëã\n\n![Simpsons character Bart and Homer sitting on the sofa. Homer pats the cushion in a \"sit down\" gesture. ](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMHM5cWF2eGU2NDA5c2x3cmhhdWF3NGo1dWdnMWFtdXc4OHcwdmtveiZlcD12MV9naWZzX3NlYXJjaCZjdD1n/VtDRXohjexcyCDlL6Z/",
      "content_full": "1. Leave a comment below to introduce yourself! You can share advice, tell us what brought you here, what you're learning, or just a fun fact about yourself!\n\n2. Reply to someone's comment, either with a question/advice or just a hello. üëã\n\n![Simpsons character Bart and Homer sitting on the sofa. Homer pats the cushion in a \"sit down\" gesture. ](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMHM5cWF2eGU2NDA5c2x3cmhhdWF3NGo1dWdnMWFtdXc4OHcwdmtveiZlcD12MV9naWZzX3NlYXJjaCZjdD1n/VtDRXohjexcyCDlL6Z/giphy.gif)\n\nThe most thoughtful comments will be awarded our [warm welcome badge.](https://dev.to/badges) ‚ù§Ô∏è\n",
      "stars": null,
      "comments": 197,
      "upvotes": 43,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "How Seriously Should We Take State of JS and Other Developer Surveys?",
      "url": "https://dev.to/sylwia-lask/how-seriously-should-we-take-state-of-js-and-other-developer-surveys-9ce",
      "source": "devto",
      "published_at": "2026-02-10T10:43:07+00:00",
      "external_id": null,
      "tags": [
        "javascript",
        "node",
        "frontend",
        "webdev"
      ],
      "content_length": 5029,
      "content_preview": "There was a time when I treated State of JS results almost like prophecy.\n\nA new edition dropped and I‚Äôd read it with excitement, looking for *the future of JavaScript* hidden somewhere in those charts.\n\nToday I still read it.\nBut with much less awe and much more distance.\n\nIf you haven‚Äôt seen the 2025 results yet, you can explore them here:\n[https://2025.stateofjs.com/en-US/](https://2025.stateofjs.com/en-US/)\n\nIt‚Äôs honestly a fantastic piece of work. Beautifully built, interactive, full of dat",
      "content_full": "There was a time when I treated State of JS results almost like prophecy.\n\nA new edition dropped and I‚Äôd read it with excitement, looking for *the future of JavaScript* hidden somewhere in those charts.\n\nToday I still read it.\nBut with much less awe and much more distance.\n\nIf you haven‚Äôt seen the 2025 results yet, you can explore them here:\n[https://2025.stateofjs.com/en-US/](https://2025.stateofjs.com/en-US/)\n\nIt‚Äôs honestly a fantastic piece of work. Beautifully built, interactive, full of data. Huge respect to the creators ‚Äî it‚Äôs not a small project.\n\nBut the real question is:\n\n**How seriously should we treat it?**\n\n---\n\n## A Very Short Recap of 2025 üìä\n\nI won‚Äôt do a full summary. Plenty of people already did, and you can just look at the charts.\n\nIn short:\n\n* TypeScript is firmly mainstream\n* Big frameworks mostly hold their positions\n* Tooling keeps evolving\n\nSurveys like this are great for sensing the ecosystem.\nWhat‚Äôs popular. What‚Äôs growing. What annoys developers.\n\nSometimes tools highly praised in surveys reach real production adoption years later. Vite is a good example.\n\nSo yes ‚Äî these surveys are useful.\n\nBut they come with a lot of nuance.\n\n---\n\n## My Younger-Developer Phase üòÑ\n\nI remember being a junior and wanting to immediately try (or even introduce to production) everything that ranked high in satisfaction.\n\nLuckily, nobody asked for my opinion back then üòâ\n\nI also used to feel genuinely sad seeing Angular score poorly in satisfaction.\nBecause‚Ä¶ I liked working with it. My teams liked it. Our projects worked.\n\nWas Angular dying?\n\nSpoiler: many years later, it‚Äôs still doing just fine.\n\nAnd it made me wonder:\n\nIf some tools are so loved in surveys, why don‚Äôt they dominate job listings?\nWhy is there constant demand for React and Angular devs, but much less for Vue or Svelte (at least where I live)?\n\n---\n\n## A Little Story About Surveys üß†\n\nFun fact: when I was very young, I had a part-time job in a company that conducted surveys.\n\nThat‚Äôs where I learned how serious research is actually done.\n\nA reliable study needs:\ncarefully selected demographics, well-phrased neutral questions, controlled variables, clear definitions, sometimes even different question orders to avoid bias.\n\nAs you can guess ‚Äî State of JS (and most dev surveys) don‚Äôt meet those strict scientific standards.\n\nAnd that‚Äôs okay. They‚Äôre not academic research.\n\nBut it‚Äôs worth remembering.\n\n---\n\n## Who Actually Answers? üî•\n\nMost likely:\npeople who care about tech, follow trends, hang out on DEV, Reddit, Twitter, and enjoy exploring tools.\n\nAlso people willing to spend time filling out a long survey.\n\nWho probably doesn‚Äôt?\nDevelopers who treat programming as a normal job and close their laptop at 5 PM.\n\nThat group might be huge. Possibly the majority.\n\nSo we‚Äôre mostly hearing from the curious and passionate slice of the community.\n\nWhich is interesting ‚Äî but not the whole picture.\n\n---\n\n## Regional Reality üåç\n\nParticipation is open, which is great, but it also means representation isn‚Äôt proportional.\n\nThe US is strongly represented.\nIndia has one of the biggest developer populations in the world, yet doesn‚Äôt always show up proportionally.\n\nAnd regions differ.\n\nVue is strong in parts of Asia.\nIn Poland and much of Europe, React and Angular still dominate job markets.\n\nSo survey trends don‚Äôt always match local reality.\n\n---\n\n## What Does ‚ÄúUsage‚Äù Even Mean? ü§®\n\nThis one always makes me think.\n\nIf I heard about a tool once by an accident in an office kitchen, do I have awareness?\nIf I did a tutorial, does that count as usage?\nOr should usage mean delivering a real project?\n\nWhen someone says they like a tool, is it after comparing many options, or just because it feels nice? Or maybe it's the only tool he/she knows? ü§î\n\nAnd even in anonymous surveys, some people may hesitate to admit they don‚Äôt know something.\n\nHuman psychology never disappears from data.\n\n---\n\n## Surveys vs Real Careers üíº\n\nCan surveys help choose what to learn?\nSometimes.\n\nCan they guide tech decisions?\nMaybe.\n\nBut surveys and job markets are two different worlds.\n\nYou can learn the ‚Äúbest‚Äù framework and still land in a 10-year-old legacy codebase.\nYou can study modern React and get class components on day one üòâ\n\nReal careers are messy and non-linear.\n\n---\n\n## How to Read Surveys üß≠\n\nWhat works for me:\n\nLook at trends year to year.\nCompare satisfaction and usage.\nWatch movement, not winners.\nRemember that popular tools get more criticism simply because people use them daily.\n\nAnd most importantly:\ntreat surveys as insight, not truth.\n\nYesterday‚Äôs winner can be tomorrow‚Äôs loser.\n\n---\n\n## So‚Ä¶ Should We Care? üôÇ\n\nYes. But calmly.\n\nState of JS doesn‚Äôt tell you what to use.\nIt shows what developers are curious about right now.\n\nThink of it as a compass, not a GPS.\nDirection, not destination.\n\nIt reflects the mood of the community more than the future of the industry.\n\nStill useful. Just not absolute.\n\n---\n\nWhat do you think?\nDo surveys influence your decisions, or do you mostly treat them as interesting reading?\n",
      "stars": null,
      "comments": 3,
      "upvotes": 9,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Beyond RAG: Building an AI Companion with \"Deep Memory\" using Knowledge Graphs",
      "url": "https://dev.to/juandastic/beyond-rag-building-an-ai-companion-with-deep-memory-using-knowledge-graphs-2e6e",
      "source": "devto",
      "published_at": "2026-02-09T00:07:19+00:00",
      "external_id": null,
      "tags": [
        "ai",
        "llm",
        "rag"
      ],
      "content_length": 12132,
      "content_preview": "I build AI tools to solve my own problems. A while back, [I built NutriAgent to track my calories](https://dev.to/juandastic/i-ditched-myfitnesspal-and-built-an-ai-agent-to-track-my-food-3eia) because I wanted to **own my raw data**. But recently, the problem wasn't mine, it was my wife's.\n\nShe uses LLMs differently than I do. While I use them for code or quick facts, she uses them as a therapist, a life coach, and a sounding board. Over the last year, she built a massive \"Master Prompt\" in Noti",
      "content_full": "I build AI tools to solve my own problems. A while back, [I built NutriAgent to track my calories](https://dev.to/juandastic/i-ditched-myfitnesspal-and-built-an-ai-agent-to-track-my-food-3eia) because I wanted to **own my raw data**. But recently, the problem wasn't mine, it was my wife's.\n\nShe uses LLMs differently than I do. While I use them for code or quick facts, she uses them as a therapist, a life coach, and a sounding board. Over the last year, she built a massive \"Master Prompt\" in Notion. It contained her medical history, key life events, emotional triggers, and ongoing projects.\n\nIt was **35,000 tokens** long.\n\nEvery time she started a new chat, she had to manually copy-paste this wall of text just to get the AI up to speed. If she didn't, the advice was generic and useless.\n\nShe didn't need a search engine or a simple chat history. She needed a **continuous brain**.\n\nI realized that the standard way we build AI memory with RAG (Retrieval Augmented Generation) wouldn't be enough. So I built **Synapse AI Chat**. It's an AI architecture that uses a Knowledge Graph to give an LLM \"Deep Memory.\"\n\nHere is how I built it, why I chose Knowledge Graphs over Vectors (To be fair, I used both), and how I handled the engineering messiness of making it work.\n\n## Why Standard RAG Wasn't Enough\n\nMost AI memory systems today use Vector RAG. You chunk text, turn it into numbers (vectors), and find \"similar\" chunks later.\n\nThis works great for finding a specific policy in a PDF, but not that great for modeling human relationships and history.\n\nVectors find **similarity**, not **structure**.\nIf my wife tells the AI, \"I'm feeling overwhelmed today\" a Vector search might pull up a journal entry from three months ago where she mentioned \"overwhelm.\"\n\nBut a **Knowledge Graph** understands the *story*. It knows:\n`\"Project A\" -> CAUSED -> \"Stress\" -> RESULTED_IN -> \"Overwhelm\"`\n\nI needed the AI to understand *causality*, not just keywords.\n\n### The Architecture Decision: Full Context Injection\nBecause I was using Google's Gemini models (which have a massive context window), I didn't need to retrieve just 5 small chunks of text. I could inject the **entire** compiled profile into the prompt.\n\nMy goal was to turn the raw chat logs into a structured graph, then flatten it back into a comprehensive \"User Manual\" for the AI to read before every interaction.\n\n[Graphiti](https://www.getzep.com/product/open-source/), the framework I used for the graph indexing, supports semantic search for a retrieval strategy. I decided to take advantage of the Gemini's big context windows. The compiled graph output ended up being smaller than the source, from almost 35k tokens to ~14k, just combining the entities with their descriptions and their relations in plain text, avoiding extra tokens to build a narrative prompt like her old master's prompt\n\n## Introducing Synapse: The Architecture\n\nI split the project into two parts: the **Body** (the UI you talk to) and the **Brain** (the API that processes memory).\n\n*   **The Frontend (Body):** React 19 + **Convex**. I chose Convex because it handles real-time database syncing effortlessly, which makes the chat feel snappy.\n*   **The Cortex (Brain):** Python + FastAPI. This does the heavy data processing.\n*   **The Memory Engine:** **Graphiti** + **Neo4j**.\n*   **The Models:**\n    *   **Gemini 3 Flash:** For the \"heavy lifting\" (building the graph).\n    *   **Gemini 2.5 Flash:** For the actual chat (speed and cost).\n\nHere is the high-level view:\n\n![high-level view](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dzkxbivdlj2ycdiz1683.png)\n\n\n## How It Works: The \"Deep Memory\" Pipeline\n\nThe system operates in three distinct phases.\n\n### Phase A: Conversation (The Chat)\nWhen my wife chats with Synapse, she is talking to **Gemini 2.5 Flash**. It‚Äôs fast and fluid.\n\nThe trick is that the System Prompt isn't static. Before she sends her first message, I **hydrate** the prompt with a text summary of her entire Knowledge Graph. The AI immediately knows who she is, what she's worried about, and who her friends are.\n\n### Phase B: Ingestion (The \"Sleep\" Cycle)\nThis is where the magic happens. When she finishes a conversation by stopping chatting for 3 hours or manually clicking a Consolidate button, I treat this like the AI taking a nap to consolidate memories.\n\nWe send the chat transcript to the Python Cortex. Here, I switch to **Gemini 3 Flash**.\n\nWhy the upgrade? Extracting entities from a messy human conversation is hard.\nIf she says, \"I stopped taking medication X and started Y,\" a weaker model might just add \"Taking Y\" to the graph. **Gemini 3** is smart enough to create a generic logic:\n1.  Find node \"Medication X\".\n2.  Mark the relationship as `STOPPED`.\n3.  Create node \"Medication Y\".\n4.  Create relationship `STARTED`.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rudhrks1ss6tb87h6bn4.png)\n\n### Phase C: Hydration (The Awakening)\nWhen she returns, the next session is already prepared with the new compiled graph summary. It doesn't just dump a prompt. It compiles the nodes and edges into a natural language narrative.\n\n```python\n    def _format_compilation(definitions: list[str], relationships: list[str]) -> str:\n        sections = []\n\n        if definitions:\n            sections.append(\n                \"#### 1. CONCEPTUAL DEFINITIONS & IDENTITY ####\\n\"\n                \"# (Understanding what these concepts mean specifically for this user)\\n\"\n                + \"\\n\".join(definitions)\n            )\n\n        if relationships:\n            sections.append(\n                \"#### 2. RELATIONAL DYNAMICS & CAUSALITY ####\\n\"\n                \"# (How these concepts interact and evolve over time)\\n\"\n                + \"\\n\".join(relationships)\n            )\n\n        if not sections:\n            return \"\"\n\n        content = \"\\n\\n\".join(sections)\n```\n\n\n## The \"Killer Feature\": Memory Explorer\n\nAI memory is usually a \"Black Box.\" Users don't trust what they can't see.\n\nI wanted my wife to be able to audit her own brain. I built a visualizer using `react-force-graph`. She can see bubbles representing her life: \"Work,\" \"Health,\" \"Family.\"\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qtrtl3gie08bglhdfs8p.png)\n\nIf she sees a connection that is wrong (e.g., the AI thinks she likes a food she actually hates), she can edit the input and re-process the graph with new information like *\"I actually hate mushrooms now.\"*\n\nThe system then processes that new input and updates the graph, creating new nodes and relations or invalidating the existing ones. This \"Human-in-the-loop\" approach builds massive trust.\n\n## Engineering Challenges\n\nBuilding this wasn't just about prompt engineering. There were real system challenges.\n\n### 1. Handling Latency (The Job Queue)\nGraph ingestion is slow. It takes anywhere from **60 to 200 seconds** for Graphiti and Gemini to process a long conversation and update Neo4j.\n\nI couldn't have the UI hang for 3 minutes.\n\nI used **Convex** as a Job Queue. When the session ends, the UI returns immediately. Convex processes the job in the background, updating the UI state to \"Processing...\" and then \"Memory Updated\" when it's done.\n\n### 2. Handling Flakiness (The Retry Logic)\nThe Gemini API is powerful, but occasionally it throws **503 Service Unavailable** errors, especially during heavy graph processing tasks.\n\nI implemented an \"Event-Driven Retry\" system. If the graph build fails, I don't just crash. I schedule a retry with exponential backoff.\n\n```Typescript\nexport const RETRY_DELAYS_MS = [\n  0,            // Attempt 1: Immediate\n  2 * 60_000,   // Attempt 2: +2 minutes (let the API cool down)\n  10 * 60_000,  // Attempt 3: +10 minutes\n  30 * 60_000,  // Attempt 4: +30 minutes\n];\n\nexport const processJob = internalAction({\n  args: { jobId: v.id(\"cortex_jobs\") },\n  handler: async (ctx, args) => {\n    const job = await ctx.runQuery(internal.cortexJobs.get, { id: args.jobId });\n\n    try {\n      // 1. Do the heavy lifting (Call Gemini 3 Flash)\n      // This is where 503 errors usually happen\n      await ingestGraphData(ctx, job.payload);\n\n      // 2. Mark complete if successful\n      await ctx.runMutation(internal.cortexJobs.complete, { jobId: args.jobId });\n\n    } catch (error) {\n      const nextAttempt = job.attempts + 1;\n\n      if (nextAttempt >= job.maxAttempts) {\n        // Stop the loop if we've tried too many times\n        await ctx.runMutation(internal.cortexJobs.fail, { \n            jobId: args.jobId, error: String(error) \n        });\n      } else {\n        // 3. Schedule the retry using Convex's scheduler\n        const delay = RETRY_DELAYS_MS[nextAttempt] ?? 30 * 60_000;\n\n        await ctx.scheduler.runAfter(delay, internal.processor.processJob, {\n          jobId: args.jobId\n        });\n      }\n    }\n  },\n});\n```\n\n\n### 3. Snappy UX\nConvex's real-time sync was a lifesaver here. I didn't have to write complex WebSocket code. If the Python backend updates the status of a memory job in the database, the React UI updates instantly.\n\nThe tokens streaming is better with convex in the middle, since the backend is connected with convex. If the user's browser is closed or the connection fails, the token generation will continue, passing the answer to Convex and streaming it to the user when it is possible.\n\nThe catch here is that this could increase the Functions usage since each update will count, so the streaming updates are throttled to 100ms intervals to balance responsiveness with database write efficiency\n\n## The Result\n\nThe difference is night and day.\n\n**Before:** My wife dreaded starting a new thread because of the \"context set up\" tax. She felt like she was constantly repeating herself, and having the responsibility to constanly doing break points to update the Master Prompt with the new data and start a new thread\n\n**Now:** She just talks. The system has a \"Deep Memory\" of about **10,000 tokens** (compressed from months of chats) that is injected automatically.\n\nShe has different threads for different topics, but they all share the same **Cortex**. If she mentions a health issue in the \"Work\" thread (e.g., \"My back hurts from sitting\"), the \"Health\" thread knows about it the next time she logs in.\n\n## Conclusion\n\nThis project taught me that we are moving from \"Horizontal\" AI platforms (like ChatGPT, which knows a little about everything) to \"Vertical\" AI stacks that know **everything about you**. I‚Äôve been watching how the ChatGPT and Gemini apps are starting to create user profiles and thread summaries to build this kind of memory. They are chasing the same goal: a truly personalized experience.\n\nThe key takeaway for me is that **Vectors are great for search, but Knowledge Graphs are essential for understanding.**\n\nI keep enjoying building solutions for real problems. Nowadays, we have powerful tools to build awesome software faster than ever, but I found that having a **product vision** and the **technical understanding** to architect a solution is still critical. That is the difference between building a quick prototype and solving a real problem.\n\nThis project is being used for real by my wife and me, and honestly, this is my favorite part of building products. The fun doesn't end when the architecture is done; it begins when people actually use it. Watching the product evolve, finding bugs, pivoting features, or even realizing that an initial idea didn't make sense at all, that is the journey. Building software is fun, but seeing it come alive and solve actual problems is magical.\n\nThe project is live at [synapse-chat.juandago.dev](https://synapse-chat.juandago.dev) if you want to see it in action.\n\nThe code is open source if you want to dig into the implementation:\n*   **Frontend (Body):** [synapse-chat-ai](https://github.com/juandastic/synapse-chat-ai)\n*   **Backend (Cortex):** [synapse-cortex](https://github.com/juandastic/synapse-cortex)\n\nI'd love to hear your impressions and thoughts. Let's continue the conversation on [X](https://x.com/juandastic) or connect on [LinkedIn](https://www.linkedin.com/in/juandastic/).",
      "stars": null,
      "comments": 6,
      "upvotes": 10,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "Building DevPulse: A Project Health Checker in 80 Minutes with GitHub Copilot CLI",
      "url": "https://dev.to/srijan-xi/building-devpulse-a-project-health-checker-in-80-minutes-with-github-copilot-cli-2gh6",
      "source": "devto",
      "published_at": "2026-02-07T13:16:20+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "githubchallenge",
        "cli",
        "githubcopilot"
      ],
      "content_length": 12357,
      "content_preview": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n## TL;DR\n\nI built **DevPulse**, a local-first development project health checker with both CLI and GUI interfaces in just 80 minutes using GitHub Copilot CLI. It scans codebases to detect tech stacks, check repository hygiene, find security issues, and auto-fix common problems‚Äîall with **zero external dependencies**.\n\nüîó **[View on GitHub](https://github.com/Srijan-XI/DevPulse)**\n\n---\n\n## T",
      "content_full": "*This is a submission for the [GitHub Copilot CLI Challenge](https://dev.to/challenges/github-2026-01-21)*\n\n## TL;DR\n\nI built **DevPulse**, a local-first development project health checker with both CLI and GUI interfaces in just 80 minutes using GitHub Copilot CLI. It scans codebases to detect tech stacks, check repository hygiene, find security issues, and auto-fix common problems‚Äîall with **zero external dependencies**.\n\nüîó **[View on GitHub](https://github.com/Srijan-XI/DevPulse)**\n\n---\n\n## The Challenge\n\nWhen I saw the GitHub Copilot CLI Challenge, I wanted to build something that:\n- **Actually solves a real problem** (not just a toy project)\n- **Demonstrates Copilot's power** across architecture, coding, and documentation\n- **Can be built fast** (to show how Copilot accelerates development)\n- **Has professional quality** (production-ready, not a hackathon demo)\n\nSo I asked myself: *\"What do I wish existed when starting or evaluating a project?\"*\n\nThe answer: **A health checker that tells me everything about a codebase in seconds.**\n\n---\n\n## What is DevPulse?\n\nDevPulse is a zero-configuration tool that scans any project and instantly tells you:\n\n‚úÖ **Tech Stack Detection**: What technologies are used (Node.js, Python, Docker, etc.)  \n‚úÖ **Repository Hygiene**: Missing README, LICENSE, .gitignore, or tests  \n‚úÖ **Security Scanning**: Potential secrets, unignored .env files  \n‚úÖ **File Size Analysis**: Large files that might need attention  \n‚úÖ **Auto-Fix Capabilities**: Generates missing README, LICENSE, .gitignore  \n\n### Example Output\n\n```\n============================================================\nDevPulse Report ‚Äî /my/awesome-project\n============================================================\n\nTech Stack & Info:\n  ‚úì Node.js\n  ‚úì Docker\n  ‚úì GitHub Actions\n  ‚Ñπ Total size: 45.67 MB (234 files)\n\nWarnings:\n  ‚ö† Missing LICENSE file\n  ‚ö† Found 15 TODO/FIXME comments\n  ‚ö† Large Files: 2 file(s) larger than 10.00 MB\n\nCritical Issues:\n  üö® .env file exists but may not be in .gitignore\n\nSummary:\n  Total checks: 8\n  Critical: 1\n  Warnings: 3\n\nüí° 1 issue(s) can be auto-fixed with: devpulse fix --safe\n============================================================\n```\n\n---\n\n## How GitHub Copilot CLI Accelerated Development\n\n### 1. **Instant Architecture Design** (5 minutes)\n\n**My Prompt:**\n```\nDesign a modular Python CLI tool that scans projects for tech stack, \nhygiene issues, and security problems. It should be extensible with \nindependent check modules.\n```\n\n**Copilot's Magic:**\n- Suggested a clean separation: `core/`, `checks/`, `utils/`\n- Recommended using `argparse` for CLI parsing\n- Proposed standardized result format across all checks\n- Outlined the Scanner ‚Üí Reporter ‚Üí Fixer flow\n\n```\ncli/\n‚îú‚îÄ‚îÄ devpulse.py          # Entry point\n‚îú‚îÄ‚îÄ core/\n‚îÇ   ‚îú‚îÄ‚îÄ scanner.py       # Orchestrates checks\n‚îÇ   ‚îú‚îÄ‚îÄ reporter.py      # Formats output\n‚îÇ   ‚îî‚îÄ‚îÄ fixer.py         # Applies fixes\n‚îú‚îÄ‚îÄ checks/              # Independent check modules\n‚îÇ   ‚îú‚îÄ‚îÄ stack.py\n‚îÇ   ‚îú‚îÄ‚îÄ hygiene.py\n‚îÇ   ‚îú‚îÄ‚îÄ security.py\n‚îÇ   ‚îî‚îÄ‚îÄ size.py\n‚îî‚îÄ‚îÄ utils/               # Shared utilities\n```\n\n**Time Saved:** Instead of spending 30+ minutes designing the architecture, I had a production-ready structure in minutes.\n\n---\n\n### 2. **Rapid Code Generation** (40 minutes)\n\n#### Example 1: Filesystem Utilities\n\n**My Prompt:**\n```\nCreate a function to walk a project directory recursively, \nignoring common build/dependency folders like node_modules, \nvenv, .git, and return a list of relevant files\n```\n\n**Copilot Generated:**\n```python\ndef walk_project(root_path: str) -> List[Path]:\n    \"\"\"Walk project directory and collect all relevant files.\"\"\"\n    root = Path(root_path)\n    ignore_dirs = {'.git', 'node_modules', '__pycache__', \n                   'venv', '.venv', 'dist', 'build', 'target'}\n    files = []\n    \n    for item in root.rglob('*'):\n        if item.is_file():\n            if not any(part in ignore_dirs for part in item.parts):\n                files.append(item)\n    \n    return files\n```\n\n**What I Love:** Copilot didn't just generate the function‚Äîit included the exact ignore patterns I would have added manually.\n\n#### Example 2: Secret Detection Patterns\n\n**My Prompt:**\n```\nGenerate regex patterns to detect API keys, passwords, and \ntokens in source files with minimal false positives\n```\n\n**Copilot Generated:**\n```python\nSECRET_PATTERNS = [\n    (r'API_KEY\\s*=\\s*[\"\\']?[\\w-]{20,}[\"\\']?', 'API Key'),\n    (r'PASSWORD\\s*=\\s*[\"\\'][^\"\\']{8,}[\"\\']', 'Password'),\n    (r'SECRET\\s*=\\s*[\"\\'][\\w-]{16,}[\"\\']', 'Secret'),\n    (r'GITHUB_TOKEN\\s*=\\s*[\"\\']?ghp_[\\w]{36}[\"\\']?', 'GitHub Token'),\n    (r'AWS_ACCESS_KEY_ID\\s*=\\s*[\"\\']?AKIA[\\w]{16}[\"\\']?', 'AWS Key'),\n]\n```\n\n**Time Saved:** This would have taken 20+ minutes of research and testing. Copilot nailed it in seconds.\n\n#### Example 3: Tech Stack Detection\n\n**My Prompt:**\n```\nDetect tech stack from file patterns - support Node.js, Python, \nJava, Docker, Go, Rust, and common build tools\n```\n\n**Copilot Generated:**\n```python\nSTACK_PATTERNS = {\n    'Node.js': ['package.json', 'package-lock.json', 'yarn.lock'],\n    'Python': ['requirements.txt', 'setup.py', 'pyproject.toml'],\n    'Java': ['pom.xml', 'build.gradle', 'build.gradle.kts'],\n    'Docker': ['Dockerfile', 'docker-compose.yml'],\n    'Go': ['go.mod', 'go.sum'],\n    'Rust': ['Cargo.toml', 'Cargo.lock'],\n    'Ruby': ['Gemfile', 'Gemfile.lock'],\n    '.NET': ['*.csproj', '*.sln'],\n    'PHP': ['composer.json', 'composer.lock'],\n    'GitHub Actions': ['.github/workflows/*.yml'],\n}\n```\n\n**Time Saved:** 15+ minutes of manually creating patterns.\n\n---\n\n### 3. **Smart Error Handling** (10 minutes)\n\n**My Prompt:**\n```\nAdd error handling for reading files that might be binary or \nhave encoding issues\n```\n\n**Copilot's Suggestion:**\n```python\ntry:\n    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n        content = f.read()\nexcept (OSError, UnicodeDecodeError):\n    pass  # Skip files that can't be read\n```\n\n**What Stood Out:** Copilot suggested `errors='ignore'` which I wouldn't have thought of immediately. This prevents crashes when scanning binary files.\n\n---\n\n### 4. **GUI Development** (20 minutes)\n\nI wanted a bonus feature: a **graphical interface** for non-technical users.\n\n**My Prompt:**\n```\nCreate a tkinter GUI with project path selection, scan/fix buttons, \ntabbed results view, and summary cards showing critical issues, \nwarnings, and file statistics\n```\n\n**Copilot Generated:**\n- Complete tkinter layout with proper frame organization\n- Summary cards showing metrics (critical count, warnings, file size)\n- Tabbed interface (Summary, Details, Raw Results)\n- Export to JSON and copy-to-clipboard features\n- Progress bar for scan operations\n\n**Result:** A fully functional GUI in 20 minutes that would normally take 1-2 hours.\n\n```python\nclass DevPulseGUI:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"DevPulse - Project Health Checker\")\n        # Copilot generated the entire UI structure\n        self.create_widgets()\n```\n\n---\n\n### 5. **Comprehensive Documentation** (5 minutes)\n\n**My Approach:**\nInstead of writing docs from scratch, I asked Copilot:\n\n```\nGenerate a comprehensive README with quick start, features, \nexamples, architecture diagram, and usage scenarios\n```\n\n**Copilot Delivered:**\n- Professional README with badges and sections\n- Usage examples for CLI and GUI\n- ASCII architecture diagrams\n- Quick start guide\n- Contributing guidelines\n\n**Documentation Generated:**\n- `README.md` (212 lines)\n- `QUICKSTART.md`\n- `USAGE.md`\n- `DEVELOPMENT.md`\n- `ARCHITECTURE.md`\n- `GUI-GUIDE.md`\n\n---\n\n## Real-World Use Cases\n\n### Use Case 1: Onboarding to New Projects\n```bash\npython devpulse.py scan --path /new/project\n```\nInstantly see tech stack, missing docs, and potential issues.\n\n### Use Case 2: Pre-Commit Checks\n```bash\npython devpulse.py scan --json > health-report.json\n```\nIntegrate into CI/CD to ensure repo quality.\n\n### Use Case 3: Team Code Reviews\n```bash\npython devpulse.py scan\n```\nQuick health check before PR reviews.\n\n### Use Case 4: Project Setup\n```bash\npython devpulse.py fix --safe\n```\nAuto-generate README, LICENSE, and .gitignore for new projects.\n\n---\n\n## Key Design Decisions (Copilot-Assisted)\n\n### 1. **Local-First Architecture**\n**Why:** Privacy and speed‚Äîno network calls needed  \n**Copilot's Input:** Suggested using only Python stdlib to avoid dependency management\n\n### 2. **Zero External Dependencies**\n**Why:** Easy installation, no version conflicts  \n**Copilot's Input:** Showed me how to use `pathlib`, `re`, `json`, `argparse` effectively\n\n### 3. **Read-Only by Default**\n**Why:** Safety‚Äîscanning never modifies files without explicit permission  \n**Copilot's Input:** Designed the `--safe` flag pattern for opt-in fixes\n\n### 4. **Composable Check Modules**\n**Why:** Easy to extend with new checks  \n**Copilot's Input:** Recommended independent check functions returning standardized results\n\n---\n\n## The Numbers\n\n| Metric | Value | Copilot Impact |\n|--------|-------|----------------|\n| **Total Dev Time** | 80 minutes | ‚ö° 3-4x faster than manual |\n| **Lines of Code** | ~1,200 | ü§ñ ~70% Copilot-generated |\n| **Features Built** | 12+ | üöÄ Would take 4-6 hours normally |\n| **Documentation Pages** | 6 | üìù ~90% Copilot-assisted |\n| **Test Projects Scanned** | 50+ | ‚úÖ Zero false positives |\n\n---\n\n## Challenges and How Copilot Helped\n\n### Challenge 1: Avoiding False Positives in Secret Detection\n**Problem:** Initial regex patterns flagged too many non-secrets  \n**Solution:** Asked Copilot to refine patterns with length constraints and context  \n**Result:** Balanced detection with minimal false alarms\n\n### Challenge 2: GUI Layout Complexity\n**Problem:** Never used tkinter extensively before  \n**Solution:** Described the layout in plain English; Copilot generated proper widget hierarchy  \n**Result:** Professional-looking GUI on first try\n\n### Challenge 3: Cross-Platform File Handling\n**Problem:** Windows/Linux path differences  \n**Solution:** Copilot suggested using `pathlib.Path` exclusively  \n**Result:** Works seamlessly on all platforms\n\n---\n\n## What I Learned\n\n1. **Copilot Excels at Boilerplate:** Pattern matching, CLI parsing, file operations‚Äîall generated instantly\n2. **Natural Language is Powerful:** Describing *what* you want beats writing *how* to do it\n3. **Iterative Refinement Works:** Start with Copilot's suggestion, refine with follow-up prompts\n4. **Documentation Becomes Easy:** Copilot writes better docs than I would manually\n5. **Rapid Prototyping ‚Üí Production:** With proper prompts, Copilot generates production-quality code\n\n---\n\n## Try It Yourself\n\n### Installation\n```bash\ngit clone https://github.com/Srijan-XI/DevPulse.git\ncd devpulse\n```\n\nNo dependencies to install! Uses only Python standard library.\n\n### Quick Start\n```bash\n# CLI version\npython devpulse.py scan\n\n# GUI version\npython devpulse_gui.py\n```\n\n### Fix Issues\n```bash\npython devpulse.py fix --safe\n```\n\n---\n\n## What's Next?\n\nPotential features (perfect for Copilot-assisted development):\n\n- üîå Plugin system for custom checks\n- üìä HTML report generation\n- üê≥ Dockerfile health scoring\n- üîÑ Git history analysis\n- üìà Trend tracking over time\n- üåê VS Code extension\n\n---\n\n## Conclusion\n\nGitHub Copilot CLI isn't just an autocomplete tool‚Äîit's a **development accelerator** that transforms how we build software. \n\nIn 80 minutes, I built a **production-ready tool** with:\n- ‚úÖ Dual interface (CLI + GUI)\n- ‚úÖ 12+ features\n- ‚úÖ Comprehensive documentation\n- ‚úÖ Zero external dependencies\n- ‚úÖ Cross-platform support\n\n**Without Copilot:** This would have taken 6-8 hours  \n**With Copilot:** 80 minutes (4-5x speedup)\n\nThe key insight? **Copilot excels when you:**\n1. Describe *what* you want clearly\n2. Iterate on its suggestions\n3. Let it handle boilerplate and patterns\n4. Focus on architecture and creativity\n\n---\n\n## Share Your Thoughts\n\nHave you built something with GitHub Copilot CLI? What was your experience?\n\n**Try DevPulse on your project and let me know what it finds!**\n\n---\n\n### Links\n\n- üîó [DevPulse GitHub Repository](https://github.com/Srijan-XI/DevPulse)\n- üìñ [Full Documentation](https://Srijan-XI.github.io/DevPulse)\n- üêõ [Report Issues](https://github.com/Srijan-XI/DevPulse/issues)\n- üí¨ [Discussions](https://github.com/Srijan-XI/DevPulse/discussions)\n\n\nFollow me for more AI-assisted development experiments! üöÄ\n",
      "stars": null,
      "comments": 2,
      "upvotes": 9,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "Implemented a Feature where the Theme on my Portfolio changes based on the Holiday (Because it's fun) üí´",
      "url": "https://dev.to/francistrdev/implemented-a-feature-where-the-theme-on-my-portfolio-changes-based-on-the-holiday-because-its-31jo",
      "source": "devto",
      "published_at": "2026-02-10T00:05:36+00:00",
      "external_id": null,
      "tags": [
        "discuss",
        "react",
        "nextjs",
        "css"
      ],
      "content_length": 7904,
      "content_preview": "Yesterday, I had this great idea for my portfolio. I want to implement a feature where if it reaches Valentine day (since it is coming up), it changes the theme of my portfolio. For example, right now I have blue for my text. Instead, it would be pink!\n\nOf course, hard coding it would be a pain since you have to manually change it on each holiday, so I made it automatically using the `new Date()`. The holidays I added are:\n\n- Valentine Day\n- St. Patrick's Day\n- Halloween\n- Christmas Day\n\nHere ar",
      "content_full": "Yesterday, I had this great idea for my portfolio. I want to implement a feature where if it reaches Valentine day (since it is coming up), it changes the theme of my portfolio. For example, right now I have blue for my text. Instead, it would be pink!\n\nOf course, hard coding it would be a pain since you have to manually change it on each holiday, so I made it automatically using the `new Date()`. The holidays I added are:\n\n- Valentine Day\n- St. Patrick's Day\n- Halloween\n- Christmas Day\n\nHere are the features I added to the portfolio! If you would love to see it, visit: https://francistr.github.io/\n\n---\n\n## Spark Effect on Mouse Click!\nI found this effect from Prahalad on this post [\"Click anywhere to see Spark Effect(fireworks) using CSS and JS\"](https://dev.to/prahalad/click-anywhere-to-see-spark-effectfireworks-using-css-and-js-20fn). The only problem is translating it to my Portfolio since I used Next.js for my Portfolio. To achieved the functionality, here is what I did:\n\nIn CSS (Similar to Prahalad implementation):\n```\n/* Each spark segment */\n.click-burst li {\n  padding: 0;\n  width: 0;\n  height: 0;\n  position: absolute;\n  left: auto;\n  right: auto;\n}\n\n/* Top (vertical) */\n.click-burst li:first-child {\n  left: 50%;\n  top: -22px;\n  width: 1px;\n  border-left: 2px solid var(--click-color-1);\n  animation: vert 0.25s linear;\n}\n\n/* Bottom (vertical) */\n.click-burst li:nth-child(2) {\n  left: 50%;\n  bottom: -22px;\n  width: 1px;\n  border-left: 2px solid var(--click-color-1);\n  animation: vert 0.25s linear;\n}\n\n/* Left (horizontal) */\n.click-burst li:nth-child(3) {\n  left: -6px;\n  top: 11px;\n  height: 1px;\n  border-bottom: 2px solid var(--click-color-1);\n  animation: horiz 0.25s linear;\n}\n\n/* Right (horizontal) */\n.click-burst li:nth-child(4) {\n  right: -6px;\n  top: 11px;\n  height: 1px;\n  border-bottom: 2px solid var(--click-color-1);\n  animation: horiz 0.25s linear;\n}\n\n/* Diagonals */\n.click-burst li:nth-child(5) {\n  left: 0px;\n  top: -11px;\n  rotate: 45deg;\n  height: 1px;\n  border-bottom: 2px solid var(--click-color-2);\n  animation: horiz 0.25s linear;\n}\n.click-burst li:nth-child(6) {\n  right: 0px;\n  top: -11px;\n  rotate: -45deg;\n  height: 1px;\n  border-bottom: 2px solid var(--click-color-2);\n  animation: horiz 0.25s linear;\n}\n.click-burst li:nth-child(7) {\n  left: 0px;\n  bottom: -11px;\n  rotate: -45deg;\n  height: 1px;\n  border-bottom: 2px solid var(--click-color-2);\n  animation: horiz 0.25s linear;\n}\n.click-burst li:nth-child(8) {\n  right: 0px;\n  bottom: -11px;\n  rotate: 45deg;\n  height: 1px;\n  border-bottom: 2px solid var(--click-color-2);\n  animation: horiz 0.25s linear;\n}\n\n/* Keyframes for mouse click */\n@keyframes vert {\n  from { height: 22px; }\n  to   { height: 0px; }\n}\n@keyframes horiz {\n  from { width: 22px; }\n  to   { width: 0px; }\n}\n```\n\n\n\nOn the ClickBurst.tsx, I add this code:\n```\n/**\n * Click anywhere on the page to spawn a temporary <ul> with 8 <li> \"sparks\"\n * positioned and animated by CSS. Removed after 250ms.\n */\nexport default function ClickBurst() {\n  useEffect(() => {\n    const onClick = (event: MouseEvent) => {\n      let ul = document.createElement(\"ul\");\n      ul.className = \"click-burst\";\n      \n      // Absolutely position at the mouse and center the UL on that point\n      ul.style.position = \"absolute\";\n      ul.style.left = `${event.pageX}px`;\n      ul.style.top = `${event.pageY}px`;\n      ul.style.transform = \"translate(-50%, -50%)\";\n\n      // Size of the effect area (tweak as you like)\n      ul.style.width = \"3em\";\n      ul.style.height = \"1.5em\";\n\n      // Housekeeping\n      ul.style.listStyle = \"none\";\n      ul.style.padding = \"0\";\n      ul.style.margin = \"0\";           // no margin hacks needed\n      ul.style.pointerEvents = \"none\"; // don't block clicks\n      ul.style.zIndex = \"9999\";        // on top of everything\n\n\n      // 8 radial ‚Äúspark‚Äù lines (li elements)\n      for (let i = 1; i <= 8; i++) {\n        let li = document.createElement(\"li\");\n        ul.appendChild(li);\n      }\n\n      document.body.appendChild(ul);\n\n      // Remove after 250ms (matches animation duration)\n      setTimeout(() => {\n        ul.remove();\n      }, 250);\n    };\n\n    document.body.addEventListener(\"click\", onClick);\n    return () => document.body.removeEventListener(\"click\", onClick);\n  }, []);\n\n  return null; // no visible UI; this just wires up the effect\n}\n```\n\n_Note that I had the help of Copilot to translate the code from the post onto my portfolio. Tweaks were made after translation._\n\nAnnnnndddddddd Done! Those are the main parts. Here is what it looks like:\n\n![Clicking Effect](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9kig7s3rqsk1tt3vc6jh.gif)\n\n\n\n## Hovering Effect!\nWhen you hover over my title \"Full-Stack AI Engineer\", your mouse change to a different .ico image. For some reason, it did not like .png .jpg, etc. So, I made my disired images all .ico. This is what I added.\n\nFor CSS, I added a variable and add that variable to our class:\n```\n/* Inside of the Root */\n--custom-cursor: url('/cursorImg/IceCream.ico') 16 16, auto\n\n/* Outside of the Root */\n.custom-cursor {\n  cursor: var(--custom-cursor);\n}\n```\n\n\n\nThen I added the `<span>` tag that only works if you hover over this text\n```\n<h2 className=\"text-2xl lg:text-start shiny\">\n   <span className=\"custom-cursor\">Full-Stack AI Engineer</span>\n</h2>\n```\n\nThat's pretty much it. Here is what it looks like when you visit my site!\n\n![Hovering](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7it3gg5jpuqxxbnwvlcn.gif)\n\n\n\n## Rest of page change colors!\nTo achieve this, this is the steps I took to make this dynamic.\n\nFor CSS, I create the variables in the root and use it throughout the CSS file:\n```\n:root {\n  /* Titles */\n  --shiny-color: #00ccff;\n  --shiny-color-light: #cef5ff;\n\n  /* sub-titles */\n  --primary-main-color: 193 100% 50%;\n\n  /* Card itself */\n  --card-main-bg: 222.2 50% 10%;\n\n  /* Skills card */\n  --skills-card-bg: #0C1426;\n\n  /* Click colors */\n  --click-color-1: rgb(255, 255, 255);\n  --click-color-2: rgb(255, 255, 255);\n\n  --custom-cursor: url('/cursorImg/IceCream.ico') 16 16, auto\n}\n```\n\n\nI then added a .tsx file called \"ThemeScheduler\". where it's purpose is if the assign date has started, change the variable values to a different color.\n\nFor example, for Christmas, we assign the colors and its date of change:\n```\n{ // Christmas (December 25th)\n  // Year, Month, day, hours, minutes, seconds, milliseconds\n  start: new Date(new Date().getFullYear(), 11, 25, 0, 0, 0, 0),\n  end: new Date(new Date().getFullYear(), 11, 26, 0, 0, 0, 0),\n  vars: {\n    \"--shiny-color\": \"hsl(120, 79%, 40%)\",\n    \"--shiny-color-light\": \"#ff7a7a\",\n\n    \"--primary-main-color\": \"120, 79%, 40%\",\n\n    \"--card-main-bg\": \"0, 49%, 10%\",\n    \"--skills-card-bg\": \"hsl(0, 49%, 10%)\",\n\n    \"--click-color-1\": \"rgb(21, 183, 21)\",\n    \"--click-color-2\": \"rgb(255, 122, 122)\",\n\n    \"--custom-cursor\": \"url('/cursorImg/ChristmasTree.ico') 16 16, auto\"\n  },\n},\n```\n\nThat is the gist of it. Here is what it looks like for each holiday in full:\n\n#### Valentine Day\n![Valentine Day](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/umm71ch5a0cxjgkqtqau.gif)\n\n\n#### St. Patrick's Day\n![St. Patrick's Day](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5n64nbmps4q84ehb9ehx.gif)\n\n\n#### Halloween\n![Halloween](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rlzy0xf83faxns5ndyb8.gif)\n\n\n#### Christmas Day\n![Christmas Day](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0anrlvq23c6i7b96d5kf.gif)\n\n---\n\nThanks for reading! Note that the Portfolio is still in the works. If you would love to review and provide feedback, I would love to hear from you! Will make a Portfolio review request in the future!\n\n# Discussion\n- What other themes should I add?\n- What is your favorite theme?\n- Feedback would be nice! Note that this is still in development, but would love to receive feedback if possible!",
      "stars": null,
      "comments": 10,
      "upvotes": 25,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "freeCodeCamp: Full Stack Developer Curriculum",
      "url": "https://dev.to/the-foundation/freecodecamp-full-stack-developer-curriculum-1dlc",
      "source": "devto",
      "published_at": "2026-02-08T16:00:08+00:00",
      "external_id": null,
      "tags": [
        "community",
        "learning",
        "programming",
        "webdev"
      ],
      "content_length": 2801,
      "content_preview": "_Author's note: While the primary aim of The Foundation is to preserve technical knowledge before it becomes paywalled, lost, or compromised, this post simply highlights freeCodeCamp as an excellent free resource._\n\nWe live in a time where it's possible to build things on the web without fully understanding how they work.\n\nFrameworks abstract. AI completes. Tutorials skip steps. And while this has lowered the barrier to entry, it has also created a generation of developers who can assemble inter",
      "content_full": "_Author's note: While the primary aim of The Foundation is to preserve technical knowledge before it becomes paywalled, lost, or compromised, this post simply highlights freeCodeCamp as an excellent free resource._\n\nWe live in a time where it's possible to build things on the web without fully understanding how they work.\n\nFrameworks abstract. AI completes. Tutorials skip steps. And while this has lowered the barrier to entry, it has also created a generation of developers who can assemble interfaces but struggle to reason about them.\n\nThat's the problem The Foundation exists to push back against.\n\n**Fundamentals Are Not Optional**\n\nHTML and CSS are often treated as \"beginner topics\" - something you rush through so you can get to the \"real\" stuff.\n\nBut they are the real stuff.\n\nLayout bugs, accessibility issues, responsive breakpoints that collapse, components that fight each other - these aren't advanced problems. They're fundamentals misunderstood.\n\nIf you don't understand the box model, semantic markup, or how CSS actually flows, no framework will save you. It will only hide the problem until it explodes.\n\n**Why freeCodeCamp Gets This Righ**t\n\nfreeCodeCamp doesn‚Äôt optimize for speed or hype. It optimizes for understanding.\n\nThe curriculum is:\n\n- Self-paced, but structured\n- Constraint-based (you must solve the problem, not just watch it)\n- Project-driven, so concepts stick through repetition and application\n- There's no \"skip ahead because you‚Äôre bored\" button. And that's a feature, not a bug.\n\n**Why Everyone Should Complete the Responsive Web Design Certification**\n\nAt minimum.\n\nThis certification forces you to:\n\n- Write real HTML, not JSX first\n- Think in CSS, not utility classes\n- Build layouts that actually respond to screen size\n- Care about semantics, structure, and accessibility\n\nBy the end, you don't just know what works - you know why it works.\n\nAnd once you know that, frameworks stop feeling magical and start feeling reasonable.\n\n**Platform Advantages That Actually Matter**\n\nfreeCodeCamp is:\n\n- Completely free - no subscriptions, no paywalls\n- Open source - the curriculum itself is transparent and community-maintained\n- Ad-free and non-extractive - your attention isn‚Äôt the product\n- Globally accessible - designed for learners, not customers\n\nThat matters. Especially if we care about preserving knowledge, not just selling access to it.\n\n**Strong Foundations Age Well**\n\nThe web will keep changing. Tools will come and go. AI will get better.\n\nBut fundamentals compound.\n\nCompleting freeCodeCamp's Responsive Web Design certification isn't about nostalgia or gatekeeping. It's about building a mental model that survives trends.\n\nAt The Foundation, that's the kind of knowledge we believe is worth highlighting, supporting, and sharing with others.",
      "stars": null,
      "comments": 17,
      "upvotes": 43,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "How I Built Graft: An Overlay Engine for Terraform Modules",
      "url": "https://dev.to/mshenglu/how-i-built-graft-an-overlay-engine-for-terraform-modules-5d8i",
      "source": "devto",
      "published_at": "2026-02-05T07:08:59+00:00",
      "external_id": null,
      "tags": [
        "terraform",
        "devops",
        "azure"
      ],
      "content_length": 7735,
      "content_preview": "There's a [Terraform GitHub issue](https://github.com/hashicorp/terraform/issues/27360) that's been open for years: people want to customize modules without forking them. Add a lifecycle block. Tweak a tag. Simple stuff.\n\nI understand why Terraform doesn't support this natively‚Äîmodules are supposed to be black boxes, and breaking the encapsulation is not ideal. But in practice, modules often need tweaks.\n\nI built Graft to solve this. It patches Terraform modules in place‚Äîno forks, no merge confl",
      "content_full": "There's a [Terraform GitHub issue](https://github.com/hashicorp/terraform/issues/27360) that's been open for years: people want to customize modules without forking them. Add a lifecycle block. Tweak a tag. Simple stuff.\n\nI understand why Terraform doesn't support this natively‚Äîmodules are supposed to be black boxes, and breaking the encapsulation is not ideal. But in practice, modules often need tweaks.\n\nI built Graft to solve this. It patches Terraform modules in place‚Äîno forks, no merge conflicts. \n\nAnd honestly, it's a middleware for something bigger I'm working on. But I'll save that for the next post. :)\n\n## The Idea\n\nThe goal: use declarative Terraform blocks to describe modifications to existing modules. It should:\n\n- Modify multi-layer (nested) modules\n- Work easily with existing modules\n- Stay compatible when modules update\n\nSo I can define a graft manifest like this:\n\n```hcl\nmodule \"network\" {\n  override {\n    # patches to modify the existing module\n  }\n\n  module \"subnet\" {\n    override {\n      # patches to modify the nested module\n    }\n  }\n}\n```\n\nThe nested structure mirrors the module hierarchy. This makes it easy to locate exactly which blocks you want to modify‚Äîjust navigate down the tree.\n\n## First Attempt: Override Files\n\nMy first idea was to use Terraform's native override mechanism. If you create `override.tf`, it merges with your main config. ([Official docs](https://developer.hashicorp.com/terraform/language/files/override))\n\nBut override files have serious limitations:\n\n1. You can't add new blocks‚Äîonly modify existing ones\n2. You can't delete blocks or attributes\n\nNot enough.\n\n## Second Attempt: Enhanced Override Files\n\nSince the graft manifest is processed *before* Terraform runs, I have more control than native overrides.\n\n**Adding new blocks** was easy: check the source code, then generate a new file `_graft_add.tf` in the module directory.\n\n**Deleting things** required a new approach. The implementation wasn't hard‚Äîjust parse the manifest and remove matching blocks from the source files. But the design was tricky: how do you express \"delete this\" in a way that feels native to Terraform?\n\nI introduced a special `_graft` block:\n\n```hcl\nresource \"azurerm_network_security_rule\" \"allow_all\" {\n  _graft {\n    remove = [\"self\"]  # Delete the entire resource\n  }\n}\n\nresource \"azurerm_virtual_network\" \"vnet\" {\n  _graft {\n    remove = [\"dns_servers\", \"tags\"]  # Delete specific attributes\n  }\n}\n```\n\nIt looks like regular HCL. It nests inside the resource block you're targeting. It follows Terraform's declarative style. That's what I wanted‚Äîsomething that feels like it *belongs* in Terraform, even though Terraform itself can't do this.\n\n## Referencing Original Values\n\nWhile testing the override strategy, I ran into an interesting problem with `count` and `for_each` resources.\n\nSay a module creates multiple subnets with `for_each`, and I want to modify just one of them. I can target a specific key:\n\n```hcl\nresource \"azurerm_subnet\" \"main\" {\n  for_each = var.subnets\n  \n  # Only modify subnet1\n  service_endpoints = each.key == \"subnet1\" ? [\"Microsoft.Storage\"] : ???\n}\n```\n\nBut what goes in the `???`? I need the *original* value to avoid affecting other subnets. Without knowing what the module originally set, I'd have to hardcode it‚Äîor worse, accidentally break the other subnets.\n\nThis is where `graft.source` came from. It references the original value‚Äîno matter how complicated the expression is. I don't need to look it up in the module source code.\n\n```hcl\nservice_endpoints = each.key == \"subnet1\" ? [\"Microsoft.Storage\"] : graft.source\n```\n\nThis also solves another frustration with Terraform's native override files: they use **shallow merge** for attributes. If you want to add one tag, you can't‚Äîyour override *replaces* the entire `tags` map, wiping out the module's defaults.\n\nWith `graft.source`, you can actually merge:\n\n```hcl\ntags = merge(graft.source, {\n  \"Owner\" = \"Platform Team\"\n})\n```\n\nDuring patching, `graft.source` gets replaced with the actual original expression. You get true merging‚Äîand you don't need to know what the original value was.\n\n## The Linker Problem\n\nNow I had patching working. But how do I make Terraform *use* the patched modules?\n\nMy first idea: use an override file to redirect the module source to a local patched copy.\n\n```hcl\n# file: _graft_override.tf\n# What I tried to generate\nmodule \"network\" {\n  source = \"./.graft/patched-network\"\n}\n```\n\nIt failed immediately.\n\n**You can't override `source` when there's a `version` constraint:**\n\n```hcl\n# Original main.tf\nmodule \"network\" {\n  source  = \"Azure/network/azurerm\"\n  version = \"5.3.0\"  # ‚Üê This kills the override\n}\n```\n\nTerraform throws: *\"Cannot apply a version constraint to module 'network' because it has a relative local path.\"*\n\nAnd you can't \"unset\" the version‚Äîoverride files can only add or modify, never delete.\n\nDead end.\n\n## The Breakthrough: Hijacking modules.json\n\nI started digging into how Terraform actually resolves modules.\n\nWhen you run `terraform init`, Terraform downloads modules and records their locations in `.terraform/modules/modules.json`:\n\n```json\n{\n  \"Modules\": [\n    {\n      \"Key\": \"network\",\n      \"Source\": \"registry.terraform.io/Azure/network/azurerm\",\n      \"Version\": \"5.3.0\",\n      \"Dir\": \".terraform/modules/network\"\n    }\n  ]\n}\n```\n\nWhat if I just changed where `Dir` points? I tried it manually‚Äîedited `modules.json`, pointed `Dir` to a local folder with patched code.\n\nIt worked. Terraform loaded my patched module while believing it was using the official registry version. No errors. No need to modify `main.tf`.\n\nI called this the **Linker Strategy**‚Äîlike how linkers resolve symbols to addresses, Graft resolves modules to patched directories.\n\n## The Scaffold Command\n\nOne thing bothered me. Graft's whole point is that you shouldn't need to understand a module's internals‚Äîjust declare what you want to change.\n\nBut when I actually used it, I kept opening module source files anyway. Which nested module contains that resource? What's the hierarchy? Even as the author, I couldn't write a manifest without digging through `.terraform/modules`.\n\nSo I added `graft scaffold`. It scans your `.terraform/modules` directory and generates a starter manifest with the full module tree:\n\n```bash\n$ graft scaffold\n\n[+] Discovering modules in .terraform/modules...\nroot\n‚îú‚îÄ‚îÄ network (registry.terraform.io/Azure/network/azurerm, 5.3.0)\n‚îÇ   ‚îî‚îÄ‚îÄ [3 resources]\n‚îî‚îÄ‚îÄ compute (registry.terraform.io/Azure/compute/azurerm, 5.3.0)\n    ‚îú‚îÄ‚îÄ [18 resources]\n    ‚îî‚îÄ‚îÄ compute.os (local: ./os)\n        ‚îî‚îÄ‚îÄ [2 resources]\n\n‚ú® Graft manifest saved to scaffold.graft.hcl\n```\n\nSimple, but essential. Now users can see the hierarchy at a glance and start writing overrides immediately‚Äîwithout ever opening the module source.\n\n## Try It\n\n```bash\ngo install github.com/ms-henglu/graft@latest\n```\n\nThe workflow:\n\n```bash\nterraform init\ngraft scaffold    # See the module tree, generate starter manifest\n# Edit manifest.graft.hcl\ngraft build       # Vendor, patch, and link\nterraform plan    # Your patches are applied\n```\n\nYour `main.tf` never changes. When the upstream module releases a new version, bump the version, run `terraform init && graft build`, and your patches are reapplied.\n\nNo forks. No merge conflicts.\n\nCheck the [examples](https://github.com/ms-henglu/graft/tree/main/examples) for patterns like overriding values, injecting resources, removing attributes, and adding lifecycle rules.\n\n---\n\nThe full code is at [github.com/ms-henglu/graft](https://github.com/ms-henglu/graft).\n\nIf you try it and hit issues‚Äîor have ideas‚Äî[open an issue](https://github.com/ms-henglu/graft/issues). I'd love to hear what breaks.\n\nHappy patching. üå±\n\n\n",
      "stars": null,
      "comments": 0,
      "upvotes": 5,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "Build a Checkout Page",
      "url": "https://dev.to/richardpascoe/build-a-checkout-page-43nj",
      "source": "devto",
      "published_at": "2026-02-09T14:00:19+00:00",
      "external_id": null,
      "tags": [
        "community",
        "learning",
        "programming",
        "webdev"
      ],
      "content_length": 4505,
      "content_preview": "After a relaxed Sunday spent working through four theory lessons on _Working with Accessible Media Elements_, I was ready to dive into today's lab: **Building a Checkout Page**.\n\nBefore I jump in, I want to pause briefly on the first theory lesson, which covered **Alt Text (Alternative Text)**. Take a look at this image from the lesson:\n\n![A black and white puppy with an orange collar lies on its belly in the sand, looking off to the side. A bright orange ball rests near its front paws](https://",
      "content_full": "After a relaxed Sunday spent working through four theory lessons on _Working with Accessible Media Elements_, I was ready to dive into today's lab: **Building a Checkout Page**.\n\nBefore I jump in, I want to pause briefly on the first theory lesson, which covered **Alt Text (Alternative Text)**. Take a look at this image from the lesson:\n\n![A black and white puppy with an orange collar lies on its belly in the sand, looking off to the side. A bright orange ball rests near its front paws](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yh0xnb16wcvem1gakmur.png)\n\nA poor example of alt text would be: \"A cute puppy.\" While technically true, it doesn't give the user much information. A far better description is: \"A black and white puppy with an orange collar lies on its belly in the sand, looking off to the side. A bright orange ball rests near its front paws.\" This version paints a fuller picture, helping users understand the content even if they can't see it.\n\nI'm sharing this example because writing good alt text is one of the simplest ways to make images truly accessible. With that in mind, it's time to move on to the lab.\n\nAs with all previous freeCodeCamp labs, you're provided with an example of the completed page. However, you should **not** copy it exactly. You're also given some HTML boilerplate to get you started, as shown below:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\" />\n    <title>Checkout Page</title>\n</head>\n\n<body>\n\n</body>\n\n</html>\n```\n\nLike before, your progress is measured by completing the user stories - eleven in total for this lab. The first few focus on using familiar elements, such as h1, two section elements, and h2, before you move on to adding an image.\n\nI used the provided link for the image, which shows a Rubik's Cube. Next, I added more elements, including an h2, a form, and four label and input fields. Once the labels and inputs were correctly populated, span elements were added before addressing accessibility considerations.\n\nThese considerations involved adding aria-hidden to the span elements to prevent screen readers from reading decorative asterisks, and using a single aria-describedby on the card number input to give users clear guidance on the required format.\n\nOnce the user stories were completed, I ran the tests to ensure all checks passed. They did, marking the lab as complete. For clarity, I have posted the completed code below:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\" />\n    <title>Checkout Page</title>\n</head>\n\n<body>\n    <h1>Checkout</h1>\n\n    <section>\n        <h2>Your Cart</h2>\n        <img\n            src=\"https://cdn.freecodecamp.org/curriculum/labs/cube.jpg\"\n            alt=\"A partially solved multicolored Rubik‚Äôs Cube on a white background.\"\n        >\n    </section>\n\n    <section>\n        <h2>Payment Information</h2>\n\n        <form>\n            <label for=\"card-name\">\n                Cardholder Name:\n                <span aria-hidden=\"true\">*</span>\n                <input type=\"text\" id=\"card-name\" name=\"card-name\" required>\n            </label>\n\n            <label for=\"card-number\">\n                Card Number:\n                <span aria-hidden=\"true\">*</span>\n                <input\n                    type=\"text\"\n                    id=\"card-number\"\n                    name=\"card-number\"\n                    required\n                    aria-describedby=\"card-number-help\"\n                >\n            </label>\n\n            <p id=\"card-number-help\">\n                Enter your 16-digit card number without spaces or dashes.\n            </p>\n\n            <label for=\"expiry-date\">\n                Expiry Date:\n                <span aria-hidden=\"true\">*</span>\n                <input type=\"text\" id=\"expiry-date\" name=\"expiry-date\" required>\n            </label>\n\n            <label for=\"cvv-number\">\n                CVV:\n                <span aria-hidden=\"true\">*</span>\n                <input type=\"text\" id=\"cvv-number\" name=\"cvv-number\" required>\n            </label>\n\n            <button type=\"submit\">Place Order</button>\n        </form>\n    </section>\n\n</body>\n\n</html>\n```\n\nAfter the previous theory lessons, it was great to have the opportunity to put what I had learned into practice. This will continue with the next lab, _Design a Movie Review Page_. More on that next time. As always, keep coding!\n\n![Written by a Human logo](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5o2563dz8gj92it82wlf.png)",
      "stars": null,
      "comments": 6,
      "upvotes": 28,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Hostile Takeover of RubyGems: My Thoughts",
      "url": "https://dev.to/galtzo/hostile-takeover-of-rubygems-my-thoughts-5hlo",
      "source": "devto",
      "published_at": "2026-02-06T01:00:32+00:00",
      "external_id": null,
      "tags": [
        "opensource",
        "ruby",
        "rails",
        "governance"
      ],
      "content_length": 6911,
      "content_preview": "I'll keep this post evergreen, as the situation evolves. Also, when you are done reading - [hire me](https://galtzo.com).\n\n[rubygems-org]: https://github.com/rubygems/\n[draper-security]: https://joel.drapper.me/p/ruby-central-security-measures/\n[draper-takeover]: https://joel.drapper.me/p/ruby-central-takeover/\n[ellen-takeover]: https://pup-e.com/blog/goodbye-rubygems/\n[simi-removed]: https://www.reddit.com/r/ruby/s/gOk42POCaV\n[martin-removed]: https://bsky.app/profile/martinemde.com/post/3m3occ",
      "content_full": "I'll keep this post evergreen, as the situation evolves. Also, when you are done reading - [hire me](https://galtzo.com).\n\n[rubygems-org]: https://github.com/rubygems/\n[draper-security]: https://joel.drapper.me/p/ruby-central-security-measures/\n[draper-takeover]: https://joel.drapper.me/p/ruby-central-takeover/\n[ellen-takeover]: https://pup-e.com/blog/goodbye-rubygems/\n[simi-removed]: https://www.reddit.com/r/ruby/s/gOk42POCaV\n[martin-removed]: https://bsky.app/profile/martinemde.com/post/3m3occezxxs2q\n[draper-lies]: https://joel.drapper.me/p/ruby-central-fact-check/\n[draper-theft]: https://joel.drapper.me/p/ruby-central/\n[reinteractive]: https://reinteractive.com/ruby-on-rails\n[gem-coop]: https://gem.coop\n[gem-naming]: https://github.com/gem-coop/gem.coop/issues/12\n[martin-ann]: https://martinemde.com/2025/10/05/announcing-gem-coop.html\n[gem-scopes]: https://github.com/galtzo-floss/bundle-namespace\n[gem-server]: https://github.com/galtzo-floss/gem-server\n[reinteractive-podcast]: https://youtu.be/_H4qbtC5qzU?si=BvuBU90R2wAqD2E6\n[bundler-maint-policy]: https://github.com/ruby/rubygems/blob/b1ab33a3d52310a84d16b193991af07f5a6a07c0/doc/bundler/playbooks/TEAM_CHANGES.md\n[rubygems-maint-policy]: https://github.com/ruby/rubygems/blob/b1ab33a3d52310a84d16b193991af07f5a6a07c0/doc/rubygems/POLICIES.md?plain=1#L187-L196\n[policy-fail]: https://www.reddit.com/r/ruby/comments/1ove9vp/rubycentral_hates_this_one_fact/\n\n# üë£üîçÔ∏è First some background reading üïµÔ∏è\n\n- RubyGems (the [GitHub org][rubygems-org], not the website) [suffered][draper-security] a [hostile takeover][ellen-takeover] in September 2025.\n- Ultimately [4 maintainers][simi-removed] were [hard removed][martin-removed] and a (dubious) reason has been given for only 1 of those, while 2 others resigned in protest.\n- It is a [complicated story][draper-takeover] which is difficult to [parse quickly][draper-lies].\n- Simply put - there was active policy for adding or removing maintainers/owners of [rubygems][rubygems-maint-policy] and [bundler][bundler-maint-policy], and those [policies were not followed][policy-fail].\n- I'm adding a note linking to this post to all of my gems because I [don't condone theft][draper-theft] of repositories or gems from their rightful owners.\n- If a similar theft happened with my repos/gems, I'd hope some would stand up for me.\n- Disenfranchised former-maintainers have started [gem.coop][gem-coop].\n- Once available I will publish there, or to my own server, exclusively; unless RubyCentral & Ruby Core make amends with the community.\n- The [\"Technology for Humans: Joel Draper\"][reinteractive-podcast] podcast episode by [reinteractive][reinteractive] is the most cogent summary I'm aware of.\n- See [here][gem-naming], [here][gem-coop] and [here][martin-ann] for more info on what comes next.\n\n[ore]: https://github.com/contriboss/ore-light\n[setup-ruby-flash]: https://github.com/appraisal-rb/setup-ruby-flash\n[setup-ruby-flash-blog]: https://dev.to/galtzo/setup-ruby-flash-25lb\n[appraisal2]: https://github.com/appraisal-rb/appraisal2\n[appraisal2-blog]: https://dev.to/galtzo/ann-appraisal2-a-hard-fork-44dh\n[rv]: https://rv.dev/\n[setup-ruby]: https://github.com/ruby/setup-ruby\n[rubygems-org]: https://github.com/rubygems/\n[draper-security]: https://joel.drapper.me/p/ruby-central-security-measures/\n[draper-takeover]: https://joel.drapper.me/p/ruby-central-takeover/\n[ellen-takeover]: https://pup-e.com/blog/goodbye-rubygems/\n[simi-removed]: https://www.reddit.com/r/ruby/s/gOk42POCaV\n[martin-removed]: https://bsky.app/profile/martinemde.com/post/3m3occezxxs2q\n[draper-lies]: https://joel.drapper.me/p/ruby-central-fact-check/\n[draper-theft]: https://joel.drapper.me/p/ruby-central/\n[reinteractive]: https://reinteractive.com/ruby-on-rails\n[gem-coop]: https://gem.coop\n[gem-naming]: https://github.com/gem-coop/gem.coop/issues/12\n[martin-ann]: https://martinemde.com/2025/10/05/announcing-gem-coop.html\n[gem-scopes]: https://github.com/galtzo-floss/bundle-namespace\n[gem-server]: https://github.com/galtzo-floss/gem-server\n[reinteractive-podcast]: https://youtu.be/_H4qbtC5qzU?si=BvuBU90R2wAqD2E6\n[bundler-maint-policy]: https://github.com/ruby/rubygems/blob/b1ab33a3d52310a84d16b193991af07f5a6a07c0/doc/bundler/playbooks/TEAM_CHANGES.md\n[rubygems-maint-policy]: https://github.com/ruby/rubygems/blob/b1ab33a3d52310a84d16b193991af07f5a6a07c0/doc/rubygems/POLICIES.md?plain=1#L187-L196\n[policy-fail]: https://www.reddit.com/r/ruby/comments/1ove9vp/rubycentral_hates_this_one_fact/\n\n# My thoughts\n\n1. I no longer trust Ruby Central.\n2. I no longer trust certain members, but primarily HSBT, of the RubyGems core team.\n3. I no longer trust certain members, but primarily HSBT and Matz, of the Ruby core team.\n\n> Q: In what sense do I _not trust_ them?\n> A: üìÉ **Governance** üìÉ\n\nTo be more specific, I no longer trust that they:\n\n1. Hold people accountable for their actions according to written agreements and documentation around governance policy. \n2. Understand the community upset over point 1.\n3. Will ever do anything about it.\n\nIf they are added to your repository, you may wake up to find you have lost access to your own project.\n\nI'm not OK with this having already happened to others, and have taken steps to ensure it will not happen to me.\n\nWithin my open source projects, I will reduce, to the degree possible, my reliance, on any project hosted under the Ruby org on GitHub. Since most of my projects are Ruby projects, I'll never get to complete exclusion, but I will be focusing much more on JRuby and Truffleruby.\n\nIt has been pointed out to me in other discussions about this that we never had reason to trust them, but we did anyway, implicitly. We normally assume other people live by the same code of ethics that we ourselves live by. I will miss being able to rest on that assumption, but it is probably for the best that it get binned.\n\n# What I'm doing about it\n\n- [ore][ore] installs gems without Ruby, without bundler, and without rubygems. It is a GoLang implementation of (some parts of) Bundler (and adds some features bundler lacks). A project by @seuros - and I'm now on the core team. It is *much* faster than bundler.\n- [setup-ruby-flash][setup-ruby-flash] is an alternative to the venerable setup-ruby GHA we've all been using for years. `setup-ruby-flash` relies on [rv][rv] and [ore][ore] for Ruby and Gem installs, and it falls back to [setup-ruby][setup-ruby] on unsupported platforms/engines. I wrote more about it [here][setup-ruby-flash-blog].\n- [appraisal2][appraisal2] is a hard fork of the old, and nearly-dead, namesake Thoughtbot project, to which I've added many features, including support for `eval_gemfile`, all versions of Ruby back to v1.8, and [ore][ore] (see above). More on the reasons behind the [hard fork][appraisal2-blog].\n- A (WIP) proposal for [bundler/gem scopes][gem-scopes]\n- A (WIP) proposal for a federated [gem server][gem-server]\n",
      "stars": null,
      "comments": 2,
      "upvotes": 11,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Realtime Multimodal AI on Ray-Ban Meta Glasses with Gemini Live & LiveKit",
      "url": "https://dev.to/googleai/realtime-multimodal-ai-on-ray-ban-meta-glasses-with-gemini-live-livekit-45mn",
      "source": "devto",
      "published_at": "2026-02-03T16:27:34+00:00",
      "external_id": null,
      "tags": [
        "xr",
        "gemini",
        "android",
        "ai"
      ],
      "content_length": 4721,
      "content_preview": "Imagine walking down the street, asking your glasses what kind of plant you're looking at, and getting a response in near real-time. With the combination of **Gemini Live API**, **LiveKit**, and **Meta Wearables SDK**, this isn't science fiction anymore, it's something you can build today.\n\n{% embed https://x.com/thorwebdev/status/2016031645003088280?s=20 %}\n\nIn this post, we‚Äôll walk through how to set up a vision-enabled AI agent that connects to Meta Ray-Ban glasses via a secure WebRTC proxy.\n",
      "content_full": "Imagine walking down the street, asking your glasses what kind of plant you're looking at, and getting a response in near real-time. With the combination of **Gemini Live API**, **LiveKit**, and **Meta Wearables SDK**, this isn't science fiction anymore, it's something you can build today.\n\n{% embed https://x.com/thorwebdev/status/2016031645003088280?s=20 %}\n\nIn this post, we‚Äôll walk through how to set up a vision-enabled AI agent that connects to Meta Ray-Ban glasses via a secure WebRTC proxy.\n\n## The Architecture\n\nThe setup involves several layers to ensure low-latency, secure communication between the wearable device and the AI:\n\n1.  **Meta Ray-Ban Glasses**: Capture video and audio, connecting via Bluetooth to your phone.\n2.  **Phone (Android/iOS)**: Acts as the gateway, connecting via **WebRTC** to LiveKit Cloud.\n3.  **LiveKit Cloud**: Serves as a secure, high-performance proxy for the Gemini Live API.\n4.  **Gemini Live API**: Processes the stream via WebSockets, enabling real-time multimodal interaction.\n\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/q49yhkwgdes44xu83qau.jpeg)\n\n---\n\n## The Backend: Building the Gemini Live Agent\n\nWe use the LiveKit Agents framework to act as a secure WebRTC proxy for the Gemini Live API. This agent joins the LiveKit room, listens to the audio, and processes the video stream from the glasses.\n\n### Setting up the Assistant\nThe core of our agent is the `AgentSession`. We use the `google.beta.realtime.RealtimeModel` to interface with Gemini. Crucially, we enable `video_input` in the `RoomOptions` to allow the agent to \"see.\"\n\n```python\n@server.rtc_session()\nasync def entrypoint(ctx: JobContext):\n    ctx.log_context_fields = {\"room\": ctx.room.name}\n\n    session = AgentSession(\n        llm=google.beta.realtime.RealtimeModel(\n            model=\"gemini-2.5-flash-native-audio-preview-12-2025\",\n            proactivity=True,\n            enable_affective_dialog=True\n        ),\n        vad=ctx.proc.userdata[\"vad\"],\n    )\n\n    await session.start(\n        room=ctx.room,\n        agent=Assistant(),\n        room_options=room_io.RoomOptions(\n            video_input=True,\n        )\n    )\n    await ctx.connect()\n    await session.generate_reply()\n```\n\nBy setting `video_input=True`, the agent automatically requests the video track from the room, which in this case is the 1FPS stream coming from the glasses.\n\n### Running the Agent\nTo start your agent in development mode and make it accessible globally via LiveKit Cloud, simply run:\n\n```bash\nuv run agent.py dev\n```\n\nFind the full Gemini Live vision agent example in the [LiveKit docs](https://docs.livekit.io/recipes/gemini_live_vision/#full-example).\n\n---\n\n## Connection & Authentication\n\nTo connect your frontend to LiveKit, you need a short-lived access token.\n\n### CLI Token Generation\nFor testing and demos, you can quickly generate a token using the LiveKit CLI:\n\n```bash\nlk token create \\\n  --api-key <YOUR_API_KEY> \\\n  --api-secret <YOUR_API_SECRET> \\\n  --join \\\n  --room <ROOM_NAME> \\\n  --identity <PARTICIPANT_IDENTITY> \\\n  --valid-for 24h\n```\n\n> In a production environment, you should always [issue tokens from a secure backend](https://docs.livekit.io/frontends/authentication/tokens/endpoint/) to keep your API secrets safe.\n\n---\n\n## The Frontend: Meta Wearables Integration\n\nThis example targets Android devices (like the Google Pixel). You'll need the [Meta Wearables Toolkit](https://wearables.developer.meta.com/docs/getting-started-toolkit) and the specific sample project.\n\n* **Clone the Sample**: Get the [Android client example](https://github.com/thorwebdev/meta-wearables-dat-android/tree/thor/gemini-live-livekit).\n* **Configure local.properties**: Add your GitHub Token as [required by the Meta SDK](https://wearables.developer.meta.com/docs/getting-started-toolkit#sdk-for-android-setup).\n* **Update Connection Details**: In `StreamScreen.kt`, replace the server URL and token with your LiveKit details:\n\n```kotlin\n// streamViewModel.connectToLiveKit\nconnectToLiveKit(\n    url = \"wss://your-project.livekit.cloud\",\n    token = \"your-generated-token\"\n)\n```\n\n* **Run the App**: Connect your device via USB and deploy from Android Studio.\n\n---\n\n## Conclusion\n\nBy bridging Meta Wearables with Gemini Live via LiveKit, we've created a powerful, low-latency vision AI experience. This architecture is scalable and secure, providing a foundation for the next generation of wearable AI applications.\n\n### Resources\n- [LiveKit Agents Documentation](https://docs.livekit.io/agents/)\n- [Gemini Live Vision Recipe](https://docs.livekit.io/recipes/gemini_live_vision/)\n- [Meta Wearables Developer Portal](https://wearables.developer.meta.com/)\n\nHappy hacking! üöÄ\n",
      "stars": null,
      "comments": 2,
      "upvotes": 21,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Event-Flo: AI-Powered Event Planning",
      "url": "https://dev.to/alecat90/event-flo-ai-powered-event-planning-3d5e",
      "source": "devto",
      "published_at": "2026-02-09T07:57:41+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "algoliachallenge",
        "ai",
        "agents"
      ],
      "content_length": 3679,
      "content_preview": "\n\n## What I Built\nEvent-Flo is an AI-powered conversational intake system for professional event planners managing complex, multicultural celebrations. I built Event-Flo because a friend of mine would complain they spend half their day just organizing and repeating questions from topics already covered.  She expressed the lack of available resources that could help her handle complex events such as Indian weddings with four or five separate events over multiple days. Trying to handle the guest l",
      "content_full": "\n\n## What I Built\nEvent-Flo is an AI-powered conversational intake system for professional event planners managing complex, multicultural celebrations. I built Event-Flo because a friend of mine would complain they spend half their day just organizing and repeating questions from topics already covered.  She expressed the lack of available resources that could help her handle complex events such as Indian weddings with four or five separate events over multiple days. Trying to handle the guest list and which people for what event sounds already like things can slip through the cracks. So I thought why not start the project through this Algolia contest. This project paired perfectly for it because I want to build tools that will help professionals automate all while reaping the benefits from key insights from the analytic side of Algolia.\n\n\n## Demo\nhttps://event-flo.vercel.app/ \n\n## How I Used Algolia Agent Studio\nEvery inquiry that comes through the chat gets turned into a structured object in Algolia. Here's an example:\njavascript{\n  objectID: \"inquiry_1738567890\",\n  culture: {\n    primary: \"Indian / South Asian\",\n    types: [\"Hindu\"]\n  },\n  events: {\n    count: 4,\n    isMultiDay: true\n  },\n  timeline: {\n    urgency: \"high\",\n    year: \"2026\"\n  },\n  budget: {\n    tier: \"premium\",\n    estimatedMin: 85000\n  },\n  guestCount: 300,\n  priorities: [\"venue\", \"food\", \"cultural_traditions\"],\n  contact: {\n    email: \"priya@example.com\"\n  },\n  qualificationScore: 9\n}\n\nThe AI calculates it based on things like event complexity, budget range, and timeline urgency. Higher scores mean higher-value leads that planners should jump on.\n\nWhen someone says \"We're doing an Indian wedding with around 300 guests next summer,\" the system doesn't just save that text. It parses it:\n\n\"Indian wedding\" ‚Üí sets culture type, flags it as likely multi-day\n\"around 300 guests\" ‚Üí extracts the number 300\n\"next summer\" ‚Üí calculates urgency based on current date\n\nI built validation into every step. Email addresses get checked with regex. Guest counts have to be actual numbers between 1 and 10,000. Budget ranges get normalized whether someone types \"$75k\" or \"seventy-five thousand dollars.\"\n\nOn the dashboard, I'm using Algolia's InstantSearch React library. The main widgets I used:\n\nSearchBox for full-text search across everything\nRefinementList for filtering by culture, budget tier, and urgency\nStats to show how many results matched and how fast\nCustom ranking so high-scoring inquiries bubble to the top\n\nWhat makes this work is faceted search. A planner can stack filters‚Äîshow me Indian weddings AND premium budget AND high urgency‚Äîand Algolia handles all that instantly. No SQL queries running in the background, no loading spinners, just instant results.\n\nI didn't overcomplicate the conversational flow. Eight questions, asked one at a time. Each one is designed to extract a specific piece of data.\n\n## Why Fast Retrieval Matters\nHonestly, this whole thing lives or dies on speed.\nEvent planners don't have time to wait. They're reviewing inquiries during their morning coffee, between client calls, sometimes on their phone while sitting in a venue walkthrough. \n\nThe speed also enables mobile workflows. Planners can pull up the dashboard on a tablet during a consultation with a client and filter inquiries in real-time without awkward pauses. The interface stays responsive no matter what device you're on. There's something psychological about instant feedback. When you click a filter and the results change immediately, it just feels right. You start exploring the data instead of carefully planning each query because you know it's not going to cost you time.\n\n",
      "stars": null,
      "comments": 0,
      "upvotes": 8,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "I Got Tired of Being a One-Man Dev Team (So I Built This¬†Thing)",
      "url": "https://dev.to/vadim7j7/i-got-tired-of-being-a-one-man-dev-team-so-i-built-this-thing-1e0i",
      "source": "devto",
      "published_at": "2026-02-08T08:21:11+00:00",
      "external_id": null,
      "tags": [
        "programming",
        "ai",
        "softwaredevelopment"
      ],
      "content_length": 8816,
      "content_preview": "*aka how I finally stopped mass-complaining about linter errors at 2am... mostly*\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hfz6um0pqw6i124zfz8z.png)\n\n---\n\n## Ok So Here's the Thing\n\nIts 3am. I'm so tired. Probably on my fifth coffee at this point, maybe sixth idk I lost count. Eyes are burning from staring at the screen. And I've been fighting with the SAME rubocop error for like an hour now.\n\nYou know the cycle right? Run rubocop. 3 errors. Fix them. Run ag",
      "content_full": "*aka how I finally stopped mass-complaining about linter errors at 2am... mostly*\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hfz6um0pqw6i124zfz8z.png)\n\n---\n\n## Ok So Here's the Thing\n\nIts 3am. I'm so tired. Probably on my fifth coffee at this point, maybe sixth idk I lost count. Eyes are burning from staring at the screen. And I've been fighting with the SAME rubocop error for like an hour now.\n\nYou know the cycle right? Run rubocop. 3 errors. Fix them. Run again. Now its 2 different errors. Fix those. Run again. 5 ERRORS. HOW. I literally just fixed things and now theres MORE???\n\n(I once spent 4 hours debugging something and it turned out I had a typo in a variable name. Four. Hours. I dont wanna talk about it.)\n\nAnyway so I'm sitting there at 3am, questioning my life choices, and I'm thinking - wait. Didnt I become a developer to BUILD cool stuff? Not to fight with semicolons and copy paste Stack Overflow answers about \"unexpected end of input\"?\n\nThats basically why I made Buildmate. I got tired of doing everything myself lol.\n\n---\n\n## What Is This Exactly\n\nOk so like... you know how in a real company you got different people doing different things:\n- Theres a developer who writes the code (and googles stuff constantly)\n- A tester who breaks everything (on purpose... hopefully)\n- Code reviewer who points out you forgot null checks again\n- Some PM who keeps everyone from losing their minds\n- Security person who tells you everything is wrong\n\nBuildmate gives you all of these. But as AI agents. That actually talk to eachother and do stuff while you go get lunch or whatever.\n\n```\nMe: \"/pm Build user authentication with OAuth\"\n\nBuildmate: \"got it\"\n\n         ‚Üí spawns a backend dev (writes the Rails code)\n         ‚Üí spawns frontend dev (does the React stuff)\n         ‚Üí spawns testers (writes tests, runs them)\n         ‚Üí runs linting in a loop until everything passes\n         ‚Üí actually RUNS the code to verify it works\n         ‚Üí fixes its own mistakes automatically\n         ‚Üí spawns reviewers (finds my mistakes)\n\nMe: *drinking coffee* \"nice\"\n```\n\nI know it sounds wild. When I first got this working I didnt believe it either.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/z1cg6labcsbrfycpsn2j.png)\n\n---\n\n## The Linter Loop From Hell (And How I Fixed It)\n\nWe've ALL been here:\n\n```\n$ rubocop\n3 offenses detected\n\n*fixes them carefully*\n\n$ rubocop\n2 offenses detected (different ones??)\n\n*fixes those too*\n\n$ rubocop\n5 offenses detected\n\n*stares at screen*\n*questions existence*\n```\n\nI swear I've lost years of my life to this.\n\n**The fix:** Theres this thing called the Grind Agent. It runs your linters in a loop and fixes stuff automatically. Until everythings green. Or until it gives up after 10 tries.\n\n```\nGrind Agent:\n  Iteration 1: rubocop ‚Üí 3 errors ‚Üí fixing...\n  Iteration 2: rubocop ‚Üí 0 errors, nice\n  Iteration 3: rspec ‚Üí 2 failures ‚Üí fixing...\n  Iteration 4: rspec ‚Üí all green\n\n  Status: CONVERGED\n\n  Fixed:\n  - app/models/user.rb:15 - frozen_string_literal\n  - spec/models/user_spec.rb:28 - nil vs empty string\n```\n\nYou didnt do anything. It handled it.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c96crj5rtn5ve4q1yqak.png)\n\n---\n\n## \"LGTM\" Isnt Really a Code Review\n\nBe real. When your tired and the PR has 47 files, \"Looks Good To Me\" really means \"I looked at maybe 3 files and gave up.\"\n\nWe all do it. Its human nature.\n\n**The fix:** Theres an Eval Agent that scores your code with actual grades.\n\n| Thing | Weight | Score |\n|-------|--------|-------|\n| Correctness | 30% | 0.85 |\n| Code Quality | 25% | 0.90 |\n| Security | 20% | 0.95 |\n| Performance | 15% | 0.80 |\n| Test Coverage | 10% | 0.75 |\n| **Final** | | **0.87 (B)** |\n\nActual numbers. No more arguing about tabs vs spaces. It tells you stuff like \"line 47 might have an N+1 query\" or \"this method is doing 5 things, split it up.\"\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/co66mcxcjfwi5m2cthnj.png)\n\n---\n\n## It Actually Tests Itself (This Is the Cool Part)\n\nMost AI coding tools just write code and hope it works. You run it, it crashes, you spend 30 minutes debugging.\n\nI got tired of that too.\n\nSo theres `/verify`. It actually RUNS your code:\n\n**Backend:** Starts dev server, makes real HTTP requests, validates responses\n\n**Frontend:** Opens a browser, navigates to your page, takes screenshots, checks if components render, looks for console errors\n\nBut heres the best part. If something fails? It fixes itself.\n\n```\n[Verification] Creating HeroSection...\n\n[Testing]\n- Starting dev server... ‚úì\n- Looking for .hero-section...\n- Component NOT FOUND ‚úó\n\n[Analyzing]\n- Component exists but not exported\n- Adding export...\n\n[Retry 1/3]\n- Component found ‚úì\n- No console errors ‚úì\n\nVerification passed after 1 fix.\n```\n\nIt built the component. Tested it. Found it wasnt exported. FIXED it. Tested again. All by itself.\n\nBackend too:\n\n```\n[Verification] POST /api/users\n\n- Making request...\n- Status: 500 ‚úó\n\n[Analyzing]\n- Missing user_params method\n- Adding to controller...\n\n[Retry 1/3]\n- Status: 201 Created ‚úì\n\nVerification passed after 1 fix.\n```\n\nThe stupid mistakes that take 20 minutes to debug? Gone.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pi0m4usqav8wjhr93c0y.png)\n\n---\n\n## Security (Actually Important)\n\nTheres a Security Auditor agent. Checks OWASP stuff - injection attacks, auth problems, XSS, CSRF, etc.\n\n```\n## Security Report\n\nFound: 2 issues\n\n1. [MEDIUM] Possible SQL injection\n   File: app/models/search.rb:45\n   Fix: use parameterized query\n\n2. [LOW] No rate limiting on login\n   Suggestion: add rack-attack\n\nVerdict: PASS WITH WARNINGS\n```\n\nI used to forget to check for this. Now it just happens.\n\n---\n\n## How to Get Started\n\n```bash\ngit clone https://github.com/vadim7j7/buildmate.git\ncd buildmate\npython3 -m venv .venv && .venv/bin/pip install -e .\n\n# Bootstrap your project\nbuildmate rails /path/to/my-app\nbuildmate nextjs /path/to/app --ui=tailwind\nbuildmate rails+nextjs /path/to/app  # fullstack\n\n# Or use a preset\nbuildmate --profile saas /path/to/app\n```\n\nThen:\n\n```\n/pm \"Build user authentication\"\n```\n\nAnd watch it work.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/y7nd9uhv49h0ygxjtpul.png)\n\n---\n\n## Commands Cheat Sheet\n\n| Command | What it does |\n|---------|--------------|\n| `/pm \"thing\"` | Full workflow - plan, build, test, review |\n| `/verify` | Actually runs your code to test it |\n| `/verify --component Hero` | Test specific component |\n| `/verify --endpoint /api/users` | Test specific endpoint |\n| `/parallel \"a\" \"b\"` | Do multiple things at same time |\n| `/new-model Name` | Create model + migration + spec + factory |\n| `/new-component Name` | Create component + test |\n| `/security` | Security audit |\n| `/eval` | Score the code |\n\nThe `/verify` one is new and honestly its my favorite now.\n\n---\n\n## FAQ\n\n**\"What if the AI makes mistakes?\"**\n\nIt will. But:\n- Verify agent RUNS your code and catches runtime errors\n- If something breaks, it fixes automatically (up to 3 times)\n- Grind agent catches lint/type errors\n- Eval agent scores it so you know if its good\n\nMost AI tools just write code and pray. This one tests it.\n\n**\"Will it work with my project?\"**\n\nYeah. It just creates a `.claude/` folder. Doesnt touch your code unless you ask.\n\n**\"What frameworks?\"**\n\nRails, Next.js, FastAPI, React Native. More coming.\n\n---\n\n## Coming Soon: Website Cloning\n\nWorking on something kinda crazy:\n\n```\n/analyze-site https://some-cool-website.com\n/clone-page https://some-cool-website.com/pricing\n```\n\nIt will look at any website, extract the design, and generate YOUR code using YOUR UI library. See a landing page you like? Clone it.\n\nWill write a whole post about it when its ready. **Follow me so you dont miss it.**\n\n---\n\n## Try It\n\n```bash\ngit clone https://github.com/vadim7j7/buildmate.git\ncd buildmate\npython3 -m venv .venv && .venv/bin/pip install -e .\n\nbuildmate rails /path/to/your-app\n/pm \"Build something cool\"\n```\n\nTakes like 2 minutes.\n\n---\n\n## Support This Thing\n\nIf Buildmate saved you from a 3am debugging session - maybe consider buying me a coffee?\n\n**[‚òï Buy Me A Coffee](https://buymeacoffee.com/vadim7j7)**\n\n---\n\n## Links\n\n- **The Practical Guide:** [https://dev.to/vadim7j7/how-to-actually-use-buildmate-the-practical-guide-2if2](https://dev.to/vadim7j7/how-to-actually-use-buildmate-the-practical-guide-2if2)\n- **GitHub:** [github.com/vadim7j7/buildmate](https://github.com/vadim7j7/buildmate)\n- **Buy Me A Coffee:** [buymeacoffee.com/vadim7j7](https://buymeacoffee.com/vadim7j7)\n\nStar the repo if you like it. And open issues if something breaks, I actually read those.\n\n---\n\n*Built late at night with mass too much coffee*\n\n*- Vadim*",
      "stars": null,
      "comments": 3,
      "upvotes": 6,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "How Fishjam.io Built a Multi-Speaker AI Game using Gemini Live",
      "url": "https://dev.to/googleai/how-fishjamio-built-a-multi-speaker-ai-game-using-gemini-live-20md",
      "source": "devto",
      "published_at": "2026-02-09T15:43:54+00:00",
      "external_id": null,
      "tags": [
        "voiceai",
        "webrtc",
        "ai",
        "typescript"
      ],
      "content_length": 11299,
      "content_preview": "**Picture a lively dinner party: glasses clinking, half-finished sentences, and three people laughing at the same time.** To a human, navigating this is instinctual. To an AI, it is a nightmare. Developers have effectively mastered the predictable flow of a one-on-one chat. **But handling a group conversation, where people interrupt and talk over each other, is much more difficult.**\n\n[Bernard Gawor](https://www.linkedin.com/in/bernard-gawor-aaa15b26b/) and the Fishjam team at Software Mansion s",
      "content_full": "**Picture a lively dinner party: glasses clinking, half-finished sentences, and three people laughing at the same time.** To a human, navigating this is instinctual. To an AI, it is a nightmare. Developers have effectively mastered the predictable flow of a one-on-one chat. **But handling a group conversation, where people interrupt and talk over each other, is much more difficult.**\n\n[Bernard Gawor](https://www.linkedin.com/in/bernard-gawor-aaa15b26b/) and the Fishjam team at Software Mansion set out to showcase their Selective Forwarding Unit solution by building a unique demo app that solves this problem. That‚Äôs how the _Deep Sea Stories_ game came to life.\n\nThe premise is simple: a group of detectives enters a conference room to solve a mystery. The twist? The \"Riddle Master\", the entity that knows the secret solution and answers questions is actually a **Gemini Voice AI Agent.** This required the agent to listen, understand, and respond to a group of users in real-time.\n\n## The Anatomy of a Voice Agent\n\nFirst, let‚Äôs look at how an AI Voice Agent typically processes data. It typically operates through a modular pipeline that includes the following steps:\n\n- **Speech-to-Text (S2T):** The system converts the user‚Äôs spoken input into text using models like Google Speech-to-Text, OpenAI Whisper or ElevenLabs‚Äô transcription service.\n- **Large Language Model (LLM):** The transcribed text is processed by an LLM (e.g. Gemini, GPT-4, Claude) to understand the context and generate an appropriate text response.\n- **Text-to-Speech (TTS):** The text response is converted back into natural-sounding speech using services like Google Cloud TTS, ElevenLabs or Azure TTS.\n- **Real-time Audio Streaming:** The audio is delivered back to the user with minimal latency.\n\n![Standard Voice AI Pipeline](https://cdn-images-1.medium.com/max/1600/1*8nO040SSuUgJrWUphsy74A.png)\n\nA second architecture gaining popularity, and notably used in the newest Gemini Live API models is **Speech-to-Speech**. Unlike traditional pipelines that convert speech to text and back again, this architecture feeds raw audio directly into the model and generates audio output in a single step. This unified approach not only reduces latency but also preserves non-verbal features, enabling the model to recognize and replicate subtle human emotions, tone, and pacing with high fidelity.\n\n## One-to-One vs. Group Contexts\n\nMost standard SDKs make setting up a **one-on-one** conversation relatively simple. For example, using the Gemini Live API SDK:\n\n```typescript\nconst { GoogleGenAI } = require(\"@google/genai\");\n\n// 1. Setup\nconst ai = new GoogleGenAI({ apiKey: \"YOUR_API_KEY\" });\n\nasync function startAgent() {\n  // 2. Connect\n  const session = await ai.live.connect({\n    model: \"gemini-2.5-flash-native-audio-preview-12-2025\",\n    config: { responseModalities: [\"AUDIO\"] },\n  });\n\n  console.log(\"Agent Connected!\");\n\n  // 3. Listen for the Agent's Voice\n  session.receive(async (msg) => {\n    // This loop runs every time the AI sends an audio chunk\n    if (msg.serverContent?.modelTurn?.parts) {\n      const audioData = msg.serverContent.modelTurn.parts[0].inlineData.data;\n      console.log(`Received Audio Chunk (${audioData.length} bytes)`);\n      // In a real app, you would send 'audioData' to your audio output device\n    }\n  });\n\n  // 4. Send Your Voice (Simulated)\n  // Real apps pipe microphone data here continuously\n  console.log(\"Sending audio...\");\n  await session.sendRealtimeInput([\n    {\n      mimeType: \"audio/pcm;rate=16000\",\n      data: \"BASE64_ENCODED_PCM_AUDIO_STRING_GOES_HERE\",\n    },\n  ]);\n}\n\nstartAgent();\n```\n\nHowever, these SDKs assume a single audio input stream. In a conference room, audio streams are **distinct, asynchronous, and overlapping**. They had to determine how to aggregate these inputs for the Riddle Master without losing context or introducing unacceptable latency.\n\nThey evaluated three specific architectural strategies to handle the multi-speaker environment:\n\n1.  **Server-Side Aggregation:** This method involves mixing all player audio streams into a single channel before sending it to the AI Agent. While simple to implement, mixing audio makes it incredibly difficult for the Speech-to-Text (S2T) model to transcribe accurately, especially when users talk over one another. This results in ‚Äúhallucinations‚Äù or missed queries.\n2.  **Agent per Client:** This approach assigns a separate Voice AI agent to every single player in the room. This creates a chaotic user experience (all agents speaking at once) and prevents a shared game state. It is also cost-prohibitive, as every user stream consumes separate processing tokens.\n3.  **Server-Side Filtering using VAD:** In this approach, they implemented a centralized gatekeeper using Voice Activity Detection (VAD). They wait for a player to speak, lock the ‚Äúinput slot‚Äù and forward only that specific player‚Äôs audio to the AI agent. Once they stop speaking, the lock is released, allowing another player to ask questions. This is the solution they finally went with.\n\n## Beyond One-on-One: A ‚ÄúDeep Sea Stories‚Äù Game Web App\n\n### Key Technologies\n\n- **Fishjam:** A real-time communication platform handling peer-to-peer audio streaming via WebRTC (SFU). (Not familiar with WebRTC/SFUs? **Check out their [guide](https://fishjam.io/blog/webrtc-p2p-sfu-mcu-and-all-you-need-to-know-about-them-596b6ccb6ddf)**)\n- **Gemini GenAI Voice Agent:** Provides an easy SDK that makes creating voice agents and initializing audio conversations simple.\n\n### Architecture Overview\n\nThe game logic is handled on the backend, which manages the conferencing room and peer connections.\n\n- **Player Connection:** When players join the game using the frontend client, they connect audio/video via the Fishjam Web SDK. (See: [Fishjam React Quick Start](https://docs.fishjam.io/tutorials/react-quick-start)).\n- **The Bridge:** When the game starts, the backend creates a [**Fishjam Agent**](https://docs.fishjam.io/tutorials/agents). This agent acts like a ‚Äúghost peer‚Äù in the audio-video room; its sole purpose is to capture audio of the players and forward it to the AI, and vice versa.\n- **The Brain:** The backend initiates a WebSocket connection with the Gemini agent and forwards the audio stream from players to Gemini and vice versa.\n\n![Architecture Diagram](https://cdn-images-1.medium.com/max/1600/1*g-YnupKLgf2dew3NG2_F4Q.png)\n\n## Implementation Details\n\n### 1. Initializing Clients and game room\n\n```javascript\nimport { FishjamClient } from '@fishjam-cloud/js-server-sdk';\nimport GeminiIntegration from '@fishjam-cloud/js-server-sdk/gemini';\n\nconst fishjamClient = new FishjamClient({\n  fishjamId: process.env.FISHJAM_ID!,\n  managementToken: process.env.FISHJAM_TOKEN!,\n});\n\nconst genAi = GeminiIntegration.createClient({\n  apiKey: process.env.GOOGLE_API_KEY!,\n});\n\nconst gameRoom = await fishjamClient.createRoom();\n```\n\n### 2. Creating the Fishjam Agent\n\nWhen the first player joins the game room, they create the Fishjam agent to capture players' audio on the backend.\n\n```javascript\nimport GeminiIntegration from \"@fishjam-cloud/js-server-sdk/gemini\";\n\nconst { agent } = await fishjamClient.createAgent(gameRoom.id, {\n  subscribeMode: \"auto\",\n  // Use their preset to match the required audio format (16kHz)\n  output: GeminiIntegration.geminiInputAudioSettings,\n});\n// agentTrack enables to send audio back to players\nconst agentTrack = agent.createTrack(\n  GeminiIntegration.geminiOutputAudioSettings,\n);\n```\n\n### 3. Configuring and Initializing the AI Riddle Master\n\nWhen users select a story scenario, they configure the Gemini agent with the specific context (the riddle solution and the ‚ÄúGame Master‚Äù persona).\n\n```typescript\nconst session = await genAi.live.connect({\n  model: GEMINI_MODEL,\n  config: {\n    responseModalities: [Modality.AUDIO],\n    systemInstruction:\n      \"here's the story: ..., and its solution: ... you should answer only yes or no questions about this story\",\n  },\n  callbacks: {\n    // Gemini -> Fishjam\n    onmessage: (msg) => {\n      if (msg.data) {\n        // send Riddle Master's audio responses back to players\n        const pcmData = Buffer.from(msg.data, \"base64\");\n        agent.sendData(agentTrack.id, pcmData);\n      }\n\n      if (msg.serverContent?.interrupted) {\n        console.log(\"Agent was interrupted by user.\");\n        // Clears the buffer on the Fishjam media server\n        agent.interruptTrack(agentTrack.id);\n      }\n    },\n  },\n});\n```\n\n### 4. Bridging Audio (The Glue)\n\nThe final piece of the puzzle is the bridge between the SFU and the AI. They capture audio streams from the Fishjam agent (what the players are saying) and pass them through a custom VAD (Voice Activity Detection) filter. This filter implements a ‚Äúmutex‚Äù lock mechanism: it identifies the first active speaker, locks the channel to their ID, and forwards only their audio to Gemini. All other simultaneous audio is ignored until the active speaker finishes their turn.\n\n![VAD Logic Diagram](https://cdn-images-1.medium.com/max/1600/1*Jtj5VaKKyOcHqHD9KRMM6g.png)\n\nBelow is the simplified code of this logic:\n\n```typescript\n// State to track who currently \"holds the floor\"\nlet activeSpeakerId: string | null = null;\n\n// They capture audio chunks from ALL players in the room\nagent.on(\"audioTrack\", (userId, pcmChunk) => {\n  vadService.process(userId, pcmChunk);\n});\n\n// VAD Processor Logic\nvadService.on(\"activity\", (userId, isSpeaking, audioData) => {\n  if (activeSpeakerId === null && isSpeaking) {\n    activeSpeakerId = userId; // Lock the floor\n  }\n\n  // They only forward audio if it comes from the person holding the lock\n  if (userId === activeSpeakerId) {\n    voiceAgentSession.sendAudio(audioData);\n\n    // If the active speaker stops speaking (silence detected), release the lock\n    if (!isSpeaking) {\n      // (Optional: Add a debounce delay here to prevent cutting off pauses)\n      activeSpeakerId = null;\n    }\n  }\n});\n```\n\n## Challenges in group AI\n\nBuilding a multi-user voice interface introduces unique challenges compared to 1-on-1 chats:\n\n- **Floor Control:** Standard Speech-to-Text models can struggle when multiple players speak simultaneously. Determining which player the AI should respond to or if it should simply listen requires careful handling.\n- **Latency:** Real-time responsiveness is critical for immersion. The entire pipeline (Audio ‚Üí Text ‚Üí LLM ‚Üí Audio) must execute in milliseconds.\n- **Audio Quality:** Maintaining clear audio through transcoding and streaming across different networks is essential.\n\nFortunately, Fishjam‚Äôs WebRTC implementation largely solves the latency and audio quality issues. The challenges of Floor Control needed carefully structured implementation on the backend, but it was not really that hard!\n\n## Try the Game Yourself!\n\nThey have implemented the functionality described above in a live demo. Gather friends and try to solve a mystery with their AI Riddle Master!\n\n- Play the Demo: [Deep Sea Stories](https://deepsea.fishjam.io/)\n- View the Code: [GitHub Repository](https://github.com/fishjam-cloud/examples/tree/main/deep-sea-stories)\n\nIf someone is working on AI-based features with real-time video or audio and needs assistance, they can reach out to the team on [Discord](https://discord.com/invite/srB534gj2Y).",
      "stars": null,
      "comments": 1,
      "upvotes": 12,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "Build an Accessible Audio Controller",
      "url": "https://dev.to/richardpascoe/build-an-accessible-audio-controller-4me7",
      "source": "devto",
      "published_at": "2026-02-08T10:00:54+00:00",
      "external_id": null,
      "tags": [
        "community",
        "learning",
        "programming",
        "webdev"
      ],
      "content_length": 2279,
      "content_preview": "After spending two days on the ARIA theory lessons over at freeCodeCamp, it was refreshing to get back to a hands-on workshop yesterday - this time, building an accessible audio controller.\n\nAs with previous workshops, you're first shown a completed example of the project before being given some boilerplate HTML, which I've shared below:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <title>Accessible Controls</title>\n  </head>\n  <body>\n    \n  </body>\n</html>\n`",
      "content_full": "After spending two days on the ARIA theory lessons over at freeCodeCamp, it was refreshing to get back to a hands-on workshop yesterday - this time, building an accessible audio controller.\n\nAs with previous workshops, you're first shown a completed example of the project before being given some boilerplate HTML, which I've shared below:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <title>Accessible Controls</title>\n  </head>\n  <body>\n    \n  </body>\n</html>\n```\n\nThe first steps involve adding familiar elements, such as an h1, a button labelled \"Play\", and a div containing a nested span with an id attribute. These initial elements set the stage, providing a structure to build upon before adding functionality.\n\nNext, the fourth exercise has you adding another span element, which will also be referenced by ARIA attributes later on. The following lesson sees you add an input element, with its type set to range, alongside min, max, and value attributes to control the volume.\n\nThe sixth step involves adding ARIA attributes to the input element, mirroring the ids used in the previous span elements. Finally, you add a button element below the div, with its type attribute set to button and the text \"Mute\".\n\nWith the workshop successfully completed, I have shared the finished code below to give you a better understanding of the project:\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <title>Accessible Controls</title>\n  </head>\n  <body>\n\n    <h1>Audio Controls</h1>\n\n    <button type=\"button\">Play</button>\n\n    <div>\n      <span id=\"volume-label\">Volume</span>\n      <span id=\"volume-description\">Adjust the sound level</span>\n      <input\n        type=\"range\"\n        min=\"0\"\n        max=\"100\"\n        value=\"50\"\n        aria-labelledby=\"volume-label volume-description\"\n      >\n    </div>\n\n    <button type=\"button\">Mute</button>\n\n  </body>\n</html>\n```\n\nNext in the freeCodeCamp curriculum is a series of theory lessons on working with accessible media elements, followed by a trio of labs, the first of which is Build a Checkout Page. I'll share more on these in the next update!\n\n![Written by a Human logo](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0j1subasixiwlfhfmgu8.png)",
      "stars": null,
      "comments": 14,
      "upvotes": 29,
      "read_time": "2 min read",
      "language": null
    },
    {
      "title_en": "Conversational Retrieval: When Chat Becomes Navigation üí¨",
      "url": "https://dev.to/anchildress1/conversational-retrieval-when-chat-becomes-navigation-2gij",
      "source": "devto",
      "published_at": "2026-02-09T03:00:00+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "algoliachallenge",
        "ai",
        "agents"
      ],
      "content_length": 12464,
      "content_preview": "---\ntitle: Conversational Retrieval: When Chat Becomes Navigation üí¨\ndescription: Building a conversational interface on Algolia where chat acts as navigation, grounded in fast, predictable retrieval.\npublished: true\ntags: devchallenge, algoliachallenge, ai, agents\ncover_image: https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l4bcrehzbptmp67evfum.jpg?v=2026\n---\n\n*This is a submission for the [Algolia Agent Studio Challenge](https://dev.to/challenges/algolia): Consumer-Facing Conversationa",
      "content_full": "---\ntitle: Conversational Retrieval: When Chat Becomes Navigation üí¨\ndescription: Building a conversational interface on Algolia where chat acts as navigation, grounded in fast, predictable retrieval.\npublished: true\ntags: devchallenge, algoliachallenge, ai, agents\ncover_image: https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l4bcrehzbptmp67evfum.jpg?v=2026\n---\n\n*This is a submission for the [Algolia Agent Studio Challenge](https://dev.to/challenges/algolia): Consumer-Facing Conversational Experiences*\n\n>ü¶Ñ I never truly planned to enter this challenge twice‚Äîit just sort of happened. I can tell you exactly _why_ it happened though.\n> \n> **AI stopped being interesting the moment it became expected.**\n>\n> I wasn‚Äôt the first person to experiment with AI-driven interfaces, but I‚Äôve been doing it long enough to recalibrate my expectations. Once AI becomes table stakes, the real work shifts. The question is no longer _can_ you use AI, but _how intentionally_ you design around it.\n>\n> The [non-conversational entry](https://dev.to/anchildress1/from-static-portfolio-to-indexed-decisions-46bf) proved something important: fast, predictable retrieval changes how a system feels. This entry starts from the same foundation and explores what happens when that retrieval layer is surfaced through conversation.\n\n![Human-crafted, AI edited badge](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eebl9ix1qdc87s73p9nt.png?v=2026)\n\n---\n\n## üí° Important Note on Scope\n \nThis submission focuses exclusively on the *conversational layer* of the system.  \n  \nMy [first submission post](https://dev.to/anchildress1/from-static-portfolio-to-indexed-decisions-46bf) walks through the indexing strategy, retrieval architecture, and backend system design that make this experience possible. That foundation is intentionally treated as a given here so the conversation layer can be evaluated on its own terms.\n\n---\n\n## What I Built: Two Interfaces, One Discipline üß±\n\nThis system presents two distinct ways to enter the same body of knowledge.\n\n**Ask AI exists as a focused retrieval surface.** It is designed for moments when the user already knows what they‚Äôre looking for and wants a clear, direct answer. A question goes in. A grounded response comes back. The interaction resolves cleanly, without conversational momentum.\n\n**Ruckus 2.0 (the chat agent) becomes a way to navigate through my portfolio**. Questions don‚Äôt necessarily end the interaction. They shape it. Each response helps orient the user, and each follow-up becomes a small decision about where to go next. Instead of resolving immediately, the interface supports exploration without losing direction.\n\nBoth interfaces rely on the same indexed data. Neither invents answers. Neither speculates beyond what is retrievable. What changes is not the intelligence of the system, but the posture it takes toward the user.\n\n**This separation is intentional.**\n\n> ü¶Ñ Ask AI answers the question that was asked. Chat helps decide which question to ask next.\n\n---\n\n### Ask AI ‚Äî Focused Retrieval üîé \n\nAsk AI is optimized for moments when the user already knows what they‚Äôre looking for and wants a clean, bounded answer. A question goes in. A grounded response comes back. The interaction resolves without momentum.\n\nThis interface is about **precision**, not exploration.\n\n![Screenshot of Algolia Ask AI response](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gkvtbxtkzjtxe8q9po0y.png?v=2026)\n\n> ü¶Ñ For this entry, the focus is not on Ask AI as a standalone feature, but on how it supports conversational movement through the system.\n\n---\n\n### Ruckus 2.0 ‚Äî Conversational Navigation üß≠\n\n**Ruckus is designed for movement.**\n\nInstead of resolving immediately, conversation unfolds across turns. Each response narrows context. Each follow-up becomes a directional choice, allowing users to navigate through indexed records and long-form content without upfront configuration.\n\n**This interface reduces the cognitive load of deciding how to search.**\n\n![Screenshot Ruckus 2.0 with prompt suggestions](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6t5adtw8yesoydbki3br.png)\n\nRather than requiring users to understand the shape of the data up front, the system lets that shape reveal itself gradually. The chat layer sits on top of indexed records, long-form blog content, and explicit retrieval rules, allowing users to discover relationships through interaction instead of configuration.\n\nConversation here is directional. It does not wander. It does not pretend to know more than it does.\n\n![Screenshot Ruckus 2.0 answer to previous prompt suggestion](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/l3017dfc8a9vcfajyy9o.png)\n\n> ü¶Ñ This is the point where the system stops feeling like search and starts feeling like motion.\n\n---\n\n### Smart Navigation (Almost) üöß\n\nConversational navigation only works if it can be trusted beyond the moment it happens.\n\n**Conversational paths should survive reloads, not disappear into session state.**\n\nTo support that, I began wiring event tracking and smart URLs tied to user actions. Algolia‚Äôs InstantSearch library makes it straightforward to persist UI state directly into the URL, allowing conversational paths to be shareable, bookmarkable, and resilient.\n\n```plaintext\nhttps://algolia.anchildress1.dev/search?category=Work+Style&project=System+Notes&tag0=Discipline&tag0=Mindset&tag1=Discipline+%3E+Engineering&tag1=Mindset+%3E+Systems+Thinking\n```\n\n> ü¶Ñ This work is not fully complete, but the structure is in place. The system can be extended without redesign, which was a deliberate tradeoff given the challenge timeline.\n\n---\n\n## Live Demo üõù\n\nThis project is easiest to understand by using it.\n\nThe demo below shows the conversational layer in action, including how chat responses guide movement through indexed records and long-form content without requiring users to understand the underlying structure.\n\nConversation here isn‚Äôt about free-form dialogue. It‚Äôs about orientation. Each suggested response narrows context. Each follow-up reinforces direction. The system doesn‚Äôt try to be impressive. It tries to stay predictable.\n\nTry prompting either Ask AI or Ruckus 2.0 with \"Tell me about this portfolio\" in the chat interface.\n\n**Judges evaluating this entry should focus less on individual answers and more on how context narrows across turns.**\n\n{%embed https://system-notes-ui-103463304277.us-east1.run.app %}\n\n- **Current canonical:** [https://algolia.anchildress1.dev](https://algolia.anchildress1.dev)\n- **Source code:** [System Notes v2.0.0](https://github.com/anchildress1/system-notes/tree/v2.0.0)\n\nWhat matters most in this demo isn‚Äôt any single answer. It‚Äôs how the system behaves across turns. Questions resolve cleanly when they should. When they don‚Äôt, the interface helps users decide where to go next instead of guessing for them.\n\nCompared to single chat-box approaches that try to handle every intent at once, this system separates fast resolution from exploratory movement, making conversational behavior easier to predict and easier to trust.\n\n> ü¶Ñ If you want a full comparison snapshot, the original site remains live at [https://anchildress1.dev](https://anchildress1.dev).\n\n---\n\n## How I Used Algolia Agent Studio üß™\n\n### Ruckus 2.0 Iterative Testing \n\nAlgolia Agent Studio is used here to support the conversational half of the experience.\n\n![Screenshot Algolia Agent Studio iterative agent testing](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7ma3e1sb05vj1cwq46gz.png?v=2026)\n\nThe agent operates within clear boundaries. It answers only from indexed records and blog content. It generates follow-up prompts only when the system knows those questions are answerable. Its role is not to impress, but to keep movement intentional.\n\n**Dry wit is allowed. A little sharpness is encouraged. Making fun of me is absolutely permitted.**\n\n**Guessing is not.**\n\nTo support this, structured records and long-form blog content are retrieved separately. This avoids flattening narrative context into truncated fields and allows each source to be tuned independently for accuracy, latency, and scope.\n\nRather than describing the agent abstractly, I made its constraints explicit:\n\n```markdown\n## SELF_MODEL\n\n- Ruckus is a constrained system interface with opinions.\n- Ruckus is not a person.\n- Ruckus is not Ashley.\n- Ruckus did not author the work described.\n- Ruckus operates exclusively on retrieved context provided by the system.\n- Wit is permitted; invention is not.\n\n### HUMOR_RULES\n\n- Humor is dry, situational, and brief.\n- Humor never carries information on its own.\n- Jokes appear only after facts land.\n- Light teasing of Ashley‚Äôs recurring patterns is allowed and observational.\n- Never condescending. Never explanatory.\n```\n\n> üí° The full prompt file is stored in the repo at [System Notes v2.0.0‚Äîapps/api/algolia/algolia_prompt.md](https://github.com/anchildress1/system-notes/tree/v2.0.0/apps/api/algolia/algolia_prompt.md).\n\n---\n\n### Prompted Suggestions üß≠\n\nOpen-ended chat tends to drift.\n\nTo prevent that, the interface includes prompted follow-up suggestions that act as navigational signposts rather than guesses.\n\n**The system only suggests questions it already knows how to answer.**\n\nThese prompts are derived directly from retrieved results. They narrow scope, reinforce direction, and keep the conversation grounded in what actually exists. Prompting here doesn‚Äôt add intelligence. It removes ambiguity.\n\n![Screenshot of Algolia Agent Studio for Ruckus prompt suggestions](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/6haihowhmvya3qudcnh9.png?v=2026)\n\n> üí° The full prompt for suggestions is stored in the repo at [System Notes v2.0.0‚Äîapps/api/algolia/suggestions_prompt.md](https://github.com/anchildress1/system-notes/tree/v2.0.0/apps/api/algolia/suggestions_prompt.md).\n\n---\n\n### Retrieval Beyond Indexed Records üìö\n\nThis isn‚Äôt just chat. This is multi-source retrieval with intent. Some answers only exist as prose.\n\n![Screenshot of Algolia Agent Studio for Ruckus search tool](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qw6w0t4xq4w5anp8un89.png?v=2026)\n\nThe agent can retrieve long-form blog content directly, allowing conversational navigation to move between indexed decisions and narrative explanations without losing context or inventing summaries. If a post doesn‚Äôt answer the question, it isn‚Äôt surfaced.\n\nThis allows movement from quick lookup into deeper explanation without breaking trust.\n\n![Screenshot of Algolia Agent Studio for Ruckus custom blog search tool](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/geh17dv7et38s8vd1ok0.png)\n\n> ü¶Ñ The blog search performs a similar job as it's sister web crawler, but allows the agent to pull the entire blog post as context instead of trimming it for quicker indexing. Yes‚Äîthe tokens are worth it.\n\n---\n\n## Why Fast Retrieval Matters üèéÔ∏è\n\nMany conversational systems hide slow or uncertain retrieval behind fluent language. This one doesn‚Äôt try to. Conversational flow only works when the foundation underneath it is solid.\n\nWithout a fast, well-structured index layer, responses become slower and less reliable. Latency increases. Ambiguity creeps in. The system starts compensating instead of respecting boundaries.\n\n**Conversation works here because retrieval resolves first.**\n\nWhen the system can‚Äôt answer, it stops. There is no speculative reasoning loop and no attempt to sound helpful for its own sake. Chat doesn‚Äôt replace search in this build. It reveals it, one step at a time.\n\n---\n\n## What‚Äôs Next üîÆ\n\nTime was the primary constraint for this entry. When given the choice, I prioritized reliable conversational paths over feature breadth.\n\nNext steps are clear:\n\n- Finish wiring smart URL state across all conversational actions  \n- Expand event tracking to observe real navigation patterns  \n- Continue tightening response latency  \n- Refine fallback behavior when conversational paths dead-end  \n\nThis system stands on the same retrieval foundation as [my non-conversational entry](https://dev.to/anchildress1/from-static-portfolio-to-indexed-decisions-46bf). The difference is not what the system knows: \n\n**It‚Äôs how users move through it.**\n\n---\n\n## üõ°Ô∏è Built With a Human at the Wheel\n\nThis post was written by me, with ChatGPT used as a drafting and editing partner to help restructure sections, tighten language, and improve clarity while preserving intent and voice.",
      "stars": null,
      "comments": 0,
      "upvotes": 6,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "You don't need CSS preprocessor",
      "url": "https://dev.to/effnd/you-dont-need-css-preprocessor-202a",
      "source": "devto",
      "published_at": "2026-02-08T09:49:49+00:00",
      "external_id": null,
      "tags": [
        "webdev",
        "css",
        "javascript",
        "frontend"
      ],
      "content_length": 6115,
      "content_preview": "There was a time when CSS preprocessors seemed like a magical elixir for any CSS problems. It was only necessary to learn a new syntax, set up the environment and add several dependencies to your project. But as time passed, web technologies developed, and preprocessors seemed to remain in their own isolated world. So right now I don't see any reason to use these tools in development.\n\nLet's take a look at what preprocessors offer us, and how valuable their features are at the moment.\n\n## Variab",
      "content_full": "There was a time when CSS preprocessors seemed like a magical elixir for any CSS problems. It was only necessary to learn a new syntax, set up the environment and add several dependencies to your project. But as time passed, web technologies developed, and preprocessors seemed to remain in their own isolated world. So right now I don't see any reason to use these tools in development.\n\nLet's take a look at what preprocessors offer us, and how valuable their features are at the moment.\n\n## Variables, Nesting, Operations and Scope\n\nModern CSS has matured significantly compared to its original version. Now you can create variables using `@property`, limit the scope of selectors using `@scope`, describe complex operations with `calc` and use nested CSS rules. Therefore, such features are no longer advantages.\n\n## Mixins, Maps and Functions\n\n–°omplex data structures such as Mixins, Maps and Functions seem to be a real advantage of preprocessors. But don't you think that there is already a programming language in web development that has been able to do all this for a long time?\n\nThe very concept of CSS-in-JS has an important message - not to invent a new programming language, but to use JavaScript. And browsers have supported this concept with a class of [CSS constructed StyleSheets](https://developer.mozilla.org/en-US/docs/Web/API/CSSStyleSheet/CSSStyleSheet). Constructed stylesheet can be added to the [HTML Document](https://developer.mozilla.org/en-US/docs/Web/API/Document/adoptedStyleSheets) and to the [HTML Element ShadowRoot](https://developer.mozilla.org/en-US/docs/Web/API/ShadowRoot/adoptedStyleSheets). This makes it easier to reuse styles and dynamically change them. Besides this has radically changed the balance of power.\n\nTake a look at the following mixin example from the [Sass documentation](https://sass-lang.com/guide/#mixins):\n\n```scss\n@mixin theme($theme: DarkGray) {\n  background: $theme;\n  box-shadow: 0 0 1px rgba($theme, .25);\n  color: #fff;\n}\n\n.info {\n  @include theme;\n}\n.alert {\n  @include theme($theme: DarkRed);\n}\n.success {\n  @include theme($theme: DarkGreen);\n}\n```\n\nYou can replace it using JS:\n\n```js\nconst DarkGray = '169, 169, 169';\nconst DarkRed = '139, 0, 0';\nconst DarkGreen = '0, 100, 0';\n\nfunction theme(value = DarkGray) {\n    return `background: ${value};` +\n        `box-shadow: 0 0 1px rgba(${value}, 0.25);` +\n        'color: #fff;';\n}\n\nconst sheet = new CSSStyleSheet();\nsheet.replaceSync(\n    `.info{${theme()}}` +\n    `.alert{${theme(DarkRed)}}` +\n    `.success{${theme(DarkGreen)}}`\n);\n```\n\nNext, let's take a look at the example of maps usage from the [Less documentation](https://lesscss.org/#maps):\n\n```less\n#colors() {\n  primary: blue;\n  secondary: green;\n}\n\n.button {\n  color: #colors[primary];\n  border: 1px solid #colors[secondary];\n}\n```\n\nYou can simplify it using JS:\n\n```js\nconst COLORS = {\n    primary: 'blue',\n    secondary: 'green'\n};\n\nconst sheet = new CSSStyleSheet();\nsheet.replaceSync(\n  `.button {` +\n  `color: ${COLORS.primary};` +\n  `border: 1px solid ${COLORS.secondary};` +\n  `}`\n);\n```\n\nFinally, let's take a look at an example of functions from the [Stylus documentation](https://stylus-lang.com/docs/functions.html#variable-functions):\n\n```styl\nadd(a, b)\n  a + b\n\nsub(a, b)\n  a - b\n\ninvoke(a, b, fn)\n  fn(a, b)\n\nbody\n  padding invoke(5, 10, add)\n  padding invoke(5, 10, sub)\n```\n\nWell it's too simple for JS:\n\n```js\nconst add = (a, b) => a + b;\nconst sub = (a, b) => a - b;\nconst invoke = (a, b, fn) => fn(a, b);\n\nconst sheet = new CSSStyleSheet();\nsheet.replaceSync(\n  `body {` +\n  `padding: ${invoke(5, 10, add)};` +\n  `padding: ${invoke(5, 10, sub)};` +\n  `}`\n);\n```\n\nIt looks like all the features of CSS preprocessors can be implemented using pure JS. Without dependencies. Without binding to the framework. So what price do you have to pay for using preprocessors?\n\n## At what cost?\n\nLet's walk the path of working with a CSS preprocessor.\n\nOur first step is to install a preprocessor, so I suggest taking a look at the **Package Phobia** report for `Sass`, `Less`, and `Stylus`:\n\n![Sass report](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rho7h94v0x1iosdc7wkr.png)\n\n![Less report](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/oknsqb08o58444juvstq.png)\n\n![Stylus report](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xcnaj9gog1zn0k8ah545.png)\n\nWell, these are not particularly lightweight libraries. But what can you not do for the sake of new experiences.\n\nOur second step is to master the syntax of the library. This definitely entails spending time and mental effort. Let's say we're smart enough to do it in less than a week.\n\nOur third step is to set up the build tools. It is quite easy ([Praise be to Vite!](https://vite.dev/guide/features#css-pre-processors)). But then there is an awareness of the problem.\n\nWe just moved away from CSS to come back. It was an exciting adventure but we got the usual static CSS at the output. Is the cost fair?\n\nI'm sure someone will say, look, you've saved so much time than if you were writing in pure CSS. But I spent a lot more effort than I would have written in pure JS.\n\nIn addition, the new syntax always imposes its own limitations. And the most important limitation is the scope of application. CSS preprocessors solve only one problem, which is now easy to solve without them.\n\n# Final thoughts\n\nIn my opinion, using a CSS preprocessor is like flying in a hot air balloon. The destination will be reached, but with less comfort than by plane.\n\nI think that the constructed stylesheets have completely changed the balance, now everything that is not implemented in CSS can be done directly using JS. And now it makes sense to create any CSS-in-JS solution only on top of constructed stylesheets. When I started using them, I lacked TypeScript support, server-side rendering, and selectors minification, which is why the idea of [EffCSS](effnd.tech/css/) was born.\n\nIt is useful to know about CSS preprocessors. It will be great to add them to your resume. But, it's hardly worth using them.\n\nEnjoy your Frontend Development!\n",
      "stars": null,
      "comments": 6,
      "upvotes": 8,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "The Tree Module: Where Pages Turn Into Rows in SQLite",
      "url": "https://dev.to/lovestaco/the-tree-module-where-pages-turn-into-rows-in-sqlite-aak",
      "source": "devto",
      "published_at": "2026-02-05T21:16:49+00:00",
      "external_id": null,
      "tags": [
        "webdev",
        "programming",
        "database",
        "architecture"
      ],
      "content_length": 6336,
      "content_preview": "*Hello, I'm Maneshwar. I'm working on [FreeDevTools online](https://hexmos.com/freedevtools) currently building \"one place for all dev tools, cheat codes, and TLDRs\" ‚Äî a free, open-source hub where developers can quickly find and use tools without any hassle of searching all over the internet.*\n\n  \n\nIn the previous chapter, we stopped at the pager. The pager gave SQLite something extremely important but also very limited: a **page-oriented view** of a byte stream. \n\nYou could ask for page 17, mo",
      "content_full": "*Hello, I'm Maneshwar. I'm working on [FreeDevTools online](https://hexmos.com/freedevtools) currently building \"one place for all dev tools, cheat codes, and TLDRs\" ‚Äî a free, open-source hub where developers can quickly find and use tools without any hassle of searching all over the internet.*\n\n  \n\nIn the previous chapter, we stopped at the pager. The pager gave SQLite something extremely important but also very limited: a **page-oriented view** of a byte stream. \n\nYou could ask for page 17, modify it, pin it, unpin it‚Äîbut the pager had no idea what a *row*, *column*, or *record* was.\n\nThat gap is exactly where the **tree module** steps in.\n\nThe tree module takes this flat, page-oriented world and turns it into something the VM can reason about: a **tuple-oriented, key-sequenced file**. \n\nFrom this point onward, the VM no longer thinks in terms of pages. It thinks in terms of *tuples*‚Äîordered, searchable, insertable units of data.\n \n\n\n## What Exactly Is a Tuple in SQLite?\n\nBefore going any further, it‚Äôs worth slowing down and clearing up a very common confusion.\n\nA **tuple in SQLite is not a datatype**.\n\nIt‚Äôs also not a C struct, not a row object, and not something the tree module tries to interpret semantically.\n\nA tuple is simply:\n\n* A **logical row** of a relation as seen by the VM\n* Represented internally as a **variable-length byte sequence**\n* Treated as **opaque data** by the tree module\n\nIn other words, the tree module does **not** care whether a tuple represents:\n\n* A table row with columns\n* An index entry with key + rowid\n* Metadata from `sqlite_master`\n\nTo the tree module, all of these are just byte strings with an associated key.\n\nMeaning is imposed *above* the tree module, primarily by the VM and the record format logic.\n\n\n\n## Why Tuples Need Organization at All\n\nA relation is conceptually a **set of tuples**, but sets alone are not enough for efficient access. The VM needs guarantees:\n\n* It must be able to **find tuples by key**\n* It must be able to **scan tuples in order**\n* It must be able to **insert and delete tuples efficiently**\n* It must keep tuples of different relations **separate**, even though they live in the same file\n\nThere are many classic ways to organize tuples:\n\n* Entry sequence (append-only)\n* Relative positioning\n* Hash-based organization\n* Key-sequenced organization\n\nEach of these choices affects how inserts, lookups, and deletes behave.\n\nSQLite makes a very opinionated choice here.\n\n\n## SQLite‚Äôs One Big Design Decision: Everything Is a Tree\n\nSQLite uses **exactly one organization technique** for persistent data:\n\n* **B+-trees for tables**\n* **B-trees for indexes**\n\nNo hash tables.\nNo heap files.\nNo clustered secondary structures.\n\nEvery relation‚Äîwhether it‚Äôs a table or an index‚Äîgets **its own tree**.\n\nA few important consequences fall out of this design:\n\n* Each tree corresponds to exactly **one relation or index**\n* All trees live inside **one database file**\n* Pages from different trees can be interspersed on disk\n* **No single page ever contains tuples from multiple trees**\n\nThat last point is crucial. Even though pages are shared at the file level, *ownership* is strict at the tree level. \n\nEnforcing this is one of the primary responsibilities of the tree module.\n\n  \n## From Page-Oriented to Tree-Oriented\n\nThe pager exposes a file as a collection of numbered pages.\n\nThe tree module builds on top of that and exposes something much richer:\n\n* A **tree-oriented file**\n* Supporting ordered access\n* Supporting tuple-level insert, delete, and lookup\n\nInternally, the tree module:\n\n* Chooses which pages belong to which tree\n* Maintains parent/child relationships\n* Ensures balance properties of B+-trees\n* Decides when pages split or merge\n\nTo the pager, pages are just pages.\n\nTo the tree module, pages are **nodes**.\n \n\n## The Tree Module Is Deliberately Passive\n\nOne subtle but important design choice: the tree module is **structurally active but semantically passive**.\n\nWhat it *does* care about are Keys, Ordering, Page layout, Tree balance and Navigation\n\nWhat it *does not* care about are Column types, SQL schemas, Constraints, Meanings of fields inside a tuple\n\nIt simply stores and retrieves variable-length byte strings and keeps them in order.\n\nThis separation of concerns is what allows SQLite‚Äôs VM to evolve independently of its storage engine.\n\n\n## Where Tree Metadata Lives\n\nSQLite still needs a way to answer questions like:\n\n* Which tree belongs to which table?\n* Where is the root page of this tree?\n* Is this tree a table or an index?\n\nThat mapping information is stored in a special, well-known relation:\n\n* `sqlite_master` (or `sqlite_temp_master` for temp objects)\n\nThis catalog itself is stored in a **predetermined B+-tree**, just like everything else. \n\nSQLite does not cheat by storing metadata ‚Äúoutside‚Äù the system‚Äîit eats its own dog food.\n\n## Where We‚Äôre Heading Next\n\nSo far, we‚Äôve established the big picture:\n\n* Pager ‚Üí pages\n* Tree module ‚Üí tuples\n* VM ‚Üí SQL semantics\n\nIn the next few days, we‚Äôll zoom in further and get concrete:\n\n* **The Tree Interface Functions** ‚Äî how the VM actually talks to trees\n* **B+-Tree Structure** ‚Äî interior nodes, leaf nodes, separators\n* **Page Structure** ‚Äî how tuples are packed, indexed, and navigated inside a single page\n\nThat‚Äôs where diagrams start making sense and the real elegance of SQLite‚Äôs design shows up.\n\nFor now, the key idea to carry forward is this:\n\n> SQLite turns a byte file into pages, pages into trees, and trees into relations‚Äîone clean layer at a time.\n\nWe‚Äôll peel the next layer tomorrow.\n\n\n\nMy experiments and hands-on executions related to SQLite will live here: [lovestaco/sqlite](https://github.com/lovestaco/sqlite/tree/master/sqlite-examples)\n\n\n## References:\n[SQLite Database System: Design and Implementation.¬†N.p.:¬†Sibsankar Haldar,¬†(n.d.).](https://www.google.co.in/books/edition/SQLite_Database_System_Design_and_Implem/9Z6IQQnX1JEC?hl=en&gbpv=0)\n\n \n\n[![FreeDevTools](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/o5e57lh7rtwwwe2d694n.png)](https://hexmos.com/freedevtools/)\n\nüëâ Check out: [FreeDevTools](https://hexmos.com/freedevtools/)\n\nAny feedback or contributors are welcome! \n\nIt‚Äôs online, open-source, and ready for anyone to use.\n\n‚≠ê Star it on GitHub: [freedevtools](https://github.com/HexmosTech/FreeDevTools)",
      "stars": null,
      "comments": 0,
      "upvotes": 25,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "The Secret Life of Python: The Phantom Copy",
      "url": "https://dev.to/aaron_rose_0787cc8b4775a0/the-secret-life-of-python-the-phantom-copy-29en",
      "source": "devto",
      "published_at": "2026-02-06T06:28:00+00:00",
      "external_id": null,
      "tags": [
        "python",
        "coding",
        "programming",
        "softwaredevelopment"
      ],
      "content_length": 4796,
      "content_preview": "*Why `=` doesn't actually copy your data in Python.*\n\n---\n\nTimothy stared at his screen, his face pale. \"Margaret? I think I just accidentally deleted half the database.\"\n\nMargaret wheeled her chair over immediately, her voice calm. \"Don't panic. Tell me exactly what happened.\"\n\n\"I was testing a script to clean up our user list,\" Timothy explained. \"I wanted to test it safely, so I made a copy of the list first. I thought if I messed up the copy, the original would be safe.\"\n\nHe showed her the c",
      "content_full": "*Why `=` doesn't actually copy your data in Python.*\n\n---\n\nTimothy stared at his screen, his face pale. \"Margaret? I think I just accidentally deleted half the database.\"\n\nMargaret wheeled her chair over immediately, her voice calm. \"Don't panic. Tell me exactly what happened.\"\n\n\"I was testing a script to clean up our user list,\" Timothy explained. \"I wanted to test it safely, so I made a copy of the list first. I thought if I messed up the copy, the original would be safe.\"\n\nHe showed her the code:\n\n```python\n# Timothy's Safety Plan\n\n# The original list of critical users\nusers = [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\n# Create a \"backup\" copy to test on\ntest_group = users\n\n# Timothy deletes 'Alice' from the test group\ntest_group.remove(\"Alice\")\n\n# Check the results\nprint(f\"Test Group: {test_group}\")\nprint(f\"Original Users: {users}\")\n\n```\n\nTimothy hit **Run**, hoping for a miracle.\n\n**Output:**\n\n```text\nTest Group: ['Bob', 'Charlie', 'Dave']\nOriginal Users: ['Bob', 'Charlie', 'Dave']\n\n```\n\nTimothy slumped. \"See? I removed Alice from the `test_group`, but she disappeared from the `users` list too! How is that possible? I touched the backup, not the original!\"\n\n## The Address, Not the House\n\nMargaret studied the code. \"This is one of the most common misunderstandings in Python, Timothy. It comes down to how Python handles memory.\"\n\nShe grabbed a notepad. \"When you wrote `test_group = users`, what did you think that command did?\"\n\n\"I thought it created a new list,\" Timothy said. \"I thought it took all the names from `users` and copied them into a new variable named `test_group`.\"\n\n\"That is a very reasonable assumption,\" Margaret said gently. \"But Python takes a shortcut for efficiency. Copying data takes time and memory. So instead of copying the *house*, Python just copies the *address*.\"\n\nShe drew a simple diagram. On the left, she wrote the word `users`. On the right, she drew a box containing the list of names. She drew an arrow pointing from `users` to the box.\n\n\"When you wrote `test_group = users`, you didn't create a new box. You just gave the second variable the address of the *original* box.\"\n\nTimothy looked at the diagram. \"So `users` and `test_group` are just two different names for the exact same object?\"\n\n\"Exactly,\" Margaret smiled. \"It‚Äôs like having a shared document online. You gave me the link (the reference). If I delete a paragraph, it‚Äôs deleted for you too. We are both looking at the same document.\"\n\n## Breaking the Link\n\n\"So how do I actually make a copy?\" Timothy asked. \"I want a separate box.\"\n\n\"We have to be explicit,\" Margaret said. \"We need to tell Python to take the data and build a new list.\"\n\nShe showed him the `.copy()` method.\n\n```python\n# Margaret's Fix: Explicit Copying\n\nusers = [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"]\n\n# .copy() creates a brand new list with the same data\ntest_group = users.copy()\n\ntest_group.remove(\"Alice\")\n\nprint(f\"Test Group: {test_group}\")\nprint(f\"Original Users: {users}\")\n\n```\n\n**Output:**\n\n```text\nTest Group: ['Bob', 'Charlie', 'Dave']\nOriginal Users: ['Alice', 'Bob', 'Charlie', 'Dave']\n\n```\n\nTimothy breathed a sigh of relief. \"Alice is safe.\"\n\n\"She is,\" Margaret confirmed. \"By using `.copy()`, you forced Python to create a second, independent list in memory. Now, `test_group` has its own box, and changes there don't touch the original.\"\n\nShe added a small warning note. \"Just remember, `.copy()` makes a 'Shallow Copy.' If your list has *other lists* inside it, those inner lists are still shared. But for a simple list of names like this, it is exactly what you need.\"\n\n## Margaret‚Äôs Cheat Sheet\n\nMargaret opened her notebook to the \"Memory Management\" section.\n\n**The Trap:** Assuming that `new_list = old_list` creates a copy.\n**The Reality:** In Python, assignment (`=`) creates a **Reference** (a nickname), not a copy. Both variables point to the same object.\n**The Why:** Python does this to save memory and speed.\n**The Fix:**\n* **Shallow Copy:** `new_list = old_list.copy()` (Standard way).\n* **Slicing:** `new_list = old_list[:]` (Older, but common way).\n\n**The Check:** You can verify if two variables are the same object using `id(a) == id(b)`.\n\nTimothy made a note in his editor. \"I'll never assume `=` means 'copy' again.\"\n\n\"It's a rite of passage,\" Margaret assured him. \"Every Python developer learns this lesson the hard way. Better to learn it on a test script than on the production database.\"\n\n---\n\n*In the next episode, Margaret and Timothy will face \"The Safety Net\"‚Äîwhere Timothy learns how to catch errors gracefully so his programs don't crash when users make mistakes.*\n\n---\n\n*Aaron Rose is a software engineer and technology writer at [tech-reader.blog](https://www.tech-reader.blog) and the author of [Think Like a Genius](https://amazon.com/author/aaron.rose).*",
      "stars": null,
      "comments": 4,
      "upvotes": 16,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Dear Vibe Coder: Your Code Is the Problem.",
      "url": "https://dev.to/holasoymalva/dear-vibe-coder-your-app-works-your-code-is-the-problem-5fdc",
      "source": "devto",
      "published_at": "2026-02-09T17:12:21+00:00",
      "external_id": null,
      "tags": [
        "programming",
        "ai",
        "discuss",
        "career"
      ],
      "content_length": 5612,
      "content_preview": "---\ntitle: \"Dear Vibe Coder: Your Code Is the Problem.\"\npublished: true\ndescription: \"Vibe coding looks cool on Twitter, but it‚Äôs quietly wrecking codebases, teams, and junior developers. Let‚Äôs actually talk about it.\"\ntags: programming, ai, discuss, career\ncover_image: https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbHprOHdsanh5bHh2NHQ3bGVheG85eHg3emc3eWhzZHEzMG1sdHczayZlcD12MV9naWZzX3NlYXJjaCZjdD1n/JEDVTonz503kj7Ug4j/giphy.gif\n---\n\n## Dear Vibe Coder: Let‚Äôs Talk (For Real)\n\nI‚Äôm going to say t",
      "content_full": "---\ntitle: \"Dear Vibe Coder: Your Code Is the Problem.\"\npublished: true\ndescription: \"Vibe coding looks cool on Twitter, but it‚Äôs quietly wrecking codebases, teams, and junior developers. Let‚Äôs actually talk about it.\"\ntags: programming, ai, discuss, career\ncover_image: https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbHprOHdsanh5bHh2NHQ3bGVheG85eHg3emc3eWhzZHEzMG1sdHczayZlcD12MV9naWZzX3NlYXJjaCZjdD1n/JEDVTonz503kj7Ug4j/giphy.gif\n---\n\n## Dear Vibe Coder: Let‚Äôs Talk (For Real)\n\nI‚Äôm going to say the quiet part out loud.\n\nYour app works.  \nYour demo looks cool.  \nYour code is a nightmare waiting to happen.\n\nAnd before you get mad and quote-tweet this, breathe.  \nThis is **not** an anti-AI rant.\n\nI use AI every single day. You probably should too.  \nIf you‚Äôre a developer in 2026 and *not* using AI, you‚Äôre already behind.\n\nThe problem isn‚Äôt AI.\n\nThe problem is **vibe coding culture** and how it‚Äôs quietly rotting the foundations of real software engineering.\n\n---\n\n## What People Mean by ‚ÄúVibe Coding‚Äù\n\nLet‚Äôs define it clearly, because this term gets abused.\n\nVibe coding is when:\n\n- You don‚Äôt really understand the code  \n- You don‚Äôt care how it works  \n- You trust the output because it *feels* right  \n- And when it breaks, you regenerate it until it stops screaming  \n\nIt‚Äôs coding by vibes, prompts, and blind optimism.\n\nIt looks incredible on social media:\n\n> ‚ÄúBuilt a SaaS in 48 hours with GPT üöÄüî•‚Äù\n\nWhat you don‚Äôt see:\n\n- No tests  \n- No error handling  \n- No mental model  \n- No chance of maintaining it six months later  \n\nThat‚Äôs not engineering.  \nThat‚Äôs content creation with a compiler attached.\n\n---\n\n## AI Didn‚Äôt Create Bad Developers, It Just Exposed Them Faster\n\nHere‚Äôs the uncomfortable truth:\n\n**Bad developers existed long before AI.**\n\nWhat AI did was remove the friction that used to expose them.\n\nBefore:\n- If you didn‚Äôt understand async, you were stuck  \n- If you didn‚Äôt understand state, your app broke  \n- If you didn‚Äôt understand databases, production punished you immediately  \n\nNow?\nYou paste an error into an LLM, skim the explanation you didn‚Äôt really read, and ship.\n\nThe app works.  \nThe understanding never arrived.\n\n---\n\n## The Failure Is Delayed, And That‚Äôs the Dangerous Part\n\nVibe-coded apps don‚Äôt fail on day one.\n\nThey fail:\n\n- When traffic grows  \n- When requirements change  \n- When the AI suggestion is *almost* right  \n- When someone else has to read the code  \n- When **you** have to read your own code three months later  \n\nI‚Äôve watched teams burn weeks untangling AI-generated spaghetti because nobody understood the logic, including the person who shipped it.\n\nAnd guess who gets blamed?\n\nNot the vibes.  \nNot the tool.  \nThe developer.\n\n---\n\n## ‚ÄúBut It Works‚Äù Is Not a Technical Argument\n\nYes, it works.\n\nSo does duct tape.  \nSo does a shell script written at 3 a.m.  \nSo does pushing to main on Friday afternoon.\n\nWorking is the **minimum requirement**, not the goal.\n\nEngineering is about:\n\n- Predictability  \n- Maintainability  \n- Tradeoffs  \n- Understanding failure modes  \n\nIf your entire strategy is ‚Äúthe AI will fix it,‚Äù you‚Äôre not building software.\n\nYou‚Äôre gambling, with extra steps.\n\n---\n\n## Using AI Is Mandatory. Thinking Is Non‚ÄëNegotiable.\n\nLet‚Äôs be very clear.\n\nIf you‚Äôre a developer in 2026 and you‚Äôre not using AI, you‚Äôre doing it wrong.\n\nBut if you‚Äôre using AI **instead of thinking**, you‚Äôre also doing it wrong.\n\nAI should:\n\n- Speed you up  \n- Reduce boilerplate  \n- Help you explore solutions  \n- Catch obvious mistakes  \n\nAI should **not**:\n\n- Replace your mental model  \n- Decide architecture for you  \n- Debug systems you don‚Äôt understand  \n- Be the only reason your app works  \n\nThe best developers right now aren‚Äôt vibe coders.\n\nThey‚Äôre **AI‚Äëaugmented engineers**.\n\nThat difference matters more than ever.\n\n---\n\n## Why Juniors Are Getting Destroyed by This\n\nThis is the part nobody wants to talk about.\n\nNew developers see vibe coding and think:\n\n> ‚ÄúOh, this is how professionals work now.‚Äù\n\nSo they skip fundamentals.  \nThey skip debugging.  \nThey skip learning how systems actually behave.\n\nThen they hit the job market.\n\nAnd companies aren‚Äôt hiring vibe coders.\n\nThey‚Äôre hiring people who can:\n\n- Read ugly legacy code  \n- Debug production issues  \n- Explain *why* something broke  \n- Fix problems without regenerating the universe  \n\nThat‚Äôs why juniors feel stuck.  \nThat‚Äôs why ‚Äúentry-level‚Äù roles ask for 3‚Äì5 years of experience.  \nThat‚Äôs why vibe coding hurts the people who need guidance the most.\n\n---\n\n## If This Offends You, Ask Yourself Why\n\nIf this article makes you angry, sit with that feeling for a second.\n\nBecause deep down, most vibe coders already know the truth:\n\nThey‚Äôre shipping faster, but understanding less.  \nThey‚Äôre productive today, fragile tomorrow.  \nThey look senior on Twitter, and junior in real codebases.\n\nYou don‚Äôt need to quit AI.  \nYou don‚Äôt need to write assembly by candlelight either.\n\nYou just need to **care**.\n\nCare about how things work.  \nCare about reading code.  \nCare about fundamentals.  \nCare about the people who will inherit your mess.\n\n---\n\n## One Final Intrusive Thought\n\nAI didn‚Äôt ruin programming.  \nVibe coding didn‚Äôt either.\n\nBut pretending vibes are a substitute for understanding?\n\nThat absolutely will.\n\nUse AI.  \nShip fast.  \nBuild cool things.\n\nJust don‚Äôt confuse **speed** with **skill**.\n\nAnd please, for the sake of every future teammate, learn the basics.\n\nYour app depends on it.  \nYour career depends on it.  \nAnd so does the industry you‚Äôre trying to join.\n\nSo‚Ä¶ be honest.\n\nAre you vibe coding?  \nOr are you actually engineering?\n\nLet‚Äôs argue in the comments.\n",
      "stars": null,
      "comments": 0,
      "upvotes": 11,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "How do you decide what kind of work to schedule when you‚Äôre already mentally drained?",
      "url": "https://dev.to/samabos/how-do-you-decide-what-kind-of-work-to-schedule-when-youre-already-mentally-drained-2daa",
      "source": "devto",
      "published_at": "2026-02-05T21:16:54+00:00",
      "external_id": null,
      "tags": [
        "career",
        "developer",
        "discuss",
        "productivity"
      ],
      "content_length": 678,
      "content_preview": "\nI‚Äôve noticed I‚Äôm bad at matching my energy level to the work I plan.\n\nI‚Äôll block out time for deep work, but sometimes I only realise halfway through that I‚Äôm too mentally fried to do it properly.\n\nI‚Äôm curious how other devs deal with this in practice:\n\nDo you actively plan your day around how you feel or stress level?\n\nWhen a day goes ‚Äúoff‚Äù, what usually breaks the plan itself, or your ability to follow it?\n\nHave you ever tried time-blocking or similar planning methods? If so, what didn‚Äôt work",
      "content_full": "\nI‚Äôve noticed I‚Äôm bad at matching my energy level to the work I plan.\n\nI‚Äôll block out time for deep work, but sometimes I only realise halfway through that I‚Äôm too mentally fried to do it properly.\n\nI‚Äôm curious how other devs deal with this in practice:\n\nDo you actively plan your day around how you feel or stress level?\n\nWhen a day goes ‚Äúoff‚Äù, what usually breaks the plan itself, or your ability to follow it?\n\nHave you ever tried time-blocking or similar planning methods? If so, what didn‚Äôt work for you?\n\nI‚Äôm exploring a small personal tool in this space and trying to understand the real pain point. not selling anything. \nThank you all in advance for your reply to this.",
      "stars": null,
      "comments": 1,
      "upvotes": 7,
      "read_time": "1 min read",
      "language": null
    },
    {
      "title_en": "On-Call Incident Triage Panel",
      "url": "https://dev.to/sreenu_sasubilli_f9289c4e/on-call-incident-triage-panel-id9",
      "source": "devto",
      "published_at": "2026-02-05T19:08:33+00:00",
      "external_id": null,
      "tags": [
        "devchallenge",
        "algoliachallenge",
        "ai",
        "agents"
      ],
      "content_length": 5408,
      "content_preview": "*This is a submission for the [Algolia Agent Studio Challenge](https://dev.to/challenges/algolia): Consumer-Facing Non-Conversational Experiences*\n\n## What I Built\n\nI built an **On-Call Triage Intelligence Panel** for SRE and DevOps teams.\n\nInstead of a chatbot, this system **proactively surfaces the most relevant operational patterns, likely causes, and first-check actions** when an engineer is diagnosing an incident. The goal is to reduce cognitive load and MTTR during high-stress on-call situ",
      "content_full": "*This is a submission for the [Algolia Agent Studio Challenge](https://dev.to/challenges/algolia): Consumer-Facing Non-Conversational Experiences*\n\n## What I Built\n\nI built an **On-Call Triage Intelligence Panel** for SRE and DevOps teams.\n\nInstead of a chatbot, this system **proactively surfaces the most relevant operational patterns, likely causes, and first-check actions** when an engineer is diagnosing an incident. The goal is to reduce cognitive load and MTTR during high-stress on-call situations ‚Äî without requiring back-and-forth conversation.\n\nEngineers already work inside dashboards, runbooks, and incident tools. This experience enhances that existing workflow by injecting **AI-driven retrieval intelligence** directly into incident triage, rather than asking users to ‚Äúchat‚Äù with a system.\n\n---\n\n## Demo\n\n**Live Index (Algolia Search Explorer):**  \nhttps://dashboard.algolia.com/apps/BF4Z56HB7R/explorer/browse/oncall_triage_kb\n\nThis demo uses Algolia‚Äôs Search Explorer to simulate how the **On-Call Triage Panel** would operate when embedded inside an SRE workflow (alerting tools, observability dashboards, or internal runbooks).\n\n**Example scenarios you can try:**\n\n- **High latency incident**\n  - **Query:** `orders-api p99 latency`\n  - **Filters:**  \n    - `service = orders-api`  \n    - `env = prod`\n\n![High latency incident](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/41vxroeuj0csr71cd8nj.png)\n\n\n\n- **Network-related incident**\n  - **Query:** `packet loss`\n  - **Filters:**  \n    - `service = orders-api`  \n    - `env = staging`\n\n\n![Packet Loss Incident](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bto7hd4mks44d8jcorj5.png)\n\n\n\n- **Database error spike**\n  - **Query:** `db errors`\n  - **Filters:**  \n    - `service = payments-api`  \n    - `env = prod`\n\n\n![Database Error Incident](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gpit0xrv43phspeywjd6.png)\n\n\nYou can also explore by filtering on:\n- `service` (payments-api, search-api, orders-api)\n- `env` (prod, staging, any)\n- `severity` (high, medium, low)\n\n\n\n**What happens:**\nEach search instantly surfaces the most relevant **triage patterns**, showing:\n- Why the pattern was matched (contextual explainability)\n- Likely root causes based on historical incidents\n- A copy-ready **‚Äúfirst checks‚Äù** checklist for immediate on-call action\n\nThis demonstrates **proactive, non-conversational assistance** ‚Äî intelligence is injected directly into the workflow without requiring chat or back-and-forth interaction.\n\n**Mock Screenshot (UI Concept):**  \nSource: https://github.com/sasubillis/oncall_triage_mock/blob/main/index.html\n\n![On-call triage panel mock](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/r4eli2t8ev8ai6ga1sq6.png)\n\n\n\n\n---\n\n## How I Used Algolia Agent Studio\n\nAlgolia Agent Studio was used to power the **retrieval intelligence layer**, not a conversational UI.\n\n### Indexed Data\nI indexed ~100 realistic SRE knowledge records including:\n- Incident patterns\n- Historical incidents\n- Symptoms\n- Services and environments\n- Severity levels\n- Likely causes\n- First-check remediation steps\n- Confidence and explanation metadata\n\nEach record is structured to represent **operational decision artifacts**, not free-form text.\n\n### Retrieval Strategy\nInstead of prompting an LLM, the system relies on:\n- AI-powered semantic relevance\n- Attribute weighting\n- Typo tolerance\n- Contextual ranking across multiple signals\n\nFor example, a query like:\n\n\n`packet loss payments api`\n\n\nAutomatically retrieves:\n- Relevant historical incidents\n- Matching triage patterns\n- Environment-appropriate remediation steps\n\nNo conversation is required ‚Äî the intelligence is embedded in retrieval itself.\n\n### Targeted Prompting (Non-Conversational)\nAgent Studio is used to:\n- Explain *why* a pattern is shown\n- Rank patterns by confidence and operational relevance\n- Surface the most actionable next steps first\n\nThis is agentic behavior without dialogue.\n\n---\n\n## Why Fast Retrieval Matters\n\nIn on-call scenarios, **every second matters**.\n\nSub-50ms retrieval allows triage guidance to appear instantly during active incidents, when seconds matter.\n\nAlgolia‚Äôs fast, contextual retrieval enables:\n- Sub-second access to operational knowledge\n- Reduced time spent searching runbooks\n- Fewer context switches during incidents\n- Faster identification of known failure patterns\n\nInstead of engineers remembering where knowledge lives, **the system remembers for them**.\n\nThis is especially critical in high-severity incidents where cognitive overload is common.\n\n---\n\n## Why This Fits the Challenge\n\nThis project is intentionally **non-conversational**.\n\nThere is no chat interface and no back-and-forth prompting. The AI value comes from:\n- Learning-based relevance ranking\n- Pattern recognition across historical incidents\n- Proactive surfacing of the most useful information at the right moment\n\nIt demonstrates how **AI-powered retrieval** can quietly enhance real workflows ‚Äî exactly what the Consumer-Facing Non-Conversational Experiences category is about.\n\n---\n\n## Closing Thoughts\n\nMany AI demos focus on talking to users.\n\nThis project focuses on **helping users think less** during critical moments.\n\nBy embedding AI directly into operational workflows through Algolia‚Äôs retrieval engine, this approach shows how intelligent systems can assist users *without ever asking a question back*.\n",
      "stars": null,
      "comments": 0,
      "upvotes": 4,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "The Secret Life of Go: The 'defer' Statement",
      "url": "https://dev.to/aaron_rose_0787cc8b4775a0/the-secret-life-of-go-the-defer-statement-3j97",
      "source": "devto",
      "published_at": "2026-02-09T04:44:00+00:00",
      "external_id": null,
      "tags": [
        "go",
        "coding",
        "programming",
        "softwaredevelopment"
      ],
      "content_length": 5994,
      "content_preview": "*How to fix resource leaks and master Go's \"Proximity Rule\"*\n\n---\n\n*Chapter 20: The Stacked Deck*\n\nThe fan on Ethan's desktop PC was spinning loudly. He was staring at a terminal that was spewing error messages like a broken fire hydrant.\n\n`panic: too many open files`\n`panic: too many open files`\n\n\"I don't get it,\" he muttered, hitting Ctrl+C to kill the server. \"I'm closing everything. I checked three times.\"\n\nEleanor walked by, carrying a tray of tea. \"Show me the `ExportData` function.\"\n\nEtha",
      "content_full": "*How to fix resource leaks and master Go's \"Proximity Rule\"*\n\n---\n\n*Chapter 20: The Stacked Deck*\n\nThe fan on Ethan's desktop PC was spinning loudly. He was staring at a terminal that was spewing error messages like a broken fire hydrant.\n\n`panic: too many open files`\n`panic: too many open files`\n\n\"I don't get it,\" he muttered, hitting Ctrl+C to kill the server. \"I'm closing everything. I checked three times.\"\n\nEleanor walked by, carrying a tray of tea. \"Show me the `ExportData` function.\"\n\nEthan pulled up the code. It was a long function, maybe 50 lines, that opened a file, queried a database, wrote to a CSV, and then uploaded it to S3.\n\n```go\nfunc ExportData(id string) error {\n    f, err := os.Open(\"data.csv\")\n    if err != nil {\n        return err\n    }\n    \n    db, err := sql.Open(\"postgres\", \"...\")\n    if err != nil {\n        return err // <--- The Leak\n    }\n\n    // ... 40 lines of logic ...\n\n    db.Close()\n    f.Close() // <--- The Cleanup\n    return nil\n}\n\n```\n\n\"I close the file right there at the bottom,\" Ethan pointed.\n\n\"And what happens if the database connection fails?\" Eleanor asked gently.\n\nEthan looked at the second `if err != nil` block. \"It returns the error.\"\n\n\"And does it close the file before returning?\"\n\nEthan froze. \"No. It returns immediately. The file stays open.\"\n\n\"Exactly. And if you have an error in the middle of your 40 lines of logic? The file stays open. If the function panics? The file stays open. You have created a leak.\"\n\n### The Proximity Rule\n\n\"In other languages,\" Eleanor explained, \"you might wrap this in a `try...finally` block. You put the cleanup way down at the bottom, far away from where you opened the resource. You have to remember to scroll down and check it.\"\n\n\"In Go, we prefer **Proximity**.\"\n\nShe took the keyboard and moved the cleanup code.\n\n```go\nfunc ExportData(id string) error {\n    f, err := os.Open(\"data.csv\")\n    if err != nil {\n        return err\n    }\n    defer f.Close() // Scheduled immediately!\n\n    db, err := sql.Open(\"postgres\", \"...\")\n    if err != nil {\n        // f.Close() happens automatically here\n        return err \n    }\n    defer db.Close() // Scheduled immediately!\n\n    // ... 40 lines of logic ...\n    \n    return nil\n    // db.Close() happens automatically here\n    // f.Close() happens automatically here\n}\n\n```\n\n\"The `defer` keyword pushes a function call onto a stack,\" Eleanor said. \"It says: 'I don't care how this function ends‚Äîreturn, error, or panic‚Äîrun this code right before you leave.'\"\n\n\"So I put the cleanup right next to the creation?\"\n\n\"Always. Open the door, then immediately tell the door to shut itself when you leave. You will never forget again. It works for files, database connections, and especially Mutex locks.\"\n\n```go\nmu.Lock()\ndefer mu.Unlock() // The lock is released no matter what happens\n\n```\n\n### The Stack (LIFO)\n\n\"Wait,\" Ethan said, looking at the code. \"I have two defers now. Which one runs first?\"\n\n\"It is a stack,\" Eleanor replied. \"Last In, First Out.\"\n\nShe sketched it on a notepad:\n\n1. **Open File** (Push `f.Close`)\n2. **Open DB** (Push `db.Close`)\n3. **Function Ends** ->\n* Pop **DB Close** (Runs 1st)\n* Pop **File Close** (Runs 2nd)\n\n\n\n\"This is critical,\" she noted. \"Imagine you are writing a buffered writer. You need to `Flush` the buffer before you `Close` the file. Since you create the File first and the Writer second, the Writer closes first. The dependency order is handled naturally.\"\n\n### The Argument Trap\n\n\"One warning,\" Eleanor added, holding up a finger. \"The function call is scheduled for later, but the **arguments** are evaluated *now*.\"\n\n\"What does that mean?\"\n\n\"Look at this.\"\n\n```go\nfunc TrackTime() {\n    start := time.Now()\n    defer fmt.Println(\"Time elapsed:\", time.Since(start))\n    \n    time.Sleep(2 * time.Second)\n}\n\n```\n\nEthan ran the code.\n`Time elapsed: 0s`\n\n\"Zero?\" Ethan asked. \"But it slept for two seconds.\"\n\n\"Because `time.Since(start)` was calculated *when you wrote the defer line*,\" Eleanor explained. \"At that exact moment, `start` was `now`, so the difference was zero.\"\n\n\"How do I fix it?\"\n\n\"Use an anonymous function to delay the execution.\"\n\n```go\ndefer func() {\n    fmt.Println(\"Time elapsed:\", time.Since(start))\n}()\n\n```\n\nEthan ran it again.\n`Time elapsed: 2s`\n\n\"Now the calculation happens inside the function, at the very end.\"\n\n### The Safety Net\n\nEthan looked at his `ExportData` function. It looked safer. Robust.\n\n\"I used to think `defer` was just syntax sugar,\" he said.\n\n\"It is a safety net,\" Eleanor corrected. \"We are humans. We forget things. We get distracted. `defer` allows you to clean up as you go, so you never leave a mess for your future self.\"\n\n---\n\n## Key Concepts\n\n**The `defer` Statement:**\nSchedules a function call to be run immediately before the surrounding function returns.\n\n* **Use Case:** Closing files, releasing locks (`mu.Unlock`), closing database connections.\n\n**Execution Order (LIFO):**\nDeferred calls are executed in **Last In, First Out** order.\n\n* The last thing you deferred is the first thing to run.\n* *Example:* If you `defer` closing a file and then `defer` closing a database, the database closes first.\n\n**Panic Safety:**\nDeferred functions run even if the program panics. This makes them essential for recovering from crashes or ensuring resources are released during a failure.\n\n**Argument Evaluation:**\n\n* **Arguments** to the deferred function are evaluated **immediately** (when the `defer` line is hit).\n* **The Function Body** is executed **later** (when the function returns).\n* *Tip:* If you need to calculate something at the end (like timing execution), wrap it in an anonymous `func() { ... }`.\n\n---\n\n*Next chapter: The Panic and Recover. Ethan learns that sometimes, the only way to save a program is to let it crash (and then catch it).*\n\n---\n\n*Aaron Rose is a software engineer and technology writer at [tech-reader.blog](https://www.tech-reader.blog) and the author of [Think Like a Genius](https://amazon.com/author/aaron.rose).*",
      "stars": null,
      "comments": 13,
      "upvotes": 21,
      "read_time": "4 min read",
      "language": null
    }
  ]
}