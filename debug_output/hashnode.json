{
  "crawler": "hashnode",
  "fetched_at": "2026-02-10T11:44:53.278616",
  "elapsed_seconds": 4.2,
  "total_articles": 49,
  "articles_with_content": 49,
  "avg_content_length": 14967,
  "articles": [
    {
      "title_en": "From Failure to Flow: How I Used Polars to Conquer Memory Issues in Our Data Pipelines",
      "url": "https://importidea.dev/from-failure-to-flow-how-i-used-polars-to-conquer-memory-issues-in-our-data-pipelines",
      "source": "hashnode",
      "published_at": "2025-04-26T18:34:35.892000+00:00",
      "external_id": null,
      "tags": [
        "Lazyframe",
        "Python",
        "Polars",
        "numpy",
        "data-engineering",
        "memory-management"
      ],
      "content_length": 13623,
      "content_preview": "Ever been bogged down by data pipelines crashing due to memory issues? It's a frustratingly common problem in data engineering projects. This post chronicles my experience of identifying and resolving memory bottlenecks in our data processing using the powerful Polars library adhering to data engineering best practices.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">I've organised my explanation using the STAR method for clarity and",
      "content_full": "Ever been bogged down by data pipelines crashing due to memory issues? It's a frustratingly common problem in data engineering projects. This post chronicles my experience of identifying and resolving memory bottlenecks in our data processing using the powerful Polars library adhering to data engineering best practices.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">I've organised my explanation using the STAR method for clarity and ease of understanding. Let's dive into how I moved from pipeline failures to a smooth, efficient flow.</div>\n</div>\n\n# The Situation\n\nThe project involved generating **Monte Carlo Projections** using **Geometric Brownian Motion (GBM)** to create thousands of potential future price paths for assets. This helps analysts estimate the probabilities of various outcomes.\n\nFirst, I would like to provide a concise explanation of the components involved in this pipeline.\n\n## Pipeline Components\n\n**Step 1:** Generate a random normal distribution and then perform a cumulative sum over it.\n\n**Step 2:** Using the Covariance matrix and two sets of random normal distributions, compute the Einstein summation.\n\n**Step 3:** Incorporating both deterministic trends (drift) and random fluctuations (volatility) using GBM.\n\n**Step 4:** Perform mean aggregation & select based on filter criteria.\n\n## Issue\n\n* The initial version used `Numpy` & `Pandas` library for all computations.\n    \n* We have to run the Projection for at least 100 years, but many times it would be even more than that. Letâ€™s break down how the computation looks based on the above steps,\n    \n* ```python\n      import numpy as np\n      \n      rng = np.random.default_rng()\n      \n      DAYS = 365\n      YEARS = 1_00\n      PATH = 20_000\n      \n      # Generating random cumulative sum normal distrubution\n      # NOTE - We required two two normal distrubution set\n      nor_dis_random1 = np.cumsum(\n          rg.normal(0, np.sqrt(1 / days), (days * fixed_years, paths)),\n          axis=0\n      )\n      nor_dis_random2 = np.cumsum(\n          rg.normal(0, np.sqrt(1 / days), (days * fixed_years, paths)),\n          axis=0\n      )\n      \n      # Computing Einstein Summation\n      ein_corr = np.einsum(\n                      \"ij,jkl->ikl\",\n                      covariance_matrix_data,\n                      np.array([nor_dis_random1, nor_dis_random2]),\n                      casting=\"same_kind\",\n                      optimize=True,\n                  )\n    ```\n    \n* As you can see above, we had to deal with extremely big arrays. `Numpy` has to allocate memory for random normal distribution generation. Business logic requires two such big arrays to compute Einstein Summation. For 100 years & above even after beefing it till `64 GB` it was failing with `Out of Memory` Error.\n    \n* Following the Einstein Summation task, numerous transformations, including the application of GBM, were performed. This was initially done using `Pandas` , which made already memory hogging code even more bulky.\n    \n* ```python\n      import pandas as pd\n      \n      # Creating two pandas dataframe from Einstein Summation aaray\n      projecttion_a = pd.DataFrame(ein_corr[0].T).cumsum(axis=0)\n      projecttion_b = pd.DataFrame(ein_corr[1].T).cumsum(axis=0)\n      \n      # Further transformation\n      sigma_s = 0.26\n      mu_s = 0.05\n      s0 = 391555\n      \n      projecttion_a = S0 * np.exp((mu_S - 0.5 * sigma_S**2) + sigma_S * projecttion_a\n      projecttion_b = S0 * np.exp((mu_S - 0.5 * sigma_S**2) + sigma_S * projecttion_b\n    ```\n    \n* It became simply unsustainable to keep increasing memory.\n    \n\n# The Tasks\n\nLet me show you how I divided the problem into smaller tasks and solved them, aiming for a cohesive and efficient process.\n\n## Assessment\n\n* The moment I stepped into this project, I realized that `Pandas` wasn't the right tool for the job. Both Numpy and Pandas store everything in memory from start to finish, and every transformation adds to memory use.\n    \n* While exploring possible solutions, I wondered about using `PySpark` because of its ability to handle distributed workloads. But then, I stumbled upon two significant issues:\n    \n    * It doesn't have first-class support for running NumPy functions across a cluster.\n        \n    * The environment is costly and bulky since Spark is cluster-based.\n        \n* Switching to Spark would have required a lot more work. That's where [Polars](https://docs.pola.rs/api/python/stable/reference/) comes in to save the day. Here are the key reasons why I chose it:\n    \n    * Like PySparkâ€™s `lazy evaluation`, Polars also supports it with [`LazyFrame`](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html).\n        \n    * First-class for Numpy functions, even in LazyFrame.\n        \n    * Runs on a single machine and generally offers excellent performance since its based on Rust & internal use of parallel processing.\n        \n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">If you're not using <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://docs.pola.rs/\" style=\"pointer-events: none\">Polars</a> in your work yet, let me tell you, you're missing out on the ultimate Swiss army knife of the Data Engineering world! It's incredibly feature-rich, lightning-fast, supports tons of I/O operations, and boasts an amazingly intuitive API!</div>\n</div>\n\n## Redesigned Pipeline components\n\n* **Step 1:** Replace all `Pandas Dataframe` code with `Polars LazyFrame`.\n    \n* **Step 2:** Since Polars fully supports Numpy functions, keep using specific Numpy functions that Polars doesn't have built-in.\n    \n* **Step 3:** Switch to `Batch Processing` for transformations. Ensure each batch creates a `LazyFrame`, so nothing is stored in memory until the final execution.\n    \n* **Step 4:** Keep adding transformation steps to the LazyFrame and execute them at the end. This approach lets Polars excel by making the most of its `Lazy API`.\n    \n\n```plaintext\nWith the lazy API, Polars doesn't run each query line-by-line but instead processes the full\nquery end-to-end. To get the most out of Polars it is important that you use the lazy API because:\n - The lazy API allows Polars to apply automatic query optimization with the query optimizer.\n - The lazy API allows you to work with larger than memory datasets using streaming.\n - The lazy API can catch schema errors before processing the data.\n```\n\n# The Actions\n\nEnough theory talking, now lets dive into real coding based actions that solved the all the above issues.\n\n## Numpy function + Polars Lazyframe\n\n```python\nimport polars as pl\nimport numpy as np\n  \nrng = np.random.default_rng()\n  \nDAYS = 365\nYEARS = 1_00\nPATH = 20_000\n\ndef unit_batch(fixed_years=10):\n        return (\n            pl.LazyFrame()\n            .with_columns(\n                # Here I am directly using NumPy's normal() & cumsum() in Polars Lazyframe since\n                # it have first class support\n                nor_dis_random=np.cumsum(\n                    rg.normal(0, np.sqrt(1 / days), (days * fixed_years, paths)).astype(np.float32),\n                    axis=0,\n                    dtype=np.float32,\n                ) # I am using Float32 as data structure instead of default Float64\n            ) \n            # I need to create lazyframe of shape (x,y) from normal distribution data of shape (x,y)\n            # Initially `nor_dis_random` is ArrayType column to which I am exploding to create y columns\n            # The beauty of Polars Lazy API is we can keep adding steps to Query plan.\n            .with_columns(pl.col(\"nor_dis_random\").arr.to_struct().alias(\"array_struct\"))\n            .unnest(\"array_struct\")\n            .drop(\"nor_dis_random\")\n        )\n\n # Creating stack of all batches of lazyframe, that will eventually get concated\nnormal_dist_random_cum_sum_frames = [\n    _unit_chunk(i)\n    for i in alive_it(\n        chunks, title=\"Normal Distribution Cumulative sum\", force_tty=True, total=len(chunks)\n    )\n]\n# This steps is very important as here I am vertically stacking all batches but still as LazyFrame\nnormal_dist = pl.concat(normal_dist_random_cum_sum_frames, how=\"vertical\")\n```\n\n## Batch Processing on Polars Lazyframe\n\n```python\nBATCH_SIZE = 1_000\n\ndef calculate_no_of_batches(row_count: int, batch_size: int = 10) -> int:\n    return row_count // batch_size + (row_count % batch_size > 0)\n\n\nein_corr_a, ein_corr_b = [], []\n\nno_of_batches = calculate_no_of_batches(\n    normal_dist1.select(pl.len()).collect().item(), batch_size=BATCH_SIZE\n)\n\n# Batch processing loop\nfor batch in alive_it(range(no_of_batches), title=\"Einstein summation\", force_tty=True):\n    # Polars Lazy API provides as with slice() to iterate over lazyframe without actually \n    # loading data in memory. Its similar to SQL Order & limit without actually loading data\n    brownian_path_array1 = normal_dist1.slice(batch * BATCH_SIZE, BATCH_SIZE)\n    brownian_path_array2 = normal_dist2.slice(batch * BATCH_SIZE, BATCH_SIZE)\n\n    # Append as array to calculate Einstein summation convention\n    einstein_summation_operands = np.array(\n        [\n            brownian_path_array1.collect().to_numpy().T,\n            brownian_path_array2.collect().to_numpy().T,\n        ],\n        dtype=np.float32,\n    )\n    ein_corr = np.einsum(\n        \"ij,jkl->ikl\",\n        covariance_matrix_data,\n        einstein_summation_operands,\n        dtype=np.float32,\n        casting=\"same_kind\",\n        optimize=True,\n    )\n\n    # Creating lazyframe from above & perform cumulative sum over entire column\n    # Once again Polars Expressive Lazy API makes code more cleaner & readable. \n    ein_corr_a.append(pl.LazyFrame(ein_corr[0].T).with_columns(pl.all().cum_sum()))\n    ein_corr_b.append(pl.LazyFrame(ein_corr[1].T).with_columns(pl.all().cum_sum()))\n\n# Just like above, even here at end all smaller batches of lazyFrame is \n# concated as Unified Lazyframe. Still nothing is stored in memory.\nein_corr_frame1 = pl.concat(ein_corr_a, how=\"vertical\")\nein_corr_frame2 = pl.concat(ein_corr_b, how=\"vertical\")\n```\n\n## Building data transformation Query graph\n\n1. Computing GBM paths\n    \n\n```python\n# Get column names from standard_brownian for explicit transformations\ncol_names = ein_corr_frame1.collect_schema().names()\n\nein_corr_frame1 = (\n    ein_corr_frame1\n    # Step 1: Calculate drift term + volatility term for each column\n    .with_columns(\n        [\n            (\n                sigma_s * pl.col(col_name) + (mu_s - 0.5 * sigma_s**2) * pl.col(\"real_number_time\")\n            ).alias(f\"{col_name}_gbm\")\n            for col_name in col_names\n        ]\n    )\n    # Step 2: Apply exponential function and scale by s0\n    .with_columns(\n        [\n            (pl.col(f\"{col_name}_gbm\").exp() * s0).alias(f\"Path_{i + 1}\")\n            for i, col_name in enumerate(col_names)\n        ]\n    )\n    # Step 3: Keep only the final sales path columns\n    .select([f\"Path_{i + 1}\" for i in range(len(col_names))])\n)\n```\n\n2. Compute Mean from GBM Paths & Use of Dynamic Scaling to fix Infinite number issue\n    \n\n```python\n# Get column names from standard_brownian for explicit transformations\ncol_names = ein_corr_frame1.collect_schema().names()\n\nein_corr_frame1 = (\n    ein_corr_frame1.with_row_index(\"_idx\", 1)\n    # select row no present in year_end_location_time\n    .filter(\n        pl.col(\"_idx\").is_in(\n            time_step.select(\"year_end_location_time\").drop_nulls().collect().to_series()\n        )\n    )\n    .drop(\"_idx\", strict=False)\n    # Handle infinities by replacing them with None\n    .with_columns(\n        [\n            pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)\n            for c in col_names\n        ]\n    )  # Calculate max value per row for Dynamic Scaling ops\n    .with_columns(max_value=pl.max_horizontal(pl.all()))\n    # Performing dynamic scale mean by scaling down --> mean --> scale up\n    .with_columns(\n        Sales=pl.mean_horizontal(  # taking mean of entire row\n            # dividing all cols by max value (scaled down)\n            pl.all().exclude(\"max_value\") / pl.col(\"max_value\")\n        )  # multiplying mean scaled down with max value (scale up)\n        * pl.col(\"max_value\")\n    )  # We only need 'Sales' column\n    .select(\"Sales\")\n)\n```\n\n3. Materializing the entire query plan into final output\n    \n\n```python\n# Till now we have performed many heavy maths based calculations, but everything is defred\n# running collect() will materialize all the queries into final dataframe\nein_corr_frame1.collect()\n```\n\n**Key Points**:\n\n* Polars expressive Lazy API is very powerful, clean & highly readable.\n    \n* From above you can easily gather that I am simple keep adding steps to Query plan without actually running them yet.\n    \n* This gives Polarsâ€™ engine many opportunities to optimize the query.\n    \n* Untill we donâ€™t run `LazyFrame.collect()` all the queries are deferred & nothing is stored in memory\n    \n\n# The Results\n\n* The entire Monte Carlo Simulation running for 100 years were able to complete under 20-25 GB of memory, which was simply failing in using Pandas even after providing 64 GB.\n    \n* Added benefit was total time because Polars is based on Rust & internally uses parrallel processing to run queries.\n    \n* The Expressive API of Polars library is very powerful & intuitive, especially for Data engineers coming from SQL world.\n    \n* This experience underscores the importance of choosing the right tools and approaches in data engineering to achieve optimal performance and efficiency.",
      "stars": null,
      "comments": 15,
      "upvotes": 117,
      "read_time": "8 min read",
      "language": null
    },
    {
      "title_en": "Building a Knowledge Graph Locally with Neo4j & Ollama",
      "url": "https://greenflux.hashnode.dev/building-a-knowledge-graph-locally-with-neo4j-and-ollama",
      "source": "hashnode",
      "published_at": "2025-04-21T10:02:54.102000+00:00",
      "external_id": null,
      "tags": [
        "Neo4j",
        "graph database",
        "knowledge graph",
        "Python",
        "Python 3",
        "huggingface",
        "llm",
        "cypher",
        "macOS",
        "obsidian",
        "ollama",
        "ontology"
      ],
      "content_length": 29360,
      "content_preview": "Knowledge graphs, also known as semantic networks, are a specialized application of graph databases used to store information about entities (person, location, organization, etc) and their relationships. They allow you to explore your data with an interactive network graph, and perform complex queries that would be difficult or impossible with SQL. Knowledge graphs are often used in fraud detection, social networks, recommendation engines, and RAG (retrieval-augmented generation).\n\nTraditionally",
      "content_full": "Knowledge graphs, also known as semantic networks, are a specialized application of graph databases used to store information about entities (person, location, organization, etc) and their relationships. They allow you to explore your data with an interactive network graph, and perform complex queries that would be difficult or impossible with SQL. Knowledge graphs are often used in fraud detection, social networks, recommendation engines, and RAG (retrieval-augmented generation).\n\nTraditionally, building a knowledge graph has involved extensive work in preprocessing the input data, carefully extracting and labeling entities and relationships based on an ontology, or schema that defines the types of data to extract. But LLMs have enabled this process to be automated, allowing large datasets to be processed into knowledge graphs quickly and easily.\n\nIn this guide, weâ€™ll be building a knowledge graph locally using a text-to-cypher model from Hugging Face, Neo4j to store and display the graph data, and Python to interact with the model and Neo4j API. This tutorial is for Mac, but Docker, Ollama and Python can all be used on Windows or Linux as well.\n\n**This guide will cover:**\n\n* Deploying Neo4j locally with Docker\n    \n* Downloading a model from HuggingFace and creating a Modelfile for Ollama\n    \n* Running the model with Ollama\n    \n* Prompting the model from a Python script\n    \n* Bulk processing local files into a knowledge graph\n    \n\n**Letâ€™s get started!**\n\n## Deploying Neo4j locally with Docker\n\nInstall Docker, then open it up and enter Neo4j in the search bar. Click **Run** on the top result with the â€˜Docker Official Imageâ€™ badge.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745074175948/5aa90e2a-f7da-4322-93af-a34f400912ac.png align=\"center\")\n\nYou should see the image download and the container start up. Select the container and click the link or open [`http://localhost:7474/`](http://localhost:7474/) in the browser.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745074423486/7991504d-3cca-4bae-a841-6cdb3eff37d3.png align=\"center\")\n\nNext you should see a login screen for Neo4j. The user name and password are both `neo4j`. Thereâ€™s also a preview of their newer browser tool (shown below). You may see an older login screen first, then an option to try the new browser. Login on the first screen, set a new password, then choose the option to try the new browser.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745074507506/d7ecad0a-2508-4c86-adfa-964280d88daf.png align=\"center\")\n\n**Note**: The browser UI serves on port 7474, but it connects to the Neo4j database instance on port 7687.\n\nOnce logged in, youâ€™ll see a command line input to run Cypher queries, similar to SQL queries, to search and manage data in the graph. Paste in the following query and run it.\n\n```plaintext\nCREATE \n  (sisko:Character {name: \"Benjamin Sisko\", rank: \"Captain\", species: \"Human\"}),\n  (kira:Character {name: \"Kira Nerys\", rank: \"Major\", species: \"Bajoran\"}),\n  (odo:Character {name: \"Odo\", rank: \"Constable\", species: \"Changeling\"}),\n  (jake:Character {name: \"Jake Sisko\", rank: \"Civilian\", species: \"Human\"}),\n  (nog:Character {name: \"Nog\", rank: \"Ensign\", species: \"Ferengi\"}),\n\n  (kira)-[:SERVES_WITH]->(sisko),\n  (odo)-[:SERVES_WITH]->(sisko),\n  (jake)-[:RELATED_TO]->(sisko),\n\n  (nog)-[:FRIEND_OF]->(jake)\n```\n\n**Run** the query, then click the **(\\*)** button under Relationships to view the graph.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745074934768/5161b219-a6c5-4267-8f09-a9d660a0a115.png align=\"center\")\n\nOk, we have Neo4j running locally and can create a graph. Next we need a way to generate Cypher queries. You could just ask ChatGPT, but there are several fine-tuned models on Hugging Face that are made for text-to-cypher generation. Weâ€™ll use Ollama to run one of these models locally so thereâ€™s no subscription cost, no internet required (after download), and no privacy or security concerns with sending data to a 3rd party.\n\nBut first, letâ€™s clear out the test query we ran earlier. Run the following command to purge the database.\n\n```plaintext\nMATCH (n)\nDETACH DELETE n\n```\n\n## Installing Ollama and Downloading a Model\n\nNext, download and install [Ollama](https://ollama.com/), then run it. You should see a Llama icon in the menu bar once itâ€™s running. The only option is `quit Ollama`. Everything else is done through the terminal.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745077078924/66ffc603-dcd5-41f5-aec6-7d888943231c.png align=\"center\")\n\nNow weâ€™ll download one of the models hosted by Ollama to test it out before trying the model from Hugging Face. This will create a Modelfile that we can use as a template, and edit it to run the Hugging Face model in Ollama.\n\nRun the following command:\n\n```bash\nollama run llama3.2\n```\n\nYouâ€™ll see several files download, then a message from the model asking you to send a message.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745077883623/f60d33c8-0a75-48fd-96a5-186232576d99.png align=\"center\")\n\nYou should be able to chat with the Llama3.2 model from the terminal now. Enter a prompt to test it out.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745077987624/b288fb35-f349-4630-8ab6-bfc171166db7.png align=\"center\")\n\nNow type `/bye` to exit the model and return to the terminal.\n\nNext, we need to copy the existing Modelfile to use as a template.\n\nRun the following command:\n\n```bash\nollama show --modelfile llama3.2\n```\n\nScroll up and find the line that starts with `FROM /Users/{YOUR_USER_NAME}/.ollama/models/blobs`\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745078413277/037d5316-3fff-4049-a464-d60a51d6eebc.png align=\"center\")\n\nThis contains the blob reference that will be used in the next section to build our own Modelfile. Copy this line to a new text file. Then save the file in a new folder to use for this project. Name the file Modelfile (no extension) and save it to the new folder. For this guide, Iâ€™m naming my folder **Neo4j**.\n\n## Downloading a model from Hugging Face and creating a Modelfile for Ollama\n\nNext, weâ€™ll be using the [neo4j/text2cypher-gemma-2-9b-it-finetuned-2024v1](https://huggingface.co/neo4j/text2cypher-gemma-2-9b-it-finetuned-2024v1) model from Hugging Face, and cloning the repo locally. Start by opening the new Neo4j folder in the terminal.\n\nHugging Face suggests using [Git Large File Storage (LFS)](https://git-lfs.com/) to clone the repo and minimize the download size by keeping larger files on the server. You can install it with `brew install git-lfs` if you have [Homebrew](https://brew.sh/) installed, or download the installer from their [website](https://git-lfs.com/).\n\nOnce **git-lfs** is installed, run:\n\n```bash\ngit lfs install\n\ngit clone https://huggingface.co/neo4j/text2cypher-gemma-2-9b-it-finetuned-2024v1\n```\n\nYou should see the repo downloaded as a new folder in the current directory.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745078920274/5988fc13-7c57-4130-88a5-0e38f27dc819.png align=\"center\")\n\nNext, we need to update the Modelfile to tell Ollama how to build and serve our model, since this is a Hugging Face model, and not one hosted by Ollama.\n\nKeep the `FROM /Users/â€¦` line at the top, and then add the remaining text like the sample below, then save the Modelfile.\n\n```plaintext\nFROM /Users/greenflux/.ollama/models/blobs/sha256-YOUR-SHA-KEY\nTEMPLATE \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>\"\nPARAMETER stop <|start_header_id|>\nPARAMETER stop <|end_header_id|>\nPARAMETER stop <|eot_id|>\n```\n\nOk, we have Ollama downloaded and running, the HF model cloned locally, and a Modelfile to tell Ollama how to use it. Weâ€™re now ready to run the model.\n\n## Running the model with Ollama\n\nNext, run:\n\n```plaintext\nollama create text2cypher -f Modelfile\n```\n\nThis tells Ollama to create a new model named *text2cypher*, using the settings in our Modelfile. You should see a few operations in the terminal, followed by a success message.\n\nNext, run:\n\n```plaintext\nollama run text2cypher\n```\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745079420160/2947dbdd-a9d2-4e6a-845d-d4e9a55110e3.png align=\"center\")\n\nYou should see the `Send a message` prompt again, but this time weâ€™re using the text2cypher model from Hugging Face. This model is fine-tuned to create Cypher queries. It works best when you provide it with the schema of your knowledge graph so that the generated query is limited to the entities and relationships you want in your graph.\n\nWhen building a new graph, you can decide on your schema first, then provide that with the prompt to generate a CREATE query to insert the new data. And to search an existing graph, just provide it with the schema and a text description of the search.\n\nHere are a prompt you can try out:\n\n```plaintext\nNode types:\n- Character(name, rank, species)\n- Station(name, location)\n- Relationships:\n    - ASSIGNED_TO (Character â†’ Station)\n    - ALLIES_WITH (Character â†” Character)\n\nWrite a Cypher query to create the following data:\n- Commander Benjamin Sisko, a Human, is assigned to Deep Space Nine (orbiting Bajor).\n- Major Kira Nerys, a Bajoran, is also assigned to Deep Space Nine.\n- Odo, a Changeling, serves as chief of security on the station.\n- Jadzia Dax, a Trill, is friends with Sisko and works as the station's science officer.\n- Quark, a Ferengi, is not part of the crew but owns a bar on the station and is friends with Odo.\n```\n\nThis tells the model what schema to use, then provides a few lines of text data to extract entities and relationships from. The response should be a Cypher CREATE query like this:\n\n```plaintext\nCREATE (s:Character {name: \"Benjamin Sisko\", rank: \"Commander\", species: \"Human\"})\nCREATE (d:Station {name: \"Deep Space Nine\", location: \"orbiting Bajor\"})\nCREATE (s)-[:ASSIGNED_TO]->(d)\n\nCREATE (k:Character {name: \"Kira Nerys\", rank: \"Major\", species: \"Bajoran\"})\nCREATE (k)-[:ASSIGNED_TO]->(d)\n\nCREATE (o:Character {name: \"Odo\", rank: \"Chief of Security\", species: \"Changeling\"})\nCREATE (o)-[:ASSIGNED_TO]->(d)\n\nCREATE (j:Character {name: \"Jadzia Dax\", rank: \"Science Officer\", species: \"Trill\"})\nCREATE (j)-[:ALLIES_WITH]->(s)\nCREATE (s)-[:ALLIES_WITH]->(o)\n\nCREATE (q:Character {name: \"Quark\", rank: \"\", species: \"Ferengi\"})\nCREATE (q)-[:OWNS_BAR_ON]->(d)\nCREATE (o)-[:ALLIES_WITH]->(q)\n```\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745080054466/96de170b-6fbb-48f3-a3c4-94f21583fb5f.png align=\"center\")\n\nNow go back to Neo4j and run the query. Then click the (\\*) again to view the new graph.\n\n**Note**: You may have to remove the inline `//comments` for the query to run.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745080361233/6cfbc00e-5e63-4a9b-bb61-80466f1ef0a7.png align=\"center\")\n\nOk, now weâ€™re able to generate Cypher queries from a text prompt. Lastly, letâ€™s write a Python script to bulk process text files into Cypher queries and insert the data into our graph.\n\n## Prompting the model from a Python script\n\nNext, open up your favorite text editor or IDE and create a python script to send prompts to Ollama. This script takes a text parameter for the prompt and will return the LLM response in the terminal.\n\n```python\nimport requests\nimport argparse\n\n# Static schema\nschema = \"\"\"\nNode types:\n- Character(name, rank, species)\n- Ship(name, class)\n- Relationships:\n    - SERVES_ON (Character â†’ Ship)\n    - FRIENDS_WITH (Character â†” Character)\n\"\"\"\n\n# Parse CLI arguments\nparser = argparse.ArgumentParser(description=\"Send prompt to Ollama text2cypher model\")\nparser.add_argument(\"prompt\", type=str, help=\"Prompt text to send (wrap in quotes)\")\nargs = parser.parse_args()\n\n# Ollama local model endpoint\nOLLAMA_URL = \"http://localhost:11434/api/generate\"\nMODEL_NAME = \"text2cypher\"\n\n# Build request payload\npayload = {\n    \"model\": MODEL_NAME,\n    \"prompt\": f\"{schema}\\n\\nQuestion: {args.prompt}\\n\\nReturn only a valid Cypher query.\",\n    \"stream\": False\n}\n\n# Send request\nresponse = requests.post(OLLAMA_URL, json=payload)\nresponse.raise_for_status()\n\n# Print result\ncypher = response.json().get(\"response\")\nprint(\"Generated Cypher Query:\\n\", cypher)\n```\n\nSave the script to the Neo4j folder and name it *send\\_prompt.py*. Then create a virtual environment and run it.\n\n```bash\npython3 -m venv venv\n```\n\n```bash\nsource venv/bin/activate\n```\n\n```bash\npip install requests\n```\n\nYou should now be able to prompt the text2cypher model from the terminal using:\n\n```bash\npython3 send_prompt.py \"Create nodes for Captain Jean-Luc Picard (Human), Lieutenant Worf (Klingon), and Counselor Deanna Troi (Betazoid). All of them serve on the USS Enterprise (Galaxy-class). Worf and Troi are friends.\"\n```\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745085175389/4d0faa99-9bf0-442d-bd12-8842fb07dd41.png align=\"center\")\n\nAlright, the Python script can return a Cypher query. Now letâ€™s update it to run that query in Neo4j.\n\n## Bulk processing local files into a knowledge graph\n\nStart out by creating a new script called run\\_cypher.py in the Neo4j folder. Paste in the following script and save. Be sure to update it with your password at the top of the script.\n\n```python\n# run_cypher.py\nimport argparse\nfrom neo4j import GraphDatabase\n\n# --- Configuration ---\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_USER = \"neo4j\"\nNEO4J_PASSWORD = \"YOUR_PASSWORD\"\n\n# --- Parse CLI Argument ---\nparser = argparse.ArgumentParser(description=\"Run a Cypher query on Neo4j\")\nparser.add_argument(\"query\", type=str, help=\"Cypher query to run (wrap in quotes)\")\nargs = parser.parse_args()\ncypher_query = args.query.strip()\n\n# --- Run Cypher Query ---\ndef run_query(query):\n    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n    with driver.session(database=\"neo4j\") as session:\n        try:\n            result = session.run(query)\n            # Collect results first before consuming\n            records = list(result)\n            summary = result.consume()\n            print(\"Query executed. Stats:\", summary.counters)\n            for record in records:\n                print(record.data())\n        except Exception as e:\n            print(\"Cypher execution error:\", str(e))\n    driver.close()\n\nrun_query(cypher_query)\n```\n\nNext, install the Neo4j driver:\n\n```bash\npip install neo4j\n```\n\nThen run the script with a prompt containing a Cypher query.\n\n```bash\npython run_cypher.py \"CREATE (sisko:Character {name: 'Benjamin Sisko', rank: 'Captain', species: 'Human'}), (kira:Character {name: 'Kira Nerys', rank: 'Colonel', species: 'Bajoran'}), (bashir:Character {name: 'Julian Bashir', rank: 'Doctor', species: 'Human'}), (defiant:Ship {name: 'USS Defiant', class: 'Defiant-class'}), (sisko)-[:SERVES_ON]->(defiant), (kira)-[:SERVES_ON]->(defiant), (bashir)-[:SERVES_ON]->(defiant), (sisko)-[:FRIENDS_WITH]->(kira), (bashir)-[:FRIENDS_WITH]->(sisko)\"\n```\n\nRefresh the Neo4j browser, and you should see the new nodes created.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745087413634/4dbea19f-51e9-4d4f-9f92-71fe3a7ca24f.png align=\"center\")\n\nNow delete all the nodes one last time before the bulk upload.\n\n```plaintext\nMATCH (n)\nDETACH DELETE n\n```\n\n### Bulk Processing Text Files\n\nOk, weâ€™re almost there! Lastly, we need a script to process a folder of text files, and call both of these scripts to generate a cypher then run it. For this example, Iâ€™m using 3 text files located in a *Notes* folder, inside the neo4j folder.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745087720620/06e4cbc9-80aa-4721-a222-6314cff5727a.png align=\"center\")\n\nHereâ€™s the files I used, in case you want to test with the same input data.\n\n**prompt1.txt**\n\n```plaintext\nCaptain Benjamin Sisko entered the operations center on Deep Space Nine and found Major Kira Nerys consulting with Constable Odo. The captain outlined a new security protocol and asked Kira to oversee its implementation. Odo adjusted his uniform and nodded as he scanned incoming reports. Kira offered suggestions for patrol routes while Odo confirmed gate readings. By the end of the briefing, all three had agreed on the plan to increase boarding inspections and monitor cargo manifests more closely.\n```\n\n**prompt2.txt**\n\n```plaintext\nLater that morning, Kira Nerys joined Science Officer Jadzia Dax in the lab for an experimental sensor test. Dr Julian Bashir arrived with new calibration data from Starfleet Medical. As Jadzia calibrated the particle analyzer, Kira reviewed station logs for unusual energy signatures. Julian smiled at their progress and noted that the anomaly readings matched a pattern he had seen during his medical travels. Together they prepared to present their findings to Captain Sisko before the next scheduled docking.\n```\n\n**prompt3.txt**\n\n```plaintext\nThat evening, Jadzia Dax walked through the promenade to visit the bar run by Quark. She found Nog polishing glasses behind the counter. Quark greeted her with a nod and gestured toward a table near the entrance. Nog filled a fresh glass of synthehol while Jadzia described the sensor anomaly discovered earlier. Quark leaned forward, offering to check his Ferengi data logs for any related transactions. Nog made a note to share the information with Dr Bashir first thing tomorrow.\n```\n\nCreate one last script to loop over the Notes folder. This script will call the `send_prompt.py` and `run_cypher.py` scripts for each file in the Notes folder.\n\n**Note**: I had to do a bit of cleanup on the response to get only the valid Cypher CREATE queries. The model was wrapping the response with ` ``` `, but not consistently. And occasionally it would add extra text at the beginning, like `Generated Cypher Query:`. So the script is a little long on the text parsing, but the main logic for looping and running the other scripts is pretty straight-forward.\n\n```python\n#!/usr/bin/env python3\nimport os\nimport re\nimport subprocess\nimport argparse\n\n# Configuration\nNOTES_DIR = \"Notes\"         # directory containing .txt files\nSEND_PROMPT_SCRIPT = \"send_prompt.py\"\nRUN_CYPHER_SCRIPT = \"run_cypher.py\"\n\ndef extract_character_ship_info(output):\n    \"\"\"Extract Character and Ship information from the output.\"\"\"\n    characters = []\n    ships = []\n    relationships = []\n    in_code_block = False\n    \n    for line in output.strip().splitlines():\n        line = line.strip()\n        \n        # Handle code blocks and filtering\n        if line.startswith(\"```\"):\n            in_code_block = not in_code_block\n            continue\n        if line.lower().startswith(\"here are\") or not line or not line.upper().startswith(\"CREATE \"):\n            continue\n        \n        # Ensure statement ends with semicolon\n        if not line.endswith(\";\"):\n            line += \";\"\n            \n        # Extract character creation\n        character_match = re.search(r'CREATE\\s+\\((\\w+):Character\\s+\\{(.+?)\\}\\)', line)\n        if character_match:\n            alias = character_match.group(1)\n            props_str = character_match.group(2)\n            \n            # Extract properties\n            name_match = re.search(r'name:\\s*\"([^\"]+)\"', props_str)\n            rank_match = re.search(r'rank:\\s*\"([^\"]*)\"', props_str)\n            species_match = re.search(r'species:\\s*\"([^\"]+)\"', props_str)\n            \n            if name_match:\n                name = name_match.group(1)\n                rank = rank_match.group(1) if rank_match else \"\"\n                species = species_match.group(1) if species_match else \"Unknown\"\n                \n                characters.append({\n                    \"alias\": alias,\n                    \"name\": name,\n                    \"rank\": rank,\n                    \"species\": species\n                })\n            continue\n                \n        # Extract ship creation\n        ship_match = re.search(r'CREATE\\s+\\((\\w+):Ship\\s+\\{(.+?)\\}\\)', line)\n        if ship_match:\n            alias = ship_match.group(1)\n            props_str = ship_match.group(2)\n            \n            # Extract properties\n            name_match = re.search(r'name:\\s*\"([^\"]+)\"', props_str)\n            class_match = re.search(r'class:\\s*\"([^\"]*)\"', props_str)\n            \n            if name_match:\n                name = name_match.group(1)\n                ship_class = class_match.group(1) if class_match else \"Unknown\"\n                \n                ships.append({\n                    \"alias\": alias,\n                    \"name\": name,\n                    \"class\": ship_class\n                })\n            continue\n        \n        # Extract relationships\n        rel_match = re.search(r'CREATE\\s+\\((\\w+)\\)-\\[:(\\w+)\\]->\\((\\w+)\\)', line)\n        if rel_match:\n            source_alias = rel_match.group(1)\n            rel_type = rel_match.group(2)\n            target_alias = rel_match.group(3)\n            \n            if rel_type in [\"SERVES_ON\", \"FRIENDS_WITH\", \"OWNED_BY\"]:\n                relationships.append({\n                    \"source\": source_alias,\n                    \"type\": rel_type,\n                    \"target\": target_alias\n                })\n    \n    return characters, ships, relationships\n\ndef process_file(file_path, send_script, run_script):\n    \"\"\"Process a single text file, extract entities and create database entries.\"\"\"\n    # Read the prompt file\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        prompt = f.read().strip()\n    \n    # Generate Cypher from text using send_prompt.py\n    result = subprocess.run(\n        [\"python3\", send_script, prompt],\n        capture_output=True,\n        text=True\n    )\n    \n    if result.returncode != 0:\n        print(f\"[Error] {result.stderr.strip()}\")\n        return\n    \n    # Extract entity information\n    characters, ships, relationships = extract_character_ship_info(result.stdout)\n    \n    # Create Characters\n    if characters:\n        print(f\"Creating {len(characters)} Character nodes:\")\n        for char in characters:\n            # MERGE ensures we don't create duplicates\n            cypher = f\"\"\"\n            MERGE (c:Character {{name: \"{char['name']}\"}})\n            ON CREATE SET \n                c.species = \"{char['species']}\",\n                c.rank = \"{char['rank']}\"\n            RETURN c\n            \"\"\"\n            \n            print(f\"Creating character: {char['name']}\")\n            cypher_result = subprocess.run(\n                [\"python3\", run_script, cypher],\n                capture_output=True,\n                text=True\n            )\n            \n            if \"error\" in cypher_result.stdout.lower() or \"error\" in cypher_result.stderr.lower():\n                print(f\"[Error] {cypher_result.stderr.strip() or cypher_result.stdout.strip()}\")\n    \n    # Create Ships\n    if ships:\n        print(f\"Creating {len(ships)} Ship nodes:\")\n        for ship in ships:\n            # MERGE ensures we don't create duplicates\n            cypher = f\"\"\"\n            MERGE (s:Ship {{name: \"{ship['name']}\"}})\n            ON CREATE SET \n                s.class = \"{ship['class']}\"\n            RETURN s\n            \"\"\"\n            \n            print(f\"Creating ship: {ship['name']}\")\n            cypher_result = subprocess.run(\n                [\"python3\", run_script, cypher],\n                capture_output=True,\n                text=True\n            )\n            \n            if \"error\" in cypher_result.stdout.lower() or \"error\" in cypher_result.stderr.lower():\n                print(f\"[Error] {cypher_result.stderr.strip() or cypher_result.stdout.strip()}\")\n    \n    # Create Relationships\n    if relationships:\n        print(f\"Creating {len(relationships)} relationships:\")\n        for rel in relationships:\n            # Find the actual character/ship entities from the lists\n            source_type = \"Character\"  # Default assumption\n            source_name = None\n            \n            # Look for source in characters\n            for char in characters:\n                if char[\"alias\"] == rel[\"source\"]:\n                    source_name = char[\"name\"]\n                    break\n            \n            # If not found in characters, check ships\n            if not source_name:\n                for ship in ships:\n                    if ship[\"alias\"] == rel[\"source\"]:\n                        source_type = \"Ship\"\n                        source_name = ship[\"name\"]\n                        break\n            \n            # If still not found, skip this relationship\n            if not source_name:\n                print(f\"Skipping relationship: source alias '{rel['source']}' not found\")\n                continue\n            \n            # Now find the target\n            target_type = \"Character\"  # Default assumption \n            target_name = None\n            \n            # For SERVES_ON, target should be a Ship\n            if rel[\"type\"] == \"SERVES_ON\":\n                target_type = \"Ship\"\n                for ship in ships:\n                    if ship[\"alias\"] == rel[\"target\"]:\n                        target_name = ship[\"name\"]\n                        break\n            else:\n                # For other relationships, look for target in characters\n                for char in characters:\n                    if char[\"alias\"] == rel[\"target\"]:\n                        target_name = char[\"name\"]\n                        break\n            \n            # If target not found, skip\n            if not target_name:\n                print(f\"Skipping relationship: target alias '{rel['target']}' not found\")\n                continue\n            \n            # Create the relationship using MATCH to find existing nodes\n            cypher = f\"\"\"\n            MATCH (a:{source_type} {{name: \"{source_name}\"}})\n            MATCH (b:{target_type} {{name: \"{target_name}\"}})\n            MERGE (a)-[r:{rel['type']}]->(b)\n            RETURN a, r, b\n            \"\"\"\n            \n            print(f\"Creating relationship: {source_name} -[{rel['type']}]-> {target_name}\")\n            cypher_result = subprocess.run(\n                [\"python3\", run_script, cypher],\n                capture_output=True,\n                text=True\n            )\n            \n            if \"error\" in cypher_result.stdout.lower() or \"error\" in cypher_result.stderr.lower():\n                print(f\"[Error] {cypher_result.stderr.strip() or cypher_result.stdout.strip()}\")\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Bulk-process text files: extract entities and create properly connected Neo4j graph\"\n    )\n    parser.add_argument(\n        \"--notes-dir\",\n        default=NOTES_DIR,\n        help=\"Path to folder containing text files (default: %(default)s)\"\n    )\n    parser.add_argument(\n        \"--script\",\n        default=SEND_PROMPT_SCRIPT,\n        help=\"Path to send_prompt.py (default: %(default)s)\"\n    )\n    parser.add_argument(\n        \"--run-script\",\n        default=RUN_CYPHER_SCRIPT,\n        help=\"Path to run_cypher.py (default: %(default)s)\"\n    )\n    args = parser.parse_args()\n\n    for fname in sorted(os.listdir(args.notes_dir)):\n        if not fname.lower().endswith(\".txt\"):\n            continue\n        \n        path = os.path.join(args.notes_dir, fname)\n        print(f\"--- Processing {fname} ---\")\n        \n        process_file(path, args.script, args.run_script)\n        print()  # blank line between files\n\nif __name__ == \"__main__\":\n    main()\n```\n\nSave the script, then clear out the database once more before testing.\n\n```plaintext\nMATCH (n) DETACH DELETE n\n```\n\nThen retest:\n\n```python\npython bulk_process.py\n```\n\nRefresh Neo4j and you should see a new set of connected nodes based on your input documents. To view all the nodes and relationships at once, run:\n\n```plaintext\nMATCH (n)\nOPTIONAL MATCH (n)-[r]->(m)\nRETURN n, r, m\n```\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1745149748625/16a7bebb-0fff-4069-a38e-40193b3ef6a7.png align=\"center\")\n\nAnd there you have it! You can now extract entities and relationships, generate Cypher queries and run them in bulk, based on a folder of text files. This will generate a collection of connected nodes based on the schema defined in your `send_prompt.py` script.\n\nCrafting an appropriate schema for your input data is one of the most important steps to generating a quality knowledge graph. Be sure to create a well-defined schema that accurately represents your data before bulk-processing your files.\n\n## Conclusion\n\nKnowledge graphs are amazing tools for visualizing data and performing complex queries to uncover new insights about relationships. They excel at tasks in fraud detection, recommendation engines, social networks, and RAG. Building a knowledge graph is as easy as running a few Cypher queries, but generating those queries from data can be challenging. This guide has shown one way you can generate these queries locally using a text-to-cypher query from Hugging Face.\n\nSpecial thanks to Jason Koo from Neo4j for this excellent [video tutorial](https://www.youtube.com/watch?v=9pdxSlxfqNY) on using a Hugging Face model in Ollama.\n\n### Whatâ€™s Next?\n\nFrom here you could connect the Neo4j database to an AI assistant or agent to perform RAG, or use the graph to discover relationships and communities of nodes that emerge as the bulk data is processed.",
      "stars": null,
      "comments": 8,
      "upvotes": 95,
      "read_time": "16 min read",
      "language": null
    },
    {
      "title_en": "Why Your Node.js App Freezes (And How Worker Threads Can Save You)",
      "url": "https://abigeal.hashnode.dev/why-your-nodejs-app-freezes-and-how-worker-threads-can-save-you",
      "source": "hashnode",
      "published_at": "2025-04-17T22:35:00.042000+00:00",
      "external_id": null,
      "tags": [
        "Node.js",
        "Beginner Developers",
        "beginner",
        "#beginners #learningtocode #100daysofcode"
      ],
      "content_length": 6919,
      "content_preview": "You've built an awesome Node.js app that your friends love using. Then disaster strikes - someone tries to generate a report and your entire application freezes. Everyone's experience grinds to a halt.\n\nThis happens because **Node.js runs on a single thread by default**. When something CPU-intensive comes along, everything else has to wait.\n\nEnter worker threads: your solution to keeping Node.js applications responsive.\n\n## Worker Threads Explained (For Real Humans)\n\nImagine you're running a piz",
      "content_full": "You've built an awesome Node.js app that your friends love using. Then disaster strikes - someone tries to generate a report and your entire application freezes. Everyone's experience grinds to a halt.\n\nThis happens because **Node.js runs on a single thread by default**. When something CPU-intensive comes along, everything else has to wait.\n\nEnter worker threads: your solution to keeping Node.js applications responsive.\n\n## Worker Threads Explained (For Real Humans)\n\nImagine you're running a pizza restaurant:\n\n**Without Worker Threads:**\n\n* You're taking orders, making pizzas, serving customers, and handling payments all by yourself\n    \n* When a large catering order comes in, you stop everything else to make 20 pizzas\n    \n* Regular customers get frustrated and leave while waiting\n    \n\n**With Worker Threads:**\n\n* You still take orders and handle payments\n    \n* But you've hired specialized pizza chefs to handle the cooking\n    \n* When that big catering order arrives, you send it to your chefs\n    \n* You keep serving other customers without missing a beat\n    \n\nThat's exactly what worker threads do in Node.js. They let you delegate heavy tasks to separate \"workers\" while keeping your main application responsive.\n\n## When Should You Use Worker Threads? Real Examples:\n\n### 1\\. Image Processing\n\n**Before:** Your app lets users upload profile pictures, but resizing and optimizing them freezes the entire site for several seconds.\n\n**After:** With worker threads, the main application keeps running smoothly while a worker thread handles the image processing in the background.\n\n### 2\\. Reports Generation\n\n**Before:** When a user generates a monthly sales report that analyzes thousands of transactions, your app becomes completely unresponsive.\n\n**After:** Your main thread instantly acknowledges the report request, then a worker thread crunches the numbers while users continue browsing.\n\n### 3\\. Search Functionality\n\n**Before:** Searching through a large database makes your app stutter for everyone.\n\n**After:** The search processing happens in a worker thread, keeping the UI snappy and responsive.\n\n## Let's Build Something Real: A Simple Worker Thread Example\n\nHere's how to implement worker threads in a way that actually makes sense. We'll create a simple web server that can handle intensive calculations without freezing.\n\n### Step 1: Set Up Your Project\n\nCreate a new folder and initialize your project:\n\n```bash\nmkdir my-responsive-app\ncd my-responsive-app\nnpm init -y\n```\n\n### Step 2: Create Your Main Application File (app.js)\n\n```javascript\nconst http = require('http');\nconst { Worker } = require('worker_threads');\n\n// Create a simple HTTP server\nconst server = http.createServer((req, res) => {\n    // This route will do heavy calculations\n    if (req.url === '/calculate') {\n        console.log('Calculation requested - sending to worker thread');\n        \n        // Create a new worker thread\n        const worker = new Worker('./worker.js');\n        \n        // Listen for the result from our worker\n        worker.on('message', (result) => {\n            res.writeHead(200, { 'Content-Type': 'application/json' });\n            res.end(JSON.stringify({ result }));\n        });\n        \n        // Handle any errors\n        worker.on('error', (error) => {\n            res.writeHead(500, { 'Content-Type': 'application/json' });\n            res.end(JSON.stringify({ error: error.message }));\n        });\n    } \n    // This route stays super fast\n    else if (req.url === '/hello') {\n        res.writeHead(200, { 'Content-Type': 'text/plain' });\n        res.end('Hello, I respond instantly because I\\'m not blocked!');\n    } \n    // Homepage\n    else {\n        res.writeHead(200, { 'Content-Type': 'text/html' });\n        res.end(`\n            <h1>Worker Threads Demo</h1>\n            <p>Try these links:</p>\n            <ul>\n                <li><a href=\"/calculate\">Run heavy calculation</a> (processed in worker thread)</li>\n                <li><a href=\"/hello\">Quick response</a> (shows main thread stays responsive)</li>\n            </ul>\n        `);\n    }\n});\n\n// Start the server\nconst PORT = 3000;\nserver.listen(PORT, () => {\n    console.log(`Server running at http://localhost:${PORT}`);\n});\n```\n\n### Step 3: Create Your Worker Thread File (worker.js)\n\n```javascript\nconst { parentPort } = require('worker_threads');\n\n// This function simulates an intensive calculation\nfunction doHeavyCalculation() {\n    console.log('Worker: Starting heavy calculation...');\n    \n    // Simulate CPU-intensive work\n    let result = 0;\n    for (let i = 0; i < 5_000_000_000; i++) {\n        // This purposely inefficient loop will keep CPU busy\n        if (i % 1_000_000_000 === 0) {\n            console.log(`Worker: ${i/1_000_000_000}/5 billion iterations completed`);\n        }\n        result += Math.sqrt(i);\n    }\n    \n    return { \n        result: result,\n        completedAt: new Date().toISOString()\n    };\n}\n\n// Start the heavy calculation\nconsole.log('Worker: I was created to do the heavy lifting!');\nconst result = doHeavyCalculation();\n\n// Send the result back to the main thread\nparentPort.postMessage(result);\nconsole.log('Worker: Done! Result sent back to main thread');\n```\n\n## Common Mistakes and How to Avoid Them\n\n1. **Creating too many workers**\n    \n    * **Problem:** Each worker consumes memory and resources\n        \n    * **Solution:** Consider using a worker pool (like `node:worker_threads` with `piscina` package)\n        \n2. **Sending too much data between threads**\n    \n    * **Problem:** Large data transfers between threads can slow things down\n        \n    * **Solution:** Only transfer the minimal data needed\n        \n3. **Using workers for I/O tasks**\n    \n    * **Problem:** Worker threads don't help much with I/O (file/network) operations\n        \n    * **Solution:** Use async/await and promises for I/O instead\n        \n\n## When NOT to Use Worker Threads\n\nWorker threads aren't always the answer. They're best for CPU-intensive work, but unnecessary for:\n\n* **Database operations** (use async/await instead)\n    \n* **API requests** (use promises instead)\n    \n* **File operations** (use the fs promises API instead)\n    \n* **Simple calculations** (the overhead isn't worth it)\n    \n\n## Next Steps: Level Up Your Worker Thread Skills\n\nReady to take your worker thread knowledge further? Try these challenges:\n\n1. Create a worker pool to manage multiple workers\n    \n2. Implement two-way communication with your workers\n    \n3. Share memory between threads for better performance\n    \n\n## Conclusion\n\nWorker threads are your secret weapon for keeping Node.js applications fast and responsive when handling CPU-intensive tasks. By offloading heavy work to separate threads, you ensure your users never experience those frustrating freezes.\n\nHave you implemented worker threads in your own projects? Share your experience in the comments below!\n\n---",
      "stars": null,
      "comments": 4,
      "upvotes": 51,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "Lessons Learned from Running Serverless Applications in Production on AWS",
      "url": "https://ujjwalmahar.hashnode.dev/lessons-learned-from-running-serverless-applications-in-production-on-aws",
      "source": "hashnode",
      "published_at": "2025-04-15T10:19:11.826000+00:00",
      "external_id": null,
      "tags": [
        "acdblr-blogathon",
        "AWS",
        "blogathon",
        "serverless"
      ],
      "content_length": 19670,
      "content_preview": "## Introduction\n\nHave you heard about serverless on AWS? It sounds amazing: your code scales up automatically, you only pay when it runs, and you don't have to worry about servers. Tools like AWS Lambda, API Gateway, and DynamoDB make this possible.\n\nGetting started is pretty easy. You can set up a basic function with a few clicks or a simple template. But using these tools for a *real* application, one that customers use every day? That's when things get interesting, and you start learning less",
      "content_full": "## Introduction\n\nHave you heard about serverless on AWS? It sounds amazing: your code scales up automatically, you only pay when it runs, and you don't have to worry about servers. Tools like AWS Lambda, API Gateway, and DynamoDB make this possible.\n\nGetting started is pretty easy. You can set up a basic function with a few clicks or a simple template. But using these tools for a *real* application, one that customers use every day? That's when things get interesting, and you start learning lessons you didn't expect.\n\nOur team jumped into serverless for some important projects because the benefits looked great. And they often are! But it wasn't always easy. We ran into some surprises. Here are the biggest things we learned the hard way while running serverless apps in production.\n\n## Lesson 1: You Need to See What Your Code is Doing\n\n**What We Thought:** \"We have logs, just like our old apps. That's probably fine.\"\n\n**What Really Happened:** We were wrong! When serverless code runs across many small pieces (Lambda, API Gateway, databases), figuring out problems is much harder than with one big application. If something broke, finding the cause felt like searching for a needle in a haystack â€“ actually, *multiple* haystacks! It took forever to track down simple errors.\n\n**How We Fixed It:**\n\n* **Better Logs:** We made sure all our logs were in a consistent format (JSON) with important info like a unique ID to track each request as it moved between services. This made searching logs much easier.\n    \n* **Using AWS X-Ray:** Think of X-Ray like a map for your requests. It shows you exactly where a request went, how long each step took, and where things slowed down or failed. It was a huge help for finding problems quickly.\n    \n* **Better Alerts:** Basic alerts tell you if a function crashed. But we started adding alerts for *business* problems (like \"failed orders\") and watching for high error rates. This way, we often knew about issues *before* users started complaining.\n    \n\n## Lesson 2: Keep an Eye on Your Spending\n\n**What We Thought:** \"Pay-per-use sounds cheap! We only pay when it runs.\"\n\n**What Really Happened:** Serverless *can* be cheap, but costs can sneak up on you if you're not careful. We found out the hard way. Maybe a function ran too long or used too much memory. Maybe a bug caused a function to call itself over and over. Maybe we logged way too much information. One small mistake even caused millions of function runs over a weekend, costing us real money!\n\n**How We Fixed It:**\n\n* **Check the Bill Often:** We started using AWS tools (like Cost Explorer) regularly to see exactly where the money was going. Adding \"tags\" (labels) to everything helped us sort costs by project or service.\n    \n* **Set Spending Limits (Budgets):** We set up AWS Budgets to send us alerts if costs started getting too high. This gave us a warning *before* the end of the month.\n    \n* **Give Functions the Right Resources:** We stopped guessing how much memory our Lambda functions needed. We looked at the actual usage and adjusted them â€“ not too much (wasteful), not too little (slow or failing).\n    \n* **Think About Costs When Designing:** We started thinking about how different choices affect the price. For example, sometimes grouping messages together is cheaper than sending lots of small ones.\n    \n\n## Lesson 3: Sometimes Serverless is Slow at First (Cold Starts)\n\n**What We Thought:** \"Lambda scales up instantly!\"\n\n**What Really Happened:** Mostly, yes. But when a function hasn't run for a little while, the *very first* request takes extra time to start up. This is called a \"cold start.\" It might only be a second or two, but if a user is waiting for an answer from your API, that delay feels really bad.\n\n**How We Fixed It:**\n\n* **Keep Some Functions Ready:** For important functions where speed matters most, we use something called \"Provisioned Concurrency.\" It costs a bit extra, but it keeps some function instances warm and ready to go, eliminating cold starts.\n    \n* **Keep Code Small and Fast:** Smaller code packages and fewer extra bits usually mean faster start times. We also learned to set up things like database connections *outside* the main part of the function, so they can be reused if the function stays warm.\n    \n* **Maybe Choose a Different Language:** Some programming languages tend to start faster than others. We looked at which ones worked best for our needs.\n    \n* **Don't Make Users Wait:** If the work doesn't need to be done instantly, we often have the API just put a message in a queue (like SQS) and tell the user \"Got it!\". Then, another Lambda function processes the message in the background. The user doesn't notice any cold start delay.\n    \n\n## Lesson 4: Handle Errors Carefully (Especially Retries)\n\n**What We Thought:** \"If a function fails, things like SQS will just retry it automatically. Easy!\"\n\n**What Really Happened:** Retries are great, but they can be dangerous! Imagine a function that charges a credit card. If it fails *after* charging but *before* saving the order, a simple retry might charge the customer *again*. We learned functions need to be \"safe to retry\" (the technical term is idempotent). Also, sometimes an error happens that retrying will *never* fix (like bad data). Retrying those just wastes time and money.\n\n**How We Fixed It:**\n\n* **Make Functions Safe to Retry:** Before doing something important (like charging money or changing data), our functions now check if that exact task was *already* completed successfully, often using a unique ID.\n    \n* **Use a \"Dead-Letter Queue\" (DLQ):** We set up a special queue (a DLQ) for messages that fail too many times. Instead of blocking everything, the failed message goes to the DLQ where we can look at it later and figure out what went wrong. We watch these DLQs closely!\n    \n* **Catch Errors Smartly:** Inside our functions, we started catching errors and thinking: Is this a temporary glitch (okay to retry)? Or is this a permanent problem (stop retrying, log the error, maybe send the message straight to the DLQ)?\n    \n\n## Lesson 5: Use Code to Set Up Your AWS Stuff (IaC)\n\n**What We Thought:** \"It's just a few functions, we can set them up by clicking in the AWS console.\"\n\n**What Really Happened:** A real serverless app isn't \"just a few functions.\" It's usually lots of functions, API parts, database tables, queues, permissions, etc. Trying to manage all that by clicking around in different environments (like development, testing, production) is a recipe for mistakes and confusion. It's slow, hard to track changes, and easy to make environments different without meaning to.\n\n**How We Fixed It:**\n\n* **Define Everything in Code:** We now use tools like AWS SAM, CDK, or Terraform to write code that describes *all* the AWS pieces our application needs. This code can be saved, shared, and versioned like any other code.\n    \n* **Automate Updates:** We set up systems (called CI/CD pipelines) that automatically test and apply changes from our infrastructure code. This is much safer and faster than manual clicking.\n    \n* **Get Identical Environments:** Using code makes it easy to create exact copies of our setup for testing or different stages. This helps avoid the \"it worked on my machine!\" problem.\n    \n\n## Wrapping Up\n\nServerless on AWS has been great for our team. It lets us move faster and build things that can handle huge amounts of traffic. But it's not magic. You need to learn how these distributed systems really work.\n\nThinking about how to *see* what's happening, watching costs, dealing with cold starts, handling errors safely, and using code to manage your setup â€“ these aren't optional extras. They are the key lessons we learned from using serverless for real. It takes some effort to learn this stuff, but once you do, you can really unlock the benefits of serverless on AWS.\n\nOur team dove headfirst into serverless for several key projects, lured by its compelling advantages. While we've absolutely reaped the benefits, the path wasn't always smooth. We encountered challenges we didn't anticipate based on the initial hype. Here are some of the most critical, hard-won lessons we learned operating serverless applications in the demanding environment of production on AWS.\n\n## Lesson 1: Observability Isn't a Feature, It's the Foundation\n\n**The Naive Start:** We initially figured, \"We have CloudWatch Logs, just like our old EC2 apps. That should be enough.\"\n\n**Production Pain:** Wrong. Debugging a distributed serverless system is vastly different from troubleshooting a monolith. A single user click might ripple through API Gateway, multiple Lambda functions, DynamoDB reads/writes, and maybe even SQS queues. When things broke, piecing together that fragmented journey across dozens of separate log streams was excruciatingly slow. Finding the root cause of even simple errors felt like searching for needles in multiple haystacks simultaneously.\n\n**The Essential Fixes:**\n\n* **Structured Logging is Mandatory:** We enforced strict JSON logging across all Lambda functions. Every log entry had to include key context like the `correlationId` (tracing the request across services), `userId` (if available), and specific function identifiers. This instantly made CloudWatch Logs Insights usable for actual debugging, not just viewing raw output.\n    \n* **Embrace AWS X-Ray:** Distributed tracing isn't optional for serverless. Implementing X-Ray was a game-changer. Suddenly, we could visualize the entire request path, see exact timings for each hop (API Gateway -&gt; Lambda -&gt; DynamoDB), identify bottlenecks immediately, and pinpoint which downstream call failed. Enabling the basic tracing checkbox in Lambda is step one; instrumenting your code with the X-Ray SDK to create custom subsegments for specific business logic or SDK calls provides invaluable granularity.\n    \n* **Go Beyond Basic Metrics:** Lambda's built-in metrics (invocations, errors, duration, throttles) are crucial, but not enough. We started emitting custom CloudWatch Metrics directly from our functions for critical business actions (e.g., `OrdersProcessed`, `PaymentFailures`, `ItemsScanned`). We then built targeted CloudWatch Alarms on these custom metrics, plus critical technical metrics like high error rates (&gt;1%), DLQ depths (&gt;0), and persistent throttles. This shifted us from reactive debugging to proactive alerting.\n    \n\n## Lesson 2: Serverless Costs Aren't Magic â€“ Monitor or Be Surprised\n\n**The Misconception:** \"Pay-per-use means it's always cheap! If it doesn't run, we don't pay.\"\n\n**Production Reality:** While often cheaper overall, serverless costs can explode unexpectedly if you're not careful. We learned this the hard way. Causes included: inefficient Lambda code (high memory and long duration), accidental recursive function calls leading to infinite loops, poorly tuned SQS retry policies causing functions to hammer a failing downstream service, and massive CloudWatch Logs ingestion from overly verbose logging. One misconfigured event trigger cost us hundreds of dollars over a weekend due to millions of unnecessary invocations.\n\n**The Cost Control Toolkit:**\n\n* **Cost Explorer is Your Best Friend:** Regularly dive into AWS Cost Explorer. Tag everything (functions, tables, queues, APIs) religiously with project codes or service names. Filter by tags and services to understand exactly where your money is going. Identify the top cost drivers â€“ is it Lambda compute, data transfer, DynamoDB reads, or CloudWatch Logs?\n    \n* **Set AWS Budgets with Alerts:** Don't wait for the monthly bill. Set up AWS Budgets for specific services, tags, or your overall account, with alert thresholds (e.g., at 50%, 80%, 100% of expected spend). Get notified before costs spiral out of control.\n    \n* **Right-Size Your Functions (Continuously):** Don't guess memory allocation. Analyze function performance using CloudWatch metrics or AWS Compute Optimizer. Over-provisioning wastes money directly; under-provisioning increases duration (costing more compute time) and can lead to timeouts. Finding the \"sweet spot\" often requires experimentation.\n    \n* **Architect with Cost in Mind:** Understand the pricing models. Is API Gateway's per-request cost significant at your scale? Would batching SQS messages reduce Lambda invocations? Are you performing costly DynamoDB scans instead of efficient `Query` or `GetItem` operations? Is extensive DEBUG logging necessary in production, or can it be controlled via environment variables?\n    \n\n## Lesson 3: Cold Starts Happen â€“ Mitigate or Accept the Latency\n\n**The Oversimplification:** \"Lambda scales instantly!\"\n\n**Production Reality:** Yes, it scales out, but the first request to a new or inactive function instance incurs a \"cold start.\" This is the time AWS needs to provision the environment, download your code package, and initialize the runtime. For user-facing synchronous APIs via API Gateway, adding hundreds of milliseconds, or sometimes even seconds, of unpredictable latency during a cold start can lead to a terrible user experience.\n\n**Strategies for Mitigation:**\n\n* **Provisioned Concurrency (The Big Gun):** For your most latency-sensitive functions, Provisioned Concurrency is the most effective solution. It keeps a specified number of execution environments warm and ready, eliminating cold starts for requests hitting those instances. Be aware: this incurs an additional cost, as you pay for the provisioned capacity whether it's used or not. Use it strategically.\n    \n* **Optimize Your Code & Package:** Keep your deployment artifacts small. Minimize dependencies; use tools like Webpack (for Node.js) or dependency pruning. Initialize expensive resources (like database connections or SDK clients) outside the main function handler. This allows them to be reused across multiple invocations within the same warm execution environment, reducing initialization time on subsequent requests.\n    \n* **Consider Runtime Choice:** We observed differences. Node.js and Python generally have faster cold starts than Java (unless using optimizations like GraalVM). Compiled languages like Go can also be very fast. Benchmark and choose based on your application's needs and your team's expertise.\n    \n* **Design Asynchronously:** Where possible, avoid making the user wait synchronously for long-running processes. An API Gateway endpoint could simply drop a message onto SQS and return a `202 Accepted` immediately. A separate Lambda function processes the message from the queue asynchronously. This masks the latency (including potential cold starts) of the background processing from the end-user.\n    \n\n## Lesson 4: Idempotency and Error Handling Are Non-Negotiable\n\n**The Initial Hope:** \"Event sources like SQS have built-in retries, so we're covered if a function fails.\"\n\n**Production Reality:** Retries are essential but dangerous if not handled carefully. If your function performs an action with side effects (charging a credit card, updating inventory, sending an email) and isn't idempotent (meaning running it multiple times with the same input produces the same result), retries can cause chaos. Imagine double-charging a customer because the function failed after payment but before confirming success. Furthermore, not all errors are retryable. A temporary network glitch? Retry. Invalid input data or a critical bug in the code? Retrying just wastes resources and delays fixing the real problem, potentially blocking your queue with \"poison pill\" messages.\n\n**Building Resilience:**\n\n* **Design for Idempotency:** This is critical for any function with side effects triggered by asynchronous events. Common techniques include:\n    \n    * Using unique transaction IDs passed in the event payload. Before processing, check a DynamoDB table (using conditional writes) to see if that ID has already been successfully processed.\n        \n    * Designing database updates to be inherently safe for re-execution (e.g., `SET status = 'processed' WHERE status = 'pending'`).\n        \n* **Utilize Dead-Letter Queues (DLQs):** Configure DLQs (usually an SQS queue or SNS topic) for your asynchronous Lambda event sources (SQS, SNS, EventBridge, etc.). When a message fails processing after the configured number of retries, AWS automatically sends it to the DLQ. This prevents failed messages from blocking the main queue and allows you to inspect, potentially fix, and manually re-drive these failed events later. Monitor your DLQ depths! A growing DLQ is a critical alert.\n    \n* **Smart Error Handling within the Function:** Use `try...catch` blocks effectively. Differentiate between transient errors (e.g., downstream service throttling, temporary network issues) that might resolve on retry (so re-throw the error or allow Lambda's retry mechanism to handle it) and permanent errors (e.g., validation failed, unrecoverable state). For permanent errors, catch them, log detailed context, potentially emit a custom metric, and return successfully (or specifically signal not to retry) to prevent pointless retries and ensure the message goes to the DLQ if configured.\n    \n\n## Lesson 5: Infrastructure as Code (IaC) Prevents Production Chaos\n\n**The Temptation:** \"It's just a few functions; we'll manage them through the AWS Console for now.\"\n\n**Production Reality:** A typical serverless application isn't just \"a few functions.\" It's a constellation of Lambda functions, API Gateway resources (routes, methods, authorizers), fine-grained IAM roles and policies, DynamoDB tables, SQS queues, SNS topics, EventBridge rules, etc. Trying to manage this complex web manually across multiple environments (dev, staging, prod) via clicks is a recipe for disaster. It's slow, error-prone, impossible to version control, and makes replicating environments reliably a nightmare. We quickly realized manual management was unsustainable.\n\n**The IaC Imperative:**\n\n* **Pick Your Framework and Stick To It:** Mandate the use of an IaC tool from day one for anything beyond trivial experimentation. AWS SAM (Serverless Application Model) and AWS CDK (Cloud Development Kit) are excellent, AWS-native options specifically tailored for serverless. Terraform and the Serverless Framework are also powerful, popular choices.\n    \n* **Automate Everything with CI/CD:** Integrate your IaC framework into a robust CI/CD pipeline (e.g., AWS CodePipeline, GitHub Actions, GitLab CI). Every commit should trigger automated builds, tests (unit, integration), and deployments. This ensures consistency, repeatability, and reduces the risk of manual deployment errors.\n    \n* **Achieve True Environment Parity:** IaC makes spinning up identical copies of your entire stack for different environments straightforward. This drastically reduces the \"but it worked in dev!\" class of problems.\n    \n\n## Conclusion\n\nAdopting serverless on AWS for production applications has been transformative for our team, enabling faster development cycles and incredible scalability. However, it's not a magic bullet. Success requires moving beyond the initial hype and embracing practices tailored to distributed, event-driven architectures. Prioritizing observability, diligently managing costs, mitigating cold starts strategically, engineering for idempotency and robust error handling, and enforcing disciplined automation through Infrastructure as Code are not optional extras â€“ they are the essential lessons learned from running serverless in the real world. The learning curve is undeniable, but mastering these principles unlocks the true power and potential of the serverless paradigm on AWS.",
      "stars": null,
      "comments": 5,
      "upvotes": 45,
      "read_time": "14 min read",
      "language": null
    },
    {
      "title_en": "Advanced Patterns & Integration with Frameworks",
      "url": "https://journeytocode.io/advanced-patterns-and-integration-with-frameworks",
      "source": "hashnode",
      "published_at": "2025-04-13T15:21:21.415000+00:00",
      "external_id": null,
      "tags": [
        "Web Components",
        "Svelte",
        "React",
        "Vue.js",
        "Angular"
      ],
      "content_length": 16507,
      "content_preview": "Web Components promise the holy grail of front-end development: truly reusable, framework-agnostic components that work anywhere. Yet many developers struggle to integrate these universal building blocks with popular frameworks like React, Angular, or Vue. In this guide, we'll explore advanced patterns that bridge these worlds, optimize performance, and ensure your Web Components shine in any ecosystem.\n\n## Why Framework Integration Matters\n\nWeb Components were designed to be universal, but the ",
      "content_full": "Web Components promise the holy grail of front-end development: truly reusable, framework-agnostic components that work anywhere. Yet many developers struggle to integrate these universal building blocks with popular frameworks like React, Angular, or Vue. In this guide, we'll explore advanced patterns that bridge these worlds, optimize performance, and ensure your Web Components shine in any ecosystem.\n\n## Why Framework Integration Matters\n\nWeb Components were designed to be universal, but the reality of modern web development means they must coexist with frameworks that have their own component models and lifecycles. Successful integration requires understanding both worlds and the boundaries between them.\n\n> \"The true power of Web Components isn't isolation from frameworks, but seamless cooperation with them.\"\n\nWhat makes integration challenging?\n\n* **Different Data Flow Models**: React's unidirectional data flow differs fundamentally from the property-based approach of Web Components.\n    \n* **Event Handling Discrepancies**: Each framework has its own event system that must be reconciled with the DOM event model.\n    \n* **Lifecycle Management**: Coordinating component lifecycle events between frameworks and Web Components requires careful orchestration.\n    \n\nLet's explore how to overcome these challenges with practical, reusable patterns.\n\n## Seamless Integration with Modern Frameworks\n\n### React Integration Strategies\n\nReact's declarative approach to UI challenges direct integration with the imperative DOM APIs of Web Components. Here are advanced patterns to bridge this gap:\n\n#### 1\\. The Ref Pattern: Direct DOM Access\n\n```jsx\nfunction MyReactComponent() {\n  const wcRef = useRef(null);\n  \n  useEffect(() => {\n    if (wcRef.current) {\n      // Direct property assignment for complex data\n      wcRef.current.complexData = { key: \"value\" };\n      \n      // Event handling with proper cleanup\n      const handleEvent = (e) => console.log(e.detail);\n      wcRef.current.addEventListener('custom-event', handleEvent);\n      \n      return () => {\n        wcRef.current.removeEventListener('custom-event', handleEvent);\n      };\n    }\n  }, []);\n  \n  return <my-web-component ref={wcRef} string-prop=\"This works\" />;\n}\n```\n\n**Key Insights:**\n\n* React passes string attributes directly, but complex data requires refs\n    \n* Event handling needs manual listeners with proper cleanup\n    \n* Ref access enables imperative methods on Web Components\n    \n\n#### 2\\. The Wrapper Pattern: Creating React-Friendly Components\n\nReact developers expect components with React-like patterns. Create wrapper components that abstract Web Component peculiarities:\n\n```jsx\n// Wrapper that handles property mapping and events\nfunction EnhancedWebComponent({ data, onCustomEvent, children }) {\n  const ref = useRef(null);\n  \n  useEffect(() => {\n    const element = ref.current;\n    if (element) {\n      // Set complex data\n      element.data = data;\n      \n      // Handle events\n      const eventHandler = (e) => onCustomEvent(e.detail);\n      element.addEventListener('custom-event', eventHandler);\n      \n      return () => element.removeEventListener('custom-event', eventHandler);\n    }\n  }, [data, onCustomEvent]);\n  \n  return (\n    <my-web-component ref={ref}>\n      {children}\n    </my-web-component>\n  );\n}\n\n// Usage feels like a normal React component\nfunction App() {\n  return (\n    <EnhancedWebComponent \n      data={{complex: \"data\"}} \n      onCustomEvent={handleEvent}\n    >\n      <p>Child content</p>\n    </EnhancedWebComponent>\n  );\n}\n```\n\n**Interactive Demo:** Try the React integration patterns yourself\n\n%[https://codepen.io/mnichols08/pen/qEBeQgd] \n\n### Angular Integration Strategies\n\nAngular has built-in support for Web Components through its `@angular/elements` package, but deeper integration still requires specific techniques:\n\n#### 1\\. Element Binding with Custom Directives\n\n```typescript\n@Directive({\n  selector: 'my-web-component'\n})\nexport class WebComponentDirective implements OnInit, OnChanges, OnDestroy {\n  @Input() complexData: any;\n  @Output() customEvent = new EventEmitter<any>();\n  \n  constructor(private el: ElementRef) {}\n  \n  ngOnInit() {\n    this.el.nativeElement.addEventListener('custom-event', \n      this.handleCustomEvent.bind(this));\n  }\n  \n  ngOnChanges(changes: SimpleChanges) {\n    if (changes.complexData) {\n      this.el.nativeElement.complexData = this.complexData;\n    }\n  }\n  \n  ngOnDestroy() {\n    this.el.nativeElement.removeEventListener('custom-event', \n      this.handleCustomEvent.bind(this));\n  }\n  \n  private handleCustomEvent(event: CustomEvent) {\n    this.customEvent.emit(event.detail);\n  }\n}\n```\n\n**Key Insights:**\n\n* Angular directives provide clean property and event binding\n    \n* OnChanges lifecycle hook updates properties efficiently\n    \n* ElementRef gives direct access to the native element\n    \n\n**Interactive Demo:** Explore Angular integrations\n\n%[https://codepen.io/mnichols08/pen/XJWvobr] \n\n### Vue Integration Strategies\n\nVue offers excellent built-in support for Web Components with its attribute and event binding syntax:\n\n```xml\n<template>\n  <my-web-component \n    :complex-prop=\"myData\" \n    @custom-event=\"handleEvent\"\n  />\n</template>\n\n<script>\nexport default {\n  data() {\n    return {\n      myData: { key: 'value' }\n    }\n  },\n  methods: {\n    handleEvent(event) {\n      console.log(event.detail);\n    }\n  },\n  mounted() {\n    // For cases where direct property access is needed\n    this.$refs.myComponent.directMethod();\n  }\n}\n</script>\n```\n\n**Key Insights:**\n\n* Vue's `:prop` syntax works with kebab-case attributes\n    \n* Native event handling with `@event-name`\n    \n* Vue's reactivity system naturally updates Web Component properties\n    \n\n**Interactive Demo:** Explore Vue integrations\n\n%[https://codepen.io/mnichols08/pen/YPzmRRW] \n\n### Svelte Integration Strategies\n\nSvelte treats Web Components as first-class citizens and makes integration particularly straightforward:\n\n```svelte\n<script>\n  import { onMount } from 'svelte';\n  \n  let el;\n  let myData = { key: 'value' };\n  \n  // Reactive assignments automatically update the Web Component\n  $: if (el) {\n    el.complexData = myData;\n  }\n  \n  function handleEvent(event) {\n    console.log(event.detail);\n  }\n  \n  onMount(() => {\n    // If needed, do direct manipulation here\n    el.addEventListener('special-event', specialHandler);\n    \n    return () => {\n      el.removeEventListener('special-event', specialHandler);\n    }\n  });\n</script>\n\n<my-web-component \n  bind:this={el} \n  on:custom-event={handleEvent}\n/>\n```\n\n**Key Insights:**\n\n* `bind:this` provides direct access to the element\n    \n* Svelte's reactivity automatically updates properties\n    \n* Svelte's event handling works natively with Web Component events\n    \n\n**Interactive Demo:** Explore Svelte integration Techniques\n\n%[https://codepen.io/mnichols08/pen/QwWezeL] \n\n**Framework Comparison Demo:** Compare integration approaches across frameworks\n\n## Performance Optimization Techniques\n\n### Lazy-Loading Strategies\n\nLoading Web Components only when needed can significantly improve initial page load performance:\n\n#### Dynamic Import Pattern\n\n```javascript\n// Only load when needed\nconst loadComponent = async () => {\n  const { MyComponent } = await import('./my-component.js');\n  customElements.define('my-component', MyComponent);\n}\n\n// Call on demand or when component is about to be needed\nbutton.addEventListener('click', loadComponent);\n```\n\n#### Intersection Observer Pattern\n\n```javascript\n// Setup observer to load component when scrolled into view\nconst observer = new IntersectionObserver((entries) => {\n  entries.forEach(entry => {\n    if (entry.isIntersecting) {\n      import('./components/lazy-component.js')\n        .then(() => {\n          // Component is now registered\n          observer.unobserve(entry.target);\n        });\n    }\n  });\n}, { rootMargin: '100px' });\n\n// Observe placeholder elements\ndocument.querySelectorAll('.component-placeholder')\n  .forEach(el => observer.observe(el));\n```\n\n**Interactive Demo:** See lazy-loading in action\n\n%[https://codepen.io/mnichols08/pen/xbxvQvb] \n\n### Shadow DOM Performance\n\nThe Shadow DOM provides encapsulation but comes with performance considerations:\n\n1. **Containment for Performance**\n    \n    ```css\n    :host {\n      contain: content; /* Limit style recalculation scope */\n      container-type: inline-size; /* For container queries */\n    }\n    ```\n    \n2. **Minimizing Selectors**\n    \n    ```css\n    /* Avoid overly complex selectors that cross shadow boundaries */\n    :host ::slotted(*) > * { /* Expensive */ }\n    \n    /* Better to use direct, simple selectors */\n    .my-item { /* More efficient */ }\n    ```\n    \n3. **Slot Change Optimization**\n    \n    ```javascript\n    // Listen only when needed\n    constructor() {\n      super();\n      this.shadowRoot.querySelector('slot')\n        .addEventListener('slotchange', this._handleSlotChange);\n    }\n    \n    // Debounce frequent updates\n    _handleSlotChange = debounce(() => {\n      this._processSlottedChildren();\n    }, 100);\n    ```\n    \n\n### Render Optimization\n\nEfficient rendering patterns can dramatically improve performance:\n\n1. **Batched DOM Updates**\n    \n    ```javascript\n    // Poor: Multiple separate DOM operations\n    this.shadowRoot.querySelector('.title').textContent = title;\n    this.shadowRoot.querySelector('.desc').textContent = description;\n    \n    // Better: Batch DOM operations with DocumentFragment\n    update(data) {\n      const fragment = document.createDocumentFragment();\n      // Build complete update in memory\n      const title = document.createElement('div');\n      title.className = 'title';\n      title.textContent = data.title;\n      fragment.appendChild(title);\n      // etc...\n      \n      // Single DOM operation\n      const container = this.shadowRoot.querySelector('.container');\n      container.innerHTML = '';\n      container.appendChild(fragment);\n    }\n    ```\n    \n2. **RequestAnimationFrame for Visual Updates**\n    \n    ```javascript\n    updateVisuals() {\n      // Schedule visual updates in animation frame\n      requestAnimationFrame(() => {\n        this.elements.forEach(el => {\n          el.style.transform = `translate(${this.x}px, ${this.y}px)`;\n        });\n      });\n    }\n    ```\n    \n\n**Interactive Demo:** Compare optimized vs. unoptimized components\n\n%[https://codepen.io/mnichols08/pen/VYwoqYm] \n\n## Advanced Patterns and Best Practices\n\n### State Management Across Boundaries\n\nManaging state between Web Components and frameworks requires thoughtful patterns:\n\n#### 1\\. The Context Pattern\n\nCreate a shared context accessible to all components:\n\n```javascript\n// context.js - Framework agnostic state management\nexport class ComponentContext {\n  constructor() {\n    this._data = {};\n    this._listeners = new Map();\n  }\n  \n  get(key) {\n    return this._data[key];\n  }\n  \n  set(key, value) {\n    this._data[key] = value;\n    if (this._listeners.has(key)) {\n      this._listeners.get(key).forEach(callback => callback(value));\n    }\n  }\n  \n  subscribe(key, callback) {\n    if (!this._listeners.has(key)) {\n      this._listeners.set(key, new Set());\n    }\n    this._listeners.get(key).add(callback);\n    \n    return () => {\n      this._listeners.get(key).delete(callback);\n    };\n  }\n}\n\n// Create shared instance\nexport const context = new ComponentContext();\n```\n\nUsage in components:\n\n```javascript\nimport { context } from './context.js';\n\nclass MyComponent extends HTMLElement {\n  connectedCallback() {\n    // Subscribe to state changes\n    this._unsubscribe = context.subscribe('userData', \n      (data) => this._updateFromState(data));\n      \n    // Initial state\n    this._updateFromState(context.get('userData'));\n  }\n  \n  disconnectedCallback() {\n    // Clean up subscription\n    this._unsubscribe();\n  }\n  \n  _updateState(data) {\n    // Update shared state\n    context.set('userData', data);\n  }\n}\n```\n\n#### 2\\. The Message Bus Pattern\n\nFor loosely coupled components, a pub/sub event bus provides flexible communication:\n\n```javascript\n// event-bus.js\nexport class EventBus {\n  constructor() {\n    this.listeners = {};\n  }\n  \n  on(event, callback) {\n    if (!this.listeners[event]) {\n      this.listeners[event] = [];\n    }\n    this.listeners[event].push(callback);\n    \n    return () => this.off(event, callback);\n  }\n  \n  off(event, callback) {\n    if (this.listeners[event]) {\n      this.listeners[event] = this.listeners[event]\n        .filter(cb => cb !== callback);\n    }\n  }\n  \n  emit(event, data) {\n    if (this.listeners[event]) {\n      this.listeners[event].forEach(callback => {\n        callback(data);\n      });\n    }\n  }\n}\n\n// Shared instance\nexport const eventBus = new EventBus();\n```\n\n### Server-Side Rendering with Web Components\n\nServer-side rendering improves initial load performance. With Declarative Shadow DOM, you can now SSR Web Components:\n\n```xml\n<!-- Server-rendered output -->\n<my-component>\n  <template shadowroot=\"open\">\n    <style>\n      /* Shadow DOM styles */\n      .card { \n        padding: 16px;\n        border: 1px solid #ddd;\n      }\n    </style>\n    <div class=\"card\">\n      <slot></slot>\n    </div>\n  </template>\n  <p>This content is slotted from light DOM</p>\n</my-component>\n\n<!-- Polyfill for browsers without Declarative Shadow DOM -->\n<script>\n  if (!HTMLTemplateElement.prototype.hasOwnProperty('shadowRoot')) {\n    document.querySelectorAll('template[shadowroot]').forEach(template => {\n      const mode = template.getAttribute('shadowroot');\n      const shadowRoot = template.parentNode.attachShadow({ mode });\n      shadowRoot.appendChild(template.content);\n      template.remove();\n    });\n  }\n</script>\n```\n\n### Testing Strategies\n\nTesting Web Components requires specialized approaches:\n\n#### 1\\. Component Unit Testing\n\n```javascript\n// Using web-test-runner and @open-wc/testing\nimport { html, fixture, expect } from '@open-wc/testing';\nimport '../src/my-component.js';\n\ndescribe('MyComponent', () => {\n  it('renders with default values', async () => {\n    const el = await fixture(html`<my-component></my-component>`);\n    \n    expect(el.shadowRoot.querySelector('.title').textContent)\n      .to.equal('Default Title');\n  });\n  \n  it('updates when properties change', async () => {\n    const el = await fixture(html`<my-component></my-component>`);\n    \n    el.title = 'New Title';\n    await el.updateComplete; // For LitElement components\n    \n    expect(el.shadowRoot.querySelector('.title').textContent)\n      .to.equal('New Title');\n  });\n  \n  it('fires custom events', async () => {\n    const el = await fixture(html`<my-component></my-component>`);\n    \n    // Setup event listener\n    let eventFired = false;\n    el.addEventListener('custom-event', () => eventFired = true);\n    \n    // Trigger event\n    el.shadowRoot.querySelector('button').click();\n    \n    expect(eventFired).to.be.true;\n  });\n});\n```\n\n#### 2\\. Integration Testing with Frameworks\n\n```javascript\n// React Testing Library example\nimport { render, fireEvent, screen } from '@testing-library/react';\nimport MyReactComponent from '../src/MyReactComponent';\n\n// Register Web Component if not done globally\nimport '../src/web-components/my-element.js';\n\ntest('React component interacts with Web Component', async () => {\n  render(<MyReactComponent initialData=\"test\" />);\n  \n  // Interact with the wrapped Web Component\n  fireEvent.click(screen.getByRole('button'));\n  \n  // Check the result\n  expect(screen.getByText('Success')).toBeInTheDocument();\n});\n```\n\n## The Future of Web Components\n\nWeb Components continue to evolve with exciting new capabilities on the horizon:\n\n* **CSS Shadow Parts**: More sophisticated styling across shadow boundaries\n    \n* **Form-associated Custom Elements**: Better integration with native forms\n    \n* **Constructable Stylesheets**: Improved performance for shared styles\n    \n* **Scoped Custom Element Registries**: Avoiding name conflicts\n    \n* **Declarative Custom Elements**: Simplified component definition\n    \n\n## Conclusion\n\nWeb Components shine brightest when they seamlessly integrate with existing frameworks and tools. By leveraging the advanced patterns covered in this guide, you can build truly reusable components that work anywhere while maintaining optimal performance and developer experience.\n\nThe future of front-end development isn't about choosing between Web Components and frameworksâ€”it's about building bridges between them to create more maintainable, performant applications.",
      "stars": null,
      "comments": 4,
      "upvotes": 29,
      "read_time": "8 min read",
      "language": null
    },
    {
      "title_en": "Rules of Structured Concurrency",
      "url": "https://vbat.dev/rules-of-structured-concurrency",
      "source": "hashnode",
      "published_at": "2025-04-06T17:34:08.906000+00:00",
      "external_id": null,
      "tags": [
        "structured concurrency",
        "iOS",
        "ios app development",
        "Swift",
        "concurrency"
      ],
      "content_length": 26677,
      "content_preview": "> ***You can find video version of this article*** [***here.***](https://youtu.be/Ys0vQtXjIOI)\n\nIn the previous [article](https://vbat.dev/behind-the-scenes-of-async-functions), we covered the key aspects of Swift Concurrency to build a solid mental model of it. In this article, we will discuss structured and unstructured tasks. Youâ€™ll see that their behavior can be tricky in some scenarios, but weâ€™ll break it down into rules to make it clear.\n\n> âš ï¸ : I assume you are familiar with the basic con",
      "content_full": "> ***You can find video version of this article*** [***here.***](https://youtu.be/Ys0vQtXjIOI)\n\nIn the previous [article](https://vbat.dev/behind-the-scenes-of-async-functions), we covered the key aspects of Swift Concurrency to build a solid mental model of it. In this article, we will discuss structured and unstructured tasks. Youâ€™ll see that their behavior can be tricky in some scenarios, but weâ€™ll break it down into rules to make it clear.\n\n> âš ï¸ : I assume you are familiar with the basic concepts of **Structured Concurrency** like **async let** and **task group.** If not, you can start [here](https://docs.swift.org/swift-book/documentation/the-swift-programming-language/concurrency/).\n\n# **Structured and Unstructured Tasks**\n\nFrom previous article, we know that to switch gears from sync context to async context we need to use tasks. However, that is not the only use case for tasks. Tasks are, in fact, a much broader concept in Swift Concurrency.\n\n> ðŸ’¡: [Tasks](https://developer.apple.com/documentation/swift/task) are **units of asynchronous work** you can run from your code.\n> \n> * **Every async function is executing in a task**. In other words, aÂ *task*Â is toÂ *asynchronous functions*, what aÂ *thread*Â is toÂ *synchronous functions.*\n>     \n> * A task runs one function at a time; a single task has no concurrency.\n>     \n> * Synchronous functions do not necessarily run as part of a task, but they can.\n>     \n\nIn terms of Structured Concurrency, there are 2 kinds of tasks:\n\n* **Structured Concurrency Tasks** (we will call them simply Structured Tasks)\n    \n    * Represented by **async let** and **task group**\n        \n* **Unstructured Concurrency Tasks** (we will call them simply Unstructured Tasks)\n    \n    * Represented by **regular unstructured task** (**Task { â€¦ })** and **detached task** (**Task.detached { â€¦ })**\n        \n\n> **â‰ï¸Â : What is the difference between Structured and Unstructured tasks?**  \n> Why they are actually called â€œstructuredâ€ and â€œunstructuredâ€? What does it mean? To understand the difference we need to understand the parent and child relation for tasks.\n\n# **Parent and Child Tasks**\n\n* **Parent Task is** a task that spawns one or more subtasks (child tasks). Parent task is responsible for managing its child tasks.\n    \n* **Child Task** is a subtask created within the context of a parent task. Child tasks return their results (if any) to the parent task (if explicitly awaited).\n    \n\nParent and child tasks together form a **task tree structure**, that simplifies control over related tasks completion, cancellation and resource management.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743868803871/d21551a2-5365-4f27-a7de-a7f0ceac769c.png align=\"center\")\n\n**Root Task** is an **initial** task in task tree (root node), which can be **parent** if it spawns child tasks.\n\n* **Structured task** can be a **child or parent task (or both). But it canâ€™t be root node in the task tree.**\n    \n* **Unstructured task** **canâ€™t be a child task**. But it **can be a parent task** if you create new child **structured** tasks from them. In other words, **unstructured task can only be a root node in task tree structure.**\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743868828960/51807485-7f39-4545-b62d-df2984df28d0.png align=\"center\")\n\nThat's why they are called \"structured\" and \"unstructured\" tasks:\n\n* **Structured tasks** always join the current task tree structure as child tasks.\n    \n* **Unstructured tasks** never join the current task tree. They start a new, independent task tree as a root node with no parent-child relation to the original task, with child tasks created later if needed.\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743868858942/b74687fb-e270-41b0-b8be-92bcafea5308.png align=\"center\")\n\n> âš ï¸ : Although we can nest one unstructured task within another, donâ€™t be misled by that - it doesnâ€™t create a parent-child relationship between them, regardless of whether the tasks are regular or detached. Later, we will demonstrate through examples that the behavior defined for a task tree hierarchy is fundamentally different for nested unstructured tasks.\n> \n> ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743868885849/c3bcccd7-3a8c-4048-a709-f4d030b1b749.png align=\"center\")\n\n## **Structured tasks**\n\nStructured tasks in Swift are represented by **async let** and **task group.**\n\nAlthough it may not be obvious with **async let**, both of them create child tasks within the parent task where they are used. The child tasks created by **async let** and **task group** are designed to be executed concurrently.\n\n### **async let**\n\nWith `async let`, you can start multiple tasks that **run concurrently.**\n\n```swift\nfunc fetchData() async {\n    async let first = fetchPart1()\n    async let second = fetchPart2()\n    async let third = fetchPart3()\n        \n    let result = await (first, second, third)\n    \n    print(result)\n}\n```\n\nAlthough it is not explicitly stated in code, three new child tasks are created here, one for each **async let.**\n\nTheÂ **lifecycle**Â ofÂ `async let`Â is bound to theÂ **local scope**Â where it is created, such as a function, closure, orÂ `do/catch`Â block.\n\n### TaskGroup\n\nIf you need to create a **dynamic** number of concurrent tasks, a **task group** is the better choice:\n\n```swift\nfunc fetchData(count: Int) async {\n    var results = [String]()\n    \n    await withTaskGroup(of: String.self) { group in\n        for index in 0..<count {\n            group.addTask {\n                await fetchPart(index)\n            }\n        }\n        for await result in group {\n            results.append(result)\n        }\n    }\n    \n    print(results)\n}\n```\n\nImportant difference with `async let` is the order of awaiting results. Unlike `async let`, where the order is code-driven, task group follows a \"**first finished â†’ first handled**\" approach, this is how an `AsyncSequence` works.\n\nLifecycle ofÂ **task group**Â is bound to the closure insideÂ `withTaskGroup`Â function.\n\n> ðŸ’¡: You can find more insights about async lets and task groups [here](https://vbat.dev/async-let-vs-task-group).\n\n## **Unstructured tasks**\n\nUnstructured tasks are usually used as a bridge between sync and async contexts.\n\nIf we create some structured child tasks inside of unstructured task, unstructured task will be a root task (node) in the task tree, and parent task for structured tasks.\n\nUnstructured tasks are represented by **regular** (it is not a common term, we will call it regular just to give it a distinctive name) and **detached** tasks.\n\n```swift\nTask { // regular task\n\tlet result = await fetchData()\n    print(result)\n}\nTask.detached { // detached task\n\tlet result = await fetchData()\n    print(result)\n}\n```\n\n> ðŸ’¡: Lifecycle of unstructured tasks does not bound to the local scope or a single closure like async let or task group. When execution leaves local scope unstructured task will simply continue itâ€™s execution.\n\n> ðŸ’¡: Unstructured tasks can be nested, but nested task doesnâ€™t have parent/child relation with outer unstructured task. You can think about unstructured task like about initiating new task tree root. It could have no child tasks at all but it is independent from parent/child relation with context where it is created.\n\n> **â‰ï¸Â : What is the difference between regular and detached tasks? When to use each of them?**\n> \n> Weâ€™ll discuss this in more detail later, but for now, we can say that a **regular task inherits the execution context**, while a detached task does not. It's quite hard to think of a specific scenario where a detached task would be useful, so in most cases, using a regular task is the better choice.\n\n# Rules of Structured Concurrency\n\nThe task tree structure enables cooperative operations, offering features such as **priority escalation**, which means when setting a higher priority on a child task, the parent taskâ€™s priority is automatically escalated.\n\nWe can consider some of those features as some kind of **structured concurrency rules**:\n\n* **Error Propagation Rule**\n    \n* **Group Cancellation Rule**\n    \n* **Group Completion Rule**\n    \n\n> ðŸ’¡Â : If you like acronyms for better memorization, you can use **EGG** ðŸ¥š.\n\nWe will explore how these rules work within structured concurrency tasks and compare them with the behavior of nested unstructured tasks. You will see that these rules apply to tasks that are part of a single task tree structure, but they do not work for nested unstructured tasks.\n\n# **Group Completion Rule**\n\n> ðŸ‘‰ **Group Completion Rule:** In a parent task, you canâ€™t forget to wait for its child tasks to complete. Parent task canâ€™t complete until child tasks are completed.\n\n```swift\nlet parentTask = Task {\n    async let first = fetchPart1()  // returns 1\n    async let second = fetchPart2() // returns 2\n    async let third = fetchPart3()  // returns 3\n    \n    let result = await (first, second, third)\n    print(result)\n}\n\nlet result = await parentTask.value\nprint(\"parentTask completes\")\n\n// Prints:\n// (1, 2, 3)\n// parentTask completes\n```\n\nAwaiting `parentTask`â€™s value means waiting until it completes. `\"parentTask completes\"` will not be printed until all child tasks created by `async let`s are completed first. For task group inside of unstructured parent task, behavior will be the same.\n\nBut it doesnâ€™t work the same way for nested unstructured tasks once there are no parent/child relation there:\n\n```swift\nlet rootTask = Task {\n    let nestedTask = Task {\n        print(\"nestedTask started\")\n        try? await Task.sleep(nanoseconds: 10_000_000_000) // 10 secs\n        print(\"nestedTask ended\")\n    }\n}\nlet result = await rootTask.value\nprint(\"rootTask completes\")\n\n// Prints:\n// rootTask completes\n// nestedTask started\n// ...after 10 seconds\n// nestedTask ended\n```\n\nAs you can see in this example, `rootTask` completes even before `nestedTask` is started.\n\nBut we can achieve the desired behavior if we explicitly await `nestedTask` inside root task:\n\n```swift\nlet rootTask = Task {\n    let nestedTask = Task {\n        print(\"nestedTask started\")\n        try? await Task.sleep(nanoseconds: 10_000_000_000) // 10 secs\n        print(\"nestedTask ended\")\n    }\n    let result = await nestedTask.value\n    print(\"nestedTask completes\")\n}\nlet result = await rootTask.value\nprint(\"rootTask completes\")\n\n// Prints:\n// nestedTask started\n// ...after 10 seconds\n// nestedTask ended\n// nestedTask completes\n// rootTask completes\n```\n\nWe just need to await both nested and root unstructured tasks.\n\nTo sum up:\n\n* async lets will be **implicitly awaited** when execution leaves the local scope, which allows parent task to complete after that as well, so group completion works as expected. âœ…\n    \n* Task group will **implicitly await** the completion of its child tasks when execution exits the task group closure, so parent task could complete after that as well, ensuring that group completion rule works as expected. âœ…\n    \n* Nested unstructured task, if not awaited, will not cause the external task (`rootTask`) to wait for its completion. It behaves like a \"spawn and forget\" mechanism, which means it doesnâ€™t follow the group completion rule. âŒÂ (which is expected, because it is unstructured and has no parent).\n    \n\n# **Group Cancellation Rule**\n\n> ðŸ‘‰ **Group Cancellation Rule:** If parent task is canceled, each of its child tasks is also automatically canceled.\n\nStructured Concurrency promotes us to think about operation cancellation. But we need to understand precisely how it works.\n\n> âš ï¸ : Keep in mind that **cancelling the task does not stop the task**, it only marks that results are not gonna be needed. Swift concurrency uses a cooperative cancellation model. Each task checks (with **Task.isCancelled** or **Task.checkCancellation()**) whether it has been canceled at the appropriate points in its execution, and responds to cancellation appropriately. Depending on what work the task is doing, responding to cancellation usually means one of the following:\n> \n> * Throwing an error likeÂ `CancellationError`\n>     \n> * ReturningÂ `nil`Â or an empty collection\n>     \n> * Returning the partially completed work\n>     \n\nLetâ€™s consider an example where we have child structured task and parent unstructured task:\n\n```swift\nfunc asyncWork() async {\n    do {\n\t\t/// If current task is canceled before the time ends,\n\t\t/// sleep function throws `CancellationError`.\n        try await Task.sleep(nanoseconds: 10_000_000_000) // 10 secs\n    } catch {\n        print(\"Cancelled as a child task with \\\\(error)\")\n        return\n    }\n    print(\"Not cancelled -> not child task!\")\n}\n\nlet parentTask = Task {\n    async let result = asyncWork()\n    await result\n}\nparentTask.cancel()\n\n// Prints:\n// \"Cancelled as a child task with CancellationError()\" \n// right after parentTask cancellation\n```\n\nOnce **unstructured task** can only be a root task in a task tree, it canâ€™t be cancelled automatically (has no parent). It can be cancelled only explicitly with `cancel()` method. Doing that will cancel all child structured tasks in the task tree. That means ðŸ‘‰Â **Group Cancellation Rule works as expected** âœ…Â for structured tasks.\n\n> **â‰ï¸Â : Can we cancel single structured child task without cancelling itâ€™s parent task?** To cancel single async lets child task, we have to cancel parent task, which will cancel all child tasks together. For task group we also have an option without parent task cancellation - call groupâ€™s `cancelAll()` method.\n\nBut ðŸ‘‰Â **Group Cancellation Rule** doesnâ€™t work âŒ for nested unstructured tasks.\n\n```swift\nlet rootTask = Task {\n    Task {\n        await asyncWork() \n        // prints \"Not cancelled -> not child task!\" after 10 seconds\n    }\n    Task.detached {\n        await asyncWork() \n        // prints \"Not cancelled -> not child task!\" after 10 seconds\n    }\n}\nrootTask.cancel()\n```\n\nNested unstructured tasks arenâ€™t cancelled when rootTask is cancelled.\n\nWe have only explicit cancellation option with `cancel` method called when rootTask is cancelled:\n\n```swift\nlet rootTask = Task {\n    let nestedTask = Task {\n        await asyncWork() \n        // prints \"Cancelled as a child task with CancellationError()\" \n        // right after rootTask cancellation\n    }\n    if Task.isCancelled {\n\t\tnestedTask.cancel()\n\t}\n}\nrootTask.cancel()\n```\n\n# **Error propagation Rule**\n\n> ðŸ‘‰ **Error Propagation Rule:** If error is propagated outside of local scope, all child tasks are implicitly cancelled and implicitly awaited.\n\nAn `async` function can throw errors by being marked with both the `async` and `throws`. **Error propagation** in Swift refers to the process of passing errors up the call stack, allowing higher-level code to handle or respond to issues that occur during execution.\n\nLetâ€™ check this rule with examples.\n\nWhen we leave the local scope due to error, async lets and task group child tasks will be **implicitly cancelled** and **implicitly awaited** âœ… **:**\n\n```swift\nfunc fast() async throws {\n    print(\"fast started\")\n    do {\n        try await Task.sleep(nanoseconds: 5_000_000_000)\n    } catch {\n        print(\"fast cancelled\", error)\n    }\n    print(\"fast ended\")\n    throw TestError1() // <- ERROR IS THROWN HERE\n}\nfunc slow() async throws {\n    print(\"slow started\")\n    do {\n        try await Task.sleep(nanoseconds: 10_000_000_000)\n    } catch {\n        print(\"slow cancelled\", error)\n    }\n    print(\"slow ended\")\n    throw TestError2() // <- ERROR IS THROWN HERE\n}\n\nfunc testErrorPropagation() async throws {\n    async let f = fast()\n    async let s = slow()\n    try await (f, s)\n    print(\"leaving local scope\") // <- will not be printed\n}\n\n//    Prints:\n//    slow started\n//    fast started\n//    fast ended // after 5 secs\n//    slow cancelled CancellationError()\n//    slow ended\n//    external catch TestError1()\n```\n\nAfter `fast` throws `TestError1`, execution leaves the local scope because the error is propagated outside of it. As a result, `slow` is implicitly cancelled and implicitly awaited.  \n`TestError1` is propagated and caught outside of local scope. `TestError2` was not propagated because `TestError1` was handled first.\n\n**What about error propagation in unstructured tasks?**\n\nThrowing an error in unstructured nested task doesnâ€™t cancel other unstructured nested tasks:\n\n```swift\nlet externalTask = Task {\n    let nestedTask = Task {\n        print(\"Nested Task 1 started\")\n        do {\n            try await Task.sleep(nanoseconds: 5_000_000_000)\n        } catch {\n            print(\"Nested Task 1 cancelled\", error)\n        }\n        print(\"Nested Task 1 ended\")\n        throw TestError1()\n    }\n    let nestedTask2 = Task {\n        print(\"Nested Task 2 started\")\n        do {\n            try await Task.sleep(nanoseconds: 10_000_000_000)\n        } catch {\n            print(\"Nested Task 2 cancelled\", error)\n        }\n        print(\"Nested Task 2 ended\")\n        throw TestError2()\n    }\n    \n    try await nestedTask.value\n    try await nestedTask2.value\n}\n  \ndo {\n    try await externalTask.value\n} catch {\n    print(\"Caught error \\\\(error)\")\n}\n\n//    Prints:\n//    Nested Task 1 started\n//    Nested Task 2 started\n//    Nested Task 1 ended // after 5 secs\n//    Caught error TestError1()\n//    Nested Task 2 ended // after 10 secs\n```\n\nAs you can see, when we throw `TestError1` from `nestedTask`, error is propagated and get caught outside. `nestedTask2` was not cancelled and ended later after that. That means:\n\n> ðŸ‘‰Â **Error Propagation Rule** doesnâ€™t work âŒ for nested unstructured tasks.\n\n> ðŸ’¡: There is another rule that works for **both structured and unstructured tasks (including nested)** related to error propagation: ðŸ‘‰Â **Error is propagated from task only if that task is awaited explicitly. No await â†’ no propagation.**\n> \n> In this example, even though error is thrown we will leave the task group closure without error propagated because child tasks are not awaited:\n> \n> ```swift\n> try await withThrowingTaskGroup(of: Void.self) { group in\n>     group.addTask {\n>         try await fast() // throws error\n>     }\n>     group.addTask {\n>         try await slow() // throws error\n>     }\n>     \n>     print(\"leaving task group closure without error propagatedâ€)\n> }\n> ```\n\n# **Context Inheritance**\n\nEven though itâ€™s not the part of defined structured concurrency rules, letâ€™s also discuss how structured and unstructured tasks inherit properties from the context they are created from.\n\n**Regular unstructured task** inherits:\n\n* **Task Priority**\n    \n    * In an async context, when you create an unstructured task without specifying a priority, it inherits the priority from the current task.\n        \n    * In sync context it will use the **priority of the thread or queue** from which it was called.\n        \n        ```swift\n        override func viewDidLoad() {\n            super.viewDidLoad()\n        \n            Task {\n                print(Task.currentPriority) // Prints `.high`\n            }\n            Task {\n                DispatchQueue.global(priority: .low).async {\n                    print(Task.currentPriority) // Prints `.low`\n                }\n            }\n            Task(priority: .background) {\n                Task {\n                    print(Task.currentPriority) // Prints `.background`\n                }\n            }\n            Task(priority: .background) {\n                DispatchQueue.main.async {\n                    Task {\n                        print(Task.currentPriority) // Prints `.high`\n                    }\n                }\n            }\n        }\n        ```\n        \n* **Task local values**\n    \n    * Swift lets us attach metadata to a task usingÂ *task-local values*, which are small pieces of information that any code inside a task can read. A task-local value is bound and read in the context of a task. It is implicitly carried with the task, and is accessible by any child tasks it creates. If we create unstructured task, is also inherit it from the task it was created from.\n        \n        ```swift\n        enum TaskLocalStorage {\n            @TaskLocal static var requestID: String?\n        }\n        \n        func checkTaskLocalValue() async {\n            await TaskLocalStorage.$requestID.withValue(\"12345\") {\n        \t\tprint(\"RequestID: \\\\(TaskLocalStorage.requestID ?? \"No Request ID\")\") \n        \t\t// Prints: Request ID: 12345\n        \t\t\t\t\n        \t\tTask {\n                    // Regular unstructured task inherits task-local values.\n                    print(\"Regular unstructured RequestID: \\\\(TaskLocalStorage.requestID ?? \"No Request ID\")\") \n                    // Prints: Regular unstructured RequestID: 12345\n                }\n                \n                // async let and task group will inherit task local values as well.\n                // Only detached task is \"detached\" from TaskLocalStorage.\n                \n                Task.detached {\n                    // Detached task does not inherit task-local values.\n                    print(\"Detached Task RequestID: \\\\(TaskLocalStorage.requestID ?? \"No Request ID\")\") \n                    // Prints: Detached Task RequestID: No Request ID\n                }\n            }\n        }\n        ```\n        \n* The **actor** weâ€™re currently running on (if any). Basically it means that we inherit the contextâ€™s executor (aka **execution context**).\n    \n\n**Structured task** inherits **the same as regular task except actor isolation:**\n\n> âš ï¸ : Some articles claim that child structured tasks created with `async let` inherit actor isolation from the context in which they are created. This can be misleading. **Structured tasks do not inherit actor isolation from their surrounding context**. It wouldnâ€™t make sense because structured tasks are designed to parallelize the work, and inheriting actor isolation would force them to run everything on the same actor (if present), sequentially.\n> \n> By default, child structured tasks run on the global concurrent executor. However, keep in mind that any async functions you call within these tasks can have their own actor isolation. For `async let`, once it is assigned with an async function call, it may look like as though it inherits the actor isolation of that function. But again, **structured tasks never inherit actor isolation from the context where they are created**.\n> \n> ```swift\n> nonisolated func fetchFirst() async -> Int { //... }\n> @MainActor func fetchSecond() async -> Int async { //... }\n> \n> async let first = fetchFirst()\n> async let second = fetchSecond()\n> \n> // is converted under the hood into something like this:\n> \n> let first = ChildTask { // global concurrent executor\n>     await fetchFirst() // global concurrent executor\n> }\n> let second = ChildTask { // global concurrent executor\n>     await fetchSecond() // main executor\n> }\n> ```\n\n**Detached task** inherits **nothing**. On practice it means that it always runs on the global concurrent executor, so it will run on any thread from cooperative thread pool. But not main thread for sure.\n\n# When to use structured and unstructured concurrency?\n\n**When to use structured concurrency?**\n\nWhen you need to run multiple concurrent operations at once. But keep in mind itâ€™s â€œimplicit awaitingâ€ behavior: when execution exits this scope â€” either normally or due to an error â€” all child tasks will beÂ **implicitly**Â **awaited**.\n\n**When to use async let vs task group?**\n\nSeveral factors to consider:\n\n* Amount of tasks: static count for async let, dynamic count for task groups.\n    \n* If you have error propagation logic, a task group may be a better choice thanÂ `async let`. The task groupâ€™s \"first thrown, first caught\" logic is more predictable and wonâ€™t be affected by the order of awaiting, unlikeÂ `async let`, where error propagation can be influenced by the awaiting order. Task groups are more suitable for cases where you want to \"fail fast\" compared toÂ `async let`.\n    \n* async let can be easier to use withÂ *heterogeneous results*Â and step-by-step initialization patterns.\n    \n\n**When to use unstructured concurrency?**\n\n* Obviously, use it when you need to switch gears from sync to async context.\n    \n* Regular unstructured task might also be used if you want to perform a piece of work independently of the function youâ€™re in, without awaiting. Some â€œfire-and-forgetâ€ style operation that will allow the function to not wait for this task completion.\n    \n    ```swift\n    func someWorkThatDontWantToWait() async {\n    \tTask {\n    \t    await work()\n        }\n    }\n    ```\n    \n* **Detached Tasks** use case is kind of tricky, because hard to say exactly when you need it:\n    \n    * Thatâ€™s why itâ€™s often referenced as a **last resort**, when nothing else would suit.\n        \n    * Anyway, make sure you really understand why usually it is not needed.\n        \n        > ðŸ’¡: Usually regular task is enough. Even though it inherits actor isolation, even if you want to avoid it and run some function on global concurrent executor, remember that this function also has itâ€™s own actor isolation context which will define where it will be executed, not the task from where we call it. For example:\n        > \n        > ```swift\n        > class ViewController: UIViewController { // Main actor isolated\n        > \tfunc someWorkOnMainThread() async {\n        > \t\tTask.detached {\n        > \t\t     await self.asyncMethod()\n        > \t\t     await self.syncMethod()\n        > \t\t}\n        >     }\n        > \t\t \n        > \tfunc asyncMethod() async {\n        >         // called on the main thread\n        >     }\n        >     \n        >     func syncMethod() {\n        >         // called on the main thread\n        >     }\n        > }\n        > ```\n        > \n        > Although we called methods from detached task, it didnâ€™t make them to be called on global concurrent executor. Both method are main actor isolated as methods of ViewController which is main actor isolated.\n        \n\nFinally, for quick reference, keep this the helper table from [Apple WWDC session](https://developer.apple.com/videos/play/wwdc2021/10134/):\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743869458360/d6800796-d6eb-4355-8f0a-3ae3d3128c1f.png align=\"left\")\n\nFor structured tasks lifecycle when local scope is left, this scheme should be useful as well (you can find more info on that matter [here](https://vbat.dev/async-let-vs-task-group)):\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743869464696/ed4d6da3-4474-46a7-aa88-76e8280d8bf0.png align=\"left\")\n\nIf you spot any inaccuracies or misleading points in this post, please share them in the comments - your feedback helps everyone!\n\nGood luck on your Swift Concurrency journey! See you in the next article!",
      "stars": null,
      "comments": 2,
      "upvotes": 29,
      "read_time": "17 min read",
      "language": null
    },
    {
      "title_en": "The Truth about Vibe Coding (feat. GitHub Copilot Agent Mode)",
      "url": "https://blog.lo-victoria.com/the-truth-about-vibe-coding-feat-github-copilot-agent-mode",
      "source": "hashnode",
      "published_at": "2025-04-07T23:58:45.931000+00:00",
      "external_id": null,
      "tags": [
        "vibe coding",
        "AI",
        "#ai-tools",
        "GitHub",
        "copilot"
      ],
      "content_length": 10887,
      "content_preview": "Hello and welcome to another Articles by Victoria! This weekend, my friends and I have been talking a lot about the revolutionary AI tools we see emerging. We came across the term \"vibe coding,\" which apparently describes the art of coding without actually doing it yourselfâ€” an AI handles it for you.\n\nThe term sounds strange to me. As someone who spends hours actually coding, I love the \"vibe\" of real coding and being in the flow state.\n\nSo in todayâ€™s article, letâ€™s explore what Vibe Coding actu",
      "content_full": "Hello and welcome to another Articles by Victoria! This weekend, my friends and I have been talking a lot about the revolutionary AI tools we see emerging. We came across the term \"vibe coding,\" which apparently describes the art of coding without actually doing it yourselfâ€” an AI handles it for you.\n\nThe term sounds strange to me. As someone who spends hours actually coding, I love the \"vibe\" of real coding and being in the flow state.\n\nSo in todayâ€™s article, letâ€™s explore what Vibe Coding actually is, how you can do it yourself and what is means for the future of software developers.\n\n# What is Vibe Coding?\n\nVibe coding is the practice of AI-dependent programming where anyone - non-coders and coders, engage with AI tools to prompt, describe and build their own software. From desktop or web apps to mobile apps, anything that can be written with code can be achieved with vibe coding (in theory).\n\nA quick lookup on the web and I found out the term was coined by [Andrej Karpathy](https://x.com/karpathy), previously Director of AI at Tesla, and a member of the founding team at OpenAI. This is his original definition of â€˜Vibe Codingâ€™:\n\n> There's a new kind of coding I call \"vibe coding\", where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. \\[â€¦\\] I'm building a project or webapp, but it's not really coding - I just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly works.\n\nSo now that we have an understanding of vibe coding, letâ€™s learn how to actually be a â€˜vibe coderâ€™.\n\n# How to Vibe Code?\n\nTo be honest, while writing this article, I was skeptical about how non-coders could truly rely on AI to handle everything. I had many questions running through my mind:\n\n* Wouldnâ€™t there be bugs or inefficiencies in the code that only a human could catch and optimize?\n    \n* What about system design and programming patternsâ€”doesnâ€™t AI still need guidance to implement them effectively?\n    \n* How complex can an AI-generated app really be, especially for someone who codes more by intuition than strict methodology?\n    \n* And if AI does all the heavy lifting, what happens to the deeper problem-solving skills that come with traditional coding?\n    \n\nVibe coding is all about working with AI as a collaborator rather than just a tool. The key is to **learn how to interact with AI efficiently**â€”like giving clear, concise instructions and iterating based on feedback.\n\nHereâ€™s how you can start:\n\n1. **Pick an AI-powered coding assistant** â€“ Tools like GitHub Copilot, Cursor, or ChatGPT can help you generate code snippets based on descriptions. For this article, Iâ€™m using Github Copilot Agent Mode since I have the paid plan.\n    \n2. **Describe what you need, not how to do it** â€“ Instead of writing step-by-step logic, tell the AI what you want, e.g., â€œCreate a user authentication system with JWT tokens.â€\n    \n3. **Refine and iterate** â€“ The AI might not get it perfect the first time, but you can tweak and refine through additional prompts. This process will take the longest for those who rarely prompts AI tools. The intuition and skill of prompt engineering takes time to build.\n    \n4. **Use AI to debug and optimize** â€“ When you run into issues, let the AI analyze error messages and suggest fixes. CTRL + C and CTRL + V (sorry Mac users) are your best friends.\n    \n\n> If you want, I can write a separate article on the details on how to setup, configure and use different AI models, both free and paid, for vibe coding. Do let me know in the comments!\n\n# Using GitHub Copilot Agent Mode to build a simple app with Vibe Coding\n\nA while back, I published an article comparing 3 coding generation tools, one of them being GitHub Copilot. But now, GitHub has released [GitHub Copilot Agent Mode](https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/) (available in preview) and since Iâ€™m curious to see how far I could take vibe coding, I decided to put it to the test using GitHub Copilot Agent Mode.\n\n%[https://lo-victoria.com/a-comparison-of-ai-coding-tools-github-copilot-qodo-and-codeium] \n\n## Step 1: Download Visual Studio Code - Insiders\n\n* Go to this [link](https://code.visualstudio.com/insiders/) to download\n    \n\n## Step 2: Open VS Code Insiders and Setup Copilot\n\nThe instructions are pretty clear and they have an onboarding window as soon as you launch Insiders.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743241329809/7917904d-2a04-4807-a38c-06adfdaa7f8c.png align=\"center\")\n\nOnce your Copilot chat window shows up, make sure the action dropdown is set to new â€œAgentâ€ mode.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743241634445/2c65d320-64d4-46bb-88ca-4f6d412e1e9f.png align=\"center\")\n\n## Step 3: Start Vibe Coding\n\nSo for this example, Iâ€™m making a simple chatbot app using [CopilotKit](https://www.copilotkit.ai/). The Agent does well by coding up a template from scratch. And then, it stops because Iâ€™m missing the API key. So I had to re-prompt to ask it to generate me the `.env` file to store it. Yes, I can easily create a single file myself but I want to be let AI code everything for this project.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743246132687/9dc089ce-5615-49c8-b103-50d294741b07.png align=\"center\")\n\nAnd of course, I still had to manually paste my API key in the `.env` file. As I instruct the Agent to run the app, it told me that the initial code it generated have issues, so it immediately applies the fixes. But the fix was still causing errors so I gave it some help and copying the sample front-end code from the CopilotKitâ€™s documentation. After that, it was fixed.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743246564198/70cf3a1e-c555-4d8a-bfaf-9ae57282653e.png align=\"center\")\n\nMy simple chatbot app is now up and running! Ta-da!\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743247081654/1982ed7d-0659-4114-a5e1-38e6044f9ef9.png align=\"center\")\n\n# **Verdict**\n\nVibe coding made app development feel effortless. Instead of manually writing code, I found myself interacting with AI in a conversational way, tweaking and refining as needed. However, some limitations quickly became apparent:\n\n## **1) Debugging is still a human task**\n\nIf I were the one building this from the start, I would have read CopilotKitâ€™s documentation and knew that I can simply use their UI component.\n\nSo when it generated a minor bug, I had to step in and debug manually. AI couldnâ€™t always fix issues without my intervention. And trying to use AI to fix their own mistakes can take lots of unproductive attempts. If you are not using free tools, these costs can accumulate really fast!\n\n## **2) AI lacks context beyond immediate prompts**\n\nWhile Copilot generated individual functions well, it didn't always connect them in the most efficient way, requiring some human restructuring. For this example, the bugs that were generated from the Copilot Agent was because it didnâ€™t know it had to check the docs and tried to implement its own UI for the chatbot.\n\n## **3) Optimization requires experience**\n\nThe AI-generated code worked, but it wasnâ€™t necessarily the most optimized or scalable solution. A human touch was still needed to refine it. As someone who can read code, itâ€™s obviously faster for me to optimize the code myself.\n\nDespite these drawbacks, the experience demonstrated how AI can dramatically speed up development, making coding more accessible to non-programmers and boosting productivity for experienced developers.\n\n## **4) Data and Security Concerns**\n\nOne of the biggest concerns with AI-driven development is the handling of sensitive data like the API key in the example. When relying on AI tools like Copilot or ChatGPT, code snippets and project details may be sent to external servers for processing. This raises questions about data privacy, security risks, and even licensing issuesâ€”especially for enterprise or proprietary projects. Itâ€™s essential to review the terms of use and ensure that confidential code isnâ€™t inadvertently exposed.\n\nAt the current limitations, vibe coding can only be used for **rapid prototyping** and would be beneficial for mainly coders. But it cannot replace software engineers who understand data structures, system design and how to build secure scalable apps.\n\n# What does Vibe Coding mean for the future of software development?\n\nWhen ChatGPT first launched, I knew that prompt engineering is a skill thatâ€™s not to be underestimated. The AI can only generate solutions as good as the prompt. **If you donâ€™t prompt well, you canâ€™t get good results. If you donâ€™t know what you are doing, you canâ€™t get good results.**\n\nVibe Coding emphasizes the importance of this skill even more! The success of vibe coding a fully functional app all depends on your prompt engineering skill.\n\nFor non-coders, I cannot imagine the process of building apps being smooth with no hiccups. AI can generate hallucinations and imperfect solutions occasionally. Knowing how to fix the AIâ€™s mistakes and catching these mistakes before it snowballs into technical debt will be a daunting task for someone without a coding background.\n\n# Conclusion\n\nThe future of software development might not be about writing code but about shaping AI-generated solutions to fit specific needs. While AI-driven coding isn't perfect yet, its rapid advancements suggest a world where developers focus more on **creativity and problem-solving** than syntax and debugging.\n\nWhether youâ€™re a seasoned developer or someone with no coding experience, AI tools like GitHub Copilot or Cursor are making it easier than ever to turn ideas into working software.\n\nI am quite excited and optimistic about the changes that vibe coding will bring. Because I donâ€™t think vibe coding is about replacing programmersâ€”itâ€™s about enhancing the way we build. As AI continues to evolve, the role of developers will shift from writing code line-by-line to guiding AI-generated solutions, shaping the future of software development in ways weâ€™re only beginning to explore.\n\n### References\n\n* Original tweet: [https://x.com/karpathy/status/1886192184808149383](https://x.com/karpathy/status/1886192184808149383)\n    \n* [Code Generation tool comparison](https://lo-victoria.com/a-comparison-of-ai-coding-tools-github-copilot-qodo-and-codeium)\n    \n* [GitHub Copilot Agent Mode](https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/)\n    \n* [CopilotKit documentation](https://docs.copilotkit.ai/quickstart)\n    \n\nThanks for reading! Hope this article has been helpful! If it has, do leave a like, share the article and comment your thoughts below! Cheers!\n\n### **Let's Connect!**\n\n* [**Twitter**](https://twitter.com/lo_victoria2666)\n    \n* [**LinkedIn**](https://www.linkedin.com/in/victoria2666/)\n    \n* [**GitHub**](https://github.com/victoria-lo)",
      "stars": null,
      "comments": 5,
      "upvotes": 62,
      "read_time": "8 min read",
      "language": null
    },
    {
      "title_en": "Markdown Editor and Knowledge Graph Notes App with Vis.js in FileMaker Pro",
      "url": "https://greenflux.hashnode.dev/markdown-editor-and-knowledge-graph-notes-app-with-visjs-in-filemaker-pro",
      "source": "hashnode",
      "published_at": "2025-04-06T13:13:40.167000+00:00",
      "external_id": null,
      "tags": [
        "knowledge graph",
        "obsidian",
        "filemaker",
        "claris",
        "markdown",
        "JavaScript",
        "data visualization"
      ],
      "content_length": 41515,
      "content_preview": "Markdown is used widely in programming, documentation, content creation, GitHub, and lots of other areas in tech. One of the most popular Markdown editors is Obsidian, a note taking app with an interesting graph view that shows how all of your notes are connected.\n\n![Obsidian](https://obsidian.md/images/screenshot-1.0-hero-combo.png align=\"left\")\n\nThe graph view is interactive and allows you to navigate through notes and see how they are connected to other records.\n\nIn this guide, Iâ€™ll show how ",
      "content_full": "Markdown is used widely in programming, documentation, content creation, GitHub, and lots of other areas in tech. One of the most popular Markdown editors is Obsidian, a note taking app with an interesting graph view that shows how all of your notes are connected.\n\n![Obsidian](https://obsidian.md/images/screenshot-1.0-hero-combo.png align=\"left\")\n\nThe graph view is interactive and allows you to navigate through notes and see how they are connected to other records.\n\nIn this guide, Iâ€™ll show how to build a Markdown editor in FileMaker Pro, and generate an interactive network graph of the notes to explore your data.\n\n**This guide will cover:**\n\n* Building a Markdown Editor in a Web Viewer with Marked.js\n    \n* Adding support for syntax highlighting code blocks with Prism.js\n    \n* Building JSON to feed the graph view\n    \n* Building the graph view with Vis.js\n    \n\n*Hereâ€™s a quick preview of the finished app*\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743928386754/ad050eb7-0ccf-4ba6-a519-9f305cca2b66.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743928389807/1ff06725-4565-4cfb-add3-d43d48df466f.png align=\"center\")\n\nAt first I was just going to build a basic Markdown editor and I needed some mock data. I had ChatGPT generate some Captainâ€™s Log entries from Star Trek, and then I got a bit carried away and decided to recreate the LCARS display! ðŸ––\n\nThen I thought the network graph from Obsidian could work well with it, and look kind of like a system of planets. This was a bit more than I originally planned on doing for this Markdown editor tutorial, but it sounded like a fun challenge. Now that I have it working, I wanted to share a guide on how to build your own.\n\n**Ready to dive in? *Letâ€™s get started!***\n\n(*Wanna skip the tutorial, and just copy the finished app?* ðŸ‘‰ [GitHub Repo](https://github.com/GreenFluxLLC/FileMaker-Experiments))\n\n## Building a Markdown Editor with Marked.js\n\nBuilding a Markdown editor from scratch may sound complex, but the [Marked.js](https://www.jsdelivr.com/package/npm/marked) library makes it quite easy to set up. Just import the library and select an element, then set the innerHTML to `marked.parse()`, passing in your Markdown text.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743929430781/b416458b-b3fa-4b95-94a3-accbad53c43f.png align=\"center\")\n\nMarked.js displays the compiled Markdown, given an input of raw Markdown text. To build an editor, you need an input or text area to feed it.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743929822377/c481a03e-7008-4be0-a71b-ed1360380a29.png align=\"center\")\n\nFirst letâ€™s set up a basic web viewer in FileMaker with Marked.js. There are lots of ways to provide the HTML to the web viewer, like entering it directly in the web viewer source field, storing it in a text field, or using a text layout object. I prefer to use the layout object method, as it avoids the issues with quotes when entering directly in the web viewer source, and avoids creating new fields.\n\nPaste the following code into a new text object, off to the side of the layout where it wonâ€™t appear on the screen. Name the object `webviewerCode` so we can reference it in the next step.\n\n```xml\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n  <title>Marked.js Live Preview</title>\n  <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n  <style>\n    body { font-family: sans-serif; display: flex; gap: 2rem; padding: 1rem; }\n    textarea { width: 45%; height: 90vh; font-family: monospace; }\n    #preview { width: 45%; height: 90vh; overflow-y: auto; border: 1px solid #ccc; padding: 1rem; }\n  </style>\n</head>\n<body>\n  <textarea id=\"input\"># Sample Markdown\n\n- **Bold item**\n- _Italic item_\n- [Link](https://example.com)\n\n</textarea>\n\n  <div id=\"preview\"></div>\n\n  <script>\n    const input = document.getElementById('input')\n    const preview = document.getElementById('preview')\n\n    const renderMarkdown = () => {\n      preview.innerHTML = marked.parse(input.value)\n    }\n\n    input.addEventListener('input', renderMarkdown)\n    renderMarkdown() // Initial render\n  </script>\n</body>\n</html>\n```\n\nThen add a web viewer and set the source to:\n\n```plaintext\nGetLayoutObjectAttribute ( \"webviewerCode\"; \"content\" )\n```\n\nYou should now see a text area for the input and a live preview of the Markdown.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743930664181/641c8fe8-7f51-433b-b277-fd3896c8ccaf.png align=\"center\")\n\n### Preloading the Editor Content from FMP Records\n\nNext we want to populate the editor from field data in FileMaker. In my case Iâ€™m using a `notes` table with a `body` field. Update the webviewerCode text object to insert a placeholder that can be used to insert the body.\n\n```xml\n  <textarea id=\"input\">{MARKDOWN_TEXT}</textarea>\n```\n\nThen update the web viewer source to insert the body field.\n\n```plaintext\nSubstitute ( \n\n  GetLayoutObjectAttribute ( \"webviewerCode\"; \"content\" );  // source text\n  \"{MARKDOWN_TEXT}\";  //  search value\n  notes::body         //  replace value\n\n )\n```\n\nYou should now be able to view Markdown from the `notes::body` field. You can also edit the Markdown and see it update in real-time on the preview pane. But thereâ€™s no way to save it yet!\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743931647809/dcdd8a0c-d745-409c-a97a-7550b4888e57.png align=\"center\")\n\n### Saving the Markdown\n\nNext, add a **Save** button to the HTML in the `webviewerCode` object, and set it to run a FileMaker script called *Save Note*, which weâ€™ll create in the next step.\n\n```xml\n   <button onclick=\"saveMarkdown()\">Save</button>\n\n  <script>\n    const input = document.getElementById('input')\n    const preview = document.getElementById('preview')\n\n    const renderMarkdown = () => {\n      preview.innerHTML = marked.parse(input.value)\n    }\n\n    const saveMarkdown = () => {\n      FileMaker.PerformScript(\"Save Note\", input.value)\n    }\n\n    input.addEventListener('input', renderMarkdown)\n    renderMarkdown() // Initial render\n  </script>\n```\n\nThen create the *Save Note* script in FileMaker Pro.\n\n```plaintext\nSet Variable [ $markdown; Value:Get(ScriptParameter) ]\nSet Field [ notes::body; $markdown ]\n```\n\nReturn to Browse Mode and test it out! You should now be able to save the note.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743932595390/452f80cc-f46a-4d7c-9d43-d96db74628d8.gif align=\"center\")\n\n## Adding syntax highlighting with Prism.js\n\nNext weâ€™ll add [Prism.js](https://www.jsdelivr.com/package/npm/prismjs) for syntax highlighting of code blocks. Import the library and use the `Prism.highlightAll()` method to automatically detect and highlight all code blocks on the page. Hereâ€™s an updated version of the complete HTML so far:\n\n```xml\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n  <title>Marked.js Live Preview</title>\n\n  <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"></script>\n  <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.css\" />\n\n  <style>\n    body { font-family: sans-serif; display: flex; gap: 2rem; padding: 1rem; }\n    textarea { width: 45%; height: 90vh; font-family: monospace; }\n    #preview { width: 45%; height: 90vh; overflow-y: auto; border: 1px solid #ccc; padding: 1rem; }\n    pre { background: #f5f5f5; padding: 10px; overflow-x: auto; }\n    code { font-family: monospace; }\n  </style>\n</head>\n<body>\n  <textarea id=\"input\">{MARKDOWN_TEXT}</textarea>\n\n  <div id=\"preview\"></div>\n\n  <button onclick=\"saveMarkdown()\">Save</button>\n\n  <script>\n    const input = document.getElementById('input')\n    const preview = document.getElementById('preview')\n\n    const renderMarkdown = () => {\n      preview.innerHTML = marked.parse(input.value)\n      Prism.highlightAll()\n    }\n\n    const saveMarkdown = () => {\n      FileMaker.PerformScript(\"Save Note\", input.value)\n    }\n\n    input.addEventListener('input', renderMarkdown)\n    renderMarkdown()\n  </script>\n</body>\n</html>\n```\n\nNow enter some Markdown with a code block. Include the language to ensure the correct formatting is applied.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743933188159/55960a60-2f12-43c6-a4c2-8750e54e4c08.png align=\"center\")\n\nOk, you can preload the editor, save the current value, and display syntax highlighting. The basics are working. From here you can use your framework and libraries of choice style the UI.\n\nHereâ€™s a more complete version using Vue3, with separate modes for Full Editor, Full Preview, and Split Screen.\n\n```xml\n<!DOCTYPE html><html lang=\"en\"><head>\n  <meta charset=\"UTF-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n  <title>Markdown Editor</title>\n  <!-- Vue & Marked -->\n  <script src=\"https://cdn.jsdelivr.net/npm/vue@3.2.37/dist/vue.global.prod.js\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/marked@4.0.16/lib/marked.esm.js\" type=\"module\"></script>\n  <!-- Prism.js (Dark Theme) -->\n  <script src=\"https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"></script>\n  <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.css\">\n  <style>\n    body {\n      font-family: Arial, sans-serif;\n      margin: 0;\n      padding: 0;\n      background-color: #000;\n    }\n    *, *::before, *::after {\n      box-sizing: border-box;\n    }\n    .toolbar {\n      display: flex;\n      justify-content: center;\n      background-color: #222;\n      padding: 10px;\n      gap: 10px;\n    }\n    .toolbar button {\n      color: #000;\n      border: none;\n      padding: 10px;\n      font-weight: bold;\n      cursor: pointer;\n    }\n    .toolbar button:nth-child(1) { background: #9944ff; }\n    .toolbar button:nth-child(2) { background: #dd4445; }\n    .toolbar button:nth-child(3) { background: #7687fe; }\n    .toolbar button:nth-child(4) { background: #ff7706; }\n    .editor-container {\n      display: flex;\n      width: 100%;\n      height: calc(100vh - 50px);\n    }\n    .editor-pane, .preview-pane {\n      flex: 1;\n      padding: 20px;\n      overflow-y: auto;\n      background-color: #121212;\n    }\n    /* Preserve Prism token colors */\n    .preview-pane :not(pre):not(code) {\n      color: #f0f0f0;\n    }\n    .editor-pane {\n      border-right: 1px solid #333;\n    }\n    textarea {\n      width: 100%;\n      height: 100%;\n      font-size: 1rem;\n      padding: 10px;\n      background: #1e1e1e;\n      color: #fff;\n      border: none;\n      resize: none;\n    }\n    pre {\n      background: #1e1e1e;\n      padding: 10px;\n      overflow-x: auto;\n    }\n    .preview-pane ul,\n    .preview-pane ol {\n      margin: 1em 0;\n      padding-left: 40px;\n    }\n    .preview-pane ul {\n      list-style-type: disc;\n    }\n    .preview-pane ol {\n      list-style-type: decimal;\n    }\n    .editor-container.editor .preview-pane {\n      display: none;\n    }\n    .editor-container.preview .editor-pane {\n      display: none;\n    }\n    .editor-container.split .editor-pane,\n    .editor-container.split .preview-pane {\n      flex: 1;\n    }\n  </style>\n</head>\n<body>\n  <div id=\"app\">\n    <div class=\"toolbar\">\n      <button @click=\"toggleView('editor')\">Full Editor</button>\n      <button @click=\"toggleView('preview')\">Full Preview</button>\n      <button @click=\"toggleView('split')\">Split Screen</button>\n      <button @click=\"saveMarkdown\">Save</button>\n    </div>\n    <div class=\"editor-container\" :class=\"viewMode\">\n      <div v-if=\"viewMode !== 'preview'\" class=\"editor-pane\">\n        <textarea v-model=\"markdown\" @input=\"renderMarkdown\"></textarea>\n      </div>\n      <div v-if=\"viewMode !== 'editor'\" class=\"preview-pane\" v-html=\"compiledMarkdown\"></div>\n    </div>\n  </div>\n  <script type=\"module\">\n    import { marked } from 'https://cdn.jsdelivr.net/npm/marked@4.0.16/lib/marked.esm.js';\n    import { createApp, ref, watch } from 'https://cdn.jsdelivr.net/npm/vue@3.2.37/dist/vue.esm-browser.prod.js';\n\n    const app = createApp({\n      setup() {\n        const initialMarkdown = document.getElementById('initial-markdown-content').textContent;\n        const markdown = ref(initialMarkdown);\n        const compiledMarkdown = ref('');\n        const viewMode = ref('split');\n\n        const renderMarkdown = () => {\n          compiledMarkdown.value = marked(markdown.value, {\n            breaks: true,\n            gfm: true\n          });\n          setTimeout(() => {\n            Prism.highlightAll();\n          }, 0);\n        };\n\n        const toggleView = (mode) => {\n          viewMode.value = mode;\n          // Re-apply syntax highlighting when view mode changes\n          setTimeout(() => {\n            Prism.highlightAll();\n          }, 10);\n        };\n\n        const saveMarkdown = () => {\n          FileMaker.PerformScript(\"Save Note\", markdown.value);\n        };\n\n        watch(markdown, renderMarkdown, { immediate: true });\n        \n        // Also watch viewMode changes to reapply highlighting\n        watch(viewMode, () => {\n          setTimeout(() => {\n            Prism.highlightAll();\n          }, 10);\n        });\n\n        return {\n          markdown,\n          compiledMarkdown,\n          viewMode,\n          toggleView,\n          saveMarkdown,\n          renderMarkdown\n        };\n      }\n    });\n\n    document.addEventListener('DOMContentLoaded', () => {\n      app.mount('#app');\n    });\n  </script>\n  <!-- Hidden element with markdown content -->\n  <div id=\"initial-markdown-content\" style=\"display: none;\">{MARKDOWN_TEXT}</div>\n</body>\n</html>\n```\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743934534329/d077f23e-80f6-4cb4-bed5-ab22624973f0.gif align=\"center\")\n\nOk, on to the graph view!\n\n## Building JSON for the graph view\n\nWith the Markdown editor, we created an HTML doc and merged in the Markdown text using a placeholder and the `Substitute()` function. For the graph view, weâ€™ll use the same approach, but we need to merge in JSON data to feed the graph. If your file is hosted on FileMaker server, I highly recommend using the FileMaker Data API to fetch your records as JSON. This is much easier than using a script or calculation, and itâ€™s more efficient because the server handles the load and it does it in a single request, instead of looping through records.\n\nHowever, in my case, Iâ€™m building this file locally and I wanted others to be able to test it without hosting the file. So weâ€™re gonna do it the hard way with a script.\n\nCreate a new script called Build JSON. Then use a `While()` loop and `ExecuteSQL()` to convert your notes into a JSON array.\n\n```plaintext\nSet Variable [ $$noteJson; \n  Value: While (\n    [\n      ~delim = \"|\" ;\n      ~sql = \"SELECT \\\"id\\\", \\\"title\\\", \\\"tags\\\", \\\"project\\\", \\\"created_by\\\" FROM \\\"notes\\\"\" ;\n      ~result = ExecuteSQL ( ~sql ; ~delim; Â¶ ) ;\n      ~rowCount = ValueCount ( ~result ) ;\n      ~i = 0 ;\n      ~output = JSONSetElement ( \"\" ; \"\" ; \"[]\" ; JSONString )\n    ] ;\n    \n    ~i < ~rowCount ;\n    \n    [\n      ~row = GetValue ( ~result ; ~i + 1 ) ;\n      ~fields = Substitute ( ~row ; ~delim ; \"Â¶\" ) ;\n      ~id = GetValue ( ~fields ; 1 ) ;\n      ~title = GetValue ( ~fields ; 2 ) ;\n      ~tags = GetValue ( ~fields ; 3 ) ;\n      ~project = GetValue ( ~fields ; 4 ) ;\n      ~created_by = GetValue ( ~fields ; 5 ) ;\n      \n      ~json = JSONSetElement ( \"{}\" ;\n        [ \"id\" ; ~id ; JSONString ] ;\n        [ \"title\" ; ~title ; JSONString ] ;\n        [ \"tags\" ; ~tags ; JSONString ] ;\n        [ \"project\" ; ~project ; JSONString ] ;\n        [ \"created_by\" ; ~created_by ; JSONString ]\n      ) ;\n      \n      ~output = JSONSetElement ( ~output ; \"[\" & ~i & \"]\" ; ~json ; JSONObject ) ;\n      ~i = ~i + 1\n    ] ;\n    \n    ~output\n  ) \n]\n\n// Optional: Show result\n// Show Custom Dialog [ Message: $$noteJson; Default Button: â€œOKâ€, Commit: â€œYesâ€; Button 2: â€œCancelâ€, Commit: â€œNoâ€ ]\n\nSet Web Viewer [ Object Name: \"webviewer\"; Action: Reload ]\n```\n\nThis will save the JSON to a global `$$notesJson` variable that we can merge into the new web viewer for the graph view. Set this script to run *On Record Load*, so that the graph view can update whenever records are navigated.\n\nAlternatively, you can call the FileMaker Data API in your script to get the JSON, then set the global variable.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743935361003/ee0670bd-cfa9-4b5e-acbb-51ebc1d413b7.png align=\"center\")\n\n## Building the graph view with Vis.js\n\nNext, add a new web viewer, and a text object to store the HTML. Link the web viewer to display the text object content like before, using `Get(LayoutObjectAttribute)`. Then enter the following code in the text object:\n\n```xml\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Basic VisJS Network Example</title>\n  <script src=\"https://unpkg.com/vis-network@9.1.2/dist/vis-network.min.js\"></script>\n  <style>\n    body {\n      font-family: sans-serif;\n      margin: 0;\n      padding: 20px;\n    }\n    \n    #network {\n      width: 100%;\n      height: 600px;\n      border: 1px solid #ccc;\n    }\n  </style>\n</head>\n<body>\n  <div id=\"network\"></div>\n  \n  <script>\n    // Create mock data directly\n    const nodes = new vis.DataSet([\n      { id: 1, label: 'Node 1', color: '#4169E1' },\n      { id: 2, label: 'Node 2', color: '#B8860B' },\n      { id: 3, label: 'Node 3', color: '#3CB371' },\n      { id: 4, label: 'Node 4', color: '#C71585' },\n      { id: 5, label: 'Node 5', color: '#4169E1' }\n    ]);\n\n    const edges = new vis.DataSet([\n      { from: 1, to: 3, label: 'connects to' },\n      { from: 1, to: 2, label: 'relates to' },\n      { from: 2, to: 4, label: 'depends on' },\n      { from: 2, to: 5, label: 'references' },\n      { from: 3, to: 5, label: 'links to' }\n    ]);\n    \n    const data = {\n      nodes: nodes,\n      edges: edges\n    };\n    \n    const options = {\n      nodes: {\n        shape: 'dot',\n        size: 16,\n        font: { size: 14 }\n      },\n      edges: {\n        arrows: 'to',\n        font: {\n          align: 'middle'\n        }\n      },\n      physics: {\n        stabilization: true,\n        barnesHut: {\n          gravitationalConstant: -8000,\n          springLength: 150,\n          springConstant: 0.04\n        }\n      },\n      interaction: {\n        hover: true,\n        tooltipDelay: 200\n      }\n    };\n    \n    const container = document.getElementById('network');\n    const network = new vis.Network(container, data, options);\n    \n    // Handle window resize\n    function resizeNetwork() {\n      network.fit();\n    }\n    \n    // Initial sizing\n    resizeNetwork();\n    \n    // Add resize event listener\n    window.addEventListener('resize', resizeNetwork);\n  </script>\n</body>\n</html>\n```\n\nThis should give you a hard-coded graph with a few nodes connected.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743936036905/77923483-5690-4aa3-a1d8-04e5705b638b.png align=\"center\")\n\nNotice how the JSON is structured to feed the graph. The nodes have an `id`, `label` and `color`, and the edges have a `to`, `from`, and `label`. But the data we want to graph is in a different format, with JSON describing each note. We can merge in the notes JSON the same way as before, then map over it with JavaScript to create the nodes and edges data.\n\nUpdate the web viewer source to merge in the `$$notesJson` variable:\n\n```plaintext\nSubstitute ( \n\n  GetLayoutObjectAttribute ( \"webviewerCode\"; \"content\" );\n  \"NOTE_JSON\";\n  $$noteJson\n\n )\n```\n\nThen update the text object:\n\n```xml\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Basic VisJS Network Example</title>\n  <script src=\"https://unpkg.com/vis-network@9.1.2/dist/vis-network.min.js\"></script>\n  <style>\n    body {\n      font-family: sans-serif;\n    }\n    \n    #network {\n      width: 100%;\n      height: 600px;\n      border: 1px solid #ccc;\n    }\n    \n    .legend {\n      position: absolute;\n      bottom: 20px;\n      right: 20px;\n      background-color: rgba(255, 255, 255, 0.8);\n      border: 1px solid #ccc;\n      border-radius: 5px;\n      padding: 10px;\n    }\n    \n    .legend-title {\n      font-weight: bold;\n      margin-bottom: 5px;\n      border-bottom: 1px solid #ccc;\n      padding-bottom: 5px;\n    }\n    \n    .legend-item {\n      display: flex;\n      align-items: center;\n      margin: 5px 0;\n    }\n    \n    .legend-color {\n      width: 15px;\n      height: 15px;\n      margin-right: 10px;\n      border-radius: 3px;\n    }\n  </style>\n</head>\n<body>\n  <div id=\"network\"></div>\n  \n  <div class=\"legend\">\n    <div class=\"legend-title\">Node Types</div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #B8860B;\"></div>\n      <div>Captain's Log</div>\n    </div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #4169E1;\"></div>\n      <div>Captain</div>\n    </div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #3CB371;\"></div>\n      <div>Project</div>\n    </div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #C71585;\"></div>\n      <div>Tag</div>\n    </div>\n  </div>\n  \n  <script>\n    const sampleLogs = {NOTE_JSON};\n    \n    const logs = sampleLogs;\n    const nodes = [];\n    const edges = [];\n    const nodeSet = new Set();\n    \n    // Function to handle tags that might be arrays or semicolon-separated strings\n    function processTags(tags) {\n      if (Array.isArray(tags)) {\n        return tags;\n      } else if (typeof tags === 'string') {\n        return tags.split(';').map(tag => tag.trim()).filter(tag => tag !== '');\n      }\n      return [];\n    }\n    \n    logs.forEach((log, i) => {\n      // Use log.id if available, otherwise use index\n      const logId = `log-${log.id || i}`;\n      \n      // Add the log node\n      nodes.push({\n        id: logId,\n        label: log.title,\n        shape: 'box',\n        group: 'log',\n        title: log.title // Tooltip on hover\n      });\n      \n      // Add captain nodes with prefix to prevent collisions with other node types\n      const captainId = `captain-${log.created_by}`;\n      if (!nodeSet.has(captainId)) {\n        nodes.push({\n          id: captainId,\n          label: log.created_by,\n          group: 'captain',\n          title: `Captain: ${log.created_by}`\n        });\n        nodeSet.add(captainId);\n      }\n      edges.push({ from: logId, to: captainId, label: 'author' });\n      \n      // Add project nodes with prefix\n      const projectId = `project-${log.project}`;\n      if (!nodeSet.has(projectId)) {\n        nodes.push({\n          id: projectId,\n          label: log.project,\n          group: 'project',\n          title: `Project: ${log.project}`\n        });\n        nodeSet.add(projectId);\n      }\n      edges.push({ from: logId, to: projectId, label: 'project' });\n      \n      // Process tags that might be arrays or semicolon-separated strings\n      const tagArray = processTags(log.tags);\n      \n      tagArray.forEach(tag => {\n        // Add tag nodes with prefix\n        const tagId = `tag-${tag}`;\n        if (!nodeSet.has(tagId)) {\n          nodes.push({\n            id: tagId,\n            label: tag,\n            group: 'tag',\n            title: `Tag: ${tag}`\n          });\n          nodeSet.add(tagId);\n        }\n        edges.push({ from: logId, to: tagId, label: 'tag' });\n      });\n    });\n    \n    const data = {\n      nodes: new vis.DataSet(nodes),\n      edges: new vis.DataSet(edges)\n    };\n    \n    const options = {\n      nodes: {\n        shape: 'dot',\n        size: 16,\n        font: { size: 14 }\n      },\n      edges: {\n        arrows: 'to',\n        font: {\n          align: 'middle'\n        }\n      },\n      groups: {\n        log: { shape: 'box', color: '#B8860B' },\n        captain: { color: '#4169E1' },\n        project: { color: '#3CB371' },\n        tag: { color: '#C71585' }\n      },\n      physics: {\n        stabilization: true,\n        barnesHut: {\n          gravitationalConstant: -8000,\n          springLength: 150,\n          springConstant: 0.04\n        }\n      },\n      interaction: {\n        hover: true,\n        tooltipDelay: 200\n      }\n    };\n    \n    const container = document.getElementById('network');\n    const network = new vis.Network(container, data, options);\n    \n    // Handle window resize\n    function resizeNetwork() {\n      network.fit();\n    }\n    \n    // Initial sizing\n    resizeNetwork();\n    \n    // Add resize event listener\n    window.addEventListener('resize', resizeNetwork);\n  </script>\n</body>\n</html>\n```\n\nThis should give you a network graph based on your actual notes:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743937127498/0fcbb096-7330-4ea1-be54-51f5c392d857.png align=\"center\")\n\nThereâ€™s a bit of JavaScript involved here, and it will be different depending on your field names and how you want to structure the relationships. Just take a sample of your source JSON and the nodes/edges JSON from the hard-coded example, and ask ChatGPT to write a JavaScript function to create the JSON to feed the graph.\n\nNext, we can add a click event to the notes nodes to navigate to that record in FileMaker:\n\n```javascript\n    // Handle node selection\n    network.on(\"selectNode\", function(params) {\n      if (params.nodes.length === 1) {\n        const nodeId = params.nodes[0];\n        const selectedNode = nodes.find(node => node.id === nodeId);\n        \n        if (selectedNode && selectedNode.group === 'log') {\n          // For log nodes, extract the log ID from the node ID\n          const logIdMatch = nodeId.match(/log-(\\d+)/);\n          if (logIdMatch && logIdMatch[1]) {\n            const logId = logIdMatch[1];\n\n            \n            // Call FileMaker script with the log ID\n            FileMaker.PerformScript(\"Select Record\", logId);\n          }\n        }\n      }\n    });\n```\n\nAnd add a *Select Record* script to call when clicking a note:\n\n```plaintext\nSet Variable [ $id; Value:Get(ScriptParameter) ]\nPerform Find [ Specified Find Requests: Find Records; Criteria: notes::id: â€œ$idâ€ ]\n```\n\nNow select a note and you should see FileMaker navigate to that record.\n\nHereâ€™s the final HTML with some extra CSS to add a gradient and some stars to the background:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743941065406/6fb9d0bc-9472-43ab-9e5f-2d41986de7bf.gif align=\"center\")\n\n```xml\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Star Trek Captains' Logs Graph</title>\n  <script src=\"https://unpkg.com/vis-network@9.1.2/dist/vis-network.min.js\"></script>\n  <style>\n    html, body {\n      margin: 0;\n      padding: 0;\n      height: 100%;\n      width: 100%;\n      font-family: sans-serif;\n      background-color: #050811;\n      color: #e1e6f0;\n      overflow: hidden;\n      position: fixed;\n    }\n    \n    #network {\n      width: 100%;\n      height: 100vh;\n      border: 1px solid #212941;\n      position: relative;\n      z-index: 10;\n      overflow: hidden;\n    }\n    \n    /* Space background with stars and galaxies */\n    body::before {\n      content: \"\";\n      position: fixed;\n      top: 0;\n      left: 0;\n      width: 100%;\n      height: 100%;\n      background-image: \n        radial-gradient(circle at 20% 35%, rgba(81, 56, 157, 0.15) 0%, transparent 50%),\n        radial-gradient(circle at 75% 44%, rgba(124, 85, 214, 0.1) 0%, transparent 40%),\n        radial-gradient(circle at 30% 70%, rgba(29, 78, 216, 0.15) 0%, transparent 45%),\n        radial-gradient(circle at 50% 80%, rgba(49, 78, 204, 0.1) 0%, transparent 30%);\n      z-index: 1;\n    }\n    \n    /* Star layers with different animations */\n    .stars {\n      position: fixed;\n      top: 0;\n      left: 0;\n      width: 100%;\n      height: 100%;\n      pointer-events: none;\n      z-index: 5;\n    }\n    \n    .stars-small {\n      background-image: \n        radial-gradient(1px 1px at 5% 10%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(1px 1px at 15% 25%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1px 1px at 30% 65%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1px 1px at 37% 22%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1px 1px at 55% 30%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(1px 1px at 75% 55%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1px 1px at 82% 3%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1px 1px at 85% 85%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1px 1px at 95% 45%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1px 1px at 42% 79%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1px 1px at 23% 53%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1px 1px at 63% 76%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(1px 1px at 68% 58%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1px 1px at 74% 92%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1px 1px at 19% 38%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(1px 1px at 26% 89%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1px 1px at 8% 76%, rgba(255, 255, 255, 0.7) 0%, transparent 100%);\n      animation: stars-move-1 150s linear infinite;\n    }\n    \n    .stars-medium {\n      background-image: \n        radial-gradient(1.5px 1.5px at 12% 15%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 27% 38%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 43% 59%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 48% 42%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 58% 12%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 73% 25%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 88% 54%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 94% 77%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 83% 93%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 32% 82%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 17% 67%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 71% 33%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(1.5px 1.5px at 63% 64%, rgba(255, 255, 255, 0.9) 0%, transparent 100%);\n      animation: stars-move-2 120s linear infinite;\n    }\n    \n    .stars-large {\n      background-image: \n        radial-gradient(2px 2px at 20% 30%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(2px 2px at 40% 70%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(2px 2px at 60% 20%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(2px 2px at 70% 90%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(2px 2px at 90% 40%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(2px 2px at 10% 60%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(2px 2px at 30% 95%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(2px 2px at 85% 15%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(2px 2px at 55% 55%, rgba(255, 255, 255, 0.7) 0%, transparent 100%);\n      animation: stars-move-3 80s linear infinite;\n    }\n    \n    /* Bright \"twinkle\" stars */\n    .stars-twinkle {\n      background-image: \n        radial-gradient(2.5px 2.5px at 15% 23%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(2.5px 2.5px at 36% 45%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(2.5px 2.5px at 58% 16%, rgba(255, 255, 255, 0.7) 0%, transparent 100%),\n        radial-gradient(2.5px 2.5px at 76% 62%, rgba(255, 255, 255, 0.9) 0%, transparent 100%),\n        radial-gradient(2.5px 2.5px at 33% 86%, rgba(255, 255, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(2.5px 2.5px at 88% 33%, rgba(255, 255, 255, 0.7) 0%, transparent 100%);\n      animation: stars-twinkle 10s ease-in-out infinite;\n    }\n    \n    /* Colorful distant stars */\n    .stars-color {\n      background-image: \n        radial-gradient(2px 2px at 25% 40%, rgba(255, 204, 204, 0.8) 0%, transparent 100%),\n        radial-gradient(2px 2px at 45% 20%, rgba(204, 204, 255, 0.8) 0%, transparent 100%),\n        radial-gradient(2px 2px at 65% 45%, rgba(204, 255, 204, 0.7) 0%, transparent 100%),\n        radial-gradient(2px 2px at 85% 75%, rgba(255, 255, 204, 0.8) 0%, transparent 100%),\n        radial-gradient(2px 2px at 22% 75%, rgba(255, 204, 255, 0.7) 0%, transparent 100%);\n      animation: stars-move-4 180s linear infinite;\n    }\n    \n    @keyframes stars-move-1 {\n      0% { background-position: 0% 0%; }\n      100% { background-position: 100% 100%; }\n    }\n    \n    @keyframes stars-move-2 {\n      0% { background-position: 100% 0%; }\n      100% { background-position: 0% 100%; }\n    }\n    \n    @keyframes stars-move-3 {\n      0% { background-position: 50% 0%; }\n      100% { background-position: 50% 100%; }\n    }\n    \n    @keyframes stars-move-4 {\n      0% { background-position: 0% 50%; }\n      100% { background-position: 100% 50%; }\n    }\n    \n    @keyframes stars-twinkle {\n      0%, 100% { opacity: 0.2; }\n      50% { opacity: 0.9; }\n    }\n    \n    /* Legend styles */\n    .legend {\n      position: absolute;\n      bottom: 20px;\n      right: 20px;\n      background-color: rgba(10, 14, 23, 0.8);\n      border: 1px solid #212941;\n      border-radius: 5px;\n      padding: 10px;\n      color: white;\n      z-index: 100;\n      box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);\n    }\n    \n    .legend-title {\n      font-weight: bold;\n      margin-bottom: 5px;\n      border-bottom: 1px solid #3A4366;\n      padding-bottom: 5px;\n    }\n    \n    .legend-item {\n      display: flex;\n      align-items: center;\n      margin: 5px 0;\n    }\n    \n    .legend-color {\n      width: 15px;\n      height: 15px;\n      margin-right: 10px;\n      border-radius: 3px;\n    }\n  </style>\n</head>\n<body>\n  <div id=\"network\"></div>\n  \n  <!-- Star layers -->\n  <div class=\"stars stars-small\"></div>\n  <div class=\"stars stars-medium\"></div>\n  <div class=\"stars stars-large\"></div>\n  <div class=\"stars stars-twinkle\"></div>\n  <div class=\"stars stars-color\"></div>\n  \n  <div class=\"legend\">\n    <div class=\"legend-title\">Node Types</div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #B8860B;\"></div>\n      <div>Captain's Log</div>\n    </div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #4169E1;\"></div>\n      <div>Captain</div>\n    </div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #3CB371;\"></div>\n      <div>Project</div>\n    </div>\n    <div class=\"legend-item\">\n      <div class=\"legend-color\" style=\"background-color: #C71585;\"></div>\n      <div>Tag</div>\n    </div>\n  </div>\n  <script>\n    // Get input data from FileMaker or use sample data\n    let logs = NOTE_JSON;\n    \n    // Handle FileMaker data if available\n    try {\n      // FileMaker WebViewer will set this variable\n      if (typeof FileMakerData !== 'undefined' && FileMakerData) {\n        logs = JSON.parse(FileMakerData);\n        console.log(\"Using FileMaker data:\", logs);\n      }\n    } catch (e) {\n      console.error(\"Error parsing FileMaker JSON data:\", e);\n      alert(\"Error parsing data from FileMaker: \" + e.message);\n    }\n\n    const nodes = [];\n    const edges = [];\n    const nodeSet = new Set();\n\n    // Function to handle tags that might be arrays or semicolon-separated strings\n    function processTags(tags) {\n      if (Array.isArray(tags)) {\n        return tags;\n      } else if (typeof tags === 'string') {\n        return tags.split(';').map(tag => tag.trim()).filter(tag => tag !== '');\n      }\n      return [];\n    }\n\n    logs.forEach((log, i) => {\n      // Use log.id if available, otherwise use index\n      const logId = `log-${log.id || i}`;\n      \n      // Add the log node\n      nodes.push({ \n        id: logId, \n        label: log.title, \n        shape: 'box', \n        group: 'log',\n        title: log.title // Tooltip on hover\n      });\n      \n      // Add captain nodes with prefix to prevent collisions with other node types\n      const captainId = `captain-${log.created_by}`;\n      if (!nodeSet.has(captainId)) {\n        nodes.push({ \n          id: captainId, \n          label: log.created_by, \n          group: 'captain',\n          title: `Captain: ${log.created_by}`\n        });\n        nodeSet.add(captainId);\n      }\n      edges.push({ from: logId, to: captainId, label: 'author' });\n\n      // Add project nodes with prefix\n      const projectId = `project-${log.project}`;\n      if (!nodeSet.has(projectId)) {\n        nodes.push({ \n          id: projectId, \n          label: log.project, \n          group: 'project',\n          title: `Project: ${log.project}`\n        });\n        nodeSet.add(projectId);\n      }\n      edges.push({ from: logId, to: projectId, label: 'project' });\n\n      // Process tags that might be arrays or semicolon-separated strings\n      const tagArray = processTags(log.tags);\n      \n      tagArray.forEach(tag => {\n        // Add tag nodes with prefix\n        const tagId = `tag-${tag}`;\n        if (!nodeSet.has(tagId)) {\n          nodes.push({ \n            id: tagId, \n            label: tag, \n            group: 'tag',\n            title: `Tag: ${tag}`\n          });\n          nodeSet.add(tagId);\n        }\n        edges.push({ from: logId, to: tagId, label: 'tag' });\n      });\n    });\n\n    const data = {\n      nodes: new vis.DataSet(nodes),\n      edges: new vis.DataSet(edges)\n    };\n\n    const options = {\n      nodes: {\n        shape: 'dot',\n        size: 16,\n        font: { size: 14, color: '#FFFFFF' }\n      },\n      edges: {\n        arrows: 'to',\n        font: { \n          align: 'middle', \n          color: '#FFFFFF',\n          strokeWidth: 0,\n          background: {\n            enabled: true,\n            color: 'rgba(10, 14, 23, 0.7)',\n            size: 5\n          }\n        },\n        color: { color: '#3A4366', hover: '#5A6386' }\n      },\n      groups: {\n        log: { shape: 'box', color: '#B8860B', font: { color: '#FFFFFF' } },\n        captain: { color: '#4169E1', font: { color: '#FFFFFF' } },\n        project: { color: '#3CB371', font: { color: '#FFFFFF' } },\n        tag: { color: '#C71585', font: { color: '#FFFFFF' } }\n      },\n      physics: {\n        stabilization: true,\n        barnesHut: {\n          gravitationalConstant: -8000,\n          springLength: 150,\n          springConstant: 0.04\n        }\n      },\n      interaction: {\n        hover: true,\n        tooltipDelay: 200\n      }\n    };\n\n    const container = document.getElementById('network');\n    const network = new vis.Network(container, data, options);\n\n    // Handle window resize\n    function resizeNetwork() {\n      network.setSize('100%', window.innerHeight + 'px');\n      network.fit({\n        animation: {\n          duration: 500,\n          easingFunction: 'easeInOutQuad'\n        }\n      });\n    }\n\n    // Initial sizing\n    resizeNetwork();\n    \n    // Add resize event listener\n    window.addEventListener('resize', resizeNetwork);\n    \n    // Handle node selection\n    network.on(\"selectNode\", function(params) {\n      if (params.nodes.length === 1) {\n        const nodeId = params.nodes[0];\n        const selectedNode = nodes.find(node => node.id === nodeId);\n        \n        if (selectedNode && selectedNode.group === 'log') {\n          // For log nodes, extract the log ID from the node ID\n          const logIdMatch = nodeId.match(/log-(\\d+)/);\n          if (logIdMatch && logIdMatch[1]) {\n            const logId = logIdMatch[1];\n\n            \n            // Call FileMaker script with the log ID\n            FileMaker.PerformScript(\"Select Record\", logId);\n          }\n        }\n      }\n    });\n  </script>\n</body>\n</html>\n```\n\nThis should give you a good starting point for building your own network graph and markdown editor. From here you can update the logic based on your fields and create edges (relationships) using backlinks, like in Obsidian, or create a knowledge graph using [NER (named-entity recognition)](https://blog.greenflux.us/named-entity-recognition-with-bert-and-hugging-face).\n\n## Conclusion\n\nWeb viewers in FileMaker Pro are a great way to extend the platform and add new features like a Markdown editor and network graph. Like all the other apps in my [FileMaker-Experiments](https://github.com/GreenFluxLLC/FileMaker-Experiments) repository, this app is meant to be a proof-of-concept and a starting point, not a complete app. Thereâ€™s a lot more you could do from here, like outputting the Markdown to PDF, sending it as an email, or adding hover effects and popups to the network graph. Feel free to copy the app and modify for your own use case.",
      "stars": null,
      "comments": 2,
      "upvotes": 24,
      "read_time": "22 min read",
      "language": null
    },
    {
      "title_en": "A Deep Dive into AWS Load Balancers: CLB, ALB, NLB, and GWLB",
      "url": "https://learnwithnitesh.hashnode.dev/a-deep-dive-into-aws-load-balancers-clb-alb-nlb-and-gwlb",
      "source": "hashnode",
      "published_at": "2025-03-29T11:16:37.096000+00:00",
      "external_id": null,
      "tags": [
        "learnwithnitesh",
        "AWS",
        "alb",
        "Load Balancer",
        "aws lambda",
        "Cloud",
        "Cloud Computing",
        "Devops",
        "education",
        "Hashnode",
        "learning",
        "networking",
        "Security",
        "technology",
        "Tutorial"
      ],
      "content_length": 7365,
      "content_preview": "Listen up, because this is important. If you're running anything on AWSâ€”or hell, even thinking about itâ€”you need to understand **load balancing**. You can't just throw traffic at a single server and hope for the best. Thatâ€™s amateur hour. AWS gives you four different types of load balancers, and if you donâ€™t know what they do, you're basically walking into a gunfight with a butter knife.\n\nSo letâ€™s break this down in a way that actually makes sense. No corporate jargon, no fluff. Just raw, aggres",
      "content_full": "Listen up, because this is important. If you're running anything on AWSâ€”or hell, even thinking about itâ€”you need to understand **load balancing**. You can't just throw traffic at a single server and hope for the best. Thatâ€™s amateur hour. AWS gives you four different types of load balancers, and if you donâ€™t know what they do, you're basically walking into a gunfight with a butter knife.\n\nSo letâ€™s break this down in a way that actually makes sense. No corporate jargon, no fluff. Just raw, aggressive knowledge so you can make smart choices.\n\n---\n\n## **The Four Load Balancers You Need to Know**\n\nAWS doesnâ€™t have just *one* load balancer. Oh no, that would be too easy. Instead, they give you **four**, and each of them serves a different purpose. Letâ€™s get into it.\n\n---\n\n## **1\\. Classic Load Balancer (CLB) â€“ The Old Grandpa of Load Balancers**\n\n### **How CLB Works (or Barely Works)**\n\nThe **Classic Load Balancer (CLB)** is the dinosaur of AWS. It operates at **Layer 4 (Transport Layer) and Layer 7 (Application Layer)** and just kind of throws traffic at EC2 instances based on very basic health checks. This was great in 2010. Now? Itâ€™s basically on life support.\n\nHereâ€™s what happens when you send traffic to a CLB:\n\n1. The CLB gets your request and checks which backend EC2 instances are still alive.\n    \n2. It distributes traffic using **round-robin** (if itâ€™s HTTP) or **least outstanding requests** (for TCP).\n    \n3. If an instance dies, it stops sending traffic to it.\n    \n\n### **Why CLB is Garbage (But Sometimes Necessary)**\n\n* **No smart routing.** It doesnâ€™t know or care whatâ€™s in your HTTP headers.\n    \n* **No WebSockets or HTTP/2.** Welcome to 2010, baby.\n    \n* **Slower and more expensive than ALB or NLB.**\n    \n\n### **When Would You Use This?**\n\n* **If you're maintaining an ancient AWS setup.**\n    \n* **If you literally have no other option.**\n    \n* **If you enjoy suffering.**\n    \n\n---\n\n## **2\\. Application Load Balancer (ALB) â€“ The Smart One**\n\n### **How ALB Works**\n\nThe **Application Load Balancer (ALB)** is the load balancer you actually want for web apps and APIs. It operates at **Layer 7 (Application Layer)**, which means it actually understands HTTP and can route traffic **intelligently**. Basically, Sherlock Holmes of Load Balancers.\n\nHereâ€™s what happens when a request hits the ALB:\n\n1. ALB inspects the **HTTP request**, looking at the URL, headers, and parameters.\n    \n2. It decides **which backend** should handle it based on pre-set rules.\n    \n3. If the backend is down, it automatically redirects traffic elsewhere.\n    \n4. It supports **WebSockets, HTTP/2, and TLS termination**, making it a beast for modern web apps.\n    \n\n### **Why ALB is a No-Brainer**\n\n* **Path-based routing** (e.g., `/api` goes one place, `/blog` goes another).\n    \n* **Host-based routing** (e.g., [`shop.example.com`](http://shop.example.com) and [`api.example.com`](http://api.example.com) can go to different places).\n    \n* **Header, Query String-based routing.** Want to send mobile users to one server and desktop users to another? Done.\n    \n* **Integrated authentication.**\n    \n\n### **When Should You Use ALB?**\n\n* **Web apps, REST APIs, and microservices.**\n    \n* **Kubernetes (EKS) and container-based workloads.**\n    \n* **Anytime you need intelligent request routing.**\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743245640964/a0d4ee86-fd30-461d-97b2-fdf483c22e97.png align=\"center\")\n\n---\n\n## **3\\. Network Load Balancer (NLB) â€“ The Speed Demon**\n\n### **How NLB Works**\n\nThe **Network Load Balancer (NLB)** is all about **raw speed**. It operates at **Layer 4 (Transport Layer - TCP/UDP)** and doesnâ€™t give a damn about HTTP requests. It just forwards traffic **as fast as humanly possible**.\n\nHereâ€™s what happens when a request hits the NLB:\n\n1. It picks a backend server based on **flow hash algorithms** (which keeps requests from the same client going to the same server).\n    \n2. It forwards the request with **single-digit millisecond latency**.\n    \n3. It supports **millions of connections per second**. Yes, *millions*.\n    \n\n### **Why NLB is a Beast**\n\n* **Handles TCP, UDP, and TLS connections with almost zero overhead.**\n    \n* **Has static IP addresses** (unlike ALB, which uses DNS-based routing).\n    \n* **Scales like a monster.**\n    \n\n### **When Should You Use NLB?**\n\n* **Real-time apps like VoIP, gaming, and financial trading.**\n    \n* **Any high-performance, low-latency app that needs static IPs.**\n    \n* **If you need to handle millions of connections.**\n    \n\n---\n\n## **4\\. Gateway Load Balancer (GWLB) â€“ The Security Bouncer**\n\n### **How GWLB Works**\n\nThe **Gateway Load Balancer (GWLB)** is a different beast. It doesnâ€™t balance traffic for web appsâ€”it acts as a **traffic cop for security appliances** like firewalls, IDS, and deep packet inspection tools. It operates at **Layer 3 (Network Layer)**.\n\nHereâ€™s what happens when traffic hits a GWLB:\n\n1. It **forwards traffic to a security appliance** (like a firewall or intrusion detection system).\n    \n2. The security appliance inspects the traffic for threats.\n    \n3. If itâ€™s safe, GWLB forwards it to the destination.\n    \n\n### **Why GWLB is a Game-Changer**\n\n* **Centralized security.**\n    \n* **Seamless integration with AWS security tools.**\n    \n* **Ensures all traffic is inspected before hitting your applications.**\n    \n\n### **When Should You Use GWLB?**\n\n* **If you need enterprise-grade network security.**\n    \n* **If youâ€™re running compliance-heavy workloads.**\n    \n* **If you want deep packet inspection before traffic hits your VPC.**\n    \n\n### **Hereâ€™s Something Extra about GWLB**\n\n**Gateway Load Balancer (GLB)**â€”the only one in the load balancing family that actually uses **route tables**. Why? Because it operates at **Layer 3 (network layer)** and, unlike its other Load balancer siblings, GLWB needs to read this so this so that it can ensure everything flows smoothly at the **network gateway level** between **cloud, on-prem, and across regions**.\n\nMeanwhile, **Application Load Balancer (ALB) and Network Load Balancer (NLB)?** They donâ€™t bother with route tables because, well, they donâ€™t *have to*. **ALB** is living the high life at **Layer 7**, routing traffic based on *content* like HTTP headers and SSL session IDsâ€”itâ€™s basically the load balancer equivalent of a food critic, analyzing every detail before making a decision. **NLB**, down at **Layer 4**, keeps things more straightforward, handling traffic based on **ports and IP addresses**, just distributing connections like a robot with a checklist. So they already know how to do their job and not have to bother with reading â€˜route tableâ€™ manual.\n\nSo, while **GLB is just trying itâ€™s best**, ALB and NLB are just out here *vibing!*\n\n---\n\n## **Which Load Balancer Should You Use?**\n\nAWS doesnâ€™t give you four load balancers just for fun. Each one has a **specific** purpose. Hereâ€™s what you should use:\n\n* **ALB** â†’ If you need smart HTTP routing.\n    \n* **NLB** â†’ If you need ultra-fast, low-latency TCP/UDP balancing.\n    \n* **GWLB** â†’ If you need security inspection before traffic hits your apps.\n    \n* **CLB** â†’ If youâ€™re stuck in the past (or migrating away from it).\n    \n\nLoad balancing isnâ€™t optionalâ€”itâ€™s **how you keep your applications from dying under load**. Now you know which one to use. No excuses.\n\n---",
      "stars": null,
      "comments": 0,
      "upvotes": 29,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "Step-by-Step Approach to Use Onion Architecture in .NET",
      "url": "https://blog.anilgurau.com/step-by-step-approach-to-use-onion-architecture-in-net",
      "source": "hashnode",
      "published_at": "2025-03-30T18:39:31.601000+00:00",
      "external_id": null,
      "tags": [
        "onion architecture",
        ".NET",
        "C#",
        "software development",
        "software architecture"
      ],
      "content_length": 21590,
      "content_preview": "Alright, let's start coding! Before we dive into the code, let's quickly recap the core principles of Onion Architecture. If you want a more in-depth look, you can always check out my previous post on [https://anilgurau.hashnode.dev/why-choose-onion-architecture-for-better-software-development-practicese](https://anilgurau.hashnode.dev/why-choose-onion-architecture-for-better-software-development-practices).\n\nBut for those who just need a quick refresher, here's the gist:\n\n![](https://cdn.hashno",
      "content_full": "Alright, let's start coding! Before we dive into the code, let's quickly recap the core principles of Onion Architecture. If you want a more in-depth look, you can always check out my previous post on [https://anilgurau.hashnode.dev/why-choose-onion-architecture-for-better-software-development-practicese](https://anilgurau.hashnode.dev/why-choose-onion-architecture-for-better-software-development-practices).\n\nBut for those who just need a quick refresher, here's the gist:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743036577620/c692b514-09b0-4697-a94f-d5275d7a1f4e.png align=\"center\")\n\nAs you can see, the key idea is that dependencies point inwards, towards the domain core. But there's another crucial aspect: layers communicate exclusively through abstractions, not concrete implementations. This keeps our application flexible, maintainable, and testable. Each layer has a specific responsibility, and we'll be exploring how to implement them step by step in the following posts.\n\nWe're going to focus on the practical application of these principles, so let's get started!\n\n## Project Setup: Creating the Solution and Layers\n\n1. **Create Solution and Solution Folders:**\n    \n    1. In Visual Studio, create a new \"Blank Solution\" named \"`OnionArchitectureGuide`\".\n        \n    2. Create Solution Folders named \"Core\", \"Infrastructure\", and \"Presentation\".\n        \n2. **Create Domain Layer:**\n    \n    1. Inside the \"Core\" Solution Folder, add a new \"Class Library (.NET Standard/.NET Library)\" project named \"`OnionArchitectureGuide.Domain`\".\n        \n3. **Create Application Layer:**\n    \n    1. Inside the \"Core\" Solution Folder, add a new \"Class Library (.NET Standard/.NET Library)\" project named \"`OnionArchitectureGuide.Application.Abstraction`\".\n        \n    2. Inside the \"Core\" Solution Folder, add a new \"Class Library (.NET Standard/.NET Library)\" project named \"`OnionArchitectureGuide.Application.Implementation`\".\n        \n4. **Create Infrastructure Layer:**\n    \n    1. Inside the \"Infrastructure\" Solution Folder, add a new \"Class Library (.NET Standard/.NET Library)\" project named \"`OnionArchitectureGuide.Persistence`\".\n        \n5. **Create Presentation Layer:**\n    \n    1. Inside the \"Presentation\" Solution Folder, add a new \"ASP.NET Core Web API\" project named \"`OnionArchitectureGuide.Api`\".\n        \n    \n    **Note:** Application layer is split into `OnionArchitectureGuide.Application.Abstraction` and `OnionArchitectureGuide.Application.Implementation`. The Presentation layer communicates with the Application layer, but it should not have direct access to implementations. Placing service interfaces and their implementations together violates the principles of Onion Architecture and creates tight coupling between layers. The outer layer should interact with the inner layer only through abstractions, not concrete implementations.\n    \n\nYour Folder Structure Look like this:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743043079247/e4962ad9-99a9-42c9-9295-ee3c04ddf58c.png align=\"left\")\n\n## **Setting up Project Dependencies:**\n\nNow, we need to set up the project dependencies according to the Onion Architecture principles. Remember, dependencies should point inwards.\n\n1. `OnionArchitectureGuide.Application.Implementation` references `OnionArchitectureGuide.Domain` and `OnionArchitectureGuide.Application.Abstraction`.\n    \n2. `OnionArchitectureGuide.Infrastructure` references `OnionArchitectureGuide.Application.Abstraction` and `OnionArchitectureGuide.Domain`.\n    \n3. `OnionArchitectureGuide.Api` references `OnionArchitectureGuide.Application.Abstraction`\n    \n\nHere is a visual Representation of the Project Dependencies.\n\n![Project dependencies Diagram](https://cdn.hashnode.com/res/hashnode/image/upload/v1743032957611/850a0a70-c1d6-406c-83de-d2b494b30ec8.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743032908281/91aa2e64-01fb-48e3-8601-04be4febf710.png align=\"center\")\n\nThis dependency structure is crucial. The API depends on the Application layer, and Infrastructure depends on the Application and Domain Layer, and finally Application layer depends on the Domain layer. This keeps our core logic independent and allows us to easily swap out implementations. This inward dependency flow also ensures that changes in outer layers have minimal impact on the inner layers, enhancing stability. Furthermore, it allows for better testability as the core business logic can be tested in isolation.\n\nWe now have the basic project structure set up. In the next post, we'll start implementing Onion Architecture layer by layer.\n\n## Layer-by-Layer Implementation\n\nAlright, we've laid the foundation, now lets construct Domain layer.\n\n### Constructing Domain Layer\n\nNow, let's define our domain entities and repository interfaces within the `OnionArchitectureGuide.Domain` project. This layer holds our core business objects and their contracts.\n\nCreate Folder named `Entities` inside `OnionArchitectureGuide.Core` project and add classes:\n\n```csharp\npublic class Book\n{\n    public int BookId { get; set; }\n    public string Title { get; set; }\n    public DateTime PublishedOn { get; set; }\n    public int AuthorId { get; set; }\n    public Author Author { get; set; }\n}\n```\n\n```csharp\npublic class Author\n{\n    public int AuthorId { get; set; }\n    public string Name { get; set; }\n    public List<Book> Books { get; set; }\n}\n```\n\nCreate `Contracts` Folder inside `OnionArchitectureGuide.Core` project and add Repository Interfaces:\n\n```csharp\npublic interface IBookRepository\n{\n    Task<List<Book>> GetAllAsync();\n}\n```\n\n```csharp\npublic interface IAuthorRepository\n{\n    Task<List<Author>> GetAllAsync();\n}\n```\n\nYour Domain layer should look like this:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743046093544/6960b24f-d4d3-4886-8017-3f100c534f99.png align=\"left\")\n\nThe Core layer holds our domain model, free from external dependencies. This ensures core logic is reusable and maintainable.\n\nNext, we'll create abstractions in the `OnionArchitectureGuide.Application.Abstraction` before implementing these contracts in the `OnionArchitectureGuide.Application.Implementation`.\n\n### **Building Application Contracts (Abstraction)**\n\nNow that we've defined our domain entities and repository contracts in the Domain layer, let's move on to creating the application contracts in the `OnionArchitectureGuide.Application.Abstraction` project. This layer will define the DTOs and interfaces that our application services will implement.\n\nInside the `OnionArchitectureGuide.Application.Abstraction` project, create a folder named `Contracts`. This folder will hold our service interface definitions.\n\n```csharp\npublic interface IBookService\n{\n    Task<List<BookDto>> GetAllBooks();\n}\n```\n\n```csharp\npublic interface IAuthorService\n{\n    Task<List<AuthorDto>> GetAllAuthors();\n}\n```\n\nCreate `DTOs` Folder inside `OnionArchitectureGuide.Application.Abstraction` project and add Data transfer objects:\n\n```csharp\npublic class BookDto\n{\n    public int BookId { get; set; }\n    public string Title { get; set; }\n    public DateTime PublishedOn { get; set; }\n}\n```\n\n```csharp\npublic class AuthorDto\n{\n    public int AuthorId { get; set; }\n    public string Name { get; set; }\n}\n```\n\nYour `OnionArchitectureGuide.Application.Abstraction` project should look like this:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743053205746/e88eccbb-0ddf-40a5-9c38-95dc585be9c5.png align=\"left\")\n\nBy defining these service interfaces and DTOs in a separate `OnionArchitectureGuide.Application.Abstraction` layer, we decouple our application's core logic from the specific implementations and the data transfer details. This allows us to easily switch out implementations or change data transfer formats without affecting the rest of the application.\n\nNext, we'll implement these service contracts in the `OnionArchitectureGuide.Application.Implementation`.\n\n### **Implementing Application Services**\n\nAlright, let's move on to the `OnionArchitectureGuide.Application.Implementation` project. We'll be focusing on implementing the service contracts and utilizing the DTOs we defined in the `OnionArchitectureGuide.Application.Abstraction`.\n\nBefore Implementing this layer, lets first install required packages from Nuget package manager or package manager console.\n\n```bash\nNuGet\\Install-Package AutoMapper -Version 14.0.0\n```\n\nLets Create Mapping Profiles now. Create Folder named `Mappings` inside `OnionArchitectureGuide.Application.Implementation` project and add AutoMapper profiles:\n\n```csharp\ninternal class ApplicationProfile: Profile\n{\n    public ApplicationProfile()\n    {\n        CreateMap<BookDto, Book>().ReverseMap();\n        CreateMap<Author, AuthorDto>().ReverseMap();\n    }\n}\n```\n\nCreate Folder named `Services` inside `OnionArchitectureGuide.Application.Implementation` project and add service implementations:\n\n```csharp\npublic class BookService : IBookService\n{\n    private readonly IBookRepository _bookRepository;\n    private readonly IMapper _mapper;\n    public BookService(IBookRepository bookRepository, IMapper mapper)\n    {\n        _bookRepository = bookRepository;\n        _mapper = mapper;\n    }\n    public async Task<List<BookDto>> GetAllBooks()\n    {\n        return _mapper.Map<List<BookDto>>(await _bookRepository.GetAllAsync());\n    }\n}\n```\n\n```csharp\ninternal class AuthorService : IAuthorService\n{\n    private readonly IAuthorRepository _authorRepository;\n    private readonly IMapper _mapper;\n    public AuthorService(IAuthorRepository authorRepository, IMapper mapper)\n    {\n        _authorRepository = authorRepository;\n        _mapper = mapper;\n    }\n    public async Task<List<AuthorDto>> GetAllAuthors()\n    {\n        return _mapper.Map<List<AuthorDto>>(await _authorRepository.GetAllAsync());\n    }\n}\n```\n\nThis layer contains the core business logic of our application. By keeping it separate from the presentation and infrastructure layers, we ensure that our business logic is reusable and testable.\n\nBefore implementing Infrastructure layer, lets register all the Dependencies for this layer.\n\nCreate a file named `ApplicationServiceExtensions.cs` in the root level of the `OnionArchitectureGuide.Application.Implementation` project.\n\nThe `OnionArchitectureGuide.Application.Implementation` layer should look like this:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743059521789/ff40c4ad-5359-4e92-a72a-85d95e2a9382.png align=\"center\")\n\nNow, register all the dependencies for this project:\n\n```csharp\npublic static class ApplicationServiceExtensions\n{\n    public static void AddApplicationServices(this IServiceCollection services)\n    {\n        //Registering all the automapper profiles\n        services.AddAutoMapper(Assembly.GetExecutingAssembly());\n\n        services.AddScoped<IBookService,BookService>();\n        services.AddScoped<IAuthorService,AuthorService>();\n    }\n}\n```\n\nNext, we'll integrate the infrastructure layer to handle data access and implement repositories.\n\n### Integrating Infrastructure\n\nNow that we've implemented our application services in the `OnionArchitectureGuide.Application.Implementation` project, let's integrate the Infrastructure layer (`OnionArchitectureGuide.Persistence`). The main purpose of this layer is to handle data access, external API integrations, and other infrastructure-related concerns. Since this is a simple implementation, we will only focus on data access and Repository implementation.\n\nMake sure you have installed `Microsoft.EntityFrameworkCore` , `Microsoft.EntityFrameworkCore.SqlServer` and `Microsoft.EntityFrameworkCore.Tools` packages for SQL Server connection, Database operations and Migrations.\n\nRun these commands in Package Manager Console:\n\n```bash\nNuGet\\Install-Package Microsoft.EntityFrameworkCore -Version 9.0.3\n```\n\n```bash\nNuGet\\Install-Package Microsoft.EntityFrameworkCore.SqlServer -Version 9.0.3\n```\n\n```bash\nNuGet\\Install-Package Microsoft.EntityFrameworkCore.Tools -Version 9.0.3\n```\n\nAfter packages are installed, create `Data` Folder, which will hold our data access(DbContext).\n\n```csharp\ninternal class ApplicationDbContext : DbContext\n{\n    public ApplicationDbContext(DbContextOptions options) : base(options)\n    {\n    }\n\n    public DbSet<Book> Books { get; set; }\n    public DbSet<Author> Authors { get; set; }\n}\n```\n\nCreate Folder named `Repositories`, which holds the repository implementations that we declared in the domain layer.\n\n```csharp\ninternal class BookRepository : IBookRepository\n{\n    private readonly ApplicationDbContext _context;\n\n    public BookRepository(ApplicationDbContext context)\n    {\n        _context = context;\n    }\n\n    public async Task<List<Book>> GetAllAsync()\n    {\n        return await _context.Books.ToListAsync();\n    }\n}\n```\n\n```csharp\ninternal class AuthorRepository : IAuthorRepository\n{\n    private readonly ApplicationDbContext _context;\n\n    public AuthorRepository(ApplicationDbContext context)\n    {\n        _context = context;\n    }\n\n    public async Task<List<Author>> GetAllAsync()\n    {\n        return await _context.Authors.ToListAsync();\n    }\n}\n```\n\nNow lets register all the dependencies for the Infrastructure layer in the Dependency Injection container.\n\nCreate a file named `InfrastructureServiceExtensions.cs` in the root level of the `OnionArchitectureGuide.Persistence` project.\n\nYour `OnionArchitectureGuide.Presistence` should look like this:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743098582438/6ec74067-8fbf-4aa3-8835-a50d21da0850.png align=\"left\")\n\nRegister Infrastructure Dependencies as:\n\n```csharp\npublic static class InfrastructureServiceExtensions\n{\n    public static void AddInfrastructureServices(this IServiceCollection services, string connectionString)\n    {\n        //Registering Application DbContext and Seeding Values\n        services.AddDbContext<ApplicationDbContext>(options =>\n        {\n            options.UseSqlServer(connectionString);\n            options.UseSeeding((context, _) =>\n            {\n                var book = context.Set<Book>().FirstOrDefault();\n                if (book == null)\n                {\n                    context.Set<Book>().AddRange(BookSeeds());\n                    context.SaveChanges();\n                }\n            })\n            .UseAsyncSeeding(async (context, _, cancellationToken) =>\n                {\n                    var book = context.Set<Book>().FirstOrDefaultAsync();\n                    if (book == null)\n                    {\n                        context.Set<Book>().AddRange(BookSeeds());\n                        await context.SaveChangesAsync();\n                    }\n                });\n\n        });\n        services.AddScoped<IBookRepository, BookRepository>();\n        services.AddScoped<IAuthorRepository, AuthorRepository>();\n    }\n    private static List<Book> BookSeeds()\n    {\n        return new List<Book> { \n            new Book { Title = \"Book 1\", Author= new Author(){ Name =\"XXX\"} }, \n            new Book { Title = \"Book 2\", Author= new Author(){ Name =\"YYY\"} }\n            };\n    }\n}\n```\n\nInfrastructure layer isolates our application's core logic from specific data access and external dependencies. This allows us to easily switch out data access implementations or integrate with different external systems without affecting the rest of the application.\n\nNext, we'll expose our application through an API in the Presentation layer.\n\n### Exposing the Application (API)\n\nNow that we've integrated the Infrastructure layer, let's expose our application's functionality through an API in the Presentation layer. This layer will handle incoming requests and outgoing responses. We'll also configure our dependencies and set up the connection string for our data access.\n\nInside the `OnionArchitectureGuide.Api/Controllers`, add new API controllers `BookController.cs` and `AuthorController.cs` and create endpoints as shown below.\n\n```csharp\n[Route(\"api/[controller]\")]\n[ApiController]\npublic class BookController : ControllerBase\n{\n    private readonly IBookService _bookService;\n    public BookController(IBookService bookService)\n    {\n        _bookService = bookService;\n    }\n\n    [HttpGet(\"get-books\")]\n    public async Task<IActionResult> GetBooks()\n    {\n        return Ok(await _bookService.GetAllBooks());\n    }\n}\n```\n\n```csharp\n[Route(\"api/[controller]\")]\n[ApiController]\npublic class AuthorController : ControllerBase\n{\n    private readonly IAuthorService _authorService;\n\n    public AuthorController(IAuthorService authorService)\n    {\n        _authorService = authorService;\n    }\n\n    [HttpGet(\"get-authors\")]\n    public async Task<IActionResult> GetBooks()\n    {\n        return Ok(await _authorService.GetAllAuthors());\n    }\n}\n```\n\nWe're almost there, but first, we must register all necessary dependencies from `OnionArchitectureGuide.Application.Implementation` and `OnionArchitectureGuide.Infrastructure`.\n\nCurrently, our `OnionArchitectureGuide.Api` project only references `OnionArchitectureGuide.Application.Abstraction`. While we *could* add references to `OnionArchitectureGuide.Application.Implementation` and `OnionArchitectureGuide.Infrastructure` directly, I prefer to handle dependency registration in a separate class library dedicated to managing these dependencies. This keeps our API project cleaner.\n\nThe `OnionArchitectureGuide.Api` project should not directly communicate with `OnionArchitectureGuide.Application.Implementation` or `OnionArchitectureGuide.Infrastructure` via concrete implementations. We should avoid using `ApplicationDbContext`, repositories, or service implementations directly in the API project. This is why all implementations have `internal` access modifiers.\n\nTo achieve this, let's create a new \"Class Library (.NET Standard/.NET Library)\" project named OnionArchitectureGuide.DependencyManager within the 'Presentation' solution folder. Then, add project references to OnionArchitectureGuide.Application.Implementation and OnionArchitectureGuide.Infrastructure to this new project.\n\nThe Dependency Flow should look like this:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743104670592/dac9c1fa-0d5f-4e10-9105-38932aea1be4.png align=\"center\")\n\nHere is a screenshot of the `OnionArchitectureGuide.Api` and Dependency Manager.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743104844512/6eb78d7b-964f-44c1-beee-9dfd5b2ecdbc.png align=\"left\")\n\nLets manage dependencies:\n\n```csharp\npublic static class DependencyManager\n{\n    public static void RegisterApplicationDependencies(this IServiceCollection services)\n    {\n        services.AddApplicationServices();\n    }\n\n    public static void RegisterInfrastructureDependencies(this IServiceCollection services, string connectionString)\n    {\n        services.AddInfrastructureServices(connectionString);\n    }\n}\n```\n\nRegister all Dependencies in `Program.cs` of `OnionArchitectureGuide.Api` project.\n\n```csharp\n//Register Application and Infrastructure services using Dependency manager\n// you can also register directly from OnionArchitectureGuide.Application.Implementation and OnionArchitectureGuide.Presistence\n//E.g. builder.Services.AddApplicationServices(); and builder.Services.AddInfrastructureServices(builder.Configuration.GetConnectionString(\"DefaultConnection\") ?? String.Empty);\n\nbuilder.Services.RegisterApplicationDependencies();\nbuilder.Services.RegisterInfrastructureDependencies(builder.Configuration.GetConnectionString(\"DefaultConnection\")?? String.Empty);\n```\n\nAdd ConnectionString to `appsettings.json`\n\n```json\n\"ConnectionStrings\": {\n  \"DefaultConnection\":  \"Your connection string\"\n}\n```\n\nOne last step before we run our application.\n\n**Running Migration:**\n\nInstall `Microsoft.EntityFrameworkCore.Design` package in `OnionArchitectureGuide.Api` project, for the migration to work.\n\n```bash\nNuGet\\Install-Package Microsoft.EntityFrameworkCore.Design -Version 9.0.3\n```\n\n* Open Package Manager Console, set Default project to `OnionArchitectureGuide.`Persistence.\n    \n* Set a Project Startup to `OnionArchitectureGuide.Api`.\n    \n* Run: `Add-Migration InitialMigration` then `Update-Database`\n    \n\n```bash\nadd-migration initialMigration\n```\n\n```bash\nupdate-database\n```\n\nWe're now ready to run our application. Ensure all dependencies are correctly registered and the connection string is properly configured before proceeding.\n\n* Run the `OnionArchitectureGuide.Api` project.\n    \n* Use Postman or Swagger to send requests to the API endpoints.\n    \n\nExample Output:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1743112220719/3df86dcd-78e5-41a5-81ac-8b3b0f892720.png align=\"center\")\n\nCongratulations! You've successfully built a simple application using Onion Architecture. We've navigated through each layer, implementing the core logic and exposing it through an API.\n\nNow that you've grasped the fundamentals, it's time to put this knowledge into practice. Start by exploring how you can apply these principles to your own projects. Consider:\n\n* **Refactoring existing code:** Identify areas where you can apply Onion Architecture to improve maintainability.\n    \n* **Expanding your application:** Add new features and functionalities, ensuring they fit within the established architecture.\n    \n* **Experimenting with different technologies:** Explore how Onion Architecture can be adapted to various databases, frameworks, and external systems.\n    \n\nKeep exploring, keep coding, and keep building robust applications. Thank you for joining me on this practical journey. Happy Coding!",
      "stars": null,
      "comments": 3,
      "upvotes": 6,
      "read_time": "10 min read",
      "language": null
    },
    {
      "title_en": "How to Set Up Full-Stack XM Cloud Local Development for Multiple Code Bases",
      "url": "https://enlightenwithamit.hashnode.dev/xm-cloud-local-development-setup",
      "source": "hashnode",
      "published_at": "2025-03-27T05:27:15.421000+00:00",
      "external_id": null,
      "tags": [
        "2025",
        "Web Development",
        "xmcloud",
        "Hashnode",
        "headless cms",
        "Programming Blogs",
        "Developer",
        "setup",
        "Sitecore"
      ],
      "content_length": 24087,
      "content_preview": "## **ðŸ¤”**Introduction to XM Cloud Local Development\n\n[XM Cloud](https://enlightenwithamit.hashnode.dev/how-can-i-get-started-learning-sitecore-xm-cloud) is a powerful platform for managing and delivering personalized digital experiences. However, when working with [multiple websites](https://enlightenwithamit.hashnode.dev/clone-sitecore-headless-sxa-renderings-step-by-step-guide), each with its own codebase and hostname, setting up a [local development](https://enlightenwithamit.hashnode.dev/how-",
      "content_full": "## **ðŸ¤”**Introduction to XM Cloud Local Development\n\n[XM Cloud](https://enlightenwithamit.hashnode.dev/how-can-i-get-started-learning-sitecore-xm-cloud) is a powerful platform for managing and delivering personalized digital experiences. However, when working with [multiple websites](https://enlightenwithamit.hashnode.dev/clone-sitecore-headless-sxa-renderings-step-by-step-guide), each with its own codebase and hostname, setting up a [local development](https://enlightenwithamit.hashnode.dev/how-to-use-github-codespaces-for-sitecore-xm-cloud-development) environment can be challenging. This article explains how to configure your environment using PowerShell scripts, Docker commands, and environment-specific variables to ensure seamless transitions between different projects.\n\n## ðŸ‘€Why Set Up a Local Development Environment for Multiple Code Bases?\n\nWhen managing multiple websites in XM Cloud, each site may have unique requirements, such as:\n\n* Different rendering hosts\n    \n* Different code bases\n    \n* Unique API endpoints and credentials\n    \n* Custom environment variables\n    \n\nA well-configured local development environment allows developers to: [**ðŸ”**](#top)\n\n* Test code changes separately\n    \n* Easily switch between different code bases\n    \n* Handle different hostnames\n    \n* Manage unique API endpoints\n    \n* Use specific environment variables for each website\n    \n* Set up a flexible and organized local development environment\n    \n* Maintain consistency across development, staging, and production environments\n    \n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">Learn more about setting up your <strong>local </strong>XM Cloud <strong>development </strong>environment <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://doc.sitecore.com/xmc/en/developers/xm-cloud/set-up-your-local-development-environment.html\" style=\"pointer-events: none\">here</a>.</div>\n</div>\n\n## ðŸ§©Prerequisites for Setting Up XM Cloud Local Environment\n\nBefore you begin, ensure you have the following: [**ðŸ”**](#top)\n\n1. **System Requirements**: Minimum 16GB RAM (32GB recommended), quad-core CPU, and at least 30GB of free disk space\n    \n2. * **Software Tools**:\n        \n        * Docker with Windows Containers enabled.\n            \n        * PowerShell.\n            \n        * Node.js LTS version.\n            \n        * .NET Core SDK and Framework.\n            \n        * A valid Sitecore license file.\n            \n3. **Sitecore XM Cloud** access and necessary API keys\n    \n4. A code repository with your website-specific code bases. [**ðŸ”**](#top)\n    \n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\"><strong>Video Companion:</strong> <strong>Understanding XM Cloud Fundamentals - </strong>Watch our YouTube tutorial to learn about XM Cloud's core concepts. Subscribing helps us make more content like this!</div>\n</div>\n\n%[https://www.youtube.com/watch?v=igOJ21AfDtc] \n\n## **ðŸ§°**Step-by-Step Guide to Setting Up XM Cloud Local Development\n\n### 1\\. Code Base Structure Overview for XM Cloud\n\nSuppose we have a code base structure that uses two different code bases for two separate websites:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742792620587/8aeb1af9-e8c1-4cbe-80ba-b1cb2c9342e8.jpeg align=\"center\")\n\nYou can clone the needed code repository that has the code base for your websites.\n\n### 2\\. Using PowerShell Scripts for Automation\n\nWe will leverage three essential scripts: [**ðŸ”**](#top)\n\n* `init.ps1` - this existing PowerShell script initializes the project with predefined settings.\n    \n* `up.ps1` - this existing PowerShell script starts the required containers and services.\n    \n* `clean-install.ps1` - this new PowerShell script performs a fresh installation.\n    \n\nThe **diagram** below **shows** how our **PowerShell scripts** handle **multiple code bases**:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742884620481/b08f1b20-bdbf-42ca-87bf-3a3f0e29173e.png align=\"center\")\n\n#### i). init.ps1 Script:\n\nThis script **starts** the **Sitecore XM Cloud Full-stack local development environment** by **setting up** necessary **services**, configuring **environment variables**, and making sure all required components are running.\n\nTo **support different code** bases or **settings** for various websites, I have **parameterized** the `.env` files based on the **website** we are setting up for XM Cloud development. These customizations help initialize the required settings for your website. Here are some details about these customizations:\n\n##### **âš¡** Access Environment Variables from a .env File:\n\nI created the custom **GetEnvVariable** function to access the path of the `.env` file and the name of the environment variable you need the value for: [**ðŸ”**](#top)\n\n```powershell\n# Get the environment variable from the .env file\nfunction GetEnvVariable {\n    param(\n        [Parameter(Mandatory = $true, HelpMessage = \"Specifies the path of the .env file.\")]\n        [string]$filePath = \".env\",  # Default path for the .env file\n        [Parameter(Mandatory = $true, HelpMessage = \"Specifies the .env name.\")]\n        [string]$varName  # Name of the environment variable to retrieve\n    )\n\n    Write-Host \"Inside GetEnvVariable\"  # Log entry into the function\n\n    $envFilePath = Resolve-Path \"$PSScriptRoot\\$filePath\"  # Resolve the full path of the .env file\n\n    if (Test-Path $envFilePath) {  # Check if the .env file exists\n        if ($varName -ne \"\" -and $envFilePath -ne \"\") {  # Ensure parameters are not empty\n            Write-Host \"Using .env file: $envFilePath\" -ForegroundColor Cyan  # Log the file being used\n            # Read the contents of the .env file\n            $envFileContent = Get-Content -Path $envFilePath  # Load the file content into an array\n\n            # Iterate through each line to find the variable\n            foreach ($line in $envFileContent) {\n                if ($line -match \"^\\s*$varName\\s*=\\s*(.+)\\s*$\") {  # Check if the line matches the variable name\n                    $varValue = $matches[1].Trim()  # Extract the variable value\n                    Write-Host \"Environment variable '$varName' found with value '$varValue'.\"  # Log success message\n                    return $varValue  # Return the found value\n                }\n            }\n\n            Write-Host \"Environment variable '$varName' not found in the .env file.\" -ForegroundColor Yellow  # Log if not found\n            return $null  # Return null if not found\n        } else {\n            Write-Host \"Invalid parameters\" -ForegroundColor Red  # Log error for invalid parameters\n            return $null  # Return null for invalid parameters\n        }\n    } else {\n        Write-Error \"The .env file does not exist at the specified path: $envFilePath\"  # Log error if file doesn't exist\n        return $null  # Return null if file doesn't exist\n    }\n}\n```\n\nUsing the syntax below, you can call this function:\n\n```powershell\nGetEnvVariable -filePath $EnvFileName -varName \"CM_HOST\"\n```\n\n##### **âš¡** Set Environment Variables in a .env File:\n\nI created the custom **SetEnvVariable** function to set the required environment variable in the `.env` file you provide as a parameter: [**ðŸ”**](#top)\n\n```powershell\n# Set the environment variable in the .env file\nfunction SetEnvVariable {\n    param(\n        [string]$filePath=\".env\",  # Default path for the .env file\n        [string]$varName,          # Name of the environment variable to set\n        [string]$varValue          # Value to assign to the environment variable\n    )\n\n    Write-Host \"Inside SetEnvVariable\"  # Log entry into the function\n\n    $envFilePath = Resolve-Path \"$PSScriptRoot\\$filePath\"  # Resolve the full path of the .env file\n\n    if (Test-Path $envFilePath) {  # Check if the .env file exists\n        if ($varName  -ne \"\" -and  $varValue  -ne \"\" -and $envFilePath  -ne \"\") {  # Ensure parameters are not empty\n            Write-Host \"Using .env file: $envFilePath\" -ForegroundColor Cyan  # Log the file being used\n            # Read the contents of the .env file\n            $envFileContent = Get-Content -Path $envFilePath  # Load the file content into an array\n\n            # Initialize a flag to check if the variable was found\n            $variableFound = $false  # Flag to track if the variable exists\n\n            # Iterate through each line and update the variable if found\n            $updatedContent = $envFileContent | ForEach-Object {\n                if ($_ -match \"^\\s*$varName\\s*=\") {  # Check if the line matches the variable name\n                    $variableFound = $true  # Set flag to true if found\n                    \"$varName=$varValue\"  # Update the line with the new value\n                } else {\n                    $_  # Keep the line unchanged if not matched\n                }\n            }\n\n            # If the variable was not found, add it to the end of the file\n            if (-not $variableFound) {\n                $updatedContent += \"$varName=$varValue\"  # Append the new variable to the content\n            }\n\n            # Write the updated content back to the .env file\n            Set-Content -Path $envFilePath -Value $updatedContent -Force  # Save changes to the file\n\n            Write-Host \"Environment variable '$varName' set to '$varValue' in the .env file.\"  # Log success message\n        } else {\n            Write-Host \"Invalid parameters\" -ForegroundColor Red  # Log error for invalid parameters\n        }\n    }\n    else {\n        Write-Error \"The .env file does not exist at the specified path: $envFilePath\"  # Log error if file doesn't exist\n    }\n}\n```\n\nUsing the syntax below, you can call this function:\n\n```powershell\n SetEnvVariable $EnvFileName \"JSS_DEPLOYMENT_SECRET_xmcloudpreview\" $jsonContent.renderingHosts.xmcloudpreview.jssDeploymentSecret\n```\n\nI have used this function in the init.ps1 script whenever I'm not using the default .env file. This way, I don't disrupt the script's default flow, preventing any errors. [**ðŸ”**](#top)\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">You can access the complete script <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/AmitKumar-AK/Sitecore.Demo.XMCloud.Verticals/blob/203e6944249e4fcf06b2974591eb59179da2b3ee/init.ps1\" style=\"pointer-events: none\">here</a> if you want to try it.</div>\n</div>\n\n#### ii). up.ps1 Script:\n\nThis script **accesses** the needed environment variables from the `.env` file and checks them before running the **docker-compose** commands to ensure the necessary **settings** are **correct**, **preventing** any Sitecore Docker Container **errors**.\n\n##### **âš¡** Validate the Path of a .env File:\n\nI updated the file and passed the .env file as a parameter to this script. Before accessing the .env file, I am checking if the file exists: [**ðŸ”**](#top)\n\n```powershell\n$envFilePath = Resolve-Path \"$PSScriptRoot\\$EnvFileName\"\nif (Test-Path $envFilePath) {\n    Write-Host \"Using .env file: $envFilePath\" -ForegroundColor Cyan\n} else {\n    Write-Error \"The .env file does not exist at the specified path: $envFilePath\"\n    return\n}\n```\n\n##### **âš¡** Improvement added to the up.ps1 script:\n\n**Earlier**, to **access** an **environment variable**, we used a **two-step process**. **First**, we retrieved the entire line from the .env file for the specific variable. **Second**, we split the line to get the variable's value:\n\n```powershell\n# Retrieve the environment variable line from the .env file\n$envContent = Get-Content .env -Encoding UTF8\n$xmCloudHost = $envContent | Where-Object { $_ -imatch \"^CM_HOST=.+\" }\n\n#Split the environment variable line to get the variable's value from the .env file.\n$xmCloudHost = $xmCloudHost.Split(\"=\")[1]\n```\n\nI **combined** these **two steps** into **one** by using the **custom** function `GetEnvVariable`. [**ðŸ”**](#top)\n\n##### **âš¡** Added error handling to the Validate-LicenseExpiry function in the upFunctions.ps1 script:\n\nIn the up.ps1 script, we load custom functions from the upFunctions.ps1 script to check if the Sitecore License is valid, preventing docker container issues early on. The current Validate-LicenseExpiry function does not check:\n\nâœ” If the **HOST\\_LICENSE\\_FOLDER** environment variable is `null`\n\nâœ” If the **HOST\\_LICENSE\\_FOLDER** environment variable already **contains** the `license.xml` string\n\nSometimes, we forget to run the `init.ps1` script or accidentally provide the wrong license file path, which usually results in an error like this: [**ðŸ”**](#top)\n\n```verilog\nlicense.xml file does not exist in the specified folder (C:\\License\\license.xml\\license.xml)\n```\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742796588046/5160c72d-5b4f-4a39-8fd2-8c1737ea1dfb.png align=\"center\")\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ¤</div>\n<div data-node-type=\"callout-text\">To fix this issue and help others in the Sitecore community avoid these errors, I have submitted a PR <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/Sitecore/Sitecore.Demo.XMCloud.Verticals/pull/263\" style=\"pointer-events: none\">here</a>.</div>\n</div>\n\n##### **âš¡** Loading Environment Variables from Your Chosen .env File\n\nThe current up.ps1 script loads environment variables from the default .env file. However, we need it to handle different configurations for various websites, load different code bases, or pass environment-specific variables to Sitecore Container Services.\n\nTo meet these needs, I changed the script to accept a specific .env file: [**ðŸ”**](#top)\n\n```powershell\n# Current method to call the docker-compose command to build the Sitecore Container services\ndocker compose build\n\n# Pass the .env file as a parameter to the docker-compose command to use variables \n# from a specific .env file in a docker-compose.yml file.\n\n# Build all containers in the Sitecore instance, forcing a pull of the latest base containers\ndocker compose --env-file $envFilePath build\n\n# Start the Sitecore instance in detached mode\ndocker compose --env-file $envFilePath up -d\n```\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">Feel free to check out the full script <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/AmitKumar-AK/Sitecore.Demo.XMCloud.Verticals/blob/203e6944249e4fcf06b2974591eb59179da2b3ee/up.ps1\" style=\"pointer-events: none\">here</a> if you're interested in giving it a try.</div>\n</div>\n\n#### ii). clean-install.ps1 Script:\n\nThis script performs a clean installation of the Sitecore XM Cloud Full-stack local development environment.\n\n##### **âš¡** Stops Containers Associated with the Compose Project Name:\n\nIn this script, I will read the environment variable COMPOSE\\_PROJECT\\_NAME from the specific .env file you choose and stop all the related containers: [**ðŸ”**](#top)\n\n```powershell\n# Stops containers matching the project name\ndocker container stop $(docker container ls -q --filter name=$composeProjectName*); \n\n# Stops and removes all containers defined in the docker-compose file\ndocker-compose stop; docker-compose down\n```\n\n##### **âš¡** Prune unused Docker resources:\n\nPruning unused Docker resources frees up disk space by removing stopped containers, unused networks, dangling images, and build cache. Running the `docker system prune` command regularly keeps your Docker environment clean and efficient, avoiding unnecessary use of resources.\n\n```powershell\n# Removes all unused data\ndocker system prune\n\n# Removes images related to the project\ndocker rmi $(docker images --format \"{{.Repository}}:{{.Tag}}\" | findstr $composeProjectName)\n```\n\n##### **âš¡** Optimizing Your Environment- Stopping IIS and Restarting HNS for Sitecore Docker Setup:\n\nBefore starting the Sitecore Docker setup, you should stop IIS and restart the Host Network Service (HNS). This ensures proper networking and prevents port conflicts. Doing this keeps the environment clean and allows Docker containers to work smoothly without interference from other services. [**ðŸ”**](#top)\n\n```powershell\n# Stops IIS and restarts the HNS\niisreset /stop; net stop hns; net start hns\n```\n\n##### **âš¡** Executing the Up Script for XM Cloud Docker Setup:\n\nOnce the environment is optimized, execute the Up Script for XM Cloud Docker Setup to initialize and start the necessary containers and services efficiently.\n\n```powershell\n# Executes the up script with the specified .env file\n.\\up.ps1 -EnvFileName $EnvFileName\n```\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">You're welcome to explore the complete script <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/AmitKumar-AK/Sitecore.Demo.XMCloud.Verticals/blob/203e6944249e4fcf06b2974591eb59179da2b3ee/clean-install.ps1\" style=\"pointer-events: none\">here</a> if you'd like to give it a go.</div>\n</div>\n\n### 3\\. Test and Validate\n\nAfter setup, verify that: [**ðŸ”**](#top)\n\n* You can also try running the script individually to set up your local XM Cloud Development environment:\n    \n    ```powershell\n    # init.ps1:\n    .\\init.ps1 -InitEnv:$true -LicenseXmlPath C:\\License\\license.xml -EnvFileName \"site-two\\.env\"\n    \n    # up.ps1:\n    .\\up.ps1 -EnvFileName \"site-two\\.env\"\n    \n    # CleanInstall.ps1:\n    .\\clean-install.ps1 -EnvFileName \"site-two\\.env\"\n    ```\n    \n* Each website loads correctly with its unique hostname.\n    \n* API endpoints and credentials are functioning as expected.\n    \n* The Pages Editor allows seamless content editing.\n    \n\n%[https://twitter.com/AmitMnath/status/1891611335659565070] \n\n## â‡Benefits of This Setup\n\n1. **Flexibility:** Easily switch between different code bases.\n    \n2. **Isolation:** Test changes on one website without impacting others.\n    \n3. **Efficiency:** Simplify development workflows with automated scripts.\n    \n4. **Consistency:** Keep local, staging, and production environments in sync.\n    \n\n## **ðŸ¤A Special Thanks to the Sitecore Demo Team**\n\nThe code repository I used here is [XM Cloud Vertical](https://github.com/Sitecore/Sitecore.Demo.XMCloud.Verticals), provided by Sitecore. It contains all the scripts and resources needed to implement this setup. Additionally, this repository offers valuable insights into XM Cloud implementation and best practicesâ€”essential for any developer looking to improve their workflow.\n\nThe **updated** `forked` version of the [XM Cloud Vertical](https://github.com/Sitecore/Sitecore.Demo.XMCloud.Verticals), which includes the **new scripts**, can be accessed using the link below. [**ðŸ”**](#top)\n\n%[https://github.com/AmitKumar-AK/Sitecore.Demo.XMCloud.Verticals/tree/feature/PASS-ENV-FILE-AS-PARAMETER] \n\n## ðŸ’¡Conclusion\n\nBy following this guide, you can easily manage multiple code bases in a full-stack XM Cloud local development setup. This method offers flexibility, efficiency, and improved project isolation.\n\n## **ðŸ™Credit/References**\n\n| [Set up your local development environment](https://doc.sitecore.com/xmc/en/developers/xm-cloud/set-up-your-local-development-environment.html) | [Set up your full-stack XM Cloud local development environment](https://doc.sitecore.com/xmc/en/developers/xm-cloud/set-up-your-full-stack-xm-cloud-local-development-environment.html) | [Preparing to run the XM Cloud foundation template locally](https://doc.sitecore.com/xmc/en/developers/xm-cloud/preparing-to-run-the-xm-cloud-foundation-template-locally.html) |\n| --- | --- | --- |\n| [Vertical Demos on XM Cloud](https://github.com/Sitecore/Sitecore.Demo.XMCloud.Verticals) | [Display all environment variables from a running PowerShell script](https://stackoverflow.com/questions/39800481/display-all-environment-variables-from-a-running-powershell-script) | [Pass variables from .env file to dockerfile through docker-compose](https://stackoverflow.com/questions/72092982/pass-variables-from-env-file-to-dockerfile-through-docker-compose) |\n| [docker-compose build args not passing to Dockerfile](https://stackoverflow.com/questions/48831447/docker-compose-build-args-not-passing-to-dockerfile) | [Environment variables in Compose using Powershell with only 1 line](https://stackoverflow.com/questions/47446092/environment-variables-in-compose-using-powershell-with-only-1-line) | [How to pass environment variable to docker-compose up](https://stackoverflow.com/questions/49293967/how-to-pass-environment-variable-to-docker-compose-up) [**ðŸ”**](#top) |\n\n## **ðŸ“Pingback**\n\n| What is an [Editing Host in XM Cloud](https://enlightenwithamit.hashnode.dev/configure-external-editing-host-xm-cloud)? A Beginnerâ€™s Guide: Learn what an editing host is in XM Cloud, its role in enabling WYSIWYG editing, and how it differs from a rendering host. | **Mastering Multi-Codebase Development in the XM Cloud**: Explore strategies for managing multiple code bases within the XM Cloud local development environment, complete with best practices and practical examples. | **Essential PowerShell Scripts for XM Cloud Local Development:** Discover the most useful PowerShell scripts to automate your XM Cloud local development environment setup and management. [**ðŸ”**](#top) |\n| --- | --- | --- |\n| **Docker Commands Every XM Cloud Developer Should Know:** Master these crucial Docker commands to efficiently manage your XM Cloud containers and local development environment. | **How to Manage Multiple .env Files in XM Cloud Projects:** Learn best practices for handling multiple environment variable files when working with different XM Cloud code bases. | **Troubleshooting Common XM Cloud Local Development Issues:** Solve frequent problems developers encounter when setting up XM Cloud local environments with these [expert tips](https://enlightenwithamit.hashnode.dev/troubleshooting-sitecore-xm-cloud-local-setup-and-sitecore-jss-errors). |\n| **Optimizing IIS for XM Cloud Development Environments:** A complete guide to configuring IIS for optimal performance in your XM Cloud local development setup. | **The Complete Guide to Sitecore XM Cloud API Keys:** Understand how to properly generate, manage, and use API keys in your XM Cloud development workflow. | **How to Implement CI/CD for XM Cloud Projects:** Learn how to set up continuous integration and deployment pipelines for your XM Cloud solutions. [**ðŸ”**](#top) |\n| **Using Docker Compose for XM Cloud Development:** A deep dive into using Docker Compose to streamline your XM Cloud local development process. | **Clean Installation Essentials for XM Cloud Developers**: Learn how to ensure clean installations in your XM Cloud local development environment using targeted scripts and commands. | **Docker Tips for XM Cloud Full-Stack Development**: Enhance your XM Cloud workflow with these Docker Compose tips for handling multiple websites and diverse code bases. |\n| Sitecore graphql queries | [Getting Started](https://enlightenwithamit.hashnode.dev/how-can-i-get-started-learning-sitecore-xm-cloud) with Sitecore XM Cloud: A Beginnerâ€™s Guide to Learning Resources | find sitecore version |\n| how does sitecore search work | what is indexing in Sitecore Search? | Sitecore Search API |\n| Sitecore Search API Crawler | Improve Sitecore Search | **Step-by-Step Guide to XM Cloud Docker Environment Setup:** Set up a fully functional XM Cloud environment using Docker and PowerShell scripts for seamless development. |\n| [Mastering Website Content Indexing withâ€‹ Sitecore Search](https://enlightenwithamit.hashnode.dev/content-indexing-with-sitecore-search) | [Troubleshooting Sitecore XM Cloud Local Setup and Sitecore JSS Errors](https://enlightenwithamit.hashnode.dev/troubleshooting-sitecore-xm-cloud-local-setup-and-sitecore-jss-errors) | Deploying Sitecore XM Cloud & JSS Apps to Vercel with Azure DevOps Pipeline: [A Step-by-Step Guide](https://enlightenwithamit.hashnode.dev/deploying-sitecore-xm-cloud-and-jss-apps-to-vercel-with-azure-devops-pipeline-a-step-by-step-guide) [**ðŸ”**](#top) |",
      "stars": null,
      "comments": 2,
      "upvotes": 5,
      "read_time": "13 min read",
      "language": null
    },
    {
      "title_en": "Understanding Build Tools: Maven, Gradle, and npm",
      "url": "https://yashpatilofficial.hashnode.dev/understanding-build-tools-maven-gradle-and-npm-1",
      "source": "hashnode",
      "published_at": "2025-03-25T05:40:28.888000+00:00",
      "external_id": null,
      "tags": [
        "npm",
        "JavaScript",
        "Devops",
        "Build tool",
        "package manager",
        "Java",
        "gradle",
        "maven",
        "Learning Journey",
        "Build In Public"
      ],
      "content_length": 14681,
      "content_preview": "In our [last-blog](https://yashpatilofficial.hashnode.dev/understanding-build-tools-maven-gradle-and-npm) we have already covered what Build Tools are, what are application artifacts, what is Maven and a thorough hands on demo to understand maven lifecycles so do check it out if you dont want to miss out.\n\nBut in this Blog post lets get an overview about what Gradle is? What is it, what is it used forâ€¦.? And also we will take a look at npm(Node Package Manager), is it a build tool? Perhaps not. ",
      "content_full": "In our [last-blog](https://yashpatilofficial.hashnode.dev/understanding-build-tools-maven-gradle-and-npm) we have already covered what Build Tools are, what are application artifacts, what is Maven and a thorough hands on demo to understand maven lifecycles so do check it out if you dont want to miss out.\n\nBut in this Blog post lets get an overview about what Gradle is? What is it, what is it used forâ€¦.? And also we will take a look at npm(Node Package Manager), is it a build tool? Perhaps not. Lets find out, shall we?\n\n## Gradle\n\nGradle is an open-source build automation tool that's primarily used for Java projects but can handle projects in other languages as well. It uses a Groovy-based domain-specific language (DSL) or Kotlin for scripting, which makes it highly flexible and powerful for managing dependencies, tasks, and builds.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742870784835/12d13d19-456d-4046-a2df-f18e2987235d.avif align=\"center\")\n\nSo similarly like Maven, Gradle is also a powerful project management tool which can be used for project management, dependencies resolution and application builds.\n\nBut what makes it different from Maven? Why have two build tools providing the same function right?\n\n**Maven vs. Gradle: Key Differences**\n\n1. **Language and Configuration:**\n    \n    * **Maven:** Utilizes XML for configuration, leading to a standardized but sometimes verbose setup.\n        \n        ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742871116825/2cfdd6b9-580b-42f9-91e1-cd28a22d477e.png align=\"center\")\n        \n    * **Gradle:** Employs a Groovy-based DSL (Domain-Specific Language) or Kotlin for configuration, offering a more concise and expressive syntax.â€‹\n        \n        ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742871193323/ccdfe902-0fbd-4dd2-b3fd-37f3f04f7c92.png align=\"center\")\n        \n        Doesnâ€™t this look much cleaner and flexible to write?\n        \n2. **Flexibility vs. Convention:**\n    \n    * **Maven:** Emphasizes convention over configuration, providing a uniform project structure and build lifecycle, which simplifies onboarding but may limit customization.â€‹\n        \n        ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742871255197/c4b8889c-7c21-46a3-af83-2b3a0954d430.png align=\"center\")\n        \n    * **Gradle:** Offers greater flexibility, allowing extensive customization of build processes, which is beneficial for complex projects requiring tailored build logic.â€‹\n        \n3. **Performance:**\n    \n    * **Maven:** Performs well but may experience longer build times as project complexity increases.â€‹\n        \n    * **Gradle:** Designed for high performance with features like incremental builds and build caching, resulting in faster build times, especially for large projects.â€‹\n        \n    * Thing here to understand is that, this doesnâ€™t mean one is better than the other. Use either one based on project needs.\n        \n4. **Multi-Language Support:**\n    \n    * **Maven:** Primarily focused on Java projects.â€‹\n        \n    * **Gradle:** Supports multiple programming languages, including Java, C++, and Python, making it versatile for polyglot projects.\n        \n\nI guess that give you a good idea how Maven and Gradle are different from one another. But now let us understand what are **Incremental Builds and Caching in Gradle.**\n\n### **1\\. Incremental Builds**\n\nGradle performs **incremental builds** by detecting what has changed since the last build and only executing necessary tasks instead of rebuilding everything from scratch.\n\n#### **How It Works:**\n\n* Gradle tracks **task inputs and outputs** (e.g., source files, dependencies, configurations).\n    \n* If inputs haven't changed, the task is **skipped** because the output is already up to date.\n    \n* If a file changes, Gradle **rebuilds only the affected components**, reducing overall build time.\n    \n\nâœ… **Example:**  \nIf you modify a single Java class in a project, Gradle recompiles **only that class** instead of rebuilding the entire project.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742872079547/9632d663-0cc2-4398-b4c8-50412c47b311.png align=\"center\")\n\n---\n\n### **2\\. Build Cache**\n\nGradle uses a **build cache** to **store outputs of previously executed tasks** and reuse them later, even across different builds or machines.\n\n#### **Types of Caching:**\n\n1. **Local Cache:**\n    \n    * Stores task outputs on the same machine.\n        \n    * If a task runs with the same inputs as a previous execution, Gradle reuses the cached output.\n        \n2. **Remote Cache:**\n    \n    * Enables teams to **share build outputs** across different machines.\n        \n    * Useful in CI/CD pipelines, where different agents can reuse cached results instead of rebuilding.\n        \n\nâœ… **Example:**  \nIf a CI server builds a project once and caches the results, subsequent builds on other agents can fetch the cached outputs instead of rebuilding everything.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742872019751/66b51cb9-1fd3-46d7-8a63-ed2ec1d9408e.png align=\"center\")\n\n**Benefits of Incremental Builds & Caching**\n\nâœ” **Faster builds** â€“ Reduces compilation time by skipping unnecessary work.  \nâœ” **Efficient CI/CD pipelines** â€“ Saves resources by reusing existing build outputs.  \nâœ” **Better scalability** â€“ Works well in large codebases where full rebuilds are expensive.\n\n### Gradle Project Structure\n\nIn a typical Gradle-based Java project, the structure often follows conventions, but it's customizable based on project needs. Hereâ€™s a simplified example structure:\n\n```bash\ncssCopyEditmy-java-project/\nâ”‚\nâ”œâ”€â”€ build.gradle       (Project-wide build script)\nâ”œâ”€â”€ settings.gradle    (Settings for the Gradle project)\nâ”‚\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ main/\nâ”‚   â”‚   â”œâ”€â”€ java/      (Main Java source code)\nâ”‚   â”‚   â””â”€â”€ resources/ (Resources like properties files)\nâ”‚   â”‚\nâ”‚   â””â”€â”€ test/\nâ”‚       â”œâ”€â”€ java/      (Test source code)\nâ”‚       â””â”€â”€ resources/ (Test resources)\nâ”‚\nâ””â”€â”€ build/              (Generated build output)\n```\n\nYou can see how similar the project structure is compared to a Maven project. The only observable difference is the final build out folder (for maven it was the target folder) and the main configuration script build.gradle file (for maven it was the pom.xml file).\n\n### Important Files in Gradle Projects\n\n1. **build.gradle:**\n    \n    * This is the core build script for the entire project.\n        \n    * It defines dependencies, plugins, tasks, and other configurations.\n        \n    * You'll specify things like Java version, plugins (e.g., Java plugin), dependencies (libraries your project relies on), and custom tasks.\n        \n        ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742871193323/ccdfe902-0fbd-4dd2-b3fd-37f3f04f7c92.png align=\"left\")\n        \n2. **settings.gradle:**\n    \n    * This file includes settings for the Gradle project.\n        \n    * It can define which subprojects are part of the build and other global settings.\n        \n\nTo initialize a Gradle project use :\n\n```bash\ngradle init \n```\n\nMake sure Gradle is installed on your system. Executing this command on the CLI will show you a bunch of options such as type of the project, programming language of implementation, type of build script, etc.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742872678533/83dd9fc6-2735-4bc2-babc-008caa9239dd.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742872834790/befa8aed-59a9-4e12-96b3-33ab21fe4aeb.png align=\"center\")\n\nAnd there you go, a Java-Gradle project is has been successfully initialized.\n\n**NOTE** : A very important thing here to note is that both Maven and Gradle use the same Maven central repository for dependencies resolution!\n\n### Gradle Wrapper\n\n**gradlew and gradlew.bat:**\n\n* These are shell scripts (Unix) and batch scripts (Windows), respectively.\n    \n* They bootstrap the Gradle environment for the project without requiring a pre-installed Gradle distribution on each developer's machine.\n    \n\n### Purpose of Gradle Wrapper\n\nThe Gradle Wrapper ensures that everyone working on the project uses the exact version of Gradle specified by the wrapper configuration. This consistency helps avoid issues caused by different Gradle versions across developers' environments. It also simplifies setup for new developers joining the project, as they only need Java installed, and Gradle will be automatically downloaded as needed by the wrapper scripts.\n\nTo build a Gradle project and create the application artifact we use the gradle wrapper command :\n\n```bash\n./gradlew build\n```\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742873280381/a5c08418-7a7a-4f24-a04f-1f1932aa4207.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742873353045/06f3a91b-e4d7-4a0e-a0b9-49ad050c9063.png align=\"center\")\n\nAnd there you go, application artifact is successfully built under ./build/libs folder. The ./build folder also contains folders such as classes, resources, test-results which store compiled classes and executed unit test results in form of a HTML file.\n\nNow this .jar artifact can we used to distribute and run the application on the server similar to Maven!\n\nAnd thatâ€™s all about Gradle you need to know to begin with! Get going on your own Gradle or Maven projects right now!\n\n## npm ( Node Package Manager)\n\nSo we studied build tools for Java based applications, but what if you are developing an application using JavaScript ?\n\nJavaScript Applications do not have special artifact types, they can just be packed into a .zip or a .tar file.\n\n### What is npm?\n\nnpm stands for Node Package Manager. It is primarily a package manager for JavaScript, and it comes bundled with Node.js installation. npm allows developers to discover, share, and reuse packages of code from hundreds of thousands of developers â€” and assemble them in powerful new ways.\n\n### Is npm a Build Tool or a Package Manager?\n\nnpm is primarily a package manager rather than a build tool. Its primary function is to manage dependencies and package distribution rather than transpiling, compressing and building the application artifact.\n\n### Applications of npm\n\nnpm is used primarily for managing dependencies in JavaScript applications, including Node.js applications, front-end web development projects, and even in some cases, in backend services built with Node.js. npm uses the npm central repository to handle installation of dependencies unless specified anything else.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742879320774/4a1242af-ae9c-42b9-bd2b-95d92805ce43.png align=\"center\")\n\n### Handling Dependencies\n\nnpm handles dependencies by defining them in a `package.json` file. This file lists all dependencies (both direct and indirect) required for the project. When you install a package using `npm install`, it reads the `package.json` and installs the required dependencies recursively.\n\nThe package.json file which is written using JSON is equivalent to the pom.xml or build.gradle file.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742879597430/ee9ecee0-57fd-4fd9-8f22-03bd5b1e53f3.png align=\"center\")\n\n### package.json vs package-lock.json\n\n* **package.json**: This file is where the metadata about a project and its dependencies are stored. It includes information such as project name, version, description, entry points, and most importantly, dependencies.\n    \n* **package-lock.json**: Introduced in npm 5, this file is automatically generated for any operations where npm modifies either the `node_modules` tree or `package.json`. It locks down the versions of dependencies to ensure consistent installs across different machines.\n    \n\n`node_modules`folder is created in your project directory having installed all the dependencies mentioned in the `package.json` file once we execute the npm install command.\n\nSo you might be wondering what will be the workflow of the JavaScript applications if npm does not produce a application artifact?\n\nWe can either use `npm pack` command. When you run `npm pack`, it creates a compressed tarball (.tgz file) of your package. This tarball includes the source code, `package.json`, and other necessary files, but it does not include dependencies unless explicitly included in your project. To run the application of the server you must install the dependencies first, unpack this zip or tar file and then run the application using the appropriate script command!\n\nLet me show you an example of how **npm** can be used in containerized environments:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742880090456/d0ab8df4-eff6-4391-8860-5a7ad91c9ab0.png align=\"center\")\n\nSo basically here is a **Dockerfile** I wrote for a frontend microservice of a MERN stack application, similarly as mentioned above what I did is :\n\n* Created a `/app` directory inside the Docker container\n    \n* Copied my `package.json` file inside the container in the /app folder\n    \n* Installed the dependencies first\n    \n* Then copied the source code of the application inside the `/app` folder.\n    \n* And then finally executed the script command to run the application!\n    \n\nAnd well thatâ€™s it, your JavaScript application is running inside your container on the specified port.\n\n### npm Scripts (`npm start`, `npm stop`, `npm test`, `npm publish`)\n\nThese commands are defined in the `scripts` section of `package.json`:\n\n* `npm start`: Typically used to start the application. It is customizable and can run any Node.js command.\n    \n* `npm stop`: Can be used to stop a running application or server.\n    \n* `npm test`: Executes tests defined in the `scripts` section.\n    \n* `npm publish`: Publishes the package to the npm registry, making it available for others to install.\n    \n\nAnd this is just for your additional information but :\n\n### Using webpack to Bundle\n\nwebpack is a module bundler for JavaScript applications. It takes modules with dependencies and generates static assets representing those modules. webpack bundles all your assets, including JavaScript, CSS, images, and fonts, into a smaller number of bundles (usually one) to be served to the browser. It helps in optimizing performance by reducing the number of requests made to the server.\n\n## Conclusion\n\nAlright we have explored build tools such as **Maven** and **Gradle** used to build and manage Java based projects and **npm** which is package management tool used for Java-Script based applications. I think this is more than enough for you to get started on using Build tools and Package Managers!\n\nConnect with me on [LinkedIn](https://www.linkedin.com/in/yash-patil-24112a258/).\n\n##",
      "stars": null,
      "comments": 3,
      "upvotes": 21,
      "read_time": "9 min read",
      "language": null
    },
    {
      "title_en": "Amazon Bedrock: A Practical Guide for Developers and DevOps Engineers",
      "url": "https://microtica.hashnode.dev/amazon-bedrock-a-practical-guide-for-developers-and-devops-engineers",
      "source": "hashnode",
      "published_at": "2025-03-24T14:57:31.466000+00:00",
      "external_id": null,
      "tags": [
        "AWS",
        "aws lambda",
        "Amazon Bedrock",
        "Devops",
        "Beginner Developers",
        "Developer",
        "Devops articles",
        "Kubernetes",
        "AI",
        "JavaScript",
        "Python",
        "React",
        "Docker",
        "APIs",
        "Artificial Intelligence"
      ],
      "content_length": 27393,
      "content_preview": "Once upon a time, building AI applications required deep experience with traditional technologies and some Machine Learning expertise. While that was the norm, developers had to configure models to their needs, provide GPUs, and manually optimize performance, which required a lot of effort and costs.\n\nWhile that approach seemed difficult, the AWS team built Amazon Bedrock, a tool that allows developers to easily create their AI applications through an API or the AWS Management Console using its ",
      "content_full": "Once upon a time, building AI applications required deep experience with traditional technologies and some Machine Learning expertise. While that was the norm, developers had to configure models to their needs, provide GPUs, and manually optimize performance, which required a lot of effort and costs.\n\nWhile that approach seemed difficult, the AWS team built Amazon Bedrock, a tool that allows developers to easily create their AI applications through an API or the AWS Management Console using its embedded foundation models. Amazon Bedrock enables developers to build generative AI applications without the stress of directly managing the underlying stack.\n\n![](https://media1.tenor.com/m/Qs2pAgi0r5kAAAAC/peace-relax.gif align=\"left\")\n\nIn this article, youâ€™ll learn everything about Amazon Bedrock, the prerequisites of using Amazon Bedrock, how to get started with Amazon Bedrock, best practices for working with Amazon Bedrock, and even the core concepts of Amazon Bedrock. Other than that, youâ€™ll also see some code samples of how you can work with AWSâ€™s Bedrock API. So, this article will serve as anÂ **A-Z guide**Â for people who are interested in using Bedrock to build their generative AI applications.\n\n## **Please, support Microtica ðŸ˜… ðŸ™**\n\nBefore moving on, Iâ€™d love it if you could support our work at Microtica joining our community! â­ï¸\n\n[**â­ï¸ Join Microticaâ€™s Discord Community â­ï¸**](https://discord.gg/N8WdXyXxZR)\n\n![](https://media1.tenor.com/m/AEq_MsuoaXsAAAAd/leonardo-dicaprio-thank-you.gif align=\"left\")\n\n## **What is Amazon Bedrock?**\n\nAmazon Bedrock is a service that lets DevOps engineers and teams build gen AI applications. Instead of building or fine-tuning models manually, Amazon Bedrock uses foundational models from AI infrastructure providers to provide a ready-made API for developers to use easily. This approach removes the complexity that comes with building generative applications and working with underlying stacks.\n\n## **Benefits of Amazon Bedrock**\n\nBefore getting in hands with Amazon Bedrock, I just thought it would be nice to share some benefits of Amazon Bedrock and include examples of how these benefits have a positive impact on your development workflow.\n\n* **Quicker Development**: Compared to working manually, where you always have to work with models and fine-tune them, AWS lets you work with a single API. With this approach, you donâ€™t have much to focus on. This approach saves a lot of time as it requires less effort than directly working with models.\n    \n    * **Example**: My colleague made an AI Assistant using Amazon Bedrock's text generation models without having to handle any ML models directly, which saved him time. He found this method quicker because he could add AI features in just a few days with an API call instead of spending months or weeks.\n        \n    \n    %[https://www.youtube.com/watch?v=U1mT6VjArKs&list=PLY0znMeIrUg8SNQrtTDO5DQ1cjV81Zq3M&index=7] \n    \n* **Scalability**: Amazon Bedrock is built on AWSâ€™s Cloud Infrastructure and uses models from various companies, including AI21 Labs, Anthropic, Cohere, DeepSeek, Luma, Meta, Mistral AI, and Stability AI. This enables teams to scale their applications easily and under heavy workloads, Bedrock ensures excellent application performance without requiring manual intervention.\n    \n    * **Example**: An e-commerce service using Amazon Bedrock for product recommendations can easily scale resources during shopping seasons without compromising performance or experiencing downtime.\n        \n* **Integration with AWS Ecosystem**: As Amazon Bedrock is an AWS product, it seamlessly integrates with Amazon SageMaker, Lambda, and S3 for building, deploying, and managing applications.\n    \n    * **Example**: A bank using Amazon Bedrock for fraud detection can create automated workflows. For instance, AWS Lambda can identify suspicious transactions, save the reports in S3, and use SageMaker to check patterns.\n        \n* **Cost-Effectiveness:**Â Amazon Bedrock has flexible pricing, so you only pay for what you use. Instead of spending a lot on expensive servers and models, you can use any of Bedrockâ€™s models that you think will help save costs while still getting powerful AI features. You can take a look at this page for Amazon Bedrock's pricing models.\n    \n    * **Example**: One of my colleagues automated blog posts with Amazon Bedrock and only paid for the API requests she used, saving money on monitoring and fine-tuning AI models.\n        \n\n## **Getting Started with Amazon Bedrock ðŸš€**\n\nNow, itâ€™s time to prepare to get our hands dirty. In this section, we will look into doing the real work and the things you should have before getting started. Even though Amazon Bedrock can be used to accomplish many tasks, this article is focused only on building generative AI applications easily with the AWS Management Console.\n\n### **Prerequisites For Using Amazon Bedrock**\n\nBefore getting started with Amazon Bedrock, here are some things you need to have ready:\n\n* Basic Python Knowledge.\n    \n* **An AWS Account**: This is primary: to get started, you need toÂ [**create an AWS account**](https://aws.amazon.com/?utm_source=DEV&utm_medium=post&utm_campaign=devrel).\n    \n* **AWS Management Console**: You need the console to interact with models if you donâ€™t want to write code. Alternatively, you can use:\n    \n    * **AWS CLI**: You can use the CLI to create AWS CLI profiles using the API. For instructions on how to use this option, refer to this documentation. This option requires some basic Python knowledge.\n        \n* **IAM Permissions**: You also need to assign the required IAM roles needed for Bedrock.\n    \n\n![](https://media1.tenor.com/m/IC_M1d1a0TsAAAAd/zohan-soletsgo.gif align=\"left\")\n\nIn this article, we will be using the AWS Management Console option for operations. You can use the CLI option to go a bit hands-on.\n\n### **Basic AI/ML Concepts**\n\nEven though Amazon Bedrock helps with removing the complexities that come with building AI applications, a basic understanding of AI and ML is helpful because, even when using Amazon Bedrock, there are things you might encounter, such as:\n\n* **Foundational Models (FMs)**: These are the pre-built AI models that Amazon Bedrock provides for operations in generative applications. You might wonder if AWS owns the entire model, but some are from different companies.\n    \n* **Prompt Engineering:**Â This is the process of creating and improving input prompts to help AI models produce accurate and great responses. Good prompt engineering improves the model's understanding and ensures it aligns with the output.\n    \n* **Model fine-tuning**: With Amazon Bedrock, itâ€™s possible to fine-tune models to your needs. You can always configure models to fit what you want; the approach for doing this is different from the manual approach.\n    \n\n## **Core Concepts of Amazon Bedrock**\n\nNow, letâ€™s take a look at some key concepts of Amazon Bedrock - by having a glance at them, youâ€™ll have a deeper understanding of how Amazon Bedrock works and how to get the best out of it\n\n### **Understanding Foundation Models**\n\nLetâ€™s take a look at some foundation models that Amazon Bedrock uses and things to consider before using them.\n\nTo find the list of models you could work with, their capabilities, and their availability in your region,Â [**refer to this guide**](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html?utm_source=DEV&utm_medium=post&utm_campaign=devrel). In that guide, youâ€™ll see everything about the models and the types of outputs they generateâ€”for example, image, text, or code.\n\nHere are some things you should consider before using any of the models:\n\n* **Use Case**: You should know if the model aligns with your applicationâ€™s needs. For example, if youâ€™re building a chatbot that generates text for responses, you need to work with one of the models that support text for their outputs.\n    \n* **Performance vs. Cost**: Now, this is where the saying \"quality over quantity\" really matters. You need to think about two things: performance and cost. Some models work quickly but are usually expensive. If you want a model that fits your budget, you might have to find a balance between how well it performs and how much it costs.\n    \n* **Customization**: Amazon Bedrock lets you adjust models for some uses. Depending on your needs, you might want a model that can be customized to fit your project.\n    \n\n## **Amazon Bedrock API**\n\nNow, let's explore Amazon Bedrock's API and SDK and learn how to use them. First, we'll have a look at the API and learn how to work with the foundational models.\n\n### **Overview**\n\nApart from interacting with the service, Amazon Bedrock API allows you to do the following:\n\n* Work with the foundation models to generate text, create images, generate code\n    \n* Adjust the modelâ€™s behaviour with settings you can change.\n    \n* Get information about the model, including its ARN and ID.\n    \n\nAmazon Bedrock APIs use AWS's normal authentication and authorization methods, and that requires IAM roles and permissions for security.\n\n### **Authentication & Access Control**\n\nTo use the Bedrock API, you need to install theÂ [**latest version of AWS CLI**](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html?utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â and log in with AWS IAM credentials. Make sure your IAM user or role has the required permissions, too.:\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"MarketplaceBedrock\",\n          \"Effect\": \"Allow\",\n          \"Action\": [\n              \"aws-marketplace:ViewSubscriptions\",\n              \"aws-marketplace:Unsubscribe\",\n              \"aws-marketplace:Subscribe\"\n          ],\n          \"Resource\": \"*\"\n      }\n  ]\n}\n```\n\nThis policy allows you to use the API to run models. To follow up with working with Amazon Bedrockâ€™s APIs, you can refer to and read these guides:\n\n* [**Amazon Bedrock API Reference**](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html?utm_source=DEV&utm_medium=post&utm_campaign=devrel): In this documentation, youâ€™ll find the service endpoints youâ€™ll likely work with.\n    \n* [**Getting Started With Amazon Bedrock API**](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html?utm_source=DEV&utm_medium=post&utm_campaign=devrel): This documentation will walk you through everything you need to know about Amazon Bedrockâ€™s APIâ€”from its installation requirements to the How-tos; itâ€™s a more detailed guide for setup.\n    \n\n### **AWS SDK Integration**\n\nAWS provides SDKs to integrate with your favourite programming languages, such as Python, Java, Go, JavaScript, Rust, etc. Now, letâ€™s have a look at some examples of how they work with different languages.\n\n* **Python (Boto3)**:\n    \n\n```python\n    import logging\n    import json\n    import boto3\n\n    from botocore.exceptions import ClientError\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    def list_foundation_models(bedrock_client):\n\n        try:\n            response = bedrock_client.list_foundation_models()\n            models = response[\"modelSummaries\"]\n            logger.info(\"Got %s foundation models.\", len(models))\n            return models\n\n        except ClientError:\n            logger.error(\"Couldn't list foundation models.\")\n            raise\n\n    def main():\n\n        bedrock_client = boto3.client(service_name=\"bedrock\")\n\n        fm_models = list_foundation_models(bedrock_client)\n        for model in fm_models:\n            print(f\"Model: {model['modelName']}\")\n            print(json.dumps(model, indent=2))\n            print(\"---------------------------\\n\")\n\n        logger.info(\"Done.\")\n\n    if __name__ == \"__main__\":\n        main()\n```\n\nThatâ€™s a clear example of how to list the available Amazon Bedrock models with Python (Boto3). To learn more about how to the Python SDK works,Â [**read this guide**](https://docs.aws.amazon.com/code-library/latest/ug/python_3_bedrock_code_examples.html).\n\n* **JavaScript Example (AWS SDK for JavaScript v3)**:\n    \n\n```jsx\nimport { fileURLToPath } from \"node:url\";\n\nimport {\n          BedrockClient,\n          ListFoundationModelsCommand,\n        } from \"@aws-sdk/client-bedrock\";\n\n        const REGION = \"us-east-1\";\n        const client = new BedrockClient({ region: REGION });\n\n        export const main = async () => {\n          const command = new ListFoundationModelsCommand({});\n\n          const response = await client.send(command);\n          const models = response.modelSummaries;\n\n          console.log(\"Listing the available Bedrock foundation models:\");\n\n          for (const model of models) {\n            console.log(\"=\".repeat(42));\n            console.log(` Model: ${model.modelId}`);\n            console.log(\"-\".repeat(42));\n            console.log(` Name: ${model.modelName}`);\n            console.log(` Provider: ${model.providerName}`);\n            console.log(` Model ARN: ${model.modelArn}`);\n            console.log(` Input modalities: ${model.inputModalities}`);\n            console.log(` Output modalities: ${model.outputModalities}`);\n            console.log(` Supported customizations: ${model.customizationsSupported}`);\n            console.log(` Supported inference types: ${model.inferenceTypesSupported}`);\n            console.log(` Lifecycle status: ${model.modelLifecycle.status}`);\n            console.log(`${\"=\".repeat(42)}\\n`);\n          }\n\n          const active = models.filter(\n            (m) => m.modelLifecycle.status === \"ACTIVE\",\n          ).length;\n          const legacy = models.filter(\n            (m) => m.modelLifecycle.status === \"LEGACY\",\n          ).length;\n\n          console.log(\n            `There are ${active} active and ${legacy} legacy foundation models in ${REGION}.`,\n          );\n\n          return response;\n        };\n\n        if (process.argv[1] === fileURLToPath(import.meta.url)) {\n          await main();\n        }\n```\n\nThe code snippet above also shows the listing of the available Bedrock foundation models. To learn more about how to use the JavaScript SDK,Â [**read this guide**](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/javascript_bedrock_code_examples.html?utm_source=DEV&utm_medium=post&utm_campaign=devrel).\n\nThere are code samples that showcase how to integrate Bedrock SDKs into your favorite programming languages. You can find them inÂ [**this guide**](https://docs.aws.amazon.com/code-library/latest/ug/bedrock-runtime_code_examples.html?utm_source=DEV&utm_medium=post&utm_campaign=devrel).\n\n### **Understanding Amazon Bedrock API Responses**\n\nNow, lets have a look at the main API operations that Amazon Bedrock provides for model prediction:\n\n* **InvokeModel**Â â€“ Sends one prompt and gets a response.\n    \n* **Converse**Â â€“ Allows ongoing conversations by including previous messages.\n    \n\nAdditionally, Amazon Bedrock supports streaming responses withÂ `InvokeModelWithResponseStream`Â andÂ `ConverseStream`.\n\nTo see the type of responses youâ€™ll get when you submit a single prompt withÂ `InvokeModel`Â andÂ `Converse`, check the following guides:\n\n* [**Converse API**](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html?utm_source=DEV&utm_medium=post&utm_campaign=devrel): This guide showcases how to use Amazon Bedrock using the Converse API. It also includes how you can make a request with anÂ [**Amazon Bedrock runtime endpoint**](https://docs.aws.amazon.com/general/latest/gr/bedrock.html#br-rt)Â and examples of the response youâ€™ll get with eitherÂ `Converse`Â orÂ `ConverseStream`.\n    \n* [**InvokeModel**](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html):Â This guide explains how to use theÂ `InvokeModel`Â operation in Amazon Bedrock. It also covers how to send requests to foundation models, set parameters for the best results, and manage responses.\n    \n\n## **Building A Conversational AI Application With Amazon Bedrock**\n\nNow, let's start building our first application in Amazon Bedrock using the AWS Management Console. For this first project, we'll create a conversational AI assistant that works only with text.\n\n### **Step 1: Get started with Amazon Bedrock in the AWS Management Console**\n\nFirstly, sign into the AWS Management Console from the main AWS sign-in URL. When, youâ€™re signed in, youâ€™ll be redirected to the Dashboard. In the dashboard, select theÂ **Amazon Bedrock**Â option (search for \"Bedrock\" in the AWS search bar).\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742323299564/3fc56342-d91b-4732-b73a-284c2b7079aa.png?auto=compress,format&format=webp align=\"left\")\n\nAfter selecting Amazon Bedrock, head over to theÂ **Model Access**Â tab and ensure you have access to anyÂ **Amazon Titan**Â text generation models by requesting access.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742376279587/9754d9ad-ef1f-431d-962a-122d25d1f44c.png?auto=compress,format&format=webp align=\"left\")\n\nAfter selecting the model you'd like to work with, hit theÂ **Next**Â button. Afterwards, you'll be redirected to a tab where you should submit a request to access the model.\n\n### **Step 2: Building the Chatbot Using Amazon Titan**\n\nHead over to theÂ **Playgrounds**Â section in the side navigation and select theÂ **Chat / Text**Â section. Enter a prompt in the playground. Click theÂ **Run**Â button to generate a response from Titanâ€™s text model.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742378364549/99f73b11-9ecc-479e-b5dc-7f7b94c7b06c.png?auto=compress,format&format=webp align=\"left\")\n\n### **Step 3: Deploy the Chatbot with AWS Lambda**\n\nNow, letâ€™s deploy the chatbot withÂ [**AWS Lambda**](https://aws.amazon.com/lambda/?utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â as a serverless application! First, we need to create an AWS Lambda Function. Here are some steps to follow to create an AWS Lambda Function:\n\n* Navigate to AWS Lambda and Create a Function.\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742383403341/530cfb7d-c5d2-46e8-962a-1b2ad0101b2b.png?auto=compress,format&format=webp align=\"left\")\n\n* Select theÂ **Author from Scratch**Â tab and make deployment configurations. Note that the runtime should beÂ **Python 3.10**.\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742383867620/9a9dd2a6-bcca-4ed2-8c35-0de846c83a4f.png?auto=compress,format&format=webp align=\"left\")\n\n* Create a role with full access to theÂ **Bedrock and CloudWatch permissions**.\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742390396766/b0aa55ad-2946-40d3-8349-b99e2b606613.png?auto=compress,format&format=webp align=\"left\")\n\n* Create function! ðŸš€\n    \n\nAdd some code to the Lambda function and hit theÂ **Deploy**Â button.\n\n```python\nimport json\nimport boto3\n\nbedrock = boto3.client('bedrock-runtime')\n\ndef lambda_handler(event, context):\n    user_input = event['queryStringParameters']['message']\n\n    response = bedrock.invoke_model(\n        body=json.dumps({\n            \"prompt\": user_input,\n            \"maxTokens\": 200\n        }),\n        modelId=\"amazon.titan-text-lite-v1\"\n    )\n\n    model_output = json.loads(response['body'].read().decode('utf-8'))\n\n    return {\n        \"statusCode\": 200,\n        \"headers\": {\"Content-Type\": \"application/json\"},\n        \"body\": json.dumps({\"response\": model_output[\"completions\"][0][\"data\"][\"text\"]})\n    }\n```\n\n### **Step 4: Deploy the API with API Gateway**\n\n* Under theÂ **Function Overview**, click theÂ **Add Trigger**Â button and select theÂ **API gateway**Â option.\n    \n* Create HTTP API and configure the security method for your API endpoint.\n    \n* Deploy the API! ðŸ¤˜\n    \n    ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742470217694/dcdb3ad8-6250-452e-870a-940950ad4a63.png?auto=compress,format&format=webp align=\"left\")\n    \n    * Note down yourÂ **Invoke URL**Â to interact with the chatbot! ðŸš€\n        \n        ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742471212653/98f4d8eb-982e-4dfa-b067-2be32c603330.png?auto=compress,format&format=webp align=\"left\")\n        \n\nFinally, you can interact with your APIâ€™s endpoint and build with it. ðŸ˜Ž\n\n## **Building a Code Generation Tool Using Amazon Bedrock and Anthropic Bedrock**\n\nNow, letâ€™s build something more fun and technical. In this section, we will be building a code generation tool using Amazon Bedrock and Anthropic Claudeâ€™s 2.0 Model (a model that generates code as its form of response). Donâ€™t fret, weâ€™ll still be working with the AWS Management Console but a basic Python knowledge is required for this use case.\n\n### **Step 1: Navigate to Bedrock and Select the Anthropic Claude 2.0 Model**\n\nJust like we did in the chatbot use case, access Amazon Bedrock in the AWS Management Console. Go to theÂ **Chat/Text**Â section under theÂ **Playground**Â section.\n\nUnder theÂ **Select Model**Â dropdown, select the Anthropic Claude 2.0 Model. Once done, you can now enter a code-related prompt in the chat.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1742478041760/4c3d60f5-d86d-4aae-8aa1-6ca39b821b37.png?auto=compress,format&format=webp align=\"left\")\n\nItâ€™s a super great model to work with as it doesnâ€™t just generate code; it also explains what the code does and how it works. Itâ€™s a super fast and effective model to work with!\n\n### **Deploy the Code Generation Use-Case With AWS Lambda**\n\nAlso just as we did in the first use-case, we will also deploy code generation using AWS Lambda.\n\n* Create a New Lambda Function\n    \n\nAdd some Python code to Lambda Function (Runtime: 3.9):\n\n```python\nimport json\nimport boto3\n\nbedrock = boto3.client('bedrock-runtime')\n\ndef lambda_handler(event, context):\n\n  prompt = event['queryStringParameters']['prompt']\n\n  response = bedrock.invoke_model(\n    body=json.dumps({\n      \"prompt\": f\"{prompt}\",\n      \"max_length\": 300,\n      \"temperature\": 0.7,\n      \"k\": 0,\n      \"p\": 0.75,\n      \"frequency_penalty\": 0,\n      \"presence_penalty\": 0\n    }),\n    modelId=\"arn:aws:bedrock::account:model/claude-v2-20221215\"\n  )\n\n  output = json.loads(response['body'].read().decode('utf-8'))\n\n  return {\n    \"statusCode\": 200,\n    \"headers\": {\"Content-Type\": \"application/json\"},\n    \"body\": json.dumps({\"code\": output[0][\"generated_text\"]})\n  }\n```\n\n### **Step 3: Deploy an API for Code Generation**\n\n* Go toÂ **API Gateway**Â and select theÂ **HTTP API**Â option.\n    \n* Integrate it with the code generator.\n    \n* Deploy the API and get theÂ **Invoke URL**Â for interactions.\n    \n\n## **Best Practices For Working With Amazon Bedrock**\n\nWhen working withÂ **Amazon Bedrock**Â you need pay attention to security, cost, and performance. By following these best practices, you can make sure your AI applications are secure, efficient, and cost-effective.\n\n### **1\\. Data Security and Privacy**\n\nIdeally, you want to keep your data private because AI models often handle sensitive user data, so security is very important in this case. To protect data when using AWS Bedrock, here some practices to follow:\n\n* **Use IAM Roles and Policies:**Â Follow theÂ [**least privilege principle**](https://community.aws/content/2dsQs3aTnwV3LKeUDFkXNSndHjp/understanding-the-principle-of-least-privilege-in-aws?lang=en&utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â to limit access to Bedrock APIs and data storage. This means only giving people the permissions they need and nothing more.\n    \n* **Encrypt Data:**Â You useÂ **AWS Key Management Service (KMS)**Â to protect sensitive data both when it's stored and when it's being sent.\n    \n* **Monitor and Audit Access:**Â EnableÂ **CloudWatch**Â andÂ **AWS Config**Â to keep track of who accesses AI models, data, and logs; and how theyâ€™re being accessed.\n    \n* **Mask Data:**Â Before sending data to Bedrock, remove any personally identifiable information to reduce the risk.\n    \n\n### **2\\. Cost Optimization (Managing Bedrock Usage and Expenses) ðŸ’¸**\n\nAWS Bedrock uses aÂ **pay-per-use**Â pricing model, so it's important to manage costs well. You get billed based on what you use. Here's how can optimize cost using AWS Bedrock:\n\n* **Choose the Right Foundation Model:**Â Different models cost different amounts; select the one that best fits your needs and budget.\n    \n* **Optimize API Calls:**Â Cut down on unnecessary API requests by using caching and batching when you can.\n    \n* **Monitor Usage:**Â UseÂ [**AWS Cost Explorer**](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/?utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â andÂ [**AWS Budgets**](https://aws.amazon.com/aws-cost-management/aws-budgets/?utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â to track your spending and set up alerts for any unexpected cost increases.\n    \n* **Use Auto Scaling:**Â When using Bedrock with AWS Lambda, adjust the number of requests to reduce unnecessary API calls.\n    \n\n### **3\\. Bias and Fairness**\n\nAI models can pick up biases based on the data they are trained on, which can cause problems. To make sure things are fair:\n\n* **Check Model Responses:**Â Regularly test the model's outputs with prompts to identify any biases or errors.\n    \n* **Use Diverse Data for Fine-Tuning:**Â When adjusting models, make sure the data includes various groups and viewpoints.\n    \n\n### **4\\. Performance Tuning**\n\nTo enhance response times and overall performance, follow these practices:\n\n* **Tune API Parameters:**Â Adjust settings likeÂ `temperature`Â andÂ `maxTokens`Â to get the best results.\n    \n* **Use an GPU-Optimized Infrastructure:**Â If you are deploying custom models, useÂ [**AWS Inferentia**](https://aws.amazon.com/ai/machine-learning/inferentia/?utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â to boost performance.\n    \n* **Load Balance Requests:**Â If you have a lot of traffic, useÂ [**AWS Application Load Balancer**](https://aws.amazon.com/elasticloadbalancing/application-load-balancer/?utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â to distribute requests more efficiently.\n    \n* **Reduce Latency:**Â Place applications closer to users withÂ [**AWS Global Accelerator**](https://aws.amazon.com/global-accelerator/?utm_source=DEV&utm_medium=post&utm_campaign=devrel)Â or AWS edge services.\n    \n\n## **Conclusion**\n\nAWS Bedrock makes it easier to integrate AI by offering scalable foundation models from Amazon and other infrastructures without the hassle of training models and managing infrastructure. To get the best results, developers should focus more on security, cost-effectiveness, and performance improvements without working manually.\n\nTo keep exploring AWS Bedrock, developers should try out different models, adjust outputs, and connect with other AWS services. Keeping up with Amazon Bedrockâ€™s guides, blogs and other resources will help make the most of Bedrock and encourage new ideas in AI-powered applications.\n\n### **Before you goâ€¦ ðŸ¥¹**\n\nThank you for taking the time to learn about building AI applications with AWS Bedrock. If you found this article helpful, please consider supporting Microtica by creating an account andÂ [**joining the community**](https://discord.gg/N8WdXyXxZR). Your support helps us keep improving and offering valuable resources for the developer community!\n\n[**Join Microtica for free! ðŸš€**](http://app.microtica.com/?utm_source=DEV&utm_medium=post&utm_campaign=devrel)\n\n![](https://media1.tenor.com/m/GNa8VEckPXQAAAAC/minions-despicable-me.gif align=\"left\")",
      "stars": null,
      "comments": 2,
      "upvotes": 34,
      "read_time": "15 min read",
      "language": null
    },
    {
      "title_en": "Depth Over Trend",
      "url": "https://blog.idrisolubisi.com/depth-over-trend",
      "source": "hashnode",
      "published_at": "2025-03-23T10:00:30.150000+00:00",
      "external_id": null,
      "tags": [
        "Developer",
        "Beginner Developers",
        "Founder",
        "software development"
      ],
      "content_length": 8026,
      "content_preview": "> ***This isn't a typical blog I write, but I'm going to share some raw thoughts along with rhetorical questions that you might find useful and helpful.***\n\nIt's interesting to see how fast the ecosystem moves today, which has made me think a lot over the past few months about what comes next. Yes, what comes after accomplishments?\n\nWhat comes after reaching the goal you set a year or two ago?  \nWhat comes after overcoming your biggest challenge, whether in skills or leadership??\n\n[![Depth not t",
      "content_full": "> ***This isn't a typical blog I write, but I'm going to share some raw thoughts along with rhetorical questions that you might find useful and helpful.***\n\nIt's interesting to see how fast the ecosystem moves today, which has made me think a lot over the past few months about what comes next. Yes, what comes after accomplishments?\n\nWhat comes after reaching the goal you set a year or two ago?  \nWhat comes after overcoming your biggest challenge, whether in skills or leadership??\n\n[![Depth not trend](https://cdn.hashnode.com/res/hashnode/image/upload/v1742049833847/8df7a55c-bd98-4142-bcc6-07c5be093e6f.png align=\"center\")](https://x.com/olanetsoft/status/1900537308798919076)\n\nHave you ever thought about the different phases of the internet we've experienced over the years and wondered what the developers from those times are doing today? Let me take you back a bit. You might know about our various internet phases: from [1995 to 2000 (Web1), 2000 to 2020 (Web2, which is still active), and now Web3](https://www.investopedia.com/web-20-web-30-5208698) and Web5 up to the present.\n\nYes, Web5. If this is your first time hearing about [Web5, you can learn about it with this breakdown from Adewale Abatti (Acekyd)](https://youtu.be/Vw3i2LeVa_Y?t=128).\n\nIt's fascinating to see how things have changed over time. But can you guess what the different eras have in common? Please take a moment to think about it.\n\n> The tech skill!\n\nYes, tech skills are transferable skills you can own, nurture, and improve without worrying about the next iteration of the internet or trends. Before I stray too far, let me bring you back to why you're reading this: it's about valuing depth over trends or, simply put, Depth over Trend. However, you prefer to call it.\n\nLet's talk about what's important, what you need, and why you need it. I will also share what I have learned from the past months of thinking about what truly matters, using the history I mentioned earlier. As you all know, data donâ€™t lie.\n\n## Depth vs Trend\n\n![Free Artistic 3D spiral structure in green hues against a solid black background, showcasing modern abstract design. Stock Photo](https://images.pexels.com/photos/28795077/pexels-photo-28795077/free-photo-of-abstract-green-spiral-design-on-black-background.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=500 align=\"center\")\n\nTo explain this concept the way I think about it, and how I believe everyone reading this should subconsciously think about it, is to use the idea of trade.\n\n\"Depth\" refers to the volume and liquidity at different **price levels** in the order book, while \"Trend\" refers to the overall direction of **price movement over time.** Think of it this way: trends are what everyone can see. They're the apparent upward or downward movements that catch attention. People chase trends because they're visible and exciting - like jumping on the AI bandwagon or whateverâ€™s hot on Twitter this week.\n\nBut depth? That's where the real value sits. Itâ€™s having deep knowledge or capability that persists regardless of market conditions. **Trends come and go, but depth gives you staying power.**\n\nWhat makes them similar is that both can lead to success. You can make quick wins following trends like day traders can profit from momentum. But the difference is **sustainability**.\n\nDepth wins in the long run because it builds compounding value. While trend-chasers constantly restart with each new wave, depth-focused players build foundations that strengthen over time. When the trend inevitably shifts, those without depth get washed out, while those with substantive knowledge or skills remain valuable.\n\nAs *Warren Buffett says - when the tide goes out, you discover whoâ€™s been swimming naked.* Depth is your protection when trends reverse.\n\n## Why should you care?\n\nEven if you think all that matters is money, who doesn't like money? Understanding the difference between depth and trends isnâ€™t just academic. It directly impacts where you invest your limited **time and energy**.\n\nChasing trends without depth is like buying into market momentum without understanding fundamentals. Sure, you might catch some upside, but you're essentially gambling. When the music stops, you're left without a chair.\n\nTrends often shine a lot, and if you aren't careful as a builder, founder, or developer, it's very easy to get carried away with them. But then, I know what you might be thinking: â€ Everyone is doing itâ€, â€ Itâ€™s the next big thing,â€ â€œI donâ€™t want to miss out,â€ and all that. But in reality, there are different phases of life, the same as tech and the tech ecosystem in general.\n\nDepth, on the other hand, is **compound interest** for your career and business. Each layer of deep understanding builds on previous layers, creating compounding returns that trends can't can't.\n\nTake developers who studied fundamental programming concepts versus those who just learned the trending framework of the month. When React inevitably gives way to the next big thing, guess who adapts faster? The ones with depth.\n\nOr consider founders who deeply understand customer problems versus those chasing funding trends. When venture capital dried up in 2022-2023, companies with genuine market depth survived while trend-chasers ran out of runway.\n\nThe market eventually rewards depth not because it's virtuous but because it's viable. And in an increasingly noisy world where everyone chases the same trends, depth is your unfair advantage.\n\n> Every person who created something different focused on depth, which often comes from a deep work stateâ€”a flow state of continuous, focused work. With trends, you stay at the top. It only requires shallow work to produce something of substance. - Blessing Adesiji\n\nWhat can you do differently?\n\nStop treating tech like a fashion show. Take a breath instead of **reflexively jumping on every new framework, language, or platform that trends on X(Formerly Twitter)**.\n\nAsk yourself: \"Does it solve a fundamental problem, or is it just shiny?\"\n\nIt doesnâ€™t mean you should isolate yourself and not keep abreast of what's going on around you.\n\nDevelop your Bulls\\*\\*\\*t detector. When everyone's hypineveryone's thing, that's your cue that's skeptical. The real opportunities often lie in areas others overlook because they're too busy following the crowd.\n\nAllocate your learning time like an investment portfolio:\n\n* **70% toward deepening your understanding of fundamentals that won't change.**\n    \n* **20% tailoring adjacent skills that complement your core.**\n    \n* **10% experimenting with trends.**\n    \n\nBuild relationships with people who prioritize depth. You become the average of the five people you spend the most time with, so surround yourself with depth-seekers who ask hard questions rather than trend-followers who chase easy answers.\n\nPerhaps most importantly, be willing to be temporarily uncool. The paradox is that ignoring what's trendy positions you to catch more immense, meaningful waves later.\n\n## Self Reflection\n\nTake a moment to audit your recent decisions. How many were made because \"everyone is doing it\" versus a â€deep understanding of underlying valueâ€?\n\nLook at your skill set critically. If the tech landscape dramatically changed tomorrow, which of your abilities would remain valuable? Those are your depth areas.\n\nConsider your heroes in tech. Are they known for riding trends or creating fundamental value regardless of market conditions? What can you learn from their approaches?\n\nAsk yourself the uncomfortable question: \"**Am I building something that will matter in five years?**\" If not, you might be optimizing for trends over depth.\n\nFinally, depth isn't just technical knowledge; it's understanding people, problems, and principles. The most profound insights often come from connecting dots across disciplines rather than narrowing down to what's happening.\n\nThe choice between depth and trend is not a career strategy. It's a philosophy about how you want to contribute to the world. Choose wisely.\n\nOlanetsoft âœï¸",
      "stars": null,
      "comments": 7,
      "upvotes": 89,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "Preventing Denial of Service Attacks in Smart Contracts: Implementing the Withdrawal Pattern",
      "url": "https://favourajaye.hashnode.dev/preventing-denial-of-service-attacks-in-smart-contracts-implementing-the-withdrawal-pattern",
      "source": "hashnode",
      "published_at": "2025-03-21T14:46:17.695000+00:00",
      "external_id": null,
      "tags": [
        "withdrawal pattern  ",
        "denial of service attack",
        "smart contract security audit",
        "pull vs push",
        "Solidity",
        "Denial of Service"
      ],
      "content_length": 10417,
      "content_preview": "There is a hidden danger that can shut down a smart contract.\n\nPicture yourself trying to withdraw money from your bank, but nothing happens every time you press the button. The bank isnâ€™t out of moneyâ€”it just refuses to process your transaction. Now imagine this happening to **thousands of people at the same time.**\n\nThis is what a **denial-of-service (DoS) attack** looks like in the blockchain world. Instead of hacking the system, attackers **exploit weaknesses in the code to block** others fr",
      "content_full": "There is a hidden danger that can shut down a smart contract.\n\nPicture yourself trying to withdraw money from your bank, but nothing happens every time you press the button. The bank isnâ€™t out of moneyâ€”it just refuses to process your transaction. Now imagine this happening to **thousands of people at the same time.**\n\nThis is what a **denial-of-service (DoS) attack** looks like in the blockchain world. Instead of hacking the system, attackers **exploit weaknesses in the code to block** others from using a smart contract. The contract is still there, but itâ€™s as good as useless for many users.\n\nIn simple terms, a DoS attack is like **blocking the only exit in a buildingâ€”people** are stuck inside, even though the door is right there.\n\nThese attacks can freeze funds, disrupt entire projects, and cause massive losses.\n\n## **How Can Withdrawals Be Blocked in Smart Contracts?**\n\nToday, we will discuss how users can get stuck in smart contracts and the hidden danger of withdrawals.\n\nIf you find yourself in a situation where the vending machine that gives out snacks was jammed by someone intentionally and the machine can no longer give anyone a snack, what would you do? Thatâ€™s exactly what happens with **withdrawal functions in smart contractsâ€”they** can be **jammed** or **blocked**, stopping people from getting their money.\n\n### **Why are withdrawal functions vulnerable?**\n\nWithdrawal functions in smart contracts are particularly vulnerable because they handle money being sent out of the contract.\n\nWhen a withdrawal happens, the contract needs to send funds to someoneâ€™s address. This means:\n\n1. The contract must interact with external addresses (EOAs or other contracts)\n    \n2. The contract has to transfer actual value (cryptocurrency)\n    \n3. The receiving address might do unexpected things when it gets money\n    \n\nNow, one broken withdrawal can block everyone. Some contracts process all withdrawals in a single transaction. If one personâ€™s withdrawal **fails**, the entire transaction **reverts**, blocking everyone else. Attackers exploit this by **deliberately making their withdrawal fail**, locking up funds for everyone.\n\nThere could also be gas limit problems. If a contract tries to send money to multiple people in a single transaction, the gas cost can become too high.\n\n## **The Problem with Push-Based Payments**\n\nSome smart contracts implement a â€œpushâ€ payment model where the contract directly sends funds to recipients. While this approach seems straightforward, it introduces a critical vulnerability: the contractâ€™s ability to function becomes dependent on the recipientâ€™s ability to receive funds.\n\nLetâ€™s examine a real-world vulnerability found in the [Putty Finance protocol](https://solodit.cyfrin.io/issues/m-06-denial-of-service-contract-owner-could-block-users-from-withdrawing-their-strike-code4rena-putty-putty-contest-git?source=post_page-----bde2eb557231---------------------------------------) and explain how the â€œwithdrawal patternâ€ can prevent such issues.\n\nIn Putty Financeâ€™s case, when users withdrew their funds, the contract first attempted to send a fee to the contract owner before sending the remaining funds to the user:\n\n```solidity\n// send the fee to the admin/DAO if fee is greater than 0%\nuint256 feeAmount = 0;\nif (fee > 0) {\n    feeAmount = (order.strike * fee) / 1000;\n    ERC20(order.baseAsset).safeTransfer(owner(), feeAmount);\n}\n\nERC20(order.baseAsset).safeTransfer(msg.sender, order.strike - feeAmount);\n```\n\nThis design created two potential DoS scenarios:\n\n1. **The Zero Address Problem**: If the contract owner is set to the zero address (0x0), many ERC20 tokens will revert the transfer, causing the entire withdrawal function to fail. This will permanently lock usersâ€™ funds in the contract.\n    \n2. **Malicious Token Hooks**: ERC777 tokens or similar standards that implement receiver hooks could be used by a malicious contract owner to implement logic that rejects incoming transfers, making it impossible for users to withdraw their funds.\n    \n\nLetâ€™s see what the address zero and the ERC777 are.\n\nThe **zero address** (`0x0000000000000000000000000000000000000000`) is a special Ethereum address that no one controls. It is commonly used as a placeholder to indicate an uninitialized or non-existent address in smart contracts. Developers use it for various purposes, such as checking if an ownership variable has been set, burning tokens by sending them to an inaccessible location, or preventing accidental transfers. Since no private key exists for the zero address, any funds or tokens sent to it are permanently lost.\n\nERC777 is an advanced Ethereum token standard that improves upon ERC20 by introducing a built-in *hook mechanism* that enables smart contracts and external accounts to react when they receive tokens. It maintains backward compatibility with ERC20, meaning it can work with existing ERC20 applications. A key feature is the `tokensReceived` hook, which allows recipients (especially smart contracts) to execute custom logic upon receiving tokens, enabling automation and security enhancements. ERC777 also supports *operator functionality*, allowing approved addresses to send tokens on behalf of others, improving usability for wallets and DeFi applications.\n\nToken hooks are callback functions that execute automatically when tokens are transferred. The most notable example is the ERC777 token standard, which introduced hooks as an improvement over the basic ERC20 standard.\n\nWhen an ERC777 token is transferred, it calls the `tokensReceived` hook on the recipient's address if the recipient is a contract. This happens before the transfer completes, allowing the recipient to react to incoming tokens.\n\nFrom the code above, if the `baseAsset` is an ERC777 token and the `owner()` is a contract, the owner could implement a malicious `tokensReceived` hook that deliberately reverts:\n\n```solidity\n// Malicious owner contract\n\ncontract MaliciousOwner {\n    function tokensReceived(\n        address operator,\n        address from,\n        address to,\n        uint256 amount,\n        bytes calldata userData,\n        bytes calldata operatorData\n    ) external {\n        // Deliberately revert when receiving tokens\n        revert(\"Denied\");\n    }\n}\n```\n\nSince the owner fee transfer happens before the user receives their funds, this malicious reversion blocks ALL user withdrawals, effectively holding user funds hostage.\n\n## **The Withdrawal Pattern Solution**\n\nThe â€œwithdrawal patternâ€ is a best practice in smart contract design that separates the tracking of balances from the actual transfer of funds. It eliminates this vulnerability by separating the accounting from the transfer.\n\n### **How It Works**\n\n1. Instead of immediately transferring funds to recipients, the contract tracks owed balances internally\n    \n2. Recipients must explicitly call a separate function to withdraw their funds\n    \n3. Each recipientâ€™s withdrawal is isolated from others, preventing chain-wide DoS attacks\n    \n\n## **Implementation for Putty Finance**\n\nThe recommended solution completely separates the fee collection from the userâ€™s withdrawal:\n\n```solidity\nmapping(address => uint256) public ownerFees;\n\nfunction withdraw(Order memory order) public {\n    // ... existing code ...\n    \n    // transfer strike to owner if put is expired or call is exercised\n    if ((order.isCall && isExercised) || (!order.isCall && !isExercised)) {\n        // Calculate fee but don't transfer it yet\n        uint256 feeAmount = 0;\n        if (fee > 0) {\n            feeAmount = (order.strike * fee) / 1000;\n            ownerFees[order.baseAsset] += feeAmount;\n        }\n\n        // Transfer user's portion immediately\n        ERC20(order.baseAsset).safeTransfer(msg.sender, order.strike - feeAmount);\n        return;\n    }\n    // ... rest of function ...\n}\n\n// Separate function for owner to collect fees\nfunction withdrawFee(address baseAsset) public onlyOwner {\n    uint256 _feeAmount = ownerFees[baseAsset];\n    ownerFees[baseAsset] = 0;\n    ERC20(baseAsset).safeTransfer(owner(), _feeAmount);\n}j\n```\n\nWith this design, users can still successfully withdraw their funds even if the ownerâ€™s fee withdrawal fails for any reason.\n\n## **Benefits of the Withdrawal Pattern**\n\n1. **Isolation of Risks**: Each partyâ€™s withdrawal is independent, preventing one failure from affecting others\n    \n2. **Reduced Attack Surface**: Eliminates vulnerabilities related to recipient behaviour\n    \n3. **Better Gas Efficiency**: Allows recipients to choose when to withdraw, potentially saving on gas costs\n    \n4. **Clearer Accounting**: This makes it easier to track who is owed what\n    \n5. **Enhanced User Trust**: Users know they can always withdraw their funds regardless of protocol owner actions\n    \n\n## **Implementing the Withdrawal Pattern in Your Contracts**\n\nTo implement the withdrawal pattern in your own smart contracts:\n\n1. Create a mapping to track balances owed to each address\n    \n2. When a payment would normally be made, update the balance in the mapping instead\n    \n3. Implement a separate withdrawal function that users call to claim their funds\n    \n4. Always follow the â€œchecks-effects-interactionsâ€ pattern when implementing withdrawals to prevent reentrancy attacks\n    \n\n%[https://gist.github.com/TehilaFavourite/d22cbc1f9bd295693ebd60cf55a040b8#file-pullvspush-md] \n\nThe withdrawal pattern is a fundamental security best practice for smart contract developers. By separating balance tracking from fund transfers, it prevents numerous DoS attack vectors and ensures that users maintain control over their assets.\n\nAs demonstrated in the Putty Finance example, even well-intentioned contract designs can contain vulnerabilities when they directly transfer funds to recipients during multi-step operations. By adopting the withdrawal pattern, you can build more resilient protocols that inspire user confidence and protect against potential DoS attacks.\n\n**Remember:** *design patterns matter in blockchain security. The withdrawal pattern isnâ€™t just a coding preferenceâ€”itâ€™s an essential security measure for any contract that handles value transfers.*\n\nAlways stay safe out there. It does not matter if you are a developer or a user. You should always ask yourself this question: **Is the withdrawal pattern followed to prevent denial of service?**\n\nI hope you enjoyed this article. I will see you in the next one.",
      "stars": null,
      "comments": 1,
      "upvotes": 52,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "How to Use the CUDO Compute CLI for Virtual Machine Lifecycle Management",
      "url": "https://yomaokobiah.com/how-to-use-the-cudo-compute-cli-for-virtual-machine-lifecycle-management",
      "source": "hashnode",
      "published_at": "2025-03-19T12:09:16.962000+00:00",
      "external_id": null,
      "tags": [
        "Cloud Computing",
        "bash script"
      ],
      "content_length": 11504,
      "content_preview": "CUDO Compute is a **decentralised cloud computing platform** that provides scalable and cost-effective computing power by utilising underutilised hardware resources from data centers, enterprises, and individual contributors. It allows users to deploy and manage virtual machines (VMs) and workloads on a distributed network of compute nodes.\n\nIn this tutorial, you will learn how to integrate Cudo Compute into your workflow for scalable, cost-effective computing.\n\n## Objectives\n\n* Create virtual m",
      "content_full": "CUDO Compute is a **decentralised cloud computing platform** that provides scalable and cost-effective computing power by utilising underutilised hardware resources from data centers, enterprises, and individual contributors. It allows users to deploy and manage virtual machines (VMs) and workloads on a distributed network of compute nodes.\n\nIn this tutorial, you will learn how to integrate Cudo Compute into your workflow for scalable, cost-effective computing.\n\n## Objectives\n\n* Create virtual machines (VMs) with CUDOâ€™s CLI\n    \n* SSH into these VMs to run a benchmark test via the CLI\n    \n* Destroy the VMs via the CLI\n    \n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">A <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://compute.cudo.org/auth/sign-in\" style=\"pointer-events: none\">CUDO account</a> will be required for this tutorial.</div>\n</div>\n\n## Get your API key\n\nYou will need to obtain your API keys from the `API Keys` section on your CUDO dashboard.\n\n![api](https://res.cloudinary.com/ichtrojan/image/upload/v1741609640/1_hh4jho.png align=\"left\")\n\nClick on the â€œCreate an API Keyâ€ button and assign a name to your API key. Your chosen API key name must be in lowercase and can be hyphenated for readability.\n\n![name](https://res.cloudinary.com/ichtrojan/image/upload/v1741609640/3_icdjzo.png align=\"left\")\n\nOn doing this, youâ€™ll be given an API key. Take note of this section of the app, as youâ€™ll need to copy your API key and use it in your CLI.\n\n![gen](https://res.cloudinary.com/ichtrojan/image/upload/v1741609640/4_ayeuxp.png align=\"left\")\n\n## Create your billing account\n\nBilling accounts pay for the use of resources in Cudo Compute. To use the cloud resources in a project, the project must be linked to an active billing account. An active billing account has a credit balance; the minimum deposit is $10.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741658109579/8cf135f9-413e-4071-a515-c27436f01298.png align=\"center\")\n\n## Setup CUDO CLI\n\nCUDO Compute has a CLI tool called `cudoctl` to manage all of your resources within the platform. Follow the steps below to get started.\n\n### Step 1: Download the binary file\n\nTo access the CLIâ€™s functionality, you must download the binary file onto your local machine. To do this, go to the [download page](https://www.cudocompute.com/docs/cli-tool).\n\n![down](https://res.cloudinary.com/ichtrojan/image/upload/v1741610112/5_ujajjx.png align=\"left\")\n\n### Step 2: Install the binary file\n\nAfter successfully downloading the binary file, you have to move it to the right path on your machine.\n\n```bash\nchmod +x <filepath> \nsudo mv <filepath> /usr/local/bin/\nsudo mv /usr/local/bin/<filename> /usr/local/bin/<newname>\n```\n\nReplace `<filepath>` with the path to the downloaded binary and `<newname>` with `cudoctl`.\n\n### Step 3: Initialise the installation\n\nRun `cudoctl init` and follow the steps.\n\n```bash\ncudoctl init\n   âœ” api key: my-api-key\n   âœ” project: my-project\n   âœ” billing account: my-billing-account\n   âœ” context: default\n   config file saved ~/.config/cudo/cudo.yml\n```\n\nA config file will be maintained in `$HOME/.config/cudo/cudo.yml`.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">Access help at any level by using <code>-h</code> e.g <code>cudoctl -h</code></div>\n</div>\n\n### Step 4: Add SSH Key\n\nAn **SSH key** is used to securely access a **VM** over the Internet or a private network. If you donâ€™t have an existing SSH key, follow the instructions [here](https://www.cudocompute.com/docs/tutorials/how-to-generate-ssh-keys) to generate a new one on your local machine.\n\nRun the following to initialise CUDO with your SSH key:\n\n```bash\ncudoctl ssh-keys init\n```\n\nSelect your desired SSH key and proceed with `<enter/return`\n\n![ssh](https://res.cloudinary.com/ichtrojan/image/upload/v1741611064/6_rf4l5f.png align=\"left\")\n\n### Executing the Bash Script\n\nHereâ€™s a [Bash script](https://gist.github.com/yomaokobiah/5da837f38fe9d1d9423ce55d34a034f9) that uses only the **Cudos CLI** to meet the objectives of this tutorial:\n\nWe begin with the script shebang (`#!/bin/bash`), which specifies that the script should be executed using Bash. The `set -e` command ensures that the script exits immediately if any command fails. Next, the user is prompted to enter a project name, which is used to create a project. Since all resources in Cudo Compute must be associated with a project, a VM cannot be made without first setting up a project.\n\n```bash\n#!/bin/bash\nset -e\nread -p \"Enter the project name: \" project_name\ncudoctl projects create $project_name\n...\n```\n\nThe `CONFIGS` array stores multiple VM configurations, with each entry following a structured format: **VM name, data center, machine type, OS image, number of vCPUs, memory size, and disk size**. Additional configuration parameters can be found in the [documentation](https://www.cudocompute.com/docs/cli-tool) if needed.\n\n```bash\n...\nCONFIGS=(\n  \"benchmarkvm1 us-santaclara-1 intel-broadwell ubuntu-2204-nvidia-535-docker-v20241017 1 1 10\"\n  \"benchmarkvm2 us-santaclara-1 intel-broadwell ubuntu-2204-nvidia-535-docker-v20241017 2 2 15\"\n  \"benchmarkvm3 us-santaclara-1 intel-broadwell ubuntu-2204-nvidia-535-docker-v20241017 4 4 20\"\n)\n...\n```\n\nWe create a file (`benchmark_results.txt`) to store the benchmark results and add a header row (`VM Name, VCPU, Memory, Disk_Size, CPU Events/sec`) to ensure the data is easily identifiable.\n\n```bash\n...\nRESULTS_FILE=\"benchmark_results.txt\"\necho \"VM Name, VCPU, Memory, Disk_Size, CPU Events/sec\" > $RESULTS_FILE\n...\n```\n\nWe define the Bash function `create_vm()` using **local** variables to store the function arguments, representing the arrayâ€™s VM configuration details. The function uses `echo` to display status messages, providing feedback on the VM creation process. The `VM_ID` variable captures the output of the `cudoctl vm create` command, allowing us to verify whether the VM was successfully created.\n\n```bash\n...\ncreate_vm() {\n  local vm_name=$1\n  local data_center=$2\n  local machine_type=$3\n  local image=$4\n  local vcpu=$5\n  local memory=$6\n  local disk_size=$7\n\n  echo \"Creating VM: $vm_name ($vcpu CPUs, $memory RAM, $disk_size Disk) in $data_center...\"\n\n  VM_ID=$(cudoctl vm create -project \"$project_name\" -id \"$vm_name\" -data-center \"$data_center\" -machine-type \"$machine_type\" \\\n          -image \"$image\" -vcpus \"$vcpu\" -memory \"$memory\" -boot-disk-size \"$disk_size\")\n\n  if [[ $VM_ID -gt 0 ]]; then\n      echo \"Failed to create VM: $vm_name\"\n      exit 1\n  fi\n\n  echo \"creating vm\"\n  echo $vm_name\n}\n...\n```\n\nThe `run_benchmark()` function automates the benchmarking process for a VM by using **local variables** to store the necessary parameters. It retrieves the list of VMs using `cudoctl vm list` and extracts the VMâ€™s **external IP address** using `grep` and `awk`. If no IP is found, the function waits and retries once. If the IP is still not found, it returns 1, indicating a failure.\n\n```bash\n...\nrun_benchmark() {\n  local vm_id=$1\n  local vcpu=$2\n  local memory=$3\n  local disk_size=$4\n\n  echo \"Running benchmark on (VM ID: $vm_id)...\"\n  \n  IPS=($(cudoctl vm list | grep -A 2 \"$vm_id\" | grep externalIP | awk '{print $2}'))\n  \n  if [[ ${#IPS[@]} -eq 0 ]]; then\n    echo \"No IPs found for VM $vm_id, waiting 30 seconds and trying again...\"\n    sleep 30\n    IPS=($(cudoctl vm list | grep -A 2 \"$vm_id\" | grep externalIP | awk '{print $2}'))\n    \n    if [[ ${#IPS[@]} -eq 0 ]]; then\n      echo \"Still no IPs found for VM $vm_id\"\n      return 1\n    fi\n  fi\n...\n```\n\nIf an IP is found, the next part of the `run_benchmark()` function checks whether SSH access is available. If SSH is accessible, it connects to the VM, updates system packages, installs `sysbench` (a benchmarking tool), and runs a CPU stress test with four threads. The function extracts the benchmark result and saves it to the results file.\n\n```bash\n...\n  echo \"Found ${#IPS[@]} IP(s) for VM $vm_id\"\n  \n  for ip in \"${IPS[@]}\"; do\n    echo \"Attempting to connect to IP: $ip\"\n    \n    if ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes root@$ip echo \"SSH Ready\" &>/dev/null; then\n      echo \"SSH connection to $ip successful\"\n      \n      OUTPUT=$(ssh -o StrictHostKeyChecking=no -o ConnectTimeout=30 root@$ip -v << 'EOF'\nexport DEBIAN_FRONTEND=noninteractive\nsudo apt-get update -y\nsudo apt-get install -y sysbench\nsysbench cpu --threads=4 run\nEOF\n      ) || { echo \"Benchmark failed on $ip\"; continue; }\n      \n      CPU_EVENTS=$(echo \"$OUTPUT\" | grep \"events per second\" | awk '{print $4}')\n      \n      if [[ -n \"$CPU_EVENTS\" ]]; then\n        echo \"$vm_id, $vcpu, $memory, $disk_size, $CPU_EVENTS\" >> $RESULTS_FILE\n        echo \"Benchmark completed successfully: $CPU_EVENTS events/sec\"\n        return 0\n      else\n        echo \"Failed to extract benchmark results from $ip\"\n      fi\n    else\n      echo \"SSH connection to $ip failed, trying next IP if available...\"\n    fi\n  done\n  \n  echo \"Failed to run benchmark on any available IP for VM $vm_id\"\n  return 1\n}\n...\n```\n\nThe `destroy_vm()` function is responsible for **deleting a virtual machine (VM)**. It takes a single argument, `vm_id` which represents the unique identifier of the VM, to be deleted. This function helps prevent resource wastage.\n\n```bash\n...\ndestroy_vm() {\n  local vm_id=$1\n  echo \"Destroying VM: (VM ID: $vm_id)...\"\n  cudoctl vm delete \"$vm_id\"\n}\n...\n```\n\nWe iterate through each configuration in the `CONFIGS` array to create the corresponding VM.\n\n```bash\n...\nfor CONFIG in \"${CONFIGS[@]}\"; do\n  set -- $CONFIG\n  VM_NAME=$1\n  DATA_CENTER=$2\n  MACHINE_TYPE=$3\n  IMAGE=$4\n  VCPU=$5\n  MEMORY=$6\n  DISK_SIZE=$7\n\n  create_vm \"$VM_NAME\" \"$DATA_CENTER\" \"$MACHINE_TYPE\" \"$IMAGE\" \"$VCPU\" \"$MEMORY\" \"$DISK_SIZE\"\n  echo \"Waiting for VM to initialize...\"\n  \n  echo \"---------------------------------------\"\ndone\n...\n```\n\nWe wait **60 seconds** to ensure the VMs have fully booted, preventing benchmarking failures due to uninitialised instances. The SSH known hosts file is cleared to avoid connection mismatches. The script then loops through each configuration in the `CONFIGS` array, runs the benchmark using the specified parameters, and saves the results. After benchmarking, each VM is deleted to free up resources. Finally, a confirmation message is displayed to indicate successful completion.\n\n```bash\n...\nsleep 60\necho '' > ~/.ssh/known_hosts\n\nfor CONFIG in \"${CONFIGS[@]}\"; do\n  set -- $CONFIG\n  VM_NAME=$1\n  VCPU=$5\n  MEMORY=$6\n  DISK_SIZE=$7\n\n  run_benchmark \"$VM_NAME\" \"$VCPU\" \"$MEMORY\" \"$DISK_SIZE\"\n  destroy_vm \"$VM_NAME\"\ndone\necho \"Benchmarking complete. Results saved in $RESULTS_FILE and VMs deleted.\"\n\nexit 0\n```\n\nTo run the entire script using the GitHub gist, run the following:\n\n```bash\n#save the script\ncurl -sSL https://gist.github.com/yomaokobiah/5da837f38fe9d1d9423ce55d34a034f9/raw > cudobenchscript.sh\n#make the script executable\nchmod +x cudobenchscript.sh\n#run the script\n./cudobenchscript.sh\n```\n\nHereâ€™s what it looks like upon successful execution:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741617218835/b1c24ca7-7a93-42d9-87ef-25273d79ed66.png align=\"center\")\n\nUsing the **Cudo Compute CLI** enables you to efficiently manage the entire lifecycle of virtual machines, from creation to benchmarking and deletion. Start experimenting today and unlock the full potential of decentralised cloud computing with **Cudo Compute**!",
      "stars": null,
      "comments": 1,
      "upvotes": 23,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "Scrolling Against the Grain: Inverting User Experience with React",
      "url": "https://enginerd.tech/scrolling-against-the-grain-inverting-user-experience-with-react",
      "source": "hashnode",
      "published_at": "2025-03-18T19:13:13.548000+00:00",
      "external_id": null,
      "tags": [
        "React",
        "JavaScript",
        "HTML5",
        "Web Development"
      ],
      "content_length": 8392,
      "content_preview": "As software engineers, we're constantly looking for ways to create unique and memorable user experiences. Today, I'm excited to share a component from a project I've been working on that completely inverts the traditional scrolling paradigm rendering the view at the bottom of the HTML element rather than the top.\n\n## The Bottom Up Approach to UI\n\nMost web experiences start at the top and scroll down, it's how we've trained users for decades. But what if we flipped this expectation on its head? T",
      "content_full": "As software engineers, we're constantly looking for ways to create unique and memorable user experiences. Today, I'm excited to share a component from a project I've been working on that completely inverts the traditional scrolling paradigm rendering the view at the bottom of the HTML element rather than the top.\n\n## The Bottom Up Approach to UI\n\nMost web experiences start at the top and scroll down, it's how we've trained users for decades. But what if we flipped this expectation on its head? That's exactly what this component does, effectively implementing a form of scroll jacking. Scroll jacking is a technique where developers override the browser's default scrolling behavior to create custom experiences.\n\n## My Inspiration: The Space Elevator\n\nBefore diving into the code, let's look at a brilliant example of this technique in action: Neal Agarwal's [Space Elevator](https://neal.fun/space-elevator/) interactive experience. This web app takes users on a journey from Earth deep into space, using scroll jacking in a way that makes perfect sense for the content.\n\nAs you \"scroll up\" from Earth's surface, you ascend into the atmosphere, then space, passing satellites, the ISS, and eventually traveling beyond our solar system. The bottom-up approach here creates an intuitive mental model that aligns with the physical concept of ascending upward.\n\n## Advantages of Scroll Jacking\n\nThis inverted scrolling paradigm offers several interesting benefits:\n\n1. **Novel user experience**: It immediately signals to users that they're interacting with something different and memorable.\n    \n2. **Metaphorical alignment**: For certain content types (like the space elevator example), it creates a more intuitive mental model.\n    \n3. **Contrasting information hierarchy**: It can emphasize importance by inverting the traditional importance cascade.\n    \n4. **Engagement boost**: The unexpected interaction pattern can increase curiosity and time-on-page.\n    \n\n## Challenges to Consider When Inverting UI\n\nOf course, this approach isn't without its challenges:\n\n1. **Breaking user expectations**: Users expect to scroll down, not up. This means you'll need to provide clear visual cues to prompt the desired behavior.\n    \n2. **Learning curve**: There's a cognitive adjustment required when users encounter your interface.\n    \n3. **Accessibility concerns**: Non-standard scrolling behaviors can create barriers for some users if not implemented thoughtfully.\n    \n4. **Mobile considerations**: Touch scrolling patterns are deeply ingrained, so mobile implementation requires extra care.\n    \n\nThe biggest hurdle is undoubtedly the need to prompt users to scroll up. This goes against decades of muscle memory and learned behavior. To overcome this, you can implement:\n\n* Animated indicators (like a bouncing scroll wheel pointing upward)\n    \n* Clear call-to-action text (\"Scroll up to continue\")\n    \n* Visual design that implies upward movement\n    \n* An animation that demonstrates the intended interaction\n    \n\n## Building the React Component\n\nNow, let's get technical. The space elevator is coded in Vue. Here's how I implemented the same component in React:\n\n```plaintext\n\"use client\";\n\nimport React, { useEffect, useRef } from \"react\";\nimport styles from \"./ScrollToTop.module.css\";\n\nconst ScrollToTop: React.FC = () => {\n  const containerRef = useRef<HTMLDivElement>(null);\n\n  useEffect(() => {\n    const observer = new MutationObserver(() => {\n      if (containerRef.current?.offsetHeight) {\n        window.scrollTo(0, document.body.scrollHeight);\n        observer.disconnect();\n      }\n    });\n\n    observer.observe(document.body, {\n      childList: true,\n      subtree: true,\n    });\n\n    return () => observer.disconnect();\n  }, []);\n\n  return (\n    <div>\n      <div className={styles.contentWrapper} ref={containerRef}>\n        <div className={styles.sectionHeader}>\n          <h1>Scroll Up</h1>\n          <p>^^^^^^</p>\n        </div>\n      </div>\n    </div>\n  );\n};\n\nexport default ScrollToTop;\n```\n\n### Breaking Down the Code\n\nLet's dissect how this component works:\n\n#### 1\\. The \"use client\" Directive\n\nThe component starts with `\"use client\"` at the top, which is crucial when working with React Server Components. This directive explicitly tells React that this module and its dependencies should be executed on the client-side (browser) rather than the server. This is necessary because we need to access browser-specific APIs like `window` and DOM manipulation functions.\n\n#### 2\\. Component Setup and References\n\nWe import the necessary React hooks: `useEffect` for handling side effects and `useRef` for creating a reference to our DOM element. The `containerRef` will be attached to the main content wrapper div, allowing us to monitor when it has been fully rendered.\n\n#### 3\\. The MutationObserver Magic\n\nThe real magic happens in the `useEffect` hook, where we use a `MutationObserver` to watch for changes to the DOM:\n\n```plaintext\nuseEffect(() => {\n  const observer = new MutationObserver(() => {\n    if (containerRef.current?.offsetHeight) {\n      window.scrollTo(0, document.body.scrollHeight);\n      observer.disconnect();\n    }\n  });\n\n  observer.observe(document.body, {\n    childList: true,\n    subtree: true,\n  });\n\n  return () => observer.disconnect();\n}, []);\n```\n\nThis code:\n\n* Creates a `MutationObserver` that watches for changes to the DOM\n    \n* When it detects that our referenced container has content with height (meaning it's been rendered), it scrolls the page to the bottom\n    \n* It then disconnects the observer since its job is complete\n    \n* The return function ensures proper cleanup by disconnecting the observer when the component unmounts\n    \n\n#### 4\\. The Component Structure\n\nThe component renders a simple structure with visual cues to prompt users to scroll up:\n\n```plaintext\nreturn (\n  <div>\n    <div className={styles.contentWrapper} ref={containerRef}>\n      <div className={styles.sectionHeader}>\n        <h1>Scroll Up</h1>\n        <p>^^^^^^</p>\n      </div>\n    </div>\n  </div>\n);\n```\n\nThe key here is connecting our `containerRef` to the content wrapper div via the `ref` attribute, which allows our code in `useEffect` to check when this element has been fully rendered.\n\n## Making It Accessible\n\nAccessibility shouldn't be an afterthought. Here's how to ensure your inverted scrolling component remains accessible:\n\n1. **Provide keyboard navigation alternatives**: Ensure users can navigate through your content using keyboard commands, not just scrolling.\n    \n2. **Use appropriate ARIA attributes**: Add `aria-label` and other relevant attributes to explain the non-standard navigation.\n    \n3. **Respect user preferences**: Check for `prefers-reduced-motion` media queries and provide alternatives for users who may experience motion sickness.\n    \n4. **Clear instructions**: Make sure instructions for navigating your UI are clear and available in multiple formats (visual, text, etc.).\n    \n5. **Test with screen readers**: Verify that screen reader users can understand and navigate your content coherently.\n    \n\n## When to Use This Approach\n\nThis inverted scrolling pattern isn't appropriate for every project. Consider using it when:\n\n* The content naturally follows a bottom-up metaphor (like the Space Elevator example)\n    \n* You're creating an artistic or experimental interface where novelty is expected\n    \n* The user journey benefits from a reversed information hierarchy\n    \n* You can provide clear navigational cues to guide users\n    \n\nAvoid this approach for content heavy websites, task oriented interfaces, or any context where efficient information retrieval is the primary goal.\n\n## Conclusion\n\nPlaying with scroll direction is more than just a novelty, it's about finding new ways to map interface interactions to mental models that make sense for specific content. When implemented thoughtfully with clear user guidance and attention to accessibility, inverting the scroll direction can create memorable, engaging experiences.\n\nThe React component shared demonstrates how to implement this technique efficiently, using modern React patterns like hooks, refs, and the [MutationObserver API](https://developer.mozilla.org/en-US/docs/Web/API/MutationObserver). It waits until content is fully rendered before triggering the scroll action and properly cleans up resources when finished.",
      "stars": null,
      "comments": 0,
      "upvotes": 14,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "Understanding AI Agents: An Overview",
      "url": "https://blog.lo-victoria.com/understanding-ai-agents-an-overview",
      "source": "hashnode",
      "published_at": "2025-03-18T02:00:25.394000+00:00",
      "external_id": null,
      "tags": [
        "AI",
        "langgraph",
        "ai-agent",
        "openai",
        "RAG "
      ],
      "content_length": 9555,
      "content_preview": "Artificial Intelligence (AI) has taken a central role in transforming how we interact with technology, from virtual assistants like Siri and Alexa to complex problem-solving systems. Among these advancements, **AI Agents** are rapidly becoming a critical tool for businesses and individuals alike.\n\nBut what exactly are AI Agents, and why should you care about them?\n\nLetâ€™s break it down, using practical examples to illustrate their power and potential.\n\n# Example\n\nLetâ€™s say I login to ChatGPT and ",
      "content_full": "Artificial Intelligence (AI) has taken a central role in transforming how we interact with technology, from virtual assistants like Siri and Alexa to complex problem-solving systems. Among these advancements, **AI Agents** are rapidly becoming a critical tool for businesses and individuals alike.\n\nBut what exactly are AI Agents, and why should you care about them?\n\nLetâ€™s break it down, using practical examples to illustrate their power and potential.\n\n# Example\n\nLetâ€™s say I login to ChatGPT and ask: *How many vacation days does Victoria have left in 2025?*\n\nChatGPT would definitely give me the wrong answer.\n\nOne of the significant limitations of Large Language Models (LLMs) like the ones used by ChatGPT is their inability to answer highly specific questions if they haven't been explicitly trained on that information. Since ChatGPT has no access to my employee database, it does not have the accurate answer to my question.\n\nThis is where **Retrieval-Augmented Generation (RAG)** comes into play.\n\n## How RAG Works:\n\n1. **Chunking Data**: Start with a document or dataset, like a PDF containing Victoriaâ€™s vacation days. The document is broken into chunks of text small enough to fit into the LLMâ€™s memory, which can handle up to 100,000 tokens.\n    \n2. **Embedding Creation**: These chunks are converted into embeddingsâ€”vector representations that capture the semantic meaning of the text.  \n    *(This topic can go deeper into transformer models which is outside this articleâ€™s scope.)*\n    \n3. **Vector Search**: When you pose a question like, â€œHow many days off did Victoria take in 2024?â€ the system performs a vector search from the vector database. If any chunk in the document contains relevant information (e.g., â€œdays offâ€ or â€œ2024â€), it finds the exact match.\n    \n4. **Answer Generation**: The relevant chunk is fed back into the LLM, which then generates a precise, context-aware answers.\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736569985320/7a8318a0-0563-41fa-8ef8-13bb9818ebf4.jpeg align=\"center\")\n\nSo what does this all means? It means that RAG enables LLMs to answer questions with increased accuracy by accessing **external, domain-specific** knowledge. Businesses have developed RAG applications for practical uses, such as a chatbot that can answer customer or product-specific questions. In 2023, RAG has dominated and enhanced the usability and scalability of LLM-powered applications.\n\n## What if?\n\nBut now, what if I ask: Based on the number of vacation days Victoria has left in 2025, suggest an itinerary for a Europe trip.\n\nIn this case, I would need my chat application to be capable of searching not only my vacation days data, but also perform web search to provide a reasonable itinerary for an X-day Europe trip.\n\nOne of the fallbacks of the naive RAG system is that it only retrieves information from a specific knowledge source that we already define for it (i.e. Victoriaâ€™s vacation days data). This means that it does not have the **agency** to decide which knowledge source to get its information from, making it a challenge when designing a solution for complex use cases.\n\n# AI Agents as Reasoning Planners\n\nRecognizing this challenge, more businesses began to adopt agentic frameworks in 2024. An agentic RAG implements AI agents into the workflow, which can go beyond answering questions. AI agents act as **reasoning planners**, capable of orchestrating multiple downstream tasks.\n\n> TLDR: AI agents are powered by LLMs to execute end-to-end tasks autonomously and make decisions in real time to achieve their given objectives.\n\nThe pillars that make up an AI agent are:\n\n* LLM (with a role and a task like a vacation days planner, etc.)\n    \n* Memory (access to chat history, previous responses, etc.)\n    \n* Planning (downstream tasks like routing, etc.)\n    \n* Access to external tools to perform actions (web search, calendars, etc.)\n    \n\n**ReAct** (not the front-end one) agent model, which stands for \"Reasoning & Acting,\" is a framework that combines both reasoning and action in AI agents. These agents are not limited to providing answers based on pre-programmed knowledge; instead, they can strategically plan and execute complex sequences of tasks to achieve specific objectives.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736571287662/08bcdf88-08e4-417f-97a3-39aa6c566352.png align=\"center\")\n\nFor example, here are some practical applications for AI Agents:\n\n* **Contract Negotiation**: Imagine an AI agent designed to negotiate leases with landlords. The agent can analyze terms, suggest counteroffers, and even simulate negotiations.\n    \n* **Personal Virtual Assistants**: Tools like Googleâ€™s Bard demonstrate the potential for AI agents to act as comprehensive personal assistants, handling tasks like scheduling meetings, generating summaries, or organizing events.\n    \n\nAnd the possibilities are endless! There are infinite ways to design agent-powered solutions, from simple single-agent to multi-agent solutions, from simple automation of daily tasks to complex task management. Letâ€™s go back to our RAG example and learn how an Agentic RAG works.\n\n## How Agentic RAG Works (very simplified):\n\n1. **Goal Identification**: The user defines a task, like creating a travel itinerary or automating a business process.\n    \n2. **Task Decomposition**: The AI agent uses reasoning to break the goal into smaller tasks. Decide which knowledge source to retrieve the necessary information from.\n    \n3. **Execution**: Tasks are executed through a combination of LLM calls (to generate the content) and API calls (e.g. fetching weather data, searching popular itineraries, etc.).\n    \n4. **Evaluation:** The AI agent then decides whether the retrieved information is relevant and can choose to re-retrieve from other sources if needed.\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736736274712/9064a6cb-0429-49c4-8379-9c4bbf1505c7.jpeg align=\"center\")\n\n# Why You Should Care?\n\nAccording to LangChainâ€™s State of AI Agents report, about 78% of businesses are planning to implement AI agents in 2024. AI Agents are not just futuristic conceptsâ€”theyâ€™re here, and theyâ€™re transforming industries.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736566005591/b2ff0437-2e5f-405c-ac3a-e73b98f3fd62.png align=\"center\")\n\nIf you are a business owner or an innovator, hereâ€™s why you should pay attention:\n\n1. **Enhanced Productivity**: AI agents can automate repetitive tasks, freeing up your time for more creative or strategic work.\n    \n2. **Customizable Intelligence**: With Agentic RAG, businesses can tailor AI solutions to their unique datasets, improving accuracy and reliability beyond the naive RAG solutions.\n    \n3. **Competitive Advantage**: Early adopters of AI agents can outpace competitors by leveraging data-driven insights and automating decision-making.\n    \n\n# Tools to Build AI Agents and How to Get Started\n\nAI Agents boomed in 2024 and now in 2025, we developers looking to create AI agents have access to a wide range of tools and frameworks. Here are 3 most popular options:\n\n## 1\\. **LangGraph**\n\nProbably the most popular framework for developing complex agentic workflows. Itâ€™s known to be highly customizable, and simplifies the creation of AI agents by providing tools for memory, chaining, and integration with APIs.\n\n> Will publish a step-by-step tutorial on this so stay tuned!\n\n## 2\\. **OpenAI Swarm**\n\nDeveloped by OpenAI, Swarm can be used to create reasoning agents that can handle text-based tasks. Itâ€™s known to be lightweight, â€œeducationalâ€ and used to create smaller agents that are highly controllable and testable.\n\n> Iâ€™m still experimenting with this but also hope to build a demo to compare with LangGraph.\n\n## 3) CrewAI\n\nItâ€™s one of the newer frameworks that is growing in popularity. It is simple to use because you can create agents with prompts, great for collaboration and developing multi-agent systems. However, it is known to be less versatile than LangGraph in terms of making highly complex workflows.\n\n## 4) Postman AI Agent Builder\n\nThis news was just out at the time of writing this article, Postman just launched its AI Agent Builder tool to help with making AI Agents easier, faster and more reliable! Iâ€™m excited to check out this one too!\n\n> Link to the [news article](https://community.postman.com/t/the-postman-drop-introducing-the-ai-agent-builder/74203)\n\n# Conclusion\n\nAs AI continues to evolve, the question isnâ€™t whether you should use AI agents but how you can best integrate them into your personal or professional life. From streamlining operations to answering complex questions, AI agents are redefining how we work, learn, and innovate.\n\nThanks for reading! Hope this article has been helpful! If it has, do leave a like, share the article and comment your thoughts below! Have you developed AI agents? Stay tuned for more AI for everyone content! Cheers!\n\n### **Let's Connect!**\n\n* [**Twitter**](https://twitter.com/lo_victoria2666)\n    \n* [**LinkedIn**](https://www.linkedin.com/in/victoria2666/)\n    \n* [**GitHub**](https://github.com/victoria-lo)\n    \n\n### References\n\n[https://www.langchain.com/stateofaiagents](https://www.langchain.com/stateofaiagents)\n\n[https://klu.ai/glossary/react-agent-model](https://klu.ai/glossary/react-agent-model)\n\n[https://weaviate.io/blog/what-is-agentic-rag](https://weaviate.io/blog/what-is-agentic-rag)\n\n[https://www.crewai.com/](https://www.crewai.com/)\n\n[https://github.com/openai/swarm](https://github.com/openai/swarm)",
      "stars": null,
      "comments": 2,
      "upvotes": 43,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "Building Your Own React State Management: A Deep Dive into Custom Stores",
      "url": "https://techover.hashnode.dev/building-your-own-react-state-management-a-deep-dive-into-custom-stores",
      "source": "hashnode",
      "published_at": "2025-03-15T11:49:50.553000+00:00",
      "external_id": null,
      "tags": [
        "React",
        "JavaScript",
        "State Management "
      ],
      "content_length": 13075,
      "content_preview": "In the ever-evolving React ecosystem, state management remains one of the most discussed and reimagined aspects of frontend development. While popular libraries like Redux, Zustand, and Jotai offer robust solutions, there's significant value in understanding how to build your own state management system. This article explores the why and how of creating custom stores in React, empowering you with knowledge that goes beyond simply importing the next trending library.\n\n## Why Consider Building You",
      "content_full": "In the ever-evolving React ecosystem, state management remains one of the most discussed and reimagined aspects of frontend development. While popular libraries like Redux, Zustand, and Jotai offer robust solutions, there's significant value in understanding how to build your own state management system. This article explores the why and how of creating custom stores in React, empowering you with knowledge that goes beyond simply importing the next trending library.\n\n## Why Consider Building Your Own Store?\n\nBefore diving into implementation details, let's consider when creating your own state management solution makes sense:\n\n1. **Educational value**: Understanding the internals of state management helps you better utilize existing libraries\n    \n2. **Simplicity**: When your needs are specific and well-defined, a custom solution can be less overhead than adapting a general-purpose library\n    \n3. **Integration requirements**: Custom stores can be designed specifically for your unique API interactions or system requirements\n    \n4. **Performance optimization**: When you need precise control over rendering behavior and state updates\n    \n5. **Project ownership**: Building your own solution creates deeper team understanding and control over critical application infrastructure\n    \n\nAs experienced React developer Kent C. Dodds often says, \"The more you understand your tools, the more effectively you can use them.\" Building a custom store provides insights into state management that will serve you regardless of which libraries you ultimately choose.\n\n## The State of State Management in React\n\nThe React state management landscape has evolved considerably:\n\n**First Wave**: Redux dominated with its predictable, centralized approach but introduced significant boilerplate.\n\n**Second Wave**: Context API with useReducer offered native solutions but with performance limitations.\n\n**Current Wave**: Libraries like Zustand, Jotai, and Recoil focus on atomic updates, composability, and developer experience.\n\nEach approach represents different philosophies about how state should be structured, updated, and consumed. Your custom solution can incorporate the best aspects of these approaches while avoiding unnecessary complexities.\n\n## Anatomy of a Custom React Store\n\nAt its core, a React store needs to:\n\n1. Hold state\n    \n2. Provide methods to update state\n    \n3. Notify components when state changes\n    \n4. Integrate with React's rendering cycle\n    \n\nLet's examine an implementation that achieves these goals using modern React patterns:\n\n```typescript\nimport { cloneDeep } from \"lodash-es\";\nimport { useSyncExternalStore } from \"react\";\n\ntype Watcher<T> = [(old: T, data: T) => boolean, (data: T) => void];\n\nexport function createStore<T>(initialState: T) {\n  let state: T = cloneDeep(initialState);\n  let listeners: (() => void)[] = [];\n  let watchers: Watcher<T>[] = [];\n\n  const subscribe = (listener: () => void) => {\n    listeners.push(listener);\n    return () => {\n      listeners = listeners.filter((f) => f !== listener);\n    };\n  };\n\n  const dispatchWatches = (newState: T) => {\n    const old = state;\n    state = newState;\n    watchers.forEach(([check, cb]) => {\n      if (check(old, newState)) {\n        cb(newState);\n      }\n    });\n  };\n\n  const dispatch = () => {\n    listeners.forEach((a) => {\n      a();\n    });\n  };\n\n  const getSnapshot = () => state;\n\n  const set = (data: T) => {\n    dispatchWatches(data);\n    dispatch();\n  };\n\n  const update = (fn: (data: T) => T) => {\n    dispatchWatches(fn(state));\n    dispatch();\n  };\n\n  const reset = () => {\n    dispatchWatches(cloneDeep(initialState));\n    dispatch();\n  };\n\n  const watch = (check: Watcher<T>[0], cb: Watcher<T>[1]) => {\n    watchers.push([check, cb]);\n    return () => {\n      watchers = watchers.filter((f) => f[0] !== check);\n    };\n  };\n\n  return {\n    subscribe,\n    getSnapshot,\n    set,\n    update,\n    reset,\n    watch,\n  };\n}\n\nexport const useStore = <T>(store: ReturnType<typeof createStore<T>>) =>\n  useSyncExternalStore(store.subscribe, store.getSnapshot, store.getSnapshot);\n```\n\nThis implementation leverages React 18's `useSyncExternalStore` hook, which was specifically designed to connect external state management systems with React's rendering cycle.\n\n## Understanding the Key Components\n\nLet's break down the essential elements of this custom store:\n\n### State Container\n\n```typescript\nlet state: T = cloneDeep(initialState);\n```\n\nThe store maintains a single source of truth, creating a deep copy of the initial state to ensure immutability.\n\n### Subscription System\n\n```typescript\nconst subscribe = (listener: () => void) => {\n  listeners.push(listener);\n  return () => {\n    listeners = listeners.filter((f) => f !== listener);\n  };\n};\n```\n\nComponents can subscribe to state changes, and the function returns a cleanup method to unsubscribe when components unmount.\n\n### Update Mechanisms\n\n```typescript\nconst set = (data: T) => {\n  dispatchWatches(data);\n  dispatch();\n};\n\nconst update = (fn: (data: T) => T) => {\n  dispatchWatches(fn(state));\n  dispatch();\n};\n```\n\nThe store provides two primary ways to update state:\n\n* `set`: Directly replace the entire state\n    \n* `update`: Use a function to transform the current state into a new one\n    \n\n### Conditional Watchers\n\n```typescript\nconst watch = (check: Watcher<T>[0], cb: Watcher<T>[1]) => {\n  watchers.push([check, cb]);\n  return () => {\n    watchers = watchers.filter((f) => f[0] !== check);\n  };\n};\n```\n\nWatchers provide a powerful mechanism for reacting to specific state changes. Unlike regular listeners that fire on every state update, watchers only trigger when their condition function returns true.\n\n### React Integration\n\n```typescript\nexport const useStore = <T>(store: ReturnType<typeof createStore<T>>) =>\n  useSyncExternalStore(store.subscribe, store.getSnapshot, store.getSnapshot);\n```\n\nThis custom hook connects our store to React's rendering system, ensuring components re-render appropriately when state changes.\n\n## Putting It Into Practice: Building a Task Management Application\n\nLet's create a simple task management application to demonstrate our custom store in action:\n\n```typescript\n// Create our task store\nconst taskStore = createStore({\n  tasks: [],\n  filter: 'all'\n});\n\n// Component using the store\nfunction TaskManager() {\n  const { tasks, filter } = useStore(taskStore);\n  const [newTask, setNewTask] = useState('');\n  \n  const filteredTasks = useMemo(() => {\n    return tasks.filter(task => {\n      if (filter === 'completed') return task.completed;\n      if (filter === 'active') return !task.completed;\n      return true;\n    });\n  }, [tasks, filter]);\n  \n  const addTask = () => {\n    if (!newTask.trim()) return;\n    \n    taskStore.update(state => ({\n      ...state,\n      tasks: [...state.tasks, {\n        id: Date.now(),\n        text: newTask,\n        completed: false\n      }]\n    }));\n    \n    setNewTask('');\n  };\n  \n  const toggleTask = (id) => {\n    taskStore.update(state => ({\n      ...state,\n      tasks: state.tasks.map(task => \n        task.id === id ? { ...task, completed: !task.completed } : task\n      )\n    }));\n  };\n  \n  return (\n    <div className=\"task-manager\">\n      {/* Implementation details */}\n    </div>\n  );\n}\n```\n\nThis example demonstrates how our custom store provides a clean, flexible state management solution without the overhead of external libraries.\n\n## Advanced Patterns and Techniques\n\nAs you become more comfortable with custom stores, consider these advanced patterns:\n\n### 1\\. Store Composition\n\nBreak your application state into domain-specific stores that can work together:\n\n```typescript\nconst userStore = createStore({ /* user state */ });\nconst taskStore = createStore({ /* task state */ });\nconst uiStore = createStore({ /* UI state */ });\n```\n\n### 2\\. Middleware Implementation\n\nAdd middleware support to intercept and transform state updates:\n\n```typescript\nconst createStoreWithMiddleware = (initialState, middlewares = []) => {\n  const store = createStore(initialState);\n  const originalUpdate = store.update;\n  \n  store.update = (fn) => {\n    let result = fn;\n    \n    // Apply middlewares in reverse to compose functions\n    middlewares.slice().reverse().forEach(middleware => {\n      const next = result;\n      result = (state) => middleware(state, next);\n    });\n    \n    originalUpdate(result);\n  };\n  \n  return store;\n};\n```\n\n### 3\\. Selectors for Performance\n\nImplement selectors to derive data from your store without unnecessary re-renders:\n\n```typescript\nconst createSelector = (store, selectorFn) => {\n  let lastState = null;\n  let lastResult = null;\n  \n  return () => {\n    const currentState = store.getSnapshot();\n    \n    if (currentState !== lastState) {\n      lastResult = selectorFn(currentState);\n      lastState = currentState;\n    }\n    \n    return lastResult;\n  };\n};\n```\n\n## Testing Your Custom Store\n\nThorough testing ensures your store behaves as expected:\n\n```typescript\ndescribe('Custom Store', () => {\n  test('should initialize with correct state', () => {\n    const initialState = { count: 0 };\n    const store = createStore(initialState);\n    \n    expect(store.getSnapshot()).toEqual(initialState);\n  });\n  \n  test('should update state correctly', () => {\n    const store = createStore({ count: 0 });\n    \n    store.update(state => ({ count: state.count + 1 }));\n    \n    expect(store.getSnapshot()).toEqual({ count: 1 });\n  });\n  \n  test('watchers should only trigger on condition', () => {\n    const store = createStore({ count: 0, name: 'test' });\n    const mockCallback = jest.fn();\n    \n    store.watch(\n      (old, current) => old.count !== current.count,\n      mockCallback\n    );\n    \n    store.update(state => ({ ...state, name: 'updated' }));\n    expect(mockCallback).not.toHaveBeenCalled();\n    \n    store.update(state => ({ ...state, count: 1 }));\n    expect(mockCallback).toHaveBeenCalledTimes(1);\n  });\n});\n```\n\n## Real-World Considerations\n\nWhen implementing custom stores in production applications, consider these practical aspects:\n\n### Performance Optimization\n\nFor large state objects, consider:\n\n1. Implementing shallow copying instead of deep cloning\n    \n2. Using structural sharing techniques similar to Immer\n    \n3. Adding memoization for derived state\n    \n\n### DevTools Integration\n\nFor debugging purposes, consider adding Redux DevTools support:\n\n```typescript\n// Simplified implementation\nif (typeof window !== 'undefined' && window.__REDUX_DEVTOOLS_EXTENSION__) {\n  const devTools = window.__REDUX_DEVTOOLS_EXTENSION__.connect({\n    name: 'Custom Store'\n  });\n  \n  devTools.init(state);\n  \n  const originalUpdate = store.update;\n  store.update = (fn) => {\n    originalUpdate((state) => {\n      const newState = fn(state);\n      devTools.send('UPDATE', newState);\n      return newState;\n    });\n  };\n}\n```\n\n### Error Handling\n\nAdd robust error handling to prevent store corruption:\n\n```typescript\nconst update = (fn: (data: T) => T) => {\n  try {\n    const newState = fn(state);\n    dispatchWatches(newState);\n    dispatch();\n  } catch (error) {\n    console.error(\"Error updating store:\", error);\n    // Potentially roll back to previous state or implement recovery strategy\n  }\n};\n```\n\n## When to Use Custom Stores vs. Established Libraries\n\nWhile building your own store is valuable, existing libraries have their place:\n\n**Use a custom store when:**\n\n* Your state management needs are specific and well-defined\n    \n* You want minimal dependencies\n    \n* Performance optimization is critical\n    \n* You need complete control over the implementation\n    \n\n**Use established libraries when:**\n\n* You need battle-tested solutions for complex state problems\n    \n* Your team is already familiar with them\n    \n* You require extensive ecosystem support (middleware, devtools, etc.)\n    \n* Time-to-market is a priority over custom implementation\n    \n\n## Conclusion: The Power of Understanding\n\nBuilding your own state management solution provides invaluable insights into how state works in React applications. Even if you ultimately choose to use established libraries, the knowledge gained from creating your own store enhances your ability to debug issues, optimize performance, and make informed architecture decisions.\n\nAs React continues to evolve, the fundamental principles of state management remain constant: maintain a single source of truth, provide predictable update patterns, and efficiently notify components of changes. By understanding these principles at their core, you'll be well-equipped to adapt to whatever new patterns and libraries emerge in the future.\n\nRemember, the goal isn't necessarily to replace existing libraries but to deepen your understanding of the problems they solve. As the saying goes, \"Give someone a library, and they'll build an application; teach someone to build a library, and they'll understand a thousand applications.\"\n\nWhether you use this custom store implementation in production or simply learn from its patterns, the knowledge gained will make you a more effective React developer.",
      "stars": null,
      "comments": 1,
      "upvotes": 28,
      "read_time": "8 min read",
      "language": null
    },
    {
      "title_en": "The internals of TCP: A deep dive",
      "url": "https://oxyprogrammer.com/the-internals-of-tcp-a-deep-dive",
      "source": "hashnode",
      "published_at": "2025-03-07T05:40:58.502000+00:00",
      "external_id": null,
      "tags": [
        "TCP",
        "TCP Handshake",
        "TCP/IP",
        "networking"
      ],
      "content_length": 22665,
      "content_preview": "# Introduction\n\nTCP has been the foundational protocol for numerous higher-level protocols, such as HTTP and WebSockets, due to its guarantee of data integrity. Imagine querying a database table and missing a few rows of dataâ€”that would be catastrophic. Yet, we make database calls without concern about missing data, thanks to TCP's integrity guarantees. In networking, balancing data integrity with latency is a challenge, especially given the unpredictable nature of the internet. We never know ho",
      "content_full": "# Introduction\n\nTCP has been the foundational protocol for numerous higher-level protocols, such as HTTP and WebSockets, due to its guarantee of data integrity. Imagine querying a database table and missing a few rows of dataâ€”that would be catastrophic. Yet, we make database calls without concern about missing data, thanks to TCP's integrity guarantees. In networking, balancing data integrity with latency is a challenge, especially given the unpredictable nature of the internet. We never know how many routers our network calls hop through, and despite many routers or switches potentially dropping packets, data loss is not an issue.\n\nIn the [first article](https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp) of this networking series, we briefly discussed TCP connections, examining how the [OS handles them internally](https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp#heading-os-facilitating-tcp-connection-under-the-hood). We concluded with several open-ended questions. I recommend reviewing [that article](https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp), as it provides an essential piece of the puzzle we aim to complete.\n\nThis article, I believe will clarify your understanding of TCP and dispel misconceptions about this enduring protocol that has stood the test of time for over the past four decades\n\n# IP Packet Header\n\nIn our [previous discussion](https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp), we explored how IP packets exist at Layer 3 of the OSI model, while TCP segments operate at Layer 4. A TCP segment, along with its header, gets stuffed inside the IP Packet data and gets sent.\n\nTo fully understand the subsequent sections, it is important to familiarize ourselves with the IP packet header. Below is a diagram illustrating the IP packet header structure:\n\n```plaintext\n    0                   1                   2                   3\n    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |Version|  IHL  |Type of Service|          Total Length         |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |         Identification        |Flags|      Fragment Offset    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |  Time to Live |    Protocol   |         Header Checksum       |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |                       Source Address                          |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |                    Destination Address                        |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n   |                    Options                    |    Padding    |\n   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n```\n\nSource: [RFC 791](https://datatracker.ietf.org/doc/html/rfc791#page-11)\n\n* **Version**: Specifies the IP protocol version, either IPv4 or IPv6.\n    \n* **IHL (Internet Header Length)**: Indicates the length of the header, including any options.\n    \n* **Type of Service**: Defines aspects like priority and quality of service.\n    \n* **Total Length**: Specifies the packet's total length, including the header and data.\n    \n* **Identification, Flags, Fragment Offset**: These fields are used for packet fragmentation and reassembly if needed.\n    \n* **Time to Live (TTL)**: Limits the lifespan of a packet by defining the maximum number of hops it can make before being discarded. Each router decrements this value by one.\n    \n* **Protocol**: Specifies the protocol used in the data portion (e.g., ICMP, TCP, UDP).\n    \n* **Header Checksum**: Used to verify the integrity of the header data in IPv4 packets.\n    \n* **Source and Destination IP Address**: Specifies the sender's and receiver's IP addresses. At Layer 3, only these IP addresses are involved in routing.\n    \n\nUnderstanding these fields will aid in navigating the complexities of network packet handling in further discussions.\n\n# TCP Connection\n\nThe Transmission Control Protocol (TCP) focuses on controlling data transmission, unlike the more lenient UDP. TCP is methodical about initiating, maintaining, and terminating data transmission. A crucial aspect is the TCP connection.\n\nA TCP connection is bidirectional, enabling protocols like HTTP and WebSocket to operate effectively. It is initiated by the client and accepted by the server. A client is typically an application used directly by the user to initiate operations, while a server is a continuously available application that handles client requests.\n\nTherefore, it's more practical for clients to identify the server rather than vice versa, aligning with software architecture, not any inherent TCP directionality.\n\nTCP, a Layer 4 protocol with port visibility (see the [first article](https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp)), requires an established connection for data transmission. A TCP connection resembles an agreement between a client and server, identified by:\n\n* Client IP\n    \n* Client Port\n    \n* Destination IP\n    \n* Destination Port\n    \n\nEstablishing a TCP connection involves a three-way handshake: SYN, SYN-ACK, and ACK. [TCP segments](https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp#heading-traveling-through-the-layers-of-network) are sequenced and acknowledged. A delay in segment acknowledgment triggers a retransmission, which we will explore shortly.\n\nMultiple connections can exist between the same client and server, with TCP segments multiplexed into a single stream. This stream is then demultiplexed and routed to the appropriate programs listening on relevant ports.\n\n## Connection Establishing\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735036838686/a608f912-0c32-4fcd-96a6-65310e0399cf.png align=\"center\")\n\n## TCP connection establishment consists of three way handshake:\n\n1. The sender sends a SYN request.\n    \n2. The receiver responds with a SYN/ACK message.\n    \n3. The initiator sends back an ACK message, finalizing the connection. Both sender and receiver now have sockets and file descriptors, signifying an established connection.\n    \n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\"><em>For information on File Descriptors and Sockets, </em><a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp#heading-os-facilitating-tcp-connection-under-the-hood\" style=\"pointer-events: none\"><em>read this article</em></a><em>.</em></div>\n</div>\n\n## Transmission of Data\n\nWith the connection established, the sender begins transmitting TCP segments. These segments are acknowledged by the receiver upon receipt.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfVhtArdMgZfcwYWYYgRizU2Dr9gkayhDSsyFkm2-UN2qGD-cP9u0noFdgrUbRV97AgP0l-qwr0twX6tjgvRkKPwAS3ybFp-hwlHoA5Fe0vQHZlTlJYMqdV4h6gW_r1F4IqwWx4rw?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nThe receiver may acknowledge multiple segments with a single acknowledgment. For instance, in the given scenario (refer the diagram above), the sender might send three segments numbered based on the initial SYN connection request sequence. The receiver acknowledges the last segment, implicitly acknowledging prior segments as well.\n\n## Re transmission of Data\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXepcpcuFr8YgJTy2q6E4zb7JAkuof0q10y33A3aNE55muNTlqTpyBPW64SxB83kfwnJWZq0iscIOXNyibwCBz3siGGU67xC7PXaGK6g2c3kQw6hBsb2BVQORymWgnA35WvV0bPn5g?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nFor illustration:\n\n1. The segment with the third sequence number is dropped.\n    \n2. The sender waits for acknowledgement and receives acknowledgment only for sequence 2.\n    \n3. After a timeout, the sender retransmits the segment marked with sequence 3.\n    \n4. The receiver then acknowledges sequence 3.\n    \n\nYou might be thinking what would happen if sequence 2 got dropped while sequence 3 went through. Well that would result in **Line of Head blocking** and we will explore that in the last section of the article.\n\n## Connection Closure and connection States\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd7t0R1GIUF7KlWEge3fb-RikXfUbVu1oJc0A5msLkldC-1mSoLiRHYyxUb7r1i11uG2U5yYhDiMVSvPJc7PiycF9dstKkWcXZlfBEITbG_xsmOZpYO2dcsC5NR_9RSiQmWGEegig?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nWhile a TCP connection is established via a three-way handshake, closing it involves a four-way handshake. The connection state transitions as follows:\n\n1. The sender initiates closure by sending a FIN request, entering the FIN WAIT state.\n    \n2. The receiver receives the FIN, sends back an ACK, and moves to the CLOSE WAIT state.\n    \n3. The sender receives the ACK, moving to the FIN WAIT 2 state.\n    \n4. The receiver enters the LAST ACK state, sending a FIN back.\n    \n5. The sender, on receiving the FIN, moves to the TIME WAIT state, sending a final ACK.\n    \n6. On receiving the last ACK, the receiver transitions the connection to the CLOSED state and waits (usually around 4 minutes) to ensure no more messages are incoming before finally moving to the CLOSED state.\n    \n\nThe responsibility for waiting and closing the connection falls on the initiator, hence the recommendation for the client to initiate the connection. Additionally, the removal of sockets and file descriptors continues even after the connection is closed, as the OS independently manages resource disposal.\n\n# Anatomy of TCP Segment\n\nThe TCP header format is as follows:\n\n*Note: Each tick mark represents one bit position.*\n\n```plaintext\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|          Source Port          |       Destination Port        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                        Sequence Number                        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                    Acknowledgment Number                      |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|  Data |           |U|A|P|R|S|F|                               |\n| Offset| Reserved  |R|C|S|S|Y|I|            Window  Size       |\n|       |           |G|K|H|T|N|N|                               |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|           Checksum            |         Urgent Pointer        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                    Options                    |    Padding    |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                             Data                              |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n```\n\nSource: [RFC 793](https://datatracker.ietf.org/doc/html/rfc793#page-15)\n\nThe header consists of 5 Ã— 4 = 20 bytes. [Additional headers](https://datatracker.ietf.org/doc/html/rfc793#page-15) may be included as options. The Source Port and Destination Port fields specify the ports for the source and destination (the IP addresses are contained in the IP packet header).\n\nThe Acknowledgment Number is only relevant if the ACK flag is set. [The Window](https://datatracker.ietf.org/doc/html/rfc793#page-15) Size field indicates the amount of data the receiver can handle (more on this in the Sliding Window section).\n\nNotable are the 9-bit flags:\n\n* **FIN**: Indicates a connection closure request.\n    \n* **SYN**: Initiates a sequence number for the initial handshake.\n    \n* **RST**: Resets the connection.\n    \n* **ACK**: Acknowledges received data.\n    \n* **URG**: Marks urgent data.\n    \n* **ECE**: Signals congestion notification.\n    \n* **CWR**: Indicates congestion window reduction.\n    \n* **NA**: Notification anomalies.\n    \n\nThe relevance of these flags will become clearer as we explore more aspects of TCP.\n\n# Flow Control\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc2tCgSIqzSO7zCkAGrLo4dWZd4FBPeZxpyc6OJo0zCRMe6meSTFizFQIcWijg_uPAqLt8SUWljPaw52N7YeDcXqI4EsXBMF-P2B1ycnOJXhJOVfbYcvp16z-3irYISxUFMJhg1PA?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nConsider a scenario where the sender wants to transmit Segments 1, 2, and 3 to the receiver. Receiving a single acknowledgment for multiple segments is more efficient. However, the sender must have a way to know how many segments to send before waiting for an acknowledgment. Sending too many segments could overwhelm the receiver's buffer, leading to dropped segments.\n\nThis is where the Window Size field (refer to TCP segment anatomy) comes into play. Each acknowledgment from the receiver includes the current Window Size, informing the sender of how many packets can be sent.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">While the Window Size is significant, there are other factors influencing flow control that we will discuss in upcoming sections.</div>\n</div>\n\n# Receiver (Sliding) Window\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVIDjPEXTIxwPQ0QyFJ2d5mep2VLdN5nR-1BvXFN-xZj85A_k5Io_U1EKzYdUBh0H04PpJ1CHJbOVtVdcH8J8oMOD7Q26z8IjniPNjWEERsCgg4KSEOFxQZ2i29dzqmq5SbHVDNg?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nThe sliding window is a critical mechanism in TCP used by the sender. As illustrated, assume the sender wants to transmit six segments to the receiver.\n\n1. The sender transmits Segments 1, 2, and 3.\n    \n2. The receiver acknowledges Segment 2.\n    \n3. The sender realizes Segment 3 is either still in transit or lost.\n    \n4. The window slides to include Segments 4 and 5, keeping Segment 3 within the window.\n    \n5. Segments 1 and 2 are excluded from the sender's window as they have moved out of the window and may be dropped.\n    \n6. The sender transmits Segments 4 and 5.\n    \n7. Upon receiving acknowledgment for Segment 3, the window slides further to include Segment 6 and remove Segment 3.\n    \n\nThis sliding window mechanism ensures orderly data transmission and efficient use of network resources.\n\n**But what should be the size of this window?**\n\nClearly, the number of segments that are to be sent to the receiver are to be included in the window. The Size of the window gets set with every acknowledgement that comes back (remember the Window Size field in the TCP header?).\n\n# Congestion Control\n\nThe flow of data from sender to receiver in TCP is not solely managed by flow control. Although flow control ensures that the receiver is not overwhelmed, the data must traverse several intermediary devices such as routers and switches. These network elements might not support rapid data flow even if the receiver can handle it. Therefore, TCP also incorporates congestion control to manage data transmission effectively.\n\nIn addition to the Receiver Window (RWND), TCP utilizes a Congestion Window (CWND), which plays a crucial role in congestion control. Itâ€™s important to note that the CWND can never exceed the RWND.\n\nThere are two primary algorithms for determining the size of the CWND:\n\n1. **TCP Slow Start**: This algorithm gradually increases the CWND size to identify the network's capacity without causing congestion.\n    \n2. **Congestion Avoidance**: This algorithm aims to optimize data flow by adjusting the CWND size to avoid congestion once the initial network capacity has been identified.\n    \n\nLet's explore each of these algorithms in detail.\n\n## TCP Slow Start\n\nIronically, despite its name, TCP Slow Start is actually the fastest among the congestion control algorithms.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe-_OUVd-jkJbTKjQ5uBNy6vhd3BE6Na9_UcRZMBgE4I_vGNDVhmjSrX8vys2DB-kmo_-eqSNd_eGk_DypXxJPtpFSdlbcLXSc3C5r_dFY86HNFrXVp8QfyKHLaAOboRHCEcd7u?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nIn the example illustrated above:\n\n1. The Congestion Window (CWND) begins with a capacity of 1 segment. Accordingly, only one segment is sent initially.\n    \n2. Upon receiving acknowledgment from the receiver, the CWND is increased by 1.\n    \n3. The sender then transmits two segments: Segments 2 and 3.\n    \n4. In response, the receiver sends back acknowledgments for both segments (2 and 3). As a result, the CWND increases by 2 (1 for each acknowledgment received).\n    \n5. With the CWND now allowing for larger transmission, the sender proceeds to send Segments 4, 5, 6, and 7.\n    \n\n## Congestion Avoidance\n\nCongestion Avoidance algorithm also increases the CWND but at a slower rate than TCP Slow start.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc6IhmnSLsQeVlDxg7BWg7OpPv78JiW6Fqf0N8q2oKnKL3rvM4mkQ527m9xD-TpcDpD_Qd2KacK_FvSo3eMHi9X8aIk_Pz8-Wd9h_q7LM8L505TWJvCJFdNSvNLD935UMX3vdofiw?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nIn the above diagram example:\n\n1. The Congestion Window (CWND) begins with the capacity for 1 segment, so only one segment is initially sent.\n    \n2. Upon receiving an acknowledgment from the receiver, the CWND is increased by 1.\n    \n3. The sender then transmits two segments: Segments 2 and 3.\n    \n4. The receiver sends back acknowledgments for both segments. The CWND is increased by 1 for this entire round trip, as it pertains to the single window of data sent.\n    \n5. With the updated CWND, the sender now sends Segments 4, 5, and 6.\n    \n\n# Congestion Notification\n\nRouters operate at Layer 3 of the OSI model, providing them visibility into IP packets. IP packets include a field known as ECN (Explicit Congestion Notification). When routers detect their buffers are becoming full, they mark the ECN field in the IP packets and forward them to the receiver. Upon receiving these packets, the receiver notes the ECN marking in the IP headers and communicates this information back to the sender. The sender willÂ  reduce the transmission rate to alleviate congestion.\n\n# Nagleâ€™s Algorithm\n\nNagleâ€™s algorithm specifies that if there are in-flight segmentsâ€”meaning segments that have been sent but for which an acknowledgment (ACK) has not yet been receivedâ€”an IP packet will only be transmitted if it is completely filled. Conversely, if there are no in-flight segments, a packet will be sent even if it is only partially filled.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfI9AzDg889hvsguUc6GZ4IUR61iuXovpIY72OK3626xxLKMk-zYR4p0ip1PE3UgzxwnDhN-M5GrImcteUb3pOuL04KxwC9JGc5U_oysNb-50D6upIJPyzGOrW1rQXUs9OnH4QrLw?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nIn the diagram example provided:\n\n* Three completely filled IP packets are sent, while a partially filled packet is held back.\n    \n* Once the acknowledgments for the three packets are received, the partially filled packet is then transmitted.\n    \n\nNagleâ€™s algorithm is often disabled in modern networking practices because it can introduce additional latency.\n\n# Delayed Acknowledgement Algorithm\n\nThe Delayed Acknowledgment Algorithm is implemented on the receiver's side. This algorithm suggests that the receiver should wait to receive multiple packets before sending an acknowledgment (ACK). By doing so, the number of acknowledgments sent is reduced, which can help decrease overall latency in the communication process.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfiuNi68X7lZ0thL7iEl3e_-A1CmM9egQMJYG-yOLegRBxk_u-8buxG4DQLSnKTLvJMeXbUfCEah0GF2QvAybSFXC3LCfOyPKrBBA6e39xZ_Qxthvi25MXrEtYoTK5Po2L-palL?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nA problem arises when both Nagleâ€™s algorithm and the Delayed Acknowledgment Algorithm are used simultaneously. Nagleâ€™s algorithm, which operates on the sender's side, holds back a packet until an acknowledgment (ACK) for previously sent segments is received. Meanwhile, the Delayed Acknowledgment Algorithm, implemented on the receiver's side, waits to receive multiple segments before sending back an ACK.\n\nThis combination can create a deadlock-like situation, where the sender is waiting for an ACK that is not being sent because the receiver is holding off until it receives additional segments. As a result, this can lead to retransmissions and timeouts, negatively impacting network performance.\n\n# TCP Head of Line blocking\n\nTCP ensures that segments are delivered in the order they are sent. Line of Head Blocking occurs when a segment in the middle of a sequence gets dropped, which particularly impacts HTTP requests because they often use the same connection to send multiple requests.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXduJXUlbZFCSo1pqqh4UhPzzjo2LFZEEQMhDdec4bVeT3hxDY2fuQoK-vL8xi42lHnRTCsQSbTFxxrPCqMg_A48rS05ECE3gmgAdsKoMdCyHM1M3ckMhKksKendaRIeEkuYrI_soA?key=allS53MX6v5H-VzZb3VLXJ-m align=\"left\")\n\nConsider the diagram example illustrated above:\n\n* Request 1 is divided into Segments 1 and 2.\n    \n* Request 2 is divided into Segments 3 and 4.\n    \n\nIf Segments 2, 3, and 4 are successfully transmitted but Segment 1 is dropped, the server will not send acknowledgments (ACKs) for Segments 2, 3, and 4 until Segment 1 is retransmitted.\n\nAs a result, Request 2 suffers delays even though it was fully transmitted, because it is dependent on the status of Request 1.\n\nThis scenario exemplifies Line of Head Blocking, where the processing of one request is held up due to the loss of an earlier packet.\n\n# Conclusion\n\nIn this comprehensive article, we built upon the foundation established in a [previous piece](https://oxyprogrammer.com/navigating-networking-layers-the-journey-of-data-through-tcp) and delved into the internals of TCP communication. We began by examining the anatomy of IP packets and then explored the mechanisms involved in TCP connection creation and closure.\n\nWe discussed various mechanisms, such as Flow Control and Congestion Control, that TCP employs to ensure smooth and reliable data transmission. The sliding window technique facilitates controlled data flow, allowing for efficient management of both flow and congestion.\n\nAdditionally, we reviewed Nagleâ€™s algorithm and the Delayed Acknowledgment Algorithm, both designed to reduce transmissions for improved efficiency. However, we noted that their combined use can lead to counterproductive outcomes. Finally, we addressed the concept of Line of Head Blocking, illustrating its impact on data transmission.\n\nI trust that this article has contributed to your understanding and provided valuable insights into the intricacies of TCP internals.",
      "stars": null,
      "comments": 3,
      "upvotes": 30,
      "read_time": "13 min read",
      "language": null
    },
    {
      "title_en": "Atomic Habits for Developers",
      "url": "https://blog.lo-victoria.com/atomic-habits-for-developers",
      "source": "hashnode",
      "published_at": "2025-02-10T01:00:20.918000+00:00",
      "external_id": null,
      "tags": [
        "Self Improvement ",
        "Developer",
        "Programming Tips"
      ],
      "content_length": 10675,
      "content_preview": "As a developer, your journey to mastery is often shaped by the small, consistent actions you take every day. Whether you're debugging code, learning a new framework, or optimizing a system, the key to continual growth lies in your habits.\n\nThis weekend, I re-read one of my favourite books, James Clear's *Atomic Habits*, and was inspired to apply the principles of habit formation to become more effective developers and accelerate our learning process.\n\n> \"Success is the sum of small efforts, repe",
      "content_full": "As a developer, your journey to mastery is often shaped by the small, consistent actions you take every day. Whether you're debugging code, learning a new framework, or optimizing a system, the key to continual growth lies in your habits.\n\nThis weekend, I re-read one of my favourite books, James Clear's *Atomic Habits*, and was inspired to apply the principles of habit formation to become more effective developers and accelerate our learning process.\n\n> \"Success is the sum of small efforts, repeated day in and day out.\" - Robert Collier\n\n# Why Habits Matter for Developers\n\nIn the fast-paced world of technology, developers face constant pressure to keep up with new languages, frameworks, tools, and best practices. But mastery doesnâ€™t happen overnight. The most successful developers arenâ€™t necessarily the most naturally talented; rather, they are the ones who build and maintain productive habits over time.\n\nHabits allow us to automate parts of our daily routines, freeing up mental bandwidth for more complex problems. By making small, incremental improvements each day, we can steadily progress and build a solid foundation of knowledge and expertise.\n\n# The Four Laws of Atomic Habits\n\nIn his book, James Clear outlines four key laws that govern habit formation:\n\n1. **Make it Obvious**\n    \n2. **Make it Attractive**\n    \n3. **Make it Easy**\n    \n4. **Make it Satisfying**\n    \n\nLetâ€™s break down how we can apply these laws to our growth as developers.\n\n## 1\\. Make it Obvious: Set Clear, Specific Goals\n\nAs a developer, the first step to cultivating new habits is to make your goals clear and specific. Rather than saying, \"Iâ€™ll improve my coding skills,\" you can break it down into specific actions like:\n\n* **Write one unit test per day.**\n    \n* **Solve one coding challenge every morning.**\n    \n* **Read one technical blog or tutorial daily.**\n    \n\nThe more specific your habit, the easier it is to follow. Make these goals visibleâ€”perhaps with a checklist on your workspace or a calendar where you can mark off completed tasks.\n\n## 2\\. Make it Attractive\n\n### Connect Habits to Something You Enjoy\n\nTo keep your motivation high, tie your technical habits to something you find rewarding. For example, after completing a task such as finishing a tutorial or debugging a complex problem, reward yourself with a small break or something you enjoy, like watching a short episode of a favorite series.\n\nIf coding feels like a grind, channel your passion into the projects you build. Choose themes or subjects that resonate with you personally. For example, I often create side projects related to PokÃ©mon, my favorite childhood game. Whether itâ€™s a PokÃ©dex built with a custom trained ML model, or a strategy simulator coded to experiment with APIs, these projects allow me to connect learning with something I love. If youâ€™d like to see an example of this approach, check out my [*Victoriaâ€™s Edition: My Journey into Tech*](https://lo-victoria.com/victorias-edition-my-journey-into-tech) article, where I shared how these personal projects fueled my growth.\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1660831885539/Jjwq9Yc5m.png?auto=compress,format&format=webp align=\"left\")\n\n### **Turning LeetCode Questions into Relatable Problem-Solving Challenges**\n\nLeetCode and similar coding challenge platforms are great for sharpening your problem-solving skills, but they can sometimes feel dry or abstract. Make these exercises more enjoyable by reframing them into challenges you can relate to. Instead of just solving a binary tree traversal, imagine you're designing a feature for a real-world application youâ€™re excited about.\n\nFor example:\n\n* **Sorting algorithms**: I would imagine Iâ€™m organizing PokÃ©mon cards by rarity or type.\n    \n* **Graph problems**: Picture yourself mapping out the shortest route to visit multiple cafes for cafe-hopping.\n    \n* **Dynamic programming**: Think of ways to optimize a strategy in a favorite video game.\n    \n\nThis mental reframing not only makes problem-solving more fun but also helps you retain concepts better by associating them with something meaningful to you. Over time, this approach can transform coding challenges into opportunities for creative expression.\n\n## 3\\. Make it Easy: Start Small and Build Gradually\n\nIncorporating habits into your daily routine can seem overwhelming at first, especially if you're trying to learn a new programming language or framework. Instead of jumping into complex projects, start small. Set achievable goals that are easy to start with and build on them over time.\n\nFor example:\n\n* **Instead of committing to write a full-feature application, begin with writing small functions.**\n    \n* **Take on the 30DaysOfCode or 100DaysOfCode challenge to stay consistent and code a bit every day.**\n    \n\nBy starting small, you can avoid the burnout that comes from setting overly ambitious goals, making it easier to build momentum. If you want to learn more about burnout, read [Why Burnouts Should Not be Ignored](https://lo-victoria.com/why-burnouts-should-not-be-ignored) (shameless plug).\n\n![Why Burnouts Should Not Be Ignored](https://cdn.hashnode.com/res/hashnode/image/upload/v1610337137584/wqaabJeJs.png?w=1600&h=840&fit=crop&crop=entropy&auto=compress,format&format=webp align=\"left\")\n\nðŸ’¡ Tip: Write down your goals and break them into smaller milestones to work towards gradually. Have a clear set of daily actions that will bring you closer to those milestones.\n\n## 4\\. Make it Satisfying: Track Your Progress and Celebrate Wins\n\nOne of the most powerful aspects of habit formation is **the sense of progress**. When we feel we're making headway, we're more likely to continue. Use a habit tracker to monitor your progress or log the tasks you've completed, whether it's with a simple note-taking app, or one you create yourself.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735916399233/061d4691-9200-4530-a55c-6ab7e08f0617.png align=\"center\")\n\nCelebrate when you hit milestonesâ€”this could be anything from completing a challenging project to mastering a new framework. For instance, after completing a month of daily coding challenges, treat yourself to a well-earned reward. These small wins reinforce the behavior and make it easier to keep going.\n\nðŸ’¡ Tip: Have an accountability buddy so they can celebrate your wins with you!\n\n# Good Habits for Every Developer\n\n## 1\\. Writing Clean, Readable Code\n\nClean code isnâ€™t just about aesthetics; itâ€™s the backbone of maintainable and scalable software. Whether youâ€™re working solo or as part of a team, readable code makes your work easier to understand, debug, and improve over time.\n\nWe should aim to write meaningful variable and function names and follow best practices and coding standards. Refactoring, though often overlooked, is another key practiceâ€”regularly revisiting and refining your code can work wonders for its longevity and clarity.\n\n## 2\\. Communicating and Collaborating Effectively\n\nWe should not only write readable code, but also build the habit of documenting it well. Strong communication and collaboration skills are never overrated. Writing clear documentation ensures your work remains accessible to anyone who encounters it. Engaging in code reviews and brainstorming sessions not only sharpens your skills but also fosters a culture of learning and collaboration.\n\n## 3\\. Staying Curious and Always Learning\n\nTechnology doesnâ€™t stand still, and neither should you. Continuous learning is a habit every developer should embrace. Explore new tools, frameworks, or programming languages that pique your interest. Keep asking questions, experimenting, and seeking new or creative solutions to problems.\n\nðŸ’¡ Tip: Joining a community is one way to keep learning, share knowledge and seek feedback to soluions!\n\n## 4\\. Prioritizing Your Health\n\nPerhaps the most overlooked habit in our field is maintaining good health. Hours of screen time, poor posture, and long periods of inactivity can lead to significant physical and mental strain. Iâ€™ve had my share of health challengesâ€”eye abrasions, sprained shoulders, the occasional static electricity shock, and persistent eye bags, to name a few. Honestly, you might feel invincible in your 20s, but these experiences have taught me that neglecting your health will eventually catch up with you.\n\nTo counter the poor default working conditions, I invested in an ergonomic chair, <s>maintain good posture</s>, <s>cut the caffeine</s>, grow plants to keep my room well-lit and set boundaries between work and personal life. Iâ€™m still very much working on it, but these are simple yet effective ways to sustain your (and probably my) well-being. Remember to also take frequent breaks to stretch and drink water.\n\n## 5\\. Reflecting on Your Journey\n\nBeing a good developer isnâ€™t measured by the number of lines of code you write. Itâ€™s about learning, growing, and appreciating the path youâ€™ve taken. Reflection is a powerful habit that allows you to recognize your achievements and identify areas for improvement.\n\nSome developers maintain daily or weekly journals to log their experiences and lessons. Personally, I prefer doing a yearly reviewâ€”a detailed reflection that encapsulates my thoughts, challenges, and milestones. My [2024 Year in Review](https://lo-victoria.com/2024-year-in-review) is an example of this process. Throughout the year, I jot down notes about what I learn or even random thoughts, and by the yearâ€™s end, I organize them into a cohesive narrative. Itâ€™s my way of looking back, thanking those whoâ€™ve supported me, and planning for whatâ€™s next. Whether you journal regularly or take a more periodic approach, the key is to find a rhythm that works for you.\n\n# Conclusion\n\nAs a developer, building strong habits isnâ€™t just about staying productiveâ€”itâ€™s about fostering a growth mindset that helps you adapt, learn, and thrive in a fast-changing field. By applying the principles of *Atomic Habits*, you can break down complex goals into achievable steps, make learning more enjoyable, and track your progress effectively. Over time, these small changes will lead to remarkable improvements in your coding abilities, your workflow, and your career.\n\nThanks for reading! I hope you have found this article helpful! Do let me know in the comments if there are any other practices to develop good habits for developers. Also, besides the ones I mentioned, what are some good habits for developers? Cheers!\n\n### **Let's Connect!**\n\n* [**Twitter**](https://twitter.com/lo_victoria2666)\n    \n* [**LinkedIn**](https://www.linkedin.com/in/victoria2666/)\n    \n* [**GitHub**](https://github.com/victoria-lo)",
      "stars": null,
      "comments": 6,
      "upvotes": 170,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "Beyond Lambda Functions: Mastering Serverless Development",
      "url": "https://soprinye.com/beyond-lambda-functions-mastering-serverless-development",
      "source": "hashnode",
      "published_at": "2025-02-04T12:00:53.350000+00:00",
      "external_id": null,
      "tags": [
        "serverless",
        "AWS",
        "aws lambda"
      ],
      "content_length": 11474,
      "content_preview": "## Introduction\n\nAWS Lambda service is a critical component of serverless applications. It is one of the compute services for FaaS on AWS. When you deploy your functions to the cloud, AWS Lambda service is responsible for the execution. Knowing how to write and deploy functions is but a small percentage of what is required to build scalable serverless applications on AWS; it is a critical part not the only part. It is important to get ideas that can help when developing serverless applications.\n",
      "content_full": "## Introduction\n\nAWS Lambda service is a critical component of serverless applications. It is one of the compute services for FaaS on AWS. When you deploy your functions to the cloud, AWS Lambda service is responsible for the execution. Knowing how to write and deploy functions is but a small percentage of what is required to build scalable serverless applications on AWS; it is a critical part not the only part. It is important to get ideas that can help when developing serverless applications.\n\nThis article contains some of the lessons learned in my 5 years of writing serverless applications that go beyond just knowing about AWS Lambda.\n\nLetâ€™s dive right in!\n\n## Use Diagrams asÂ a Guide\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735887029760/75225274-b26b-4991-a602-fbceb0b0c975.png align=\"left\")\n\nDiagrams have been a great tool for software engineers for a long time and it did not change with serverless development. Iâ€™ve worked with engineering teams that start working on large-scale systems without a simple diagram and what you hear is: â€œAs you use the application you will get to know what it does and what happens when request â€˜xâ€™ is madeâ€. Maybe because of some requirements or something Iâ€™m not aware of, there is no simple documentation of the systemâ€™s architecture. I have used diagrams in most of my large-scale architectures and they have been helpful. Some reasons for having a diagram as a guide include:\n\n### Birdâ€™s-eye view of required components\n\nIn developing micro-services, itâ€™s safe to say that the unit of deployment is a serviceâ€Šâ€”â€Ša service is self-contained and has all the required functions it needs to carry out its operation. For serverless APIs, the unit of deployment is a function. Yes!, a function. However, in most, if not all cases, this function is not the only piece of your API. The function may need to read or write data from a data store, interact with data in an SQS queue or EventBridge, and many other components. The fact is there are a lot of moving parts that need to be effectively coordinated. With a diagram, you can see those parts and how they interact with each other to bring a feature to life.\n\n### Similar component concern\n\nThe cloud offers similar products to achieving a result, of course with subtle differences and it is important to pick the right service given the right context to enable us to achieve our goal efficiently. After a system has been designed, with a diagram, you can quickly and easily depict if a service in use is the most efficient given the context of the requirement â€Šâ€”â€Š*you must however understand how each service operates in detail and avoid assumptions*. Sometimes, you may have designed your architecture based on some limited requirement or even on assumptions. It is also possible that some parts of the system were rushed to quickly come up with a POC - that is fine. Having your solution in a diagram helps you understand what changes need to be done and how they affect the overall architectural solution. Diagrams also help in cases where thereâ€™s a change to the requirement; it becomes easy to spot the service that should be replaced if needed and where.\n\n### Review solutions across the development life cycle\n\nSoftware development is likened to the Civil Engineering discipline where in most cases, a building plan is drawn before actual construction begins. If this happens, then having a diagram allows you the opportunity to have your thoughts *on paper(on screen)* which effectively helps you review your solutions before implementation. There could be a better solution than what you originally documented. Let me also advocate that the software architecture diagram be seen as a â€œliving documentâ€ in the sense that, it could change or be updated as the actual development progresses. This happens as you learn more about a domain or realise a better service for the job. Hence, you do not need a 100% complete diagram before you commence development. Having a visual representation of your solution, helps you make adjustments even faster as you implement your solution.\n\n### Fast development experience\n\nI have noticed I go faster if I have a diagram that I can refer to as I implement a solution. This can be likened to goal setting and working backwards to implement the goal. It gives a feeling of having done it before. When you know the requirements and have documented the necessary services required to fulfil them, it's only a matter of your knowledge of the implementation. You really can go faster with a prepared diagram versus having to plan, think and develop almost simultaneously. It is useful here to remember that your code is not the solution - it is only an implementation of a solution.\n\n## Understand the details of a service\n\nIt is typical in software engineering to start with a â€œhello worldâ€ type example when trying to learn a technology. It is ok and I am not against itâ€”â€Šitâ€™s ok to get started with an example of how to use a service; it helps you quickly see requirements as well as simple outputs. However, to get the best and customise to your requirements, you will need more than a hello-world type experience. To become really good at using a service, you need to get practical with the service with as many use cases as you need to. Some of the ways to get started with a service include reviewing the documentation and reading an in-depth blog or an open-source repository with examples. However, please note that while those ways of learning are really good, they could focus on certain aspects of services, not the full capabilities. For example, itâ€™s general knowledge that Single Table Design in DynamoDB is very good. However, may not be suitable for all types of operations. Articles, blog posts, etc. have a focusâ€Šâ€”â€Što get you to see a few things. Aside from all these, a progressive hands-on on the service is by far the best way to get a very good understanding of how the service operates, including its limitationsâ€Šâ€”â€Šthis helps you decide when to pick â€˜*service aâ€™* over â€˜*service b*â€™ when solving your requirements challenge.\n\n## Adopt EDA Early - For Efficient ScalingÂ needs\n\nOne of the frequently talked about features of Lambda is that; it scales according to the number of requests. This means that a new instance of your lambda function is created to handle requests as they come in especially if a previous instance is â€œ*still busyâ€*. It is true that lambda scales. However, understanding its limitations is key to its efficient use and mitigation of unexpected behaviours.\n\nLambda does not scale infinitely, it has certain limits relating to the region of deployment and the number of functions that can be executed per time. Lambdaâ€™s concurrency limit causes requests to be throttled when the limit is reached. Therefore, to build efficient systems with efficient scaling capabilities, you need to consider Event-Driven Architectures. It is very easy to postpone learning about EDAs in Serverless Architecture to a later time but I recommend that it is brought forward because it is very important and fundamental to your learning and understanding as a serverless developer and in developing efficient and scalable serverless architectures.\n\nTwo very fundamental aspects of EDAs are Queues and EventBuses. Fundamental knowledge of Amazon SQS and Amazon EventBridge can help you build efficient and scalable solutions.\n\n## What does that framework do? You may need to isolate itâ€™s orchestration\n\nFrameworks help develop serverless applications because they provide some level of abstraction and structure as to how the application is written coupled with the ease of deployment.\n\nHaving developed several serverless applications using frameworks like AWS SAM and Serverless Framework, it is important to note that some aspects of the service or component are automated for the engineer. A good example is when AWS SAM or Serverless Framework automatically creates one of the critical components in the serverless API: the API Gateway.\n\nIn creating a custom API Gateway resource, the following resources are needed: `RestApi`, `Resource`, `Stage`, `Method`, `Deployment` and if you are using a custom domain, you will need both a `BasePathMapping` and `DomainName` resource.\n\nIn the SAM snippet below, just by specifying the `Api` type in the event source of the function, AWS SAM automatically creates those components for you - (talk about making your life easy) - and after deployment is done, youâ€™re greeted with a nicely formatted API Gateway URL with which you can access your APIs. A similar orchestration happens with the Serverless Framework.\n\n```yaml\n  CheckoutFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: src/Checkout/index.handler\n      Description: Performs order checkout\n      MemorySize: 128\n      Events:\n        Api:\n          Type: Api\n          Properties:\n            Path: /checkout\n            Method: POST\n```\n\nThis is useful as necessary - however, there will be times when you will need to orchestrate an API Gateway - hence, will need to define custom resources as listed above. This means you will need to know how to build an API Gateway resource just like the framework does so that you can decompose, isolate and build up as necessary. The lesson here is defaults are good, however, custom resources will need more than defaults.\n\n## Get Comfortable with IaC - Infrastructure as Code\n\nIâ€™ve been involved in projects where some resources like custom domain names are created via the AWS console. You soon find out that as the project progresses, such resources created become a breaking point for a smooth CI/CD process because they have been manually created and it is possible to forget that such resources were created (this happens a lot with S3 buckets).\n\nIaC becomes critical when you work in a multi-account setup and you have to deploy services or environments across accounts in an efficient way - most times, offloaded to a CI/CD process - which is the best practice.\n\nIt is possible to become overly focused on functions. However, Infrastructure as Code is critical to your success in developing serverless applications. This includes defining resources like Buckets, Queues, Event Bridges, and others using code that can be checked into source control and integrated with a CI/CD process for an efficient and fast integration and deployment loop. This ensures consistency across environments, as multi-accounts are becoming common. You need to get comfortable with IaC.\n\n## Summary\n\nThis article shares insight from my journey in developing Serverless Applications on AWS. It looks beyond Lambda functions as the focus of Serverless Applications and shares ideas that I can help an engineer become better at building serverless applications including using flow diagrams for architecture visualisation, understanding the limitations of Lambda and the need for event-drive architectures. It also talks about understanding cloud service detail to make the right judgement as to what service to incorporate in architecture, with a note on the usefulness of IaC.\n\nSome common tools used include:\n\n* Diagramming\n    \n    * Lucid Chart\n        \n    * Draw.io\n        \n    * ExcaliDraw\n        \n* IaC/Frameworks\n    \n    * Pulumi\n        \n    * Terraform\n        \n    * AWS CDK\n        \n    * AWS SAM\n        \n    * Serverless Framework\n        \n\nI hope this is helpful. Let me know your thoughts in the comment section.",
      "stars": null,
      "comments": 0,
      "upvotes": 11,
      "read_time": "9 min read",
      "language": null
    },
    {
      "title_en": "The Latest JavaScript Security Vulnerabilities in NPM Packages",
      "url": "https://sandworm.hashnode.dev/the-latest-javascript-security-vulnerabilities-in-npm-packages",
      "source": "hashnode",
      "published_at": "2025-02-03T12:01:09.312000+00:00",
      "external_id": null,
      "tags": [],
      "content_length": 5775,
      "content_preview": "# The Latest JavaScript Security Vulnerabilities in NPM Packages\n\nHello and welcome to another edition of our ongoing series on JavaScript security! This week, Sandworm Monitor has identified several high-risk vulnerabilities in a wide array of NPM packages. We will delve into some of the most pressing issues and provide insights into best practices for developers to mitigate these threats. \n\n## Unmasking Malicious Postinstall Scripts\n\nOne repeat offender in our vulnerability assessments is the ",
      "content_full": "# The Latest JavaScript Security Vulnerabilities in NPM Packages\n\nHello and welcome to another edition of our ongoing series on JavaScript security! This week, Sandworm Monitor has identified several high-risk vulnerabilities in a wide array of NPM packages. We will delve into some of the most pressing issues and provide insights into best practices for developers to mitigate these threats. \n\n## Unmasking Malicious Postinstall Scripts\n\nOne repeat offender in our vulnerability assessments is the misuse of postinstall scripts. These scripts are meant to perform setup tasks when a package is installed, but they can also be exploited to carry out harmful actions. \n\nTake the case of [`soroswap-utils`](https://sandworm.dev/npm/security-vulnerabilities/package/soroswap-utils). The package's postinstall script can source environment variables from sensitive files, providing a backdoor for attackers to access private information or run malicious Node.js scripts. This vulnerability highlights the need for developers to closely scrutinize the postinstall logic within third-party packages. Consider disabling these scripts entirelyâ€”or reviewing and auditing them rigorouslyâ€”before executing.\n\nAnother notable example comes from [`sysaid-infra-analytics`](https://sandworm.dev/npm/security-vulnerabilities/package/sysaid-infra-analytics), where the postinstall script scrapes sensitive system information and exfiltrates it to an external server. This kind of data breach can have widespread ramifications, from privacy violations to more targeted attacks based on harvested data. As a countermeasure, developers should evaluate the need for postinstall scripts, remove unnecessary external communications during setup, and employ strict network egress controls.\n\n## Exposing Critical System Files and User Data\n\nSeveral packages were found using scripts that read and transmit system-critical files to remote servers. For instance, [`pages-e2e`](https://sandworm.dev/npm/security-vulnerabilities/package/pages-e2e) uses a preinstall script to send the `/etc/passwd` file to a remote location, which is an alarming security risk.  \n\nSending such files without explicit user consent violates privacy and can lead to extensive damage, such as unauthorized access to user accounts. Developers must avoid granting their packages unnecessary access to system files and ensure any system interaction is essential, transparent, and secure. Moreover, it is crucial to conduct thorough code reviews and audits of any scripts that access system files.\n\n## Reverse Shells and Arbitrary Code Execution\n\nSome packages showed the tendency to establish reverse shell connections, which is a grave security danger. The [`overture2osmjs`](https://sandworm.dev/npm/security-vulnerabilities/package/overture2osmjs) package is one such example, where a reverse shell is spawned, allowing an attacker full control over the victim's machine. These kinds of scripts highlight the importance of ensuring package integrity and understanding the trustworthiness of package sources.\n\nThe risk here can be curbed by validating shell scripts and ensuring that software is only fetched from legitimate and secure sources. Integrating a tool like Sandworm Audit into your development workflow can help track direct and transitive dependencies for potential vulnerabilities, assisting in better management and reconciliation of risk dependencies. Check it out here: [Sandworm Audit](https://github.com/sandworm-hq/sandworm-audit).\n\n## Protecting Against Code Execution Vulnerabilities\n\nMany packages have scripts that can execute arbitrary commands, either through shell constructs or by altering existing code bases. A significant example is the [`requests-async`](https://sandworm.dev/npm/security-vulnerabilities/package/requests-async) package, which downloads remote PowerShell scripts, adjusts execution policies, and even sets scripts to run persistently at system startup. These actions create vectors for attackers to plant malware or establish enduring backdoors.\n\nTo counter such threats, developers should avoid executing commands dynamically from external sources within scripts. If such behaviors are absolutely necessary, stringent validation and integrity checks should be enforced. Employ network-based security policies and scrutinize all code dependencies for unwanted command execution abilities.\n\n## Recommendations for Developers\n\nIn light of these vulnerabilities, it is vital for developers to incorporate robust security practices into their CI/CD pipelines as well as into their development lifecycle:\n\n- **Review and Audit:** Regularly perform comprehensive security reviews of all code including third-party dependencies.\n- **Limit Script Permissions:** Utilize package.json scripting functionality judiciously, and limit post-installation scripts' permissions and capabilities whenever possible.\n- **Use Sandworm Audit:** Make use of Sandworm Audit to gain insights into vulnerabilities and stay updated with the latest security advisories.\n- **Securely Manage Secrets:** Avoid storing sensitive configuration directly within the codebase, and leverage secure vaults or environment variables for the safe management of credentials.\n- **Implement Network Controls:** Employ network egress filters and multi-factor authentication to protect against unauthorized data exfiltration and account compromise.\n\nBy adopting these best practices, developers can significantly enhance the security posture of their JavaScript applications, keeping potential threats at bay, and ensuring user data remains protected.\n\nStay safe and secure in your coding endeavors, and remember: constant vigilance is our first line of defense against the ever-evolving threat landscape. Happy coding!\n\n",
      "stars": null,
      "comments": 0,
      "upvotes": 19,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Run Deepseek-R1 Locally with Ollama & Open WebUI in Docker â€” Access It Anywhere!",
      "url": "https://prakasrana.hashnode.dev/run-deepseek-r1-locally-with-ollama-open-webui-in-docker-access-it-anywhere",
      "source": "hashnode",
      "published_at": "2025-01-29T02:21:03.329000+00:00",
      "external_id": null,
      "tags": [],
      "content_length": 5308,
      "content_preview": "---\n\n### Introduction:\n\nThe groundbreaking Chinese reasoning model, **Deepseek-R1**, has stunned the world with its exceptional performance across benchmarks. While its capabilities are exciting, some users may feel uneasy about running it directly through the official site due to privacy concerns.\n\nThe good news? Since Deepseek-R1 is open source, you can run it locally on your computer â€” and better still, make it accessible from anywhere in the world. Yes, itâ€™s possible, and hereâ€™s how to do it",
      "content_full": "---\n\n### Introduction:\n\nThe groundbreaking Chinese reasoning model, **Deepseek-R1**, has stunned the world with its exceptional performance across benchmarks. While its capabilities are exciting, some users may feel uneasy about running it directly through the official site due to privacy concerns.\n\nThe good news? Since Deepseek-R1 is open source, you can run it locally on your computer â€” and better still, make it accessible from anywhere in the world. Yes, itâ€™s possible, and hereâ€™s how to do it!\n\n---\n\n### How to Get Started:\n\nWeâ€™ll use **Ollama** to run the model locally, **Open WebUI** for a clean and user-friendly interface, and **ngrok** to expose your local instance globally. Letâ€™s break it down step by step.\n\n---\n\n### Step 1: Setting Up Ollama & Deepseek\n\n1. **Download Ollama:**  \n    Grab it from the official website: [ollama.com/download](https://ollama.com/download).\n    \n2. **Search and Download Deepseek-R1:**  \n    Visit [ollama.com/search](https://ollama.com/search) and find the Deepseek-R1 model.\n    \n    * If youâ€™re working with limited resources, you can use the 1.5B parameter version. Run the following command to download and start it locally:\n        \n        ```bash\n        ollama run deepseek-r1:1.5b\n        ```\n        \n\n---\n\n### Step 2: Setting Up Open WebUI\n\nNow that Deepseek-R1 is running in your terminal, letâ€™s set up a polished user interface with Open WebUI.\n\n1. **Visit the Open WebUI GitHub:**  \n    Find setup instructions on their official page: [github.com/open-webui/open-webui](https://github.com/open-webui/open-webui).\n    \n2. **Run the Docker Image:**  \n    Use this command to pull and run the Open WebUI Docker image:\n    \n    ```bash\n    docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n    ```\n    \n3. **Access Open WebUI:**  \n    Open your browser and navigate to [http://localhost:3000/](http://localhost:3000/). Register and log in, then select **Deepseek-R1:1.5b** from the dropdown menu. Thatâ€™s it! Your model is now live on a sleek web interface.\n    \n\n---\n\n### Step 3: Make It Accessible Anywhere with ngrok\n\nTo access Open WebUI from any device, weâ€™ll use ngrok to expose your local server.\n\n1. **Download ngrok:**  \n    For macOS users, download it here: [ngrok.com/downloads/mac-os](https://ngrok.com/downloads/mac-os).\n    \n2. **Set Up Your Auth Token:**  \n    Use the following command to configure ngrok:\n    \n    ```bash\n    ngrok config add-authtoken <your_token>\n    ```\n    \n3. **Expose Your Port:**  \n    Run this command to expose port 3000 (where Open WebUI is running):\n    \n    ```bash\n    ngrok http 3000\n    ```\n    \n\n**ngrok display:**\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1738194704691/e57d374e-067e-4e19-9d24-17163d439b33.png align=\"center\")\n\n**Access from Anywhere:**  \nCopy the provided URL (e.g., `https://my-localhost123.ngrok-free.app`) and open it on any device. Your locally hosted Deepseek-R1 instance is now accessible globally!\n\n---\n\n### Extra Steps:\n\n### Using a Docker Compose YML File:\n\nCreate a **docker-compose.yml** file with the following configuration.\n\n```bash\nversion: \"3.8\"\nservices:\n  ollama:\n    container_name: ollama\n    image: ollama/ollama\n    restart: always\n    ports:\n      - \"11434:11434\"\n    volumes:\n      - ollama_data:/root/.ollama\n\n  open-webui:\n    container_name: open-webui\n    image: ghcr.io/open-webui/open-webui:main\n    restart: always\n    ports:\n      - \"3000:8080\"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n    depends_on:\n      - ollama\n\nvolumes:\n  ollama_data:\n```\n\nRun the following commands:\n\n```bash\ndocker-compose up -d\ndocker exec -it ollama bash\n\n#Run below commands on docker container tty:\nollama list\nollama rm deepseek-r1:1.5b\nollama run deepseek-r1:1.5b\n```\n\nDisplay:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1738199570502/70c3e78a-092f-4b7f-a035-e33e5d34d5dc.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1738199926036/ffcf6a53-910f-40e8-8933-3abe09bce3da.png align=\"center\")\n\nVisit the URL https://localhost:3000 and log in using the username and password in Open WebUI.\n\n---\n\n### Conclusion\n\nAnd there you have it! A simple guide to running Deepseek-R1 privately on your computer while making it accessible from anywhere in the world.\n\nGot feedback? Let me know â€” and happy experimenting! ðŸ˜Š\n\n*(P.S. Check out the attached image of Deepseek running on my laptop with laptop localhost!)*\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1738194966735/1103561e-d883-49a3-aceb-0781c4fe8da2.png align=\"center\")\n\n*(P.S. Check out the attached image of Deepseek running on my with laptop ngrok!)*\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1738195013143/db59b825-2b79-445d-86de-015572bf165c.png align=\"center\")\n\n*(P.S. Check out the attached image of Deepseek running on my mobile!)*\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1738195514073/f54f14be-eb3e-4055-a0c8-7d3c0d18a060.jpeg align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1738195539574/e7eeeb5b-279d-4c04-939e-bc15170ee038.jpeg align=\"center\")\n\n---\n\nThank You! ðŸ˜Š\n\nFollow me on YouTube below:\n\n%[https://www.youtube.com/@LinuxCloudGuy]",
      "stars": null,
      "comments": 2,
      "upvotes": 41,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "What Text Area Popovers Taught Me About Browser APIs",
      "url": "https://blackgirlbytes.dev/what-text-area-popovers-taught-me-about-browser-apis",
      "source": "hashnode",
      "published_at": "2025-01-29T06:08:34.701000+00:00",
      "external_id": null,
      "tags": [
        "Web Development",
        "Browsers",
        "JavaScript",
        "Open Source",
        "React"
      ],
      "content_length": 7387,
      "content_preview": "I recently went down several rabbit holes about building WYSIWYG editors and popovers. While on maternity leave, I finally have time to deeply explore random technical problems without deadlines - a rare opportunity.\n\nThus far, I've focused on contributing to an open source AI agent called [codename goose](https://block.github.io/goose) built with a Rust backend and Electron-based chat interface. I submitted a [pull request](https://github.com/block/goose/pull/542) to add a WYSIWYG editor to its",
      "content_full": "I recently went down several rabbit holes about building WYSIWYG editors and popovers. While on maternity leave, I finally have time to deeply explore random technical problems without deadlines - a rare opportunity.\n\nThus far, I've focused on contributing to an open source AI agent called [codename goose](https://block.github.io/goose) built with a Rust backend and Electron-based chat interface. I submitted a [pull request](https://github.com/block/goose/pull/542) to add a WYSIWYG editor to its chat interface, opting to build a custom solution instead of using existing packages. The maintainers appreciated this approach since editor dependencies bloat bundle size, but raised concerns about the toolbar consuming too much visual space. They suggested implementing a popover toolbar instead.\n\nI wrongly assumed creating a floating toolbar would be simple. I aimed to:\n\n* Show a popover toolbar when text is selected\n    \n* Position it precisely above the selection\n    \n* Handle word-wrapped text spanning multiple lines\n    \n* Maintain accurate positioning during scroll\n    \n\nHandling popovers in a text area element turned out to be more complex than expected. In this blog post, Iâ€™ll share what I learned.\n\n## Text areas arenâ€™t your average DOM elements.\n\nUnlike typical HTML elements where we can manipulate contents, measure positions, and add new elements, text areas only expose raw text content and basic selection APIs. Browsers control their rendering behind the scenes.\n\nI asked Claude to generate an analogy for further illustration:\n\n* **Regular HTML elements** are like having a house where you can move the furniture around, add new items, measure distances between things, etc.\n    \n* **Text areas** are more like looking through a window into a room you can't enter. You can see what's inside and make some basic changes (like adding/removing text), but you can't reach in and manipulate things directly. The browser handles all the internal workings using native OS text editing capabilities.\n    \n\n## Popovers outside of text areas\n\n### The Popover API\n\nAll modern browsers include a built-in Popover API for creating popup elements. Here's an example:\n\n%[https://codesandbox.io/embed/knpf7w] \n\n#### Limitations\n\nWhile this API is cross-browser compatible and straightforward to implement, it comes with several limitations:\n\n* It only works with button elements since the required `popovertarget` attribute is only available on buttons\n    \n* You have to use CSS to position the popover relative to its target element\n    \n* And, my biggest limitation is that it doesnâ€™t work within text areas.\n    \n\nShout out to Mark Techson for introducing me to the Popover API via Una Kravetsâ€™ conference talk called [Less Cruft, More Power: Leverage the Power of the Web Platform](https://youtu.be/-0qa-UqTh_Q?si=IiwhJiXJbi_u4n-n&t=642).\n\n### The Selection API\n\nI wanted my popover to appear wherever the user selected text. This required me to:\n\n* Know the position of the selected text\n    \n* Listen for events that happened when text was selected and deselected\n    \n\nI came across Colby Fayockâ€™s blog post called [How to Share Selected Text in React with the Selection API](https://spacejelly.dev/posts/how-to-share-selected-text-in-react-with-the-selection-api). While Colby's focus was on text sharing functionality, his post introduced me to the Selection API - which could help me position my popover relative to selected text.\n\nThe Selection API lives in `window.getSelection()`. When you call this method, it returns a Selection object that tells you about the text a user has selected on the page.\n\n#### getRangeAt(0)\n\nFrom this Selection object, you can call `getRangeAt(0)` to find out exactly where the selection begins and ends. It gives you two numbers:\n\n* `startOffset` - where the selection begins\n    \n* `endOffset` - where the selection ends\n    \n\nIn a selection, each character has an index. For the text \"Hello, World! Welcome.\", the indexes look like this:\n\n%[https://codesandbox.io/embed/vj49mj] \n\nIf you select the word \"World\", then `startOffset` is 7 (where \"W\" begins) and `endOffset` is 12 (right after \"d\").\n\n**Side note:** I learned that the parameter 0 in getRangeAt(0) tells the browser which selection you want information about. When you select text, each selection gets stored at different indexes in an array. Most browsers only let you select one piece of text at a time, so you'll only have one item at index 0. But browsers like Firefox let you hold Ctrl to select multiple pieces of text. If you try to access indexes greater than 0 in browsers that don't support multiple selections, you'll get an error.\n\n#### getBoundingClientRect()\n\n`getRangeAt(0)` gives you access to `getBoundingClientRect()`.\n\n`getBoundingClientRect()` returns a box of measurements around your selected text. It tells you the top, right, bottom, and left positions of this box, plus its width and height.\n\nWith these measurements, I could place my popover right above any text the user selects, like this:\n\n%[https://codesandbox.io/embed/mhxtkr] \n\nWhile this approach worked for most HTML elements, text area elements provide limited access to the Selection API, so I needed an alternative approach.\n\n### The Mirrored Div\n\nThrough rubber ducking with Claude, I learned about the mirrored div approach, a workaround for determining selection coordinates in a text area.\n\nHere's how it works: create an invisible div that overlays the text area, containing the exact same content and styling. When a user selects text, they're actually interacting with this invisible div rather than the text area underneath it. This gives you access to the full Selection API while maintaining what looks and feels like a standard text area to the user.\n\nI found validation for this technique in Jhey Thompkins' blog post \"[HOW TO: Where's the text cursor?](https://jh3y.medium.com/how-to-where-s-the-caret-getting-the-xy-position-of-the-caret-a24ba372990a)\", which introduced me to the `getComputedStyle()` method. This API returns the computed CSS styles of HTML elements, letting developers match the text area's appearance in the overlay div with precision.\n\nBut just like with a real mirror, things aren't always what they seem. Similar to the warning on car mirrors that \"objects may be closer than they appear,\" our mirrored div can distort reality in subtle ways:\n\n* Text can wrap at different points between the div and text area\n    \n* Browsers handle spacing and font rendering differently, causing text positions to shift unexpectedly\n    \n\n{% codesandbox wtsqcv %}\n\n%[https://codesandbox.io/embed/wtsqcv] \n\n## Why not use an NPM package?\n\nIâ€™ve tried a few packages, and I found that most of these packages work well with regular DOM elements. However, they struggle with text areas due to the same fundamental limitations I stated earlier - limited access to the text area's internal rendering and positioning.\n\n## Conclusion\n\nWhile browsers have come a long way in supporting rich text interactions, working with text areas remains surprisingly complex. I had fun learning about these browser APIs, but I still haven't found a solution that fits my use case. Maybe future APIs will make tasks like selection-based popovers more straightforward.\n\nIf you've tackled text area customization in a way I havenâ€™t explored, I'd love to hear about your approaches.",
      "stars": null,
      "comments": 1,
      "upvotes": 15,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "The OSI Model Simplified: A Practical Guide for DevOps Engineers",
      "url": "https://croniumx.hashnode.dev/the-osi-model-simplified-a-practical-guide-for-devops-engineers",
      "source": "hashnode",
      "published_at": "2025-01-27T03:30:41.382000+00:00",
      "external_id": null,
      "tags": [
        "Devops",
        "Cloud",
        "DevSecOps",
        "SRE",
        "Platform Engineering ",
        "OSI Model"
      ],
      "content_length": 5832,
      "content_preview": "The OSI (Open Systems Interconnection) model explains how computers communicate over a network. It breaks down the process into seven layers, each with a specific role. Understanding the OSI model is crucial for DevOps engineers because it helps with troubleshooting, designing systems, and ensuring smooth communication between applications and servers. Let's explore each layer with simple explanations and examples tailored for DevOps.\n\n## Layer 1: Physical Layer\n\nThe Physical Layer involves the ",
      "content_full": "The OSI (Open Systems Interconnection) model explains how computers communicate over a network. It breaks down the process into seven layers, each with a specific role. Understanding the OSI model is crucial for DevOps engineers because it helps with troubleshooting, designing systems, and ensuring smooth communication between applications and servers. Let's explore each layer with simple explanations and examples tailored for DevOps.\n\n## Layer 1: Physical Layer\n\nThe Physical Layer involves the hardware components. It includes cables, switches, and network cardsâ€”items you can physically touch.\n\n* **Scenario:** Setting up a data center with servers.\n    \n* **Example Task:** Ensuring network cables are properly connected to avoid hardware communication issues.\n    \n\n## Layer 2: Data Link Layer\n\nThe Data Link Layer makes sure data moves without errors between devices on the same network. It handles things like MAC addresses and switches.\n\n* **Scenario:** Fixing problems with local server communication.\n    \n* **Example Task:** Verifying the MAC address of a network interface to ensure the right device is getting the data.  \n    Command: `ifconfig | grep HWaddr`\n    \n\n[![Diagram of the OSI model with seven layers: Application, Presentation, Session, Transport, Network, Data Link, and Physical. Each layer has a brief description of its function.](https://cdn.hashnode.com/res/hashnode/image/upload/v1737548309228/932a6994-15c2-485f-a899-6569d28b5643.png align=\"center\")](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/)\n\n## Layer 3: Network Layer\n\nThe Network Layer determines how data moves between devices on different networks. It uses IP addresses to route the data.\n\n* **Scenario:** Troubleshooting connectivity between two servers in different subnets.\n    \n* **Example Task:** Use a tool like `ping` to check if a serverâ€™s IP address can be reached.  \n    Command: `ping 192.168.1.10`\n    \n\n## Layer 4: Transport Layer\n\nThe Transport Layer ensures data is delivered reliably. It uses protocols like TCP, which is reliable, and UDP, which is faster but less reliable.\n\n* **Scenario:** Troubleshooting slow API responses.\n    \n* **Example Task:** Use `netstat` or `ss` to check open ports and ensure the server is listening on the correct port.  \n    Command: `ss -tuln`\n    \n\n## Layer 5: Session Layer\n\nThe Session Layer manages and controls connections between devices, like keeping a phone call active until you hang up.\n\n* **Scenario:** Maintaining stable connections for database queries.\n    \n* Example Task: Check session timeouts in the database or web server configuration.\n    \n    Command: `grep 'timeout' /etc/nginx/nginx.conf`\n    \n\n## Layer 6: Presentation Layer\n\nThe Presentation Layer ensures data is in the correct format for the application. It handles encryption, compression, and data translation.\n\n* **Scenario:** Debugging issues with SSL certificates.\n    \n* Example Task: Check if SSL/TLS certificates are properly configured for secure communication.\n    \n    Command: `openssl s_client -connect example.com:443`\n    \n\n## Layer 7: Application Layer\n\nThe Application Layer is where users interact with the network. It includes applications like web browsers, email clients, and APIs.\n\n* **Scenario:** Debugging API failures in a microservices architecture.\n    \n* Example Task: Use tools like curl or Postman to test API endpoints.\n    \n    Command: `curl -X GET`[`https://api.example.com/v1/resource`](https://api.example.com/v1/resource)\n    \n\n## Why the OSI Model Matters for DevOps\n\nFor DevOps, the OSI model serves as a guide that helps with:\n\n* **Troubleshooting:** Quickly identifying where a problem occursâ€”whether it's a broken cable (Layer 1) or a misconfigured API (Layer 7).\n    \n* **System Design:** Understanding how data moves through the layers aids in building reliable systems.\n    \n* **Collaboration:** Communicating effectively with network engineers, developers, and system administrators by using a common language.\n    \n\nBy understanding the OSI model, DevOps engineers can tackle complex issues confidently and ensure systems operate smoothly from top to bottom.\n\n## Reference\n\n1. **Introduction to the OSI Model**  \n    [https://www.cloudflare.com/learning/network-layer/what-is-the-osi-model/](https://www.cloudflare.com/learning/network-layer/what-is-the-osi-model/)  \n    A beginner-friendly explanation of the OSI model, detailing all seven layers with practical examples.\n    \n2. **OSI Model and Its Layers Explained**  \n    [https://www.cisco.com/c/en/us/solutions/small-business/resource-center/networking/what-is-the-osi-model.html](https://www.cisco.com/c/en/us/solutions/small-business/resource-center/networking/what-is-the-osi-model.html)  \n    Ciscoâ€™s guide to the OSI model, including its relevance to networking and troubleshooting.\n    \n3. **Networking Commands for Each OSI Layer**  \n    [https://www.comptia.org/blog/the-osi-model-explained-and-how-to-easily-remember-its-seven-layers](https://www.comptia.org/blog/the-osi-model-explained-and-how-to-easily-remember-its-seven-layers)  \n    A detailed explanation of the OSI layers, paired with practical tools and commands for troubleshooting.\n    \n4. **How the OSI Model Applies to Real-World Scenarios**  \n    [https://www.ibm.com/cloud/learn/osi-model](https://www.ibm.com/cloud/learn/osi-model)  \n    IBMâ€™s guide on the OSI model and its application in cloud environments, including DevOps-relevant use cases.\n    \n5. **OSI Model in DevOps and Troubleshooting**  \n    [https://www.techtarget.com/searchnetworking/definition/OSI](https://www.techtarget.com/searchnetworking/definition/OSI)  \n    A comprehensive overview of the OSI model and its use in diagnosing network and application issues, specifically useful for DevOps and IT professionals.",
      "stars": null,
      "comments": 0,
      "upvotes": 24,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Adding Custom Rate Limiting to Your AWS Lambda API Using a Middleware",
      "url": "https://florian.hashnode.dev/adding-custom-rate-limiting-to-your-aws-lambda-api-using-a-middleware",
      "source": "hashnode",
      "published_at": "2025-01-24T15:19:56.240000+00:00",
      "external_id": null,
      "tags": [
        "AWS",
        "GraphQL",
        "ratelimit",
        "lambda"
      ],
      "content_length": 17652,
      "content_preview": "Rate limiting an API or service is a common use case. But how can you do this for your AWS Lambda? API Gateway offers usage planes and quotas, so you can put a API in front of the lambda, but quotas have some downsides there:\n\n> Usage plan throttling and quotas are not hard limits, and are applied on a best-effort basis. In some cases, clients can exceed the quotas that you set. Donâ€™t rely on usage plan quotas or throttling to control costs or block access to an API.\n\nAnother option is to pair A",
      "content_full": "Rate limiting an API or service is a common use case. But how can you do this for your AWS Lambda? API Gateway offers usage planes and quotas, so you can put a API in front of the lambda, but quotas have some downsides there:\n\n> Usage plan throttling and quotas are not hard limits, and are applied on a best-effort basis. In some cases, clients can exceed the quotas that you set. Donâ€™t rely on usage plan quotas or throttling to control costs or block access to an API.\n\nAnother option is to pair API Gateway, your Lambda and AWS WAF, but if you donâ€™t want to find yourself in the AWS WAF configuration jungle, another simpler option could do the trick for you. Letâ€™s explore how you can easily setup rate limits for your Lambda.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736955448976/c7842dfa-a0ac-4a1e-b9f3-8b9d16e0adb5.png align=\"center\")\n\nThe above image shows the architecture of the rate-limiter. As I use [Middy](https://middy.js.org/) in almost all of my projects for shared middlewares, I built the rate-limiter as a middleware as well. The code is split in two parts:\n\n1. Checking for the current quotas of the user and checking it against the allowed quotas in `before` phase\n    \n2. Increasing the rate limit after the actual invoke of my API in the `after` phase\n    \n\nFor storage, I decided to go with something that as fast read and write accesses and in the best case supports TTLs. So a cache made the race and I did go with [momento](https://www.gomomento.com/platform/cache/). You can of course use other caches like Redis or even a database to save the current quotas of a user.\n\nThat is the basics we need to implement rate-limiting. Of course, the allowed quotas have to be defined somewhere and the current quotas have to be stored somewhere. Before we touch on these topics, letâ€™s explore why rate limiting is even needed.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">â„¹</div>\n<div data-node-type=\"callout-text\">I used the rate limits introduced here for an AppSync API, where I needed to have fixed quotas for certain mutations based on a userâ€™s subscription to the service. Thus, there is no API Gateway involved. You can of course put the middleware code into <strong>API Gateway Lambda authorizers</strong> to check for quotas and increase them accordingly. Be aware with though, that authorizers are need some cache tweaks to make it work. An additional requirement for my implementation was, that quotas are only increased on successful invokes of the mutation.</div>\n</div>\n\n# What Is Rate Limiting and Why Implement It?\n\nRate limiting controls traffic to a network, app, or API. It does this by limiting the number of requests a client can make in a set time, like 100 requests per minute. It ensures efficient use of system resources. It also protects the infrastructure from misuse or overload.\n\nWhy implement rate limiting?\n\n1. Rate limiting safeguards against malicious activities. These include Denial of Service (DoS) attacks, brute force login attempts, and excessive API scraping. It does this by capping the request frequency.\n    \n2. It prevents servers from becoming overwhelmed by limiting request volume. This ensures a consistent, reliable experience for all users.\n    \n3. In shared environments, rate limiting ensures fair access to resources. It prevents certain users from monopolizing bandwidth or server capacity.\n    \n4. Limiting traffic cuts costs by reducing server load. This is key for APIs that do resource-heavy tasks. It ensures they are used wisely.\n    \n5. Rate limiting can help monetize. For example, you can offer a free tier with a limit on requests. This will attract new users. Charge for any extra usage for resource-intensive APIs. These include those for complex data processing, machine learning, or real-time analytics. Heavy usage incurs high costs. Higher-tier plans can include elevated rate limits, providing premium access and encouraging upgrades.\n    \n6. For usage-based or subscription-based APIs, rate limiting ensures users stay within their plans. It helps avoid unexpected costs and encourages users to scale up their usage.\n    \n\nRate limiting can act as a gateway to premium services. It offers a baseline of free requests. This lets users test an API's value before committing to higher prices.\n\n# **Implementing the Middleware**\n\nIn this post we explore an option to rate limit users based on their available quota. This information can come from some hard limit or be based on the current subscription of a user within our product.\n\nNow, letâ€™s get started with the implementation. First, we only want to rate limit some operations of our API. So we can define them for later use.\n\n```typescript\nexport const QUOTA_OPERATION = [\n  'generate',\n  'analyze'\n] as const;\n\nexport type QuotaOperation = (typeof QUOTA_OPERATION)[number];\n```\n\nAs a first step, w define what rate limits a user can have based on the operation and entitlement. Letâ€™s assume there are`free`, `advanced` and `enterprise`. We can use a simple mapping between an entitlement a user can have within our app and the resulting amount of request they can make. Additionally, we want to save when these rate limits are resting (e.g. seconds, minutes, days or even weeks).\n\n```typescript\n\nexport const QUOTA_PERIOD = [\n  'second',\n  'minute',\n  'hour',\n  'day'\n] as const;\n\nexport type QuotaEntitlements =\n  | 'free'\n  | 'advanced'\n  | 'enterprise';\n\n\n\nexport const RATE_LIMITING_QUOTAS: Record<\n  QuotaEntitlements,\n  Record<\n    QuotaOperation,\n    {\n      limit: number;\n      period: QuotaPeriod;\n    }\n  >\n> = {\n  free: {\n    analyze: {\n      limit: 100,\n      period: 'day'\n    },\n    generate: {\n      limit: 10,\n      period: 'minute'\n    }\n  },\n  advanced: {\n     analyze: {\n      limit: 1000,\n      period: 'day'\n    },\n    generate: {\n      limit: 100,\n      period: 'minute'\n    }\n  },\n  enterprise: {\n     analyze: {\n      limit: 1000,\n      period: 'hour'\n    },\n    generate: {\n      limit: 100,\n      period: 'second'\n    }\n  }\n};\n```\n\nThe following is defined from the above code:\n\n1. Every user on the free plan can call `analyze` *100* times a day and `generate` *10* time per minute\n    \n2. Every user on the advanced plan can call `analyze` 1000 times a day and `generate` 100 time per minute\n    \n3. Every user on the enterprise plan can call `analyze` 1000 times an **hour** and `generate` 100 time per **second**\n    \n\nIn our case, that means it is important what the **highest** entitlement of the user is.\n\n```typescript\n\nexport const ENTITLEMENT_ORDER: Record<QuotaEntitlement, number> = {\n  free: 1,\n  advanced: 2,\n  enterprise: 3\n} as const;\n\nexport const getHighestEntitlement = (entitlements: QuotaEntitlement[]): QuotaEntitlement => {\n  return entitlements.reduce(\n    (highest, current) =>\n      ENTITLEMENT_ORDER[current] > ENTITLEMENT_ORDER[highest]\n        ? current\n        : highest,\n    'free'\n  );\n};\n```\n\nWe need to store everything in a format so that we know, for which rate limit we need to check, what the current used quota is and when the quota will reset itself. In the case of our momento, this is rather easy. We can come up with a key structure that saves the most important thinks: `userId` and `operation`, use the automatic TTLs for resets and save the current used quota as the value:\n\n```typescript\nconst generateCacheKey = (args: {\n  userId: string;\n  operation: QuotaOperation;\n}) => {\n  return `user:${args.userId}:operation:${args.operation}`;\n};\n```\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">âš </div>\n<div data-node-type=\"callout-text\">Caution: momento (and some other caches) have an initial limit of 24hrs for any TTL. If you need higher TTLs for your use-case, consider increasing the limit, using another cache or another store altogether.</div>\n</div>\n\nNow we can create a helper function that will do the following return the following things:\n\n1. `currentCount`: Currently used quota, will be 0 if no entry is found on the cache\n    \n2. `limit`: How many invokes the user can currently make to this API\n    \n3. `period`: When the rate limit will reset as a unit\n    \n4. `ttl`: Remaining time until the quota will reset\n    \n5. `cache`: Cache related information.\n    \n\n```typescript\n\nconst parseGetResponseValue = (response: CacheGet.Response) => {\n  switch (response.type) {\n    case CacheGetResponse.Hit:\n      return { currentCount: parseInt(response.valueString()), exists: true };\n    case CacheGetResponse.Miss:\n      return { currentCount: 0, exists: false };\n    case CacheGetResponse.Error:\n      // In case of an error, we should throw and denie access\n      throw new Error (\"Unable to get the user's quota\") \n  }\n};\n\nexport const periodInSeconds = (period: QuotaPeriod): number => {\n  switch (period) {\n    case 'second':\n      return 1; // 1 second in seconds\n    case 'minute':\n      return 60; // 1 minute in seconds\n    case 'hour':\n      return 60 * 60; // 1 hour in seconds\n    case 'day':\n      return 24 * 60 * 60; // 1 day in seconds\n  }\n};\n\nexport async function getCurrentQuotaOfUser(\n  args: {\n     operation: QuotaOperation;\n     cacheClient: CacheClient;\n     entitlement: QuotaEntitlement[],\n     userId: string\n   }\n) {\n  // Get the current highest entitlement for the user\n  const userEntitlement = getHighestEntitlement(args.entitlement);\n  \n  // Determinate the max rate limit based on the plan \n  const operationLimit = RATE_LIMITING_QUOTAS[userPlan][args.operation];\n\n  // Generate the cache key\n  const cacheKey = generateCacheKey({ userId:args.userId, operation: args.operation });\n\n  // Get the current count from Momento\n  const response = await args.cacheClient.get(\n    'rate-limit-cache',\n    cacheKey\n  );\n\n  // Parse the result \n  const { currentCount, exists } = parseGetResponseValue(response);\n\n  // Parse the current period to seconds for TTL usage\n  let ttl: number = periodInSeconds(operationLimit.period);\n\n  if (exists) {\n    // Check for the current TTL if we found a entry in the cache\n    const ttlResponse = await args.cacheClient.itemGetTtl(\n      'rate-limit-cache',\n      cacheKey\n    );\n\n    const ttlMillis = ttlResponse.remainingTtlMillis();\n    \n    // Calculate the remaining TTL for the operation's rate limit\n    ttl = ttlMillis ? ttlMillis * 1000 : ttl;\n  }\n\n  return {\n    currentCount,\n    limit: operationLimit.limit,\n    period: operationLimit.period,\n    ttl,\n    cache: {\n      key: cacheKey,\n      exists,\n      ttl\n    }\n  };\n}\n```\n\nThese helper functions are the base for our rate limiting middleware. Now, before we put the middleware together, we create a reusable momento client. Letâ€™s write a small middleware that injects a client to our execution context of our lambda function.\n\n```typescript\nimport {\n  CacheClient,\n  Configurations,\n  CredentialProvider\n} from '@gomomento/sdk';\nimport { logger } from '@instameal/lambda-logger';\nimport middy, { MiddlewareObj } from '@middy/core';\n\nasync function createCacheClient(secret: string) {\n  return CacheClient.create({\n    configuration: Configurations.Laptop.v1(),\n    credentialProvider: CredentialProvider.fromString(secret),\n    defaultTtlSeconds: 600\n  });\n}\n\nlet cachedClient: CacheClient | null = null;\n\nexport type MomentoClientContext = {\n  cacheClient: CacheClient;\n};\n\nexport type WithMomentoClientCache<T> = T & {\n  momento: MomentoClientContext;\n};\n\ntype MomentoOptions = {\n  secret: string;\n};\n\nconst assignMomentoClientToContext = (\n  options: MomentoOptions\n): middy.MiddlewareObj => {\n  const before: middy.MiddlewareFn = async (request) => {\n    if (!cachedClient) {\n      cachedClient = await createCacheClient(options.secret);\n    } else {\n      logger.debug('using cached MomentoClient client');\n    }\n    Object.assign(request.context, {\n      momento: {\n        // Access to the whole cache abstraction\n        cacheClient: cachedClient\n      }\n    });\n  };\n\n  return {\n    before\n  };\n};\n\n/**\n * Middleware assigning an instance of the MomentoClientSDK on the context object.\n * The client instance is cached and reused for the same execution environment.\n */\nexport const momento: (options: MomentoOptions) => MiddlewareObj[] = (\n  options\n) => [assignMomentoClientToContext(options)];\n```\n\nThe rate limiting middleware is now easily put together. We need to\n\n1. Get the operation from the incoming event\n    \n2. Check if the operation is even rate limited. If not, we can execute the request directly\n    \n3. Use the `getCurrentQuotaOfUser` to get the required information\n    \n4. Check if the user has exceeded the limit. If so, we will throw a `LimitExceededException`\n    \n5. Assign some information we have gathered to the context, so we can use it in the `after` phase\n    \n6. In the `after` phase, create or increment the current entry in our cache\n    \n\n```typescript\n\ntype RateLimitingStash = {\n  cacheKey: string;\n  currentCount: number;\n  exists: boolean;\n  ttl: number;\n};\n\nconst rateLimitingMiddleware = (): MiddlewareObj<\n  AppSyncResolverEvent<unknown, unknown>,\n  APIGatewayProxyResult,\n  Error,\n  WithMomentoClientCache<LambdaContext>\n> => {\n  return {\n    before: async (handler) => {\n      const { context, event } = handler;\n      const {\n        momento: { cacheClient }\n      } = context;\n\n      // Get the operation name somewhere from the request\n      // This could be either the path or a mutation name from AppSync    \n      const operation = mapFieldNameToOperation({\n        fieldName: event.info.fieldName\n      });\n\n      if (!operation || !QUOTA_OPERATION.includes(operation)) {\n        logger.debug('No operation or operation is not rate-limited', {\n          operation\n        });\n        // If there's no operation, or the operation is not rate-limited, proceed\n        return;\n      }\n\n      const { currentCount, limit, ttl, cache, period } =\n        await getCurrentQuotaOfUser({\n          cacheClient,\n          environment,\n          identity: event.identity,\n          operation\n        });\n\n      // Check if the current request exceeds the allowed limit\n      if (currentCount >= limit) {\n        throw new LimitExceededException(\n          'You have reached the limit for the operation',\n          {\n            operation,\n            limit,\n            invokes: currentCount,\n            resetsAt: DateTime.fromSeconds(ttl).toISO()\n          }\n        );\n      }\n      \n      // Save the information to the stash so we can use it after the API execution\n      const rateLimitingStash: RateLimitingStash = {\n        cacheKey: cache.key,\n        currentCount,\n        exists: cache.exstis,\n        ttl,\n      };\n\n      Object.assign(handler.event.stash, {\n        rateLimit: rateLimitingStash\n      });\n    },\n\n    after: async (handler) => {\n      const { context, event, response } = handler;\n      const {\n        momento: { cacheClient }\n      } = context;\n\n      // Only increase the limit if the operation was executed succesfully \n      if (response) {\n        // Check if there's a valid response\n        const rateLimitingStash = event.stash.rateLimit as\n          | RateLimitingStash\n          | undefined;\n\n        // No rate limiting stash, so we don't need to update the cache\n        if (!rateLimitingStash) {\n          return;\n        }\n\n        const { cacheKey, currentCount, ttl, exists } = rateLimitingStash;\n\n        // Entry already exists, we can use the atomic increment\n        if (exists) {\n          await cacheClient.increment(\n            generateCacheName(environment, 'rate-limit-cache'),\n            cacheKey,\n            1,\n            { ttl }\n          );\n        } else {\n          // Set a new value with a TTL for the period\n          await cacheClient.set(\n            generateCacheName(environment, 'rate-limit-cache'),\n            cacheKey,\n            (currentCount + 1).toString(),\n            { ttl }\n          );\n        }\n      }\n    }\n  };\n};\n\nexport default rateLimitingMiddleware;\n```\n\nFinally, we can enhance our Lambda handler with out two newly created middlewares. Now, any request that uses the `analyze` or `generate` mutations should be rate limited base on our defined quotas.\n\n```typescript\nconst hanlder = () => {\n  // ... your implementation\n};\n\nexport const main = middy(handler)\n  .use(momento())\n  .use(rateLimitingMiddleware())\n```\n\n# Final Thoughts\n\nWe have implemented a simple rate limiting middleware. It allows us to limit any execution of the operations we have defined in the timeframes we have defined. Also, this is not on a best-effort basis. It will block any request that exceeds the pre-defined limit.\n\nWe can enhance our implementation in many ways. We can make it more dynamic by storing more information. If we change our quotas in the future, existing accounts would not be affected. We can make the entitlements and operation mapping more modular. This would help apps with dynamic, not strict, plans and entitlements. Or, and this is a big decision, use a different storage engine. It can have benefits.\n\nChoosing the correct storage can be tough. The advantage of momento and other caches is that they use atomic writes (Redis does as well). This comes in handy, as it prevents race conditions from different requests. You may also want to check the concurrency settings on your lambda in regard to that.\n\nThe decision should also depend on the throughput you are expecting for your API. Caches are fast and cheap, but have some other limitations. Databases can get expensive if you have many reads and writes to manage your API's rate limits. However, they offer other benefits.\n\nAs in so many cases, it really depends on your use case and what you want to achieve.\n\nSee you next time. ðŸ‘‹",
      "stars": null,
      "comments": 0,
      "upvotes": 22,
      "read_time": "11 min read",
      "language": null
    },
    {
      "title_en": "Automate Code Formatting with Prettier, ESLint, Husky, and lint-staged",
      "url": "https://blog.alyssaholland.me/automate-code-formatting",
      "source": "hashnode",
      "published_at": "2025-01-24T12:30:18.070000+00:00",
      "external_id": null,
      "tags": [
        "Developer",
        "Prettier",
        "eslint"
      ],
      "content_length": 13162,
      "content_preview": "## Introduction\n\nMaintaining a formatted and tidy codebase is the ideal scenario for projects. However, ensuring that codebases adhere to these standards requires some work to make this a reality. In this article, Iâ€™ll describe the libraries and automations Iâ€™ve employed to achieve a polished and organized codebase a reality.\n\nIf you'd like to set up all these tools in a sample project, you can clone this [GitHub repository](https://github.com/Cool-Runningz/blog-automate-code-formatting). All th",
      "content_full": "## Introduction\n\nMaintaining a formatted and tidy codebase is the ideal scenario for projects. However, ensuring that codebases adhere to these standards requires some work to make this a reality. In this article, Iâ€™ll describe the libraries and automations Iâ€™ve employed to achieve a polished and organized codebase a reality.\n\nIf you'd like to set up all these tools in a sample project, you can clone this [GitHub repository](https://github.com/Cool-Runningz/blog-automate-code-formatting). All the examples I provide will assume you're following along with the sample project unless I mention otherwise.\n\n## Benefits\n\nBefore we dive into *how* to automate this process, let's first explore *why* you would want to automate code formatting and linting. Instead of bike-shedding over rules, you can automatically format or lint files using a small set of configuration files. Prettier is used for formatting, and linters are used for catching bugs. ESLint offers a wide range of options, including framework-specific checks and accessibility warnings, to name a few. Both Prettier and ESLint work together to improve the structure and quality of the codebase.\n\n## Prettier\n\n[Prettier](https://prettier.io/) is an opinionated code formatter that integrates with many code editors and supports a variety of programming languages. Prettier intentionally limits the amount of options you can customize so that you spend less time debating styles like whether to use tabs vs. spaces or single quotes vs. double quotes and more time focusing on important things. Simply put, Prettier saves you time and energy by allowing you to agree on a set of configuration options and move on with your day.\n\n![Prettier logo](https://cdn.hashnode.com/res/hashnode/image/upload/v1737250221884/b2017706-8643-47f8-9266-17b51568ae26.png align=\"center\")\n\n### Installation & Configuration\n\nTo install Prettier into your project run the following command:\n\n```bash\nnpm install --save-dev --save-exact prettier\n```\n\nThen, create an empty config file at the root of your project:\n\n```bash\ntouch .prettierrc.json\n```\n\nThis `.prettierrc.json` file will house the configuration options for our project. To test out a few options, open the file and add the following text to enforce double quotes and semicolons.\n\n```json\n{\n  \"singleQuote\": false,\n  \"semi\": true\n}\n```\n\n### Testing\n\nRun the following command at the root of the project to utilize [Prettierâ€™s CLI](https://prettier.io/docs/en/cli) and check for any formatting issues in `src/counter.js`. :\n\n```bash\nnpx prettier --check src/counter.js\n```\n\nAfter checking that file, Prettier will output the following to the terminal:\n\n```bash\nChecking formatting...\n[warn] src/counter.js\n[warn] Code style issues found in the above file. Run Prettier with --write to fix.\n```\n\nTo fix the issue, Prettier gives us a big clue here telling us to use the `â€”write` flag to address the issues it found so letâ€™s try doing just that with this update:\n\n```bash\nnpx prettier --write src/counter.js\n```\n\nNow if you run a diff against the `counter.js` you will notice that semicolons have been added to the end of statements and the single quotes have been replaced with double quotes.\n\n![git diff showing the quote and semicolon changes made by Prettier in .](https://cdn.hashnode.com/res/hashnode/image/upload/v1737259999456/74bcee35-26a1-4908-97df-598df277fa58.png align=\"center\")\n\n## ESLint\n\n[ESLint](https://eslint.org/) is a powerful tool that performs static analysis of your code to identify potential issues and enforce coding standards. ESLint can detect syntax errors, potential bugs, and deviations from your defined coding style. This tool is widely integrated into many text editors, providing real-time feedback as you write code. Additionally, ESLint can be configured to run as part of your continuous integration pipeline, ensuring that all code changes adhere to your project's quality standards before they are merged. This helps maintain a consistent codebase and reduces the likelihood of introducing errors into your application.\n\n![ESLint logo](https://cdn.hashnode.com/res/hashnode/image/upload/v1737250298794/aefcb3fc-0b12-4fc2-b8ff-cab98b5a9466.png align=\"center\")\n\n### Installation & Configuration\n\nTo install ESLint into your project run the following command:\n\n```bash\nnpm install eslint --save-dev\n```\n\nYou can configure ESLint using this command:\n\n```bash\nnpm init @eslint/config@latest\n```\n\nThis will prompt you with a series of questions and after installation is complete, create a `eslint.config.js` file that you can edit and configure to your liking. By default the recommended configuration will be applied and you can configure some rules so that the file looks like this:\n\n```javascript\n// eslint.config.js\nimport globals from \"globals\";\nimport pluginJs from \"@eslint/js\";\n\n/** @type {import('eslint').Linter.Config[]} */\nexport default [\n  {languageOptions: { globals: globals.browser }},\n  pluginJs.configs.recommended,\n  {\n    rules: {\n      \"no-console\": \"warn\"\n    }\n  }\n];\n```\n\nThe name `\"no-console\"` is an example of a [rule](https://eslint.org/docs/latest/rules) in ESLint. When overriding rules the severity can be one of the following:\n\n* `0` or `\"off\"`\n    \n* `1` or `\"warn\"`\n    \n* `2` or `\"error\"`\n    \n\nThe three severity levels provide you with fine-grained control over how ESLint applies rules (for more configuration options and details, see the [configuration docs](https://eslint.org/docs/latest/use/configure/)).\n\n<details data-node-type=\"hn-details-summary\"><summary>â„¹ï¸ Bonus Tip</summary><div data-type=\"detailsContent\">You can extend ESLint with <a target=\"_self\" rel=\"noopener noreferrer nofollow\" href=\"https://eslint.org/docs/latest/extend/ways-to-extend#plugins\" style=\"pointer-events: none\">plugins</a>. The most common ones Iâ€™ve used include: <code>eslint-plugin-jsx-a11y</code>, <code>eslint-plugin-react</code>, <code>eslint-plugin-react-hooks</code> and <code>eslint-plugin-storybook</code>.</div></details>\n\n### eslint-config-prettier\n\n> â„¹ï¸ Use Prettier for code formatting concerns, and linters for code-quality concerns.\n\nLinters often include rules for both code quality and style. However, most style rules aren't needed when using Prettier, so we need to install the [eslint-config-prettier](https://github.com/prettier/eslint-config-prettier) package to prevent ESLint from conflicting with Prettier.\n\n```bash\nnpm install eslint-config-prettier --save-dev\n```\n\nThen add `eslint-config-prettier` to the ESLint config so that it includes the following:\n\n```javascript\n// eslint.config.js\nimport globals from \"globals\";\nimport pluginJs from \"@eslint/js\";\nimport eslintConfigPrettier from \"eslint-config-prettier\"; //Import package\n\n/** @type {import('eslint').Linter.Config[]} */\nexport default [\n  {languageOptions: { globals: globals.browser }},\n  pluginJs.configs.recommended,\n  eslintConfigPrettier, //Ensure this is the last option in the list\n  {\n    rules: {\n      \"no-console\": \"warn\"\n    }\n  }\n];\n```\n\nTo ensure everything is set up correctly and that none of the ESLint rules conflict with Prettier, we can use the [CLI helper tool](https://github.com/prettier/eslint-config-prettier?tab=readme-ov-file#cli-helper-tool) included with eslint-config-prettier. For example, if I run `npx eslint-config-prettier src/counter.js`, it will output, *â€œNo rules that are unnecessary or conflict with Prettier were found.â€*\n\n### Testing\n\nRun the following command at the root of the project to utilize [ESLintâ€™s CLI](https://eslint.org/docs/latest/use/command-line-interface) and check for any formatting issues in `counter.js`:\n\n```bash\nnpx eslint src/counter.js\n```\n\nYouâ€™ll notice that ESLint does not output anything, indicating that no issues were found. To purposely create an error and a warning, update `counter.js` to add a `console.log()` statement and create a new variable without referencing it anywhere.\n\nIf youâ€™re using an editor like VSCode and have the [ESLint extension installed](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint), you might notice that the editor shows the following tooltips when you hover over the code associated with the issues:\n\n![VSCode highlighting two ESLint warnings. One warns about an unexpected console statement, and the other notifies that a variable 'empty' is assigned a value but never used.](https://cdn.hashnode.com/res/hashnode/image/upload/v1737509903628/1ac88dff-2d09-4d1f-b445-058c89d0a7ce.png align=\"left\")\n\nHowever, you can use ESLintâ€™s CLI tool to also check for any issues by running the following command:\n\n```javascript\nnpx eslint src/counter.js\n```\n\nAfter running the command, the terminal should output the following error and warning messages:\n\n![CLI output with two ESLint warnings. One warns about an unexpected console statement, and the other notifies that a variable 'empty' is assigned a value but never used.](https://cdn.hashnode.com/res/hashnode/image/upload/v1737510280965/bcc28b8e-7da2-4b29-9123-4b33aad9bc58.png align=\"left\")\n\nThis indicates that ESLint is set up properly and correctly identifying issues.\n\n## Husky\n\n[Husky](https://typicode.github.io/husky/) is a tool that allows you to manage Git hooks more easily. Git hooks are scripts that run automatically at certain points in the Git workflow, such as before a commit or push. Husky allows you to configure these hooks making it easier to enforce code quality standards, run tests, or perform other tasks automatically as part of the development process.\n\n### **Installation & Configuration**\n\nTo install husky run the following command:\n\n```bash\nnpm install --save-dev husky\n```\n\nNext, we need to run the `init` command which will simplify setting up husky in a project:\n\n```bash\nnpx husky init\n```\n\nThis creates a `pre-commit` script in `.husky/` and updates the `prepare` script in `package.json`. In the next section weâ€™ll see how we can use this to run Prettier and ESLint whenever committing new files.\n\n## lint-staged\n\n[lint-staged](https://www.npmjs.com/package/lint-staged) enhances the code quality assurance process by running linters on files that are ***staged*** for commit, rather than the entire project. This approach ensures that only the files intended for the next commit are checked, preventing errors from entering the repository and enforcing consistent code style. By focusing on staged files, lint-staged significantly reduces the time and resources required for linting, making it a practical solution for maintaining code quality without unnecessary overhead. The tool provides the ability to execute custom shell tasks on these files, filtered by specified glob patterns, streamlining the pre-commit workflow.\n\n![Sample output from running git commit with a lint-staged configuration](https://cdn.hashnode.com/res/hashnode/image/upload/v1737600311186/59263b80-54b8-4db9-85e3-e30ad0d8923a.png align=\"left\")\n\n### Installation & Configuration\n\nRun the following command to install `lint-staged`\n\n```bash\nnpm install --save-dev lint-staged\n```\n\nNext, we need to setup the `pre-commit` hook to run lint-staged. If you remember from the previous section, husky created this file for us so we can open the file directly and edit it or run the following command:\n\n```bash\necho \"npx lint-staged\" > .husky/pre-commit\n```\n\nNow we need to instruct lint-staged to run Prettier and ESLint. We do this by creating a configuration file named `.lintstagedrc.cjs` and adding the following code to it:\n\n```javascript\n//.lintstagedrc.cjs\nmodule.exports = {\n    'src/**/*.js': ['eslint --fix', 'prettier --write --ignore-unknown']\n}\n```\n\nThis file uses a glob pattern to match all JavaScript files in the `src` directory, and it lints and formats all staged files that are about to be committed.\n\n### Testing\n\nNow if we add and commit `counter.js` with the intentional issues that were added earlier in the article, we will see the following output:\n\n![Command line output showing a failed ESLint and Prettier task during a pre-commit script run. The errors include an unused variable and an unexpected console statement, resulting in one error and one warning. ](https://cdn.hashnode.com/res/hashnode/image/upload/v1737689398082/0431469a-8c26-4e86-8dd8-4ce097804100.png align=\"left\")\n\n<details data-node-type=\"hn-details-summary\"><summary>â„¹ï¸ Bonus Tip</summary><div data-type=\"detailsContent\">If you want to test the lint-staged output and configuration without adding and undoing commits, you can run <code>npx lint-staged --debug</code></div></details>\n\n## El Fin ðŸ‘‹ðŸ½\n\nHopefully, you can start to see the benefits this combination of tools offers. Prettier formats files, ESLint helps you catch potential bugs, Husky makes it easier to create pre-commit hooks, and lint-staged is the glue that keeps this automation process consistent.\n\nBy leveraging these tools, you can automate code formatting and maintain a clean, efficient codebase. These tools not only enhance code quality but also streamline the development process, allowing you to focus more on building and less on manual formatting.\n\nAs always, thank you for reading, and happy coding!\n\n%[https://youtu.be/tIO9cBRm4hc?si=2YMCppoX6T80tf63]",
      "stars": null,
      "comments": 0,
      "upvotes": 11,
      "read_time": "8 min read",
      "language": null
    },
    {
      "title_en": "Don't be a ticket engineer.",
      "url": "https://blog.alanvarghese.me/dont-be-a-ticket-engineer",
      "source": "hashnode",
      "published_at": "2025-01-19T18:30:00+00:00",
      "external_id": null,
      "tags": [
        "software development",
        "AI",
        "engineering",
        "upskilling"
      ],
      "content_length": 5877,
      "content_preview": "Imagine one day, youâ€™re scrolling through your Jira tickets, and the latest ones are marked as resolved by AI. Why wouldnâ€™t it be? If it had access to the code, understands the requirements, and has been trained on previous human input, itâ€™s perfectly capable of completing the task. You however, are left wondering: *Whatâ€™s the point of me doing this anymore?*\n\nThis isnâ€™t a hypothetical scenario anymore. As AI rapidly reshapes software development landscape, a large percentage of engineers face a",
      "content_full": "Imagine one day, youâ€™re scrolling through your Jira tickets, and the latest ones are marked as resolved by AI. Why wouldnâ€™t it be? If it had access to the code, understands the requirements, and has been trained on previous human input, itâ€™s perfectly capable of completing the task. You however, are left wondering: *Whatâ€™s the point of me doing this anymore?*\n\nThis isnâ€™t a hypothetical scenario anymore. As AI rapidly reshapes software development landscape, a large percentage of engineers face a clear choice: evolve and bring strategic value or risk being replaced.\n\nWith tools like ChatGPT, GitHub Copilot, and other LLM-based workflows, much of the repetitive coding, bug-fixing, and routine feature development can be automated. Engineers who focus solely on picking up tickets (or narrowly defined tasks) are setting themselves up to become obsolete.\n\n**The Ticket Engineer Trap**\n\nPicture a typical workday in a large enterprise. You log in, grab a task from the backlog, implement a solution, and move on. Thereâ€™s no room for reflection, no question of why this task is important or if there's a better approach. Youâ€™re simply executing orders, day in and day out. Itâ€™s a reactive, task-based routine and it's where many engineers get stuck. These engineers are often the first to face layoffs when companies need to trim their teams.\n\n1. **AI excels at predictable tasks:**\n    \n    AI shines when it comes to predictable tasks with explicit instructions. Give it a ticket with clear, well-defined requirements, and it will churn out code faster and often more accurately than you could. If your role is to execute these tasks, youâ€™re competing directly with LLMs and ultimately, youâ€™re losing.\n    \n2. **There is no room for creativity or leadership:**\n    \n    As a ticket engineer, you rarely get involved in the creative parts of the process: system design, architectural decisions, and product innovation. These are the areas where human ingenuity thrives and where AI still falls short. If youâ€™re absent from these discussions, you're sidelining your career potential.\n    \n3. **Limited ownership breeds stagnation:**\n    \n    Limited ownership in development is akin to working at an assembly line, churning out parts without envisioning the final product. By only working within narrow boundaries, you miss out on broader, cross-functional experiences that can enhance your skill set like product thinking, UX design, and business strategy. This lack of exposure can restrict your value within the team and your future opportunities.\n    \n\n---\n\n## The solution: Be value-oriented.\n\nFor every ticket you pick up, pause for a moment and ask yourself:\n\n* *Why is this important?*\n    \n* *What value does it bring to the user?*\n    \n* *Is there a better solution?*\n    \n\nBy understanding the intent behind a task, youâ€™ll not only be able to implement it more effectively, but youâ€™ll also be able to propose better solutions or even eliminate unnecessary work. This is how you demonstrate strategic thinking as an engineer not just by executing tasks but solving problems in a way that drives real impact.\n\n1. **Work cross-functionally**\n    \n    If possible, work with designers and product managers to understand the bigger picture behind the tasks that youâ€™re assigned. Early on in the design process, you could offer suggestions and even influence the final product rather than merely carrying them out. Consider yourself a user, foresee potential problems, provide suggestions for UI/UX enhancements, and make sure your work satisfies user needs.\n    \n2. **Upskill in areas AI canâ€™t easily replace**\n    \n    Learn how to design systems that are scalable, effective, and maintainable to make sure youâ€™re contributing at a level AI isnâ€™t equipped to reach yet. Communication, leadership, and mentoring are also human-centric qualities that AI finds difficult to mimic. In technical discussions, code reviews, and team alignment, take the initiative.\n    \n3. **Learn to leverage AI**\n    \n    But donâ€™t just view AI as competition and use it to your advantage. I could write an entire post about this one but the key here is in mastering AI tools to exponentially increase your productivity by automating mundane tasks, freeing you up to focus on more complex challenges.\n    \n\n---\n\nI recently came across a post on [daily.dev by Saqib Tahir](https://app.daily.dev/posts/you-get-paid-based-on-the-level-of-abstraction-you-can-work-at--soryhdhti) that helped me visualize the journey from task-oriented work to strategic ownership. The post breaks down the seniority progression into six levels:\n\n> * **Level 1:** Hereâ€™s the problem, the solution, and how to implement it.\n>     \n> * **Level 2:** Hereâ€™s the problem and the solution. Figure out how to implement it.\n>     \n> * **Level 3:** Hereâ€™s the problem. Figure out the solution.\n>     \n> * **Level 4:** Hereâ€™s a list of problems. Identify the most impactful one to solve.\n>     \n> * **Level 5:** Find all the problems and determine which are worth solving.\n>     \n> * **Level 6:** Predict future problems and create systems to prevent them.\n>     \n\nThis progression illustrates what Iâ€™m advocating for. Moving from executing tasks to becoming value-oriented is how you climb the seniority ladder. When you think beyond solving immediate problems, identifying, prioritizing, and even preventing them, youâ€™ll not only evolve from being a ticket engineer but also elevate your contribution to the team and accelerate your career growth.\n\nAs an engineering lead at an AI-first company, Iâ€™ve seen firsthand the difference between engineers who simply follow orders and those who take ownership of their work. The latter group brings ideas, challenges assumptions, and drives projects forward in ways that no AI tool can. As professionals in a rapidly changing industry, being adaptable is what sets us apart.",
      "stars": null,
      "comments": 1,
      "upvotes": 43,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "YAML vs YML: Developerâ€™s Guide to Syntax and Ease of Use",
      "url": "https://keploy.hashnode.dev/yaml-vs-yml-developers-guide-to-syntax-and-ease-of-use",
      "source": "hashnode",
      "published_at": "2025-01-20T05:02:37.905000+00:00",
      "external_id": null,
      "tags": [
        "YAML",
        "yml",
        "community",
        "difference",
        "languages",
        "syntax",
        "formatting",
        "extension",
        "Developer Tools",
        "Developer Blogging",
        "Developer",
        "Infrastructure as code",
        "XML to YAML",
        "yaml zero to hero",
        "YAML, JSON,DEVOPS"
      ],
      "content_length": 10962,
      "content_preview": "It sounds funny to know, but [**YAML** stands for \"**YAML Ain't Markup Language**.](https://keploy.io/blog/community/building-custom-yaml-dsl-in-python)\" Well, contrary to its unserious nomenclature, itâ€™s considered a pretty widely used data serialization format known for good human readability and scalability.\n\nThe theory becomes funnier when we realize thatÂ **YML is simply a brief alternative to YAML,**Â which has been done for some practical purposes. Yet, there are some interesting difference",
      "content_full": "It sounds funny to know, but [**YAML** stands for \"**YAML Ain't Markup Language**.](https://keploy.io/blog/community/building-custom-yaml-dsl-in-python)\" Well, contrary to its unserious nomenclature, itâ€™s considered a pretty widely used data serialization format known for good human readability and scalability.\n\nThe theory becomes funnier when we realize thatÂ **YML is simply a brief alternative to YAML,**Â which has been done for some practical purposes. Yet, there are some interesting differences in the story of their evolution, including each of their use cases, in this blog.\n\n## **What is YAML?**\n\nYAML is a format for structuring and storing data that is easy for both humans and machines to understand. It allows humans to write and edit the data easily, while computers can read and process it efficiently. Therefore,Â **YAML is defined as a human-readable data serialization format.**\n\n**Data serialization**Â essentially means converting data structures (arrays/objects) to linear format (string/binary data) and storing them in a file or exchanging them between systemsâ€”all without changing their structure. Consider this similar to packing a parcel box, ensuring its items stay intact while shipping.\n\n## **Evolution story of YAML**\n\nIt was initially in **2001** when three developers named Clark Evans, along with Ingy dÃ¶t Net and Oren Ben-Kiki, designed the **YAML format**. At the initial, it represented â€œYet Another Markup Language,â€ but for some reason, it was later changed to â€œYAML Ainâ€™t Markup Language.â€.\n\n## **Why was YAML even created?**\n\nAlthough existing formats like XML and [JSON](https://keploy.io/blog/community/4-ways-to-write-comments-in-json) have been widely accepted and used for data serialization, they fail to be flexible, brief, and human-readable. There is a gap and an absence of an intuitive, extensible, lightweight, and concise format option. YAML was designed to cater to all such use cases and serve developers worldwide in a â€˜better-to-work-withâ€™ format.\n\nAs part of the YAML configuration, any files under this format were assigned the file extension `.yaml` officially.\n\n## **How did YML come into the picture?**\n\nAfter the YAML format was designed and finally introduced, it secured massive adoption by developers worldwide throughout the early 2000s. However, with this, a couple of technical limitations and difficulties were also faced:\n\n### **1) Three-Character Limit for Extensions:**\n\nThe prevailing legacy operating systems at the time included early versions of Windows and primarily MS-DOS (Microsoft Disk Operating System). These systems essentially only allowed a length of up to three characters. As a result, there was an unsaid convention by the developers to start shortening the extension to `.yml` instead of using the official `.yaml` extension.\n\nSurprisingly, YML could successfully fit within the systemâ€™s constraints and environments. Thatâ€™s how YML came into the picture, implicitly!\n\n### **2) Developersâ€™ Convenience:**\n\nLater, after the systems were upgraded and evolved to accommodate high character limits to extensions, the developers still somehow chose `.yml` the shorter version due to reasons like ease of typing and command-line workflows.\n\nHowever, even though the official `.yaml` extension format could have been used, developers made it `.yml` more acceptable to use.\n\n## YAML vs. YML - Whatâ€™s the Correct Syntax?\n\nYAML parsers refer to the libraries or tools that read and process files formatted in the YAML language. When any file with `.yaml` an `.yml` the extension will be executed, the YAML parser wouldnâ€™t differentiate but rather treat them as the same.\n\nTherefore, in the case of file extensions, both mean the same. The data stored inside the file will be read and processed in the same way.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736858989879/718794c3-c5dc-4f2a-87df-3f77a804674d.png align=\"center\")\n\n### Why Both `.yml` and `.yaml` Still Exist Today?\n\nIt often creates a big confusion about the existence of `.yml` and `.yaml` together, while both are pretty widely used. Hereâ€™s how we differentiate the two:\n\n* **YAML:** It is a data-serialization format/standard in itself similar to XML, JSON, etc. It is a language that is used for data serialization. YAML is a format, a language, and a file extension altogether.\n    \n* **YML:** Just a file extension used and seen as `.yml` for the YAML-formatted data files.\n    \n\nThe continued existence of `.yml` alongside `.yaml` is due to a mix of **legacy reasons**, **developer habits**, and **tooling flexibility**.\n\nAny file named with `.yaml` or `.yml` file extension typically represents that the data inside is in the YAML format.\n\n## **What do we use YAML for ?**\n\nThere are a couple of benefits of the YAML format over others. Some of the **benefits** include:\n\n* human-friendly structure, easy to visualize\n    \n* clean syntax\n    \n* simplicity for brackets or tags\n    \n* efficient for machines,\n    \n* lightweight and flexible format\n    \n* allows customizable data structures based on configuration needs\n    \n\nThus, YAML is set to be an ideal format for data serialization. Letâ€™s see where we can use YAML as a real-world use case:\n\n### **Real-World Use Cases of YAML**\n\n* **Configuration Files**:\n    \n    * Similar to the role a blueprint plays in a building construction, YAML defines how any service or application must be configured. For instance, it `docker compose` uses YAML in order to define the networks, services, and volume of any [Docker application](https://keploy.io/blog/technology/secure-your-database-communications-with-ssl-in-docker-containers-learn-to-set-up-ssl-for-mongodb-and-postgresql-efficiently) out there.\n        \n* **Data Serialization:**\n    \n    * YAML helps serialize data (converting complex data structures into a format that can be saved or transmitted). This could again be related to the blueprint and construction analogy. Just like a blueprint provides a model about how everything gets assembled, YAML helps serialize data so that data transfer becomes seamless and easy across different software components (like microservices, web servers, [CI/CD pipelines](https://keploy.io/blog/community/how-cicd-is-changing-the-future-of-software-development), etc.).\n        \n* **Infrastructure as Code**:\n    \n    * Just the way any buildingâ€™s blueprint tells about its planned infrastructure and its organization, YAML could be used in cloud services like **infrastructure as code** tools (for instance: **Kubernetes**, **Ansible**). By this, YAML is responsible for delivering the necessary information related to the infrastructure, config, and deployment pipelines for any opted cloud servers/services.\n        \n* **Example of YAML (Blueprint for an App Configuration):**\n    \n    `app: name: MyApp version: 1.0 settings: debug: true max_connections: 100 environment: production database: host: db.example.com port: 5432 user: admin password: secret`\n    \n* Just the way any build Therefore, any YAML file significantly helps in breaking down any applicationâ€™s configurations into logical components like app name, database information, settings, etc.\n    \n\n## YAML in Modern Tools\n\nYAML has become a cornerstone in modern software development. Some key areas where YAML plays a vital role include:\n\n* **Kubernetes**: YAML files define pods, deployments, and services in Kubernetes clusters. For example, a deployment manifest in YAML might look like:\n    \n\n```yaml\napiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    name: my-app\n  spec:\n    replicas: 3\n    template:\n      metadata:\n        labels:\n          app: my-app\n      spec:\n        containers:\n        - name: app-container\n          image: my-app-image:v1\n```\n\n* [**Keploy:**](http://www.keploy.io) YAML plays an integral role in [automating testing](https://keploy.io/blog/community/exploring-cypress-and-keploy-streamlining-test-automation) workflows. Autogenerated test cases and mocks are stored in YAML format within the test suite, making them both human-readable and easily editable.\n    \n\n```yaml\nversion: api.keploy.io/v1beta1\nkind: Http\nname: test-1\nspec:\n    metadata: {}\n    req:\n        method: POST\n        proto_major: 1\n        proto_minor: 1\n        url: http://localhost:8010/product\n        header:\n            Accept: '*/*'\n            Content-Length: \"43\"\n            Content-Type: application/json\n            Host: localhost:8010\n            User-Agent: curl/7.88.1\n        body: \"{\\n    \\\"name\\\":\\\"Bubbles\\\", \\n    \\\"price\\\": 123\\n}\"\n        timestamp: 2024-06-05T16:53:56.839574946+05:30\n    resp:\n        status_code: 201\n        header:\n            Content-Length: \"37\"\n            Content-Type: application/json\n            Date: Wed, 05 Jun 2024 11:23:56 GMT\n        body: '{\"id\":2,\"name\":\"Bubbles\",\"price\":123}'\n        status_message: Created\n        proto_major: 0\n        proto_minor: 0\n        timestamp: 2024-06-05T16:53:58.94794204+05:30\n    objects: []\n    assertions:\n        noise:\n            header.Date: []\n    created: 1717586638\ncurl: \"curl --request POST \\\\\\n  --url http://localhost:8010/product \\\\\\n  --header 'Accept: */*' \\\\\\n  --header 'Content-Type: application/json' \\\\\\n  --header 'Host: localhost:8010' \\\\\\n  --header 'User-Agent: curl/7.88.1' \\\\\\n  --data '{\\n    \\\"name\\\":\\\"Bubbles\\\", \\n    \\\"price\\\": 123\\n}'\"\n```\n\n## **Conclusion**\n\nYAML is widely accepted to be an indispensable data serialization format due to its simplicity, flexibility, and readability. Thus, itâ€™s ideal for any of the modern software development needs. The humorous origin of its name and the confusing coexistence of `.yaml` and `.yml` file formats were the primary focus of this blog. Weâ€™ve seen how YAML proves to be a really versatile tool for bridging the gap between human-friendly data representation and machine efficiency.\n\n## FAQâ€™s\n\n### **Can we conver**t `.yml` **to**`.yaml` ?\n\nYes, you can convert the former `.yaml` and vice versa by simply renaming the file.\n\n### **Why are some tools still used** `.yml` **by default?**\n\nThough thereâ€™s no difference in functionality, some tools are still used. YAML tool by default due to its frequent usage out of developer preferences, historical reasons, and legacy support.\n\n### **What are some of the use cases of YAML?**\n\nIt is used in CI/CD pipelines (like GitHub Actions and GitLab CI), DevOps, cloud services (like Infrastructure as Code), and configuration management (for instance, Kubernetes, Docker, etc.).\n\n### **Is there any difference between .yml and .yaml file extensions?**  \n\nNo, thereâ€™s no difference in functionality between `.yml` and `.yaml`. Both refer to YAML (YAML Ainâ€™t Markup Language) files and are treated the same by parsers and tools.\n\n### **Can YAML files contain comments?**  \n\nYes, YAML supports comments using the `#` symbol. Anything written after `#` on a line is ignored by the parser, making it useful for documentation or clarifying configuration sections.",
      "stars": null,
      "comments": 2,
      "upvotes": 58,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "Migrating Web Components from Vue 2 to Vue 3 at Open Library",
      "url": "https://blog.rayberger.org/vue-3-web-components-open-library",
      "source": "hashnode",
      "published_at": "2025-01-18T00:56:08.766000+00:00",
      "external_id": null,
      "tags": [
        "Vue.js",
        "vite",
        "Web Development",
        "technology",
        "software development",
        "Open Library"
      ],
      "content_length": 11381,
      "content_preview": "*Update Feb, 2025: [Vue Migration Part 2 - A Simple Solution Emerges](https://blog.rayberger.org/vue-migration-part-2)*\n\n> TL;DR: Migrating Vue 2 Web Components to Vue 3 isn't as straightforward as the docs suggest. We tackled three main challenges: setting up Vite builds, managing multiple components, and handling plugins.\n\nYou can see the final pull request [here](https://github.com/internetarchive/openlibrary/pull/10298/).\n\nThis is the story of how a seemingly simple migration turned into a 2",
      "content_full": "*Update Feb, 2025: [Vue Migration Part 2 - A Simple Solution Emerges](https://blog.rayberger.org/vue-migration-part-2)*\n\n> TL;DR: Migrating Vue 2 Web Components to Vue 3 isn't as straightforward as the docs suggest. We tackled three main challenges: setting up Vite builds, managing multiple components, and handling plugins.\n\nYou can see the final pull request [here](https://github.com/internetarchive/openlibrary/pull/10298/).\n\nThis is the story of how a seemingly simple migration turned into a 20-hour adventure through the world of Vite and Vue 3. If you're considering a similar migration or just curious about the state of Vue 3 [Web Components](https://developer.mozilla.org/en-US/docs/Web/API/Web_components), this is for you.\n\n# Understanding Vue and Web Components\n\nIn contrast to typical Single Page Applications (SPAs), where Vue renders the entire application, [Open Library](https://openlibrary.org) leverages Vue for specific interactive elements within traditional HTML pages. This is accomplished by integrating custom HTML tags, or Web Components, which are then transformed into Vue components.\n\nFor example, when you look at the source of an Open Library page, you might see an `<ol-barcode-scanner>` tag. This custom tag, along with its JavaScript, allows Vue to render just that specific element rather than managing the entire page. It's like having mini Vue apps scattered throughout traditional HTML pages.\n\n## Vue 3 Migration Challenges\n\nWhile Vue 3 is largely compatible with Vue 2, and there are many resources available for migrating from Vue 2 to Vue 3, our challenge lay in building web components in Vue 3. The Vue 3 documentation covers Web Components, but the approach was not directly applicable to our use case, which involves building one component per page with isolated scripts.\n\n## Vue 2 Build Setup\n\nThe process of building Web Components for Vue 2 was very straightforward. We would simply run `vue-cli-service build BarcodeScanner.vue`, and a JS file would be generated that we could use on any page.\n\n# Challenges with Vue 3 Web Components\n\nLet me break down our journey into three main challenges. While each challenge has a straightforward solution in hindsight, finding these solutions took considerable exploration.\n\n## Building one Web Component with Vite\n\nThe [deprecated Vue CLI](https://cli.vuejs.org/) we used to build Vue 2 Web Components does not support Vue 3 Web Components and has not been updated in years. As a result, I investigated the latest and greatest build tool: Vite. There's one big difference with Vite: it doesn't accept `.vue` files (Single File Components) like Vue CLI did. Instead, Vite requires a configuration file as its input.\n\nThis posed a challenge because I was 1) disbelieving that it couldn't take .vue files and instead required two new files for every component, 2) trying to get it to create all the components without so many new files, and 3) attempting to get a Vue plugin working (more on that later). That being said, the docs are fairly straightforward on how to do this for just one component. I just needed two new files:\n\n```js\n// vite.config.js\nexport default defineConfig({\n    plugins: [vue({ customElement: true })], // Because all of our Vue components are customElements\n    build: {\n        outDir: 'PRODUCTION_DIR',\n        emptyOutDir: false, // Preserve existing files since we build components individually\n        target: 'es2015', // The oldest browsers Vite supports out of the box\n        rollupOptions: {\n            input: join(BUILD_DIR, `BarcodeScanner.js`),\n            output: {\n                entryFileNames: `ol-barcode-scanner.js`,\n                inlineDynamicImports: true, // needed for components to work with just one js file\n                format: 'iife' // use iife to support old browsers without type=\"module\"\n            },\n        },\n    },\n});\n```\n\n```js\n// BarcodeScanner.js\nimport { defineCustomElement } from 'vue';\nimport ele from './BarcodeScanner.vue';\ncustomElements.define('ol-barcode-scanner', defineCustomElement(ele));\n```\n\n## Building multiple Web Components\n\nAt this point, we have a Vite config file that can only have one input, which is a JavaScript file, and it produces a single output, the desired JavaScript file. This limitation poses a challenge since we have numerous components to build. While Vite config files support multiple input and output setups, this functionality is not available when using the [inlineDynamicImports](https://rollupjs.org/configuration-options/#output-inlinedynamicimports) option, which we need for the components to work in isolation.\n\nThe problem we face is that we need a single configuration file to be mapped to one JavaScript file, which in turn is mapped to one Vue file. This setup seems overly complicated. Three files now for each component instead of just one .vue file. The two new files are basically identical for each component. There has to be a way around this, right?\n\n[![](https://cdn.hashnode.com/res/hashnode/image/upload/v1737059841771/fbc0cade-6441-430e-929d-7b98d523e330.png align=\"center\")](https://mermaid.live/edit#pako:eNqFU11P20AQ_Cure44tH05C8ANS84GE1EqVgIqQRNX5vEmusu-s-yCkcf47Zx80UIrqB2u8O7M7HvsOhKsCSUbWpdrxLdMWbqdLCf4K9_vFmOmWc8OZlKjjR4criKLL5jszBguwCiZfrxuYL3wn4qWIDOpHwRFyJ8piFcbMg0SrwnE0UHfACiVhLUps4GGhyigPmyLzsuqXWb218uVTK9dVrbT1ZvJ9A-PAnnR-dphHXPm2RGmjnWZ1jTqIbhU4g-BZUJduI6TJQLwb9Pc-IWtnT67G3Zi7NgRmoGs2MP1gUliMuZJrsTlJp_9KcLZoue9im_0ntqtPYwt64_KNf-ct_HB45RUmlLs0T_A-QJTFB924NfNzoqqKyeKNenaC8z9q0iMV6oqJwv9Qh7a8JHaLFS5J5mHOjEdLefQ85qy62UtOMqsd9ohWbrMl2ZqVxj-5umAWp4J5E9UrpWaSZAfyRDJK07hPaTI6T9IhTc9GtEf2vpzEyWDQT0dJnw59j54de-S3Un5CEl-kCaUXw6STDc8H3byHrvmyFAthlf4WjkP4YOT4DKMUAaU)\n\nThe left half is the old Vue CLI setup, the right half is the new vite setup.\n\n### Trial and Error\n\nThrough reading docs, trial and error, and ChatGPT, we painfully accepted that we indeed need a Vue file, a JavaScript file that points to the Vue file, and a config file that points to the JavaScript file (please someone come and show me how I'm wrong). If we were to approach this in a traditional manner, we would require a Vue file, a JavaScript file, and a Vite config file for each component. Currently, we have five components, which means we would end up adding ten new files that are essentially identical. There has to be a way around this, right?\n\n### Solution\n\nIt seems too hacky for some of the best webdev tools (Vue and Vite), but I found a solution that actually worked without adding 10 new files to the codebase. It comes in two parts.\n\nFirst, set an environment variable `COMPONENT_NAME` that `vite.config.js` reads so that we don't need one config file for every component:\n\n```js\nconst COMPONENT_NAME = process.env.COMPONENT;\nexport default defineConfig({\n    build: {\n        /// shorted to show the change\n        rollupOptions: {\n            input: join(BUILD_DIR, `vue-tmp-${COMPONENT_NAME}.js`),\n            output: { entryFileNames: `ol-${COMPONENT_NAME}.js`, },\n        },\n    },\n});\n```\n\nSecond, we need to deal with these pesky input files. So we just generate them from a string... in the config file itself.\n\n```js\n\ngenerateViteEntryFile(COMPONENT_NAME);\n\nexport default defineConfig({...})\n\nfunction generateViteEntryFile(componentName) {\n    const template = `\nimport { defineCustomElement } from 'vue';\nimport ele from './${componentName}.vue';\ncustomElements.define('${kebabCase(componentName)}', defineCustomElement(ele));\n`;\n\n    try {\n        writeFileSync(join(BUILD_DIR, `vue-tmp-${componentName}.js`), template);\n    } catch (error) {\n        // eslint-disable-next-line no-console\n        console.error(`Failed to generate Vite entry file: ${error.message}`);\n        process.exit(1);\n    }\n}\n```\n\nAll things considered, this solution is simple and keeps us from having many nearly identical files to keep in sync. That being said, I really can't believe these hacks (environment variables and generated input files) are the best way.\n\n## Plugin Support\n\nEverything was functioning smoothly for all of our components, except for one that utilizes a Vue plugin: `vue-async-computed`. We previously considered removing this plugin since it didn't support Vue 3, but we ultimately decided against it because it helps maintain a clean structure in our code. Drini actually made the [PR to add Vue 3 support](https://github.com/foxbenjaminfox/vue-async-computed/pull/124) years ago. Given the lack of updates to that plugin, maybe it's worth reconsidering switching to newer alternatives like [computedAsync](https://vueuse.org/core/computedAsync/), but dang it, I'm on a mission to upgrade to Vue 3. Besides, Drini made a good point that there are nice plugins we should consider using after the upgrade.\n\nI explored various approaches to integrate the plugin into our Vue component. The guides I found often went into Vue's internals, particularly regarding how to incorporate the plugin for the component. Our team does not consist of Vue experts; we simply want a little more interactivity on parts of the site.\n\nUltimately, I decided to use the `vue-web-component-wrapper` library. To be fair, I saw it earlier, but resisted adding another dependency for something that should be easy. Anyway, the library simplifies the process by allowing us to pass in the desired plugin along with our component. Short and simple, and I even added a conditional so the plugin is only added for the one component that needs it.\n\n# Summary\n\nI faced many challenges while migrating from Vue 2 Web Components to Vue 3 Web Components. However, they don't seem too hard now that we have solutions.\n\nI am certain that there are technical reasons for the difficulties encountered during this migration. However, if I had a magic wand, here are the changes I would like:\n\n1. Vite (rollup under the hood) should allow for multiple inputs and outputs while utilizing `inlineDynamicImports`. This feature alone would have greatly simplified the entire process. If this cannot be implemented, then the [Web Components guide](https://vuejs.org/guide/extras/web-components) should be updated to include a reasonable workaround. Related issue [here](https://github.com/rollup/rollup/issues/5601).\n    \n2. The Web Component guide should be updated to include information about plugins. It would be helpful to explain how to add them without introducing another dependency, or at the very least, to point to `vue-web-component-wrapper` as a viable solution. Issue opened [here](https://github.com/vuejs/docs/issues/3151).\n    \n3. Vite should be able to accept `.vue` files in the `vite.config.js`.\n\nOverall, this migration process required approximately 20 hours of my effort (not counting the attempts by other folks in previous years). Throughout this process, I did not find a single example that addressed all of these challenges. I hope that this account will be helpful to those poor souls who find themselves migrating from Vue 2 to Vue 3 in 2025 and beyond. Perhaps the Vue and Vite teams will take note of this and improve the official guide and possibly even the tooling. Happy open sourcing!\n\n*Found this helpful? Consider joining our* [*Open Library*](https://github.com/internetarchive/openlibrary) *community as a dev, librarian, designer, or any other way you want to help.*\n\nPS: Huge thanks to Drini for reviewing and merging this pull request!",
      "stars": null,
      "comments": 0,
      "upvotes": 12,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "Debugging Authorization: How Cerbos Makes Troubleshooting Access Issues a Breeze",
      "url": "https://tech-on-diapers.hashnode.dev/debugging-authorization-how-cerbos-makes-troubleshooting-access-issues-a-breeze",
      "source": "hashnode",
      "published_at": "2025-01-15T09:05:41.202000+00:00",
      "external_id": null,
      "tags": [
        "debugging",
        "authorization",
        "troubleshooting",
        "YAML",
        "policies",
        "IAM",
        "backend",
        "audit logs",
        "Testing",
        "Beginner Developers",
        "Security"
      ],
      "content_length": 10071,
      "content_preview": "*When you hear the word \"authorization,\" what comes to your mind? Before I learned about it properly, I thought it was just about checking if someone was allowed to do something - like a simple yes or no gate. While that basic idea isn't wrong, there's so much more beneath the surface. Authorization is really about creating an intricate web of trust and permissions that determines not just who can access what, but how different parts of a system interact with each other.*\n\n*And that's where tool",
      "content_full": "*When you hear the word \"authorization,\" what comes to your mind? Before I learned about it properly, I thought it was just about checking if someone was allowed to do something - like a simple yes or no gate. While that basic idea isn't wrong, there's so much more beneath the surface. Authorization is really about creating an intricate web of trust and permissions that determines not just who can access what, but how different parts of a system interact with each other.*\n\n*And that's where tools like* [*Cerbos*](https://www.cerbos.dev/) *come into play. Just like how we evolved from using simple keys to sophisticated access control systems, we have moved beyond basic allow/deny rules to powerful* [*policy-based authorization*](https://www.cerbos.dev/features-benefits-and-use-cases/pbac)*. Cerbos lets you define these complex permission rules in a way that's both powerful and elegant - think of it as the master conductor orchestrating who gets to do what across your entire system, making sure every access request follows the exact rules you've set up, without missing a beat.*\n\n### **Pre-requisites**\n\nTo get the most out of this article, readers should have a foundational understanding of the following:\n\n* **Basic Authorization Concepts:** Authentication vs. authorization, roles, permissions, and access control.\n    \n* **IAM Systems:** Role-based or attribute-based access control models.\n    \n* **YAML Configuration:** Syntax and editing basics.\n    \n* **Distributed Systems:** Managing access in microservices/cloud setups.\n    \n* **Cerbos Basics:** Overview of Cerbos and its role in simplifying authorization.\n    \n* **Debugging Tools:** Experience with logs and troubleshooting.\n    \n\n### **Table of Contents**\n\n* Introduction\n    \n* Understanding Common Authorization Debugging Challenges\n    \n* Cerbos Debugging Tools and Features\n    \n* Step-by-Step Debugging with Cerbos\n    \n* Benefits of Using Cerbos for Debugging\n    \n* Conclusion\n    \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736936821297/00258d41-ccba-4e93-871d-bafeeb6385fe.jpeg align=\"center\")\n\n### Introduction\n\nDebugging authorization can feel like solving a puzzle with missing pieces, especially when users are denied access despite having the right permissions. In this article, we explore common authorization issues and how Cerbos simplifies debugging access problems.\n\n**The Pain of Debugging Authorization**\n\n* Misconfigured roles/permissions.\n    \n* Lack of visibility into access decisions.\n    \n* Complexities in distributed systems.\n    \n\nTraditional debugging approaches often rely on guesswork, leading to time-consuming and error-prone processes. Cerbos, on the other hand, offers clear insights and simplifies access troubleshooting.\n\n**How Cerbos Simplifies the Process**\n\nEnter Cerbosâ€”a policy-based authorization tool thatâ€™s designed to make debugging access issues not just easier, but almost enjoyable. With Cerbos, you get:\n\n* **Clear Audit Logs:** Understand every access decision at a glance.\n    \n* **Readable Policies:** No more struggling with cryptic configuration files.\n    \n* **Policy Testing Tools:** Validate changes before they go live, saving you from future headaches.\n    \n\nIn the sections ahead, weâ€™ll break down how Cerbos addresses the pain points of traditional debugging and gives you the tools to resolve access issues efficiently and with confidence.\n\n### Understanding Common Authorization Debugging Challenges\n\nWhen it comes to debugging authorization, things can get tricky fast. Itâ€™s not just about getting a simple \"yes\" or \"no\" on access; there are layers of complexity. Letâ€™s explore some of the most common challenges developers face when debugging authorization issues.\n\n* **Misconfigured Roles and Permissions**\n    \n\nPermissions may not align as expected, leading to access denials despite correct roles.\n\n* **Lack of Transparency**\n    \n\nWithout clear feedback on access decisions, troubleshooting can become inefficient.\n\n* **Distributed Systems**\n    \n\nDebugging becomes more complicated in environments where services and policies are spread across multiple systems.\n\nBy understanding these common challenges, you can start thinking about ways to make your authorization debugging process smoother, more efficient, and way less frustrating.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736931241959/5cc60965-25b5-481b-b6cb-1f114d2ed72a.jpeg align=\"center\")\n\n### Cerbos Debugging Tools and Features\n\nWhen it comes to debugging authorization issues, Cerbos is like having a clear roadmap in the middle of a foggy forest. Hereâ€™s how its tools make the journey a lot easier:\n\n* **Audit Logs: Gaining Visibility into Access Decisions**\n    \n    Think of audit logs as your applicationâ€™s diary. They track every access decision, giving you a full breakdown of who accessed what and why. Instead of guessing where things went wrong, you can dive into detailed logs and pinpoint exactly where a misstep occurred. No more blind troubleshootingâ€”Cerbos makes it transparent.\n    \n* **Human-Readable Policies: Identifying Misconfigurations Easily**\n    \n    Configuration files can often feel like a secret code, but Cerbosâ€™ human-readable policies cut through the complexity. Instead of struggling with cryptic rules, you can quickly scan through policies in plain language, making it easier to spot any misconfigurations. Itâ€™s like reading a map instead of a maze.\n    \n* **Testing Policies: Validating Before Deploying**\n    \n    Cerbos lets you test your policies before they go live. This means you can validate your rules in a controlled environment, ensuring they work as intended without breaking anything in production. Itâ€™s like running a rehearsal before the big performanceâ€”no surprises, just smooth execution.\n    \n\nWith these tools, debugging authorization issues becomes a breeze, and the process of fixing them is both faster and more accurate.\n\n### Step-by-Step Debugging with Cerbos\n\n**A Common Scenario: Denied Access with Correct Role**\n\nLetâ€™s start with the problem at hand: a user is assigned the correct role, but they canâ€™t access the resource they need. You check the permissions, and everything seems fine. But the system still wonâ€™t let them through. This is a classic case where debugging tools like Cerbos can save you time and sanity.\n\n**Using Audit Logs to Trace Issues**\n\nThe first thing youâ€™ll want to do is dig into the **audit logs**. Cerbos provides detailed logs of every authorization decision made. By reviewing these logs, you can quickly see what happened when the user attempted to access the resource. Was it a permissions issue? Or maybe the role didnâ€™t get applied as expected? The logs will give you the visibility you need to start tracing the problem.\n\n**Reviewing and Adjusting YAML Policies**\n\nNext up: **YAML policies**. With Cerbos, you define your access control rules in YAML, making it easy to see exactly whatâ€™s happening under the hood. If the logs point to a permissions issue, itâ€™s time to take a closer look at the policies. Was the correct role linked to the right permissions? Sometimes, a small typo or misconfiguration can cause big headaches, so give those policies a thorough review.\n\n**Testing and Deploying Fixes**\n\nOnce you've tracked down the issue and adjusted the policies, itâ€™s time to test. Cerbos lets you simulate access control decisions, so you can ensure that everything works as expected before pushing changes live. Run a few tests to confirm that the denied user can now access the resource. If everything checks out, deploy the fix to production with confidence.\n\nBy following these steps, youâ€™ll not only resolve the access issue but also build a better understanding of how Cerbos helps streamline authorization debugging.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736931290906/26574e36-9fb7-44ca-a68c-9d204c220e40.jpeg align=\"center\")\n\n### Benefits of Using Cerbos for Debugging\n\n**Time Savings with Clear Insights**\n\nWith Cerbos, debugging authorization issues no longer feels like searching for a needle in a haystack. The clear, human-readable audit logs provide instant insights into access decisions, saving you valuable time. Instead of diving into complex logs or running endless tests, you can quickly pinpoint what went wrong and get back to business.\n\n**Improved Accuracy with Readable and Testable Policies**\n\nCerbos doesnâ€™t just make debugging easierâ€”it makes your policies more reliable. Its readable policy language and built-in testing tools allow you to validate changes before they go live, ensuring you catch potential issues early. No more guessing if a policy change will work as expectedâ€”Cerbos gives you the confidence that everything is set up correctly.\n\n**Enhanced Collaboration Between Teams**\n\nDebugging authorization often requires input from multiple teamsâ€”security, development, operations. Cerbos simplifies collaboration by making policies transparent and easy to understand. With the ability to test policies together and see who has access to what, everyone can stay on the same page and solve problems faster.\n\n### Conclusion\n\nIn a nutshell, Cerbos makes debugging authorization a whole lot easier. With features like [detailed audit logs](https://www.cerbos.dev/features-benefits-and-use-cases/audit-logs), [human-readable policies](https://www.cerbos.dev/features-benefits-and-use-cases/human-readable-authorization), and [testing tools](https://www.cerbos.dev/news/new-tools-for-effortless-policy-creation-and-testing-in-cerbos-hub), Cerbos helps you quickly identify and fix access issues. Gone are the days of sifting through endless lines of code or grappling with confusing configurations.\n\nIf youâ€™re tired of the headache that comes with troubleshooting access problems, itâ€™s time to give Cerbos a try. Its simple, powerful approach can save you time, boost your confidence in your authorization setup, and make collaboration with your team smoother than ever. So, take the plunge and see how Cerbos can make your life easierâ€”and your access control, a breeze!",
      "stars": null,
      "comments": 0,
      "upvotes": 42,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "I Built A Serverless Ephemeral AWS Account Vending Machine",
      "url": "https://benoitboure.com/i-built-a-serverless-ephemeral-aws-account-vending-machine",
      "source": "hashnode",
      "published_at": "2025-01-07T07:59:31.982000+00:00",
      "external_id": null,
      "tags": [
        "AWS",
        "serverless",
        "aws-cdk",
        "aws learning ",
        "AWS Management",
        "AWS Cost Optimization",
        "AWS Account Management"
      ],
      "content_length": 15350,
      "content_preview": "Last November 2024, I attended an [AWS user group meetup in Barcelona](https://www.meetup.com/barcelona-amazon-web-services-meetup/events/304525815/). I found [Joan GarcÃ­a](https://www.linkedin.com/in/jggtic/)'s sessionsÂ particularly interesting. He explained how they addressed some recurring challenges at [Ocado](https://www.linkedin.com/company/ocado-technology/posts/?feedView=all), such as safely conducting proofs of concept or running hackathons without disrupting production environments whi",
      "content_full": "Last November 2024, I attended an [AWS user group meetup in Barcelona](https://www.meetup.com/barcelona-amazon-web-services-meetup/events/304525815/). I found [Joan GarcÃ­a](https://www.linkedin.com/in/jggtic/)'s sessionsÂ particularly interesting. He explained how they addressed some recurring challenges at [Ocado](https://www.linkedin.com/company/ocado-technology/posts/?feedView=all), such as safely conducting proofs of concept or running hackathons without disrupting production environments while keeping costs under control.\n\nUnfortunately, this session was not recorded. I wonâ€™t get into the details but, in short, they implemented a way for their teams to request ephemeral, self-destructed AWS accounts. Users request an account to run a PoC, a hackathon, a workshop, etc. After a set time, or when a specific budget is reached, all the resources in the account are automatically deleted, and the account is closed. In other words: an AWS account vending machine.\n\nI thought it was a great idea! We all love to play with new AWS services, run quick PoCs, etc. but I often forget or am too lazy to clean up after myself, which clutters my AWS accounts with unnecessary resources. In some cases, it can even incur unnecessary costs.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736019458647/b6b5a412-d375-49a5-ab86-12e4933c927a.png align=\"center\")\n\nAlthough Infrastructure as code helps with the tear-down process, I donâ€™t always dot it. Sometimes, I donâ€™t even use IaC, especially if I just want to test or try something quickly. The idea of getting an ephemeral AWS account that I can mess with, and knowing that everything in it will automatically get destroyed later sounded very attractive. The bad news was: Ocadoâ€™s solution is not open source... So I re-built it myself.\n\n## Requirements\n\nHere are the requirements I had in mind before starting this project:\n\n* **Security**\n    \n    * All sandbox accounts should stay secure within the same AWS Organization.\n        \n    * Users can only access accounts they are supposed to. e.g. they canâ€™t access other userâ€™s accounts.\n        \n    * Users should access the vending machine and sandbox accounts using their SSO credentials.\n        \n* **Low Cost:** The solution should be cheap to run (Whatâ€™s the point of building this to save on costs if the solution itself ends up costing more than the savings?). For that reason, I wanted the solution to be 100% serverless.\n    \n* **Simple**:\n    \n    * Users should be able to easily request new accounts through a simple Web App.\n        \n    * Users should access sandbox accounts from the AWS access portal, or the AWS CLI, using SSO, just like they do for any other long-lived account (e.g. dev, prod).\n        \n\n## Solution Overview\n\nThis solution uses IAM Identity Center. Users sign in using SSO to access AWS accounts under the same organization. They [access a web application](https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-applications.html) where they can request a new sandbox account. When a user introduces a request, an account is randomly picked from a pool of AWS accounts specially created for this purpose. It is assigned to the user with a pre-determined [Permission Set](https://docs.aws.amazon.com/singlesignon/latest/userguide/permissionsetsconcept.html) in IAM Identity Center. The user can then sign into the account using the AWS console, or the AWS CLI with SSO. When the sandbox expires, or when the user requests it, the account assignment is removed. The user loses access to the account and all the resources are deleted using [aws-nuke](https://github.com/ekristen/aws-nuke) (`aws-nuke` is an open-source utility that deletes all the resources in an AWS account). Finally, the account is recycled and put back into the account pool for future assignments. Administrators can control who can access the web application.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736015186656/8763dd4e-a755-4603-994f-d122c68faf2e.png align=\"center\")\n\n![Request an AWS Sandbox Account](https://cdn.hashnode.com/res/hashnode/image/upload/v1736004642617/9dd20563-bd9a-43fa-932e-e10614c1c69e.png align=\"center\")\n\n![Sandbox Account Ready](https://cdn.hashnode.com/res/hashnode/image/upload/v1736004724880/33691fbb-345b-490a-a601-13972c13531b.png align=\"center\")\n\n![List of Sandbox accounts](https://cdn.hashnode.com/res/hashnode/image/upload/v1736004764782/8455ae43-beed-40dc-9a10-20dd78b21ad0.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736015374201/969c5b3a-5358-478c-bf0e-c07e29ee29b7.png align=\"center\")\n\n## How it works\n\nHere is an overview of the architecture.\n\n![](https://img.plantuml.biz/plantuml/png/dHPDJzimz9vVyPRBWaIR8IvJXH2b7GXf1KJ6E4mxk8wRMYHsP3ljkgd_VS_v3cbWur3vVN-_brveGvJ9ajZv4B8L5mocJy4zuh0s9jKJtrTaEuwuMMVBJ3D5fJ1Cc36LYK-sEYPBRTyHHuOUhQGQfJ4Hrg2_EVay_kI7N1ld0nSqpiBQk8_lJ2Q95ECqzts07_0aZVcAit189aK-9OPBSOD1HIe7BJdO2Vf_Ie5XwLKcg4NqWEgrabcgRMXJIcM6HJWi5p2QAMvsDo4M2bzC57qIGPbVaT00qtf118bOWgG76RDtM9ikQYe-J0sOQFnSomrJ8bU-Kn4H_7UUlduzeRVrsmY97mKVCZKdXYo9Plvy9qWYvvS3SZCSCuBJgBH_HT2u6IhFG3-_R33QIyN3Y0LqpS8i7gpEBJDRgPwvY6R5RCzPp37DdJ-BPHPUAJdSQICLLiRFHvMLMC3KXjDtj7Cc8ooSB1GTZH6bH944Ogo3sQKCKXdlBE8upWeP3Do0Y70fVq7PF-q2qQ1BuXy7uBM1yt1lRxDdPk5ZS352Yu55NSJT8zG_D2KUATxZddyugHLRjc5q3gNA1BuneY2KMsVlE8HYnS2rPoKFt0AEq-nNld1UKWSzhVqsYLIktQEtyyMP2B7D2qBNMCKoxUyTOZVxte9vMaja8hts10K7l22uEoerz_qikhuRlr0vxkPXPCJC6irQ1A2yQHv9kUrKojtoEShPX-RFqwUsGGPRH-69BJKtJM80pMnxj0QHtT1XZfTRyMLcUz_MBCRKeyLhTGe87h5_S2zbN4llyTVFfdDiFQ8rZqJJsscEr_NeKuem3csufoi8jLe2K4kq8dihBiLYXfUg2UoX8BGZUqagGYilLhLVnMw11RlkgnjOBPIsajKc5v8ePRS2HZ5R6ToZfbSLvwnc5Lr0UYDKw-bJfEjJ6E7g2RkFARTBclAQqastOYeUhQr-fqaLyyUQXxW5Fpk-orau6wDJBISGJ1TVRHeVeMYZtKPhe6qGnYZTtyw9bShmpYb4NkgKSLUbwx7FwrcmcLVmwFn5ddbaSuQJBdS8Txd_65TNgH_OONun7CsfX76MehlMuPpOajL-KB8VYhsjUPcl5VxhyX_OHgBDlNE1alWP-Gi0 align=\"left\")\n\n[Source](https://editor.plantuml.com/uml/dHPDJzimz9vVyPRBWaIR8IvJXH2b7GXf1KJ6E4mxk8wRMYHsP3ljkgd_VS_v3cbWur3vVN-_brveGvJ9ajZv4B8L5mocJy4zuh0s9jKJtrTaEuwuMMVBJ3D5fJ1Cc36LYK-sEYPBRTyHHuOUhQGQfJ4Hrg2_EVay_kI7N1ld0nSqpiBQk8_lJ2Q95ECqzts07_0aZVcAit189aK-9OPBSOD1HIe7BJdO2Vf_Ie5XwLKcg4NqWEgrabcgRMXJIcM6HJWi5p2QAMvsDo4M2bzC57qIGPbVaT00qtf118bOWgG76RDtM9ikQYe-J0sOQFnSomrJ8bU-Kn4H_7UUlduzeRVrsmY97mKVCZKdXYo9Plvy9qWYvvS3SZCSCuBJgBH_HT2u6IhFG3-_R33QIyN3Y0LqpS8i7gpEBJDRgPwvY6R5RCzPp37DdJ-BPHPUAJdSQICLLiRFHvMLMC3KXjDtj7Cc8ooSB1GTZH6bH944Ogo3sQKCKXdlBE8upWeP3Do0Y70fVq7PF-q2qQ1BuXy7uBM1yt1lRxDdPk5ZS352Yu55NSJT8zG_D2KUATxZddyugHLRjc5q3gNA1BuneY2KMsVlE8HYnS2rPoKFt0AEq-nNld1UKWSzhVqsYLIktQEtyyMP2B7D2qBNMCKoxUyTOZVxte9vMaja8hts10K7l22uEoerz_qikhuRlr0vxkPXPCJC6irQ1A2yQHv9kUrKojtoEShPX-RFqwUsGGPRH-69BJKtJM80pMnxj0QHtT1XZfTRyMLcUz_MBCRKeyLhTGe87h5_S2zbN4llyTVFfdDiFQ8rZqJJsscEr_NeKuem3csufoi8jLe2K4kq8dihBiLYXfUg2UoX8BGZUqagGYilLhLVnMw11RlkgnjOBPIsajKc5v8ePRS2HZ5R6ToZfbSLvwnc5Lr0UYDKw-bJfEjJ6E7g2RkFARTBclAQqastOYeUhQr-fqaLyyUQXxW5Fpk-orau6wDJBISGJ1TVRHeVeMYZtKPhe6qGnYZTtyw9bShmpYb4NkgKSLUbwx7FwrcmcLVmwFn5ddbaSuQJBdS8Txd_65TNgH_OONun7CsfX76MehlMuPpOajL-KB8VYhsjUPcl5VxhyX_OHgBDlNE1alWP-Gi0)\n\n### Sandbox Accounts\n\nSandbox accounts are organized in an Organization Unit (OU). These are the accounts that are assigned to a user when they request one. They donâ€™t hold any resources except for SSO roles and a special role: `AWSNuke` to which weâ€™ll come back later.\n\n### Management Account\n\nThis is the management account for the AWS organization structure. Users live in this account (in IAM Identity Center). This account also holds a special cross-account role: `VendingMachine`. This role is assumed by the Vending Machine service to assign or revoke access to an account for a specific user.\n\n### Vending Machine Account\n\nThis is the Vending Machine service. The service is composed of the following components:\n\n#### A static website\n\nThis is a React SPA which the users use to request a new sandbox. It is stored on S3 and served by AWS CloudFront distribution.\n\n#### AppSync API\n\nAn AppSync GraphQL API is the gateway between the users and the service. It triggers the `Assign Account` and `Release Account` Step Function workflows (see below).\n\n**Amazon Cognito**\n\nA Cognito user pool controls user authentication to AWS AppSync. [IAM Identity Center is used as an Identity provider using SAML](https://repost.aws/knowledge-center/cognito-user-pool-iam-integration).\n\n#### Accounts DynamoDB Table\n\nThis DynamoDB table contains information about the sandbox accounts such as their id and status (e.g. `USED` or `FREE`). When in use, it also stores who is using the account (user id), when it expires, etc.\n\n#### EventBridge Scheduler\n\nWhen a user requests an account, a schedule is created at the expiration date to destroy it and return it to the account pool.\n\n#### Assign Account Step Functions Workflow\n\nWhen a user requests a new account, the AppSync API starts this Step Function Workflow, which orchestrates the assignation of an account to the user.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736003288838/770adbcf-b439-4213-80d6-3f01de05864a.png align=\"center\")\n\nFirst, it tries to find an available account (i.e. an account that is not already used by another user (`status = FREE`) and immediately locks it in DynamoDB (`status = USED`) so that no other users can have the same account assigned to them.\n\nAfter that, the workflow invokes the [CreateAccountAssignment](https://docs.aws.amazon.com/singlesignon/latest/APIReference/API_CreateAccountAssignment.html) command to assign the account to the requester user. It does so assuming the `VendingMachine` role in the management account.\n\nFinally, we schedule the execution of the `Release Account` workflow at the expiration date of the account. By default, itâ€™s 14 days after the request time, but the user can request a shorter or longer period. We also put an `accountAssigned` event into an Event Bridge bus to let other services know about it. This event is used by the [EventBridge AppSync integration](https://docs.aws.amazon.com/eventbridge/latest/userguide/target-appsync.html) to notify the user that the account is ready, in real-time.\n\n#### Release Account Step Functions Workflow\n\nThis other Step Function workflow orchestrates the destruction of an account; either when it expires (triggered by the EventBridge scheduler), or because the user requested it through the web app when itâ€™s no longer needed.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736003850594/4fdd1708-8dd8-4803-9703-39204618e803.png align=\"center\")\n\nFirst, we check if the user initiated the destruction. When this is the case, we delete the schedule that we created when setting up the account. When triggered by the EventBridge scheduler, the schedule is automatically deleted (We set `ActionAfterCompletion: 'DELETE'`).\n\nThen, we remove access to the account from the user using the [DeleteAccountAssignment](https://docs.aws.amazon.com/singlesignon/latest/APIReference/API_DeleteAccountAssignment.html) command. We also remove the user association in the `accounts` DynamoDB table.\n\nThen, we invoke `aws-nuke`. Since itâ€™s a long-running process, it is executed within an ECS task running on Fargate. `aws-nuke` assumes the `AWSNuke` role within the targeted account and uses that role to delete all resources.\n\nFinally, we release the account in DynamoDB so that it can be re-assigned to another user later.\n\n## Cost Estimations\n\nAssuming a moderate usage of this service, the operational cost should be close to zero.\n\nWith an average of 10 account requests per month, here are the cost estimations of the main components:\n\n**AppSync API**\n\n* API requests: &lt; 1000 requests(1) at $4/million requests = ~$0.004\n    \n* ~10 real-time updates at $2/million = ~$0.00002\n    \n* &lt; 10 connection minutes at $0.08/million minute = &lt;$0.0000008\n    \n\n(1) Assuming that users consult the web app more often to check expiration times, etc.\n\n**DynamoDB**\n\nUsage should stay under the free tier. Outside free tier:\n\n* ~50 write requests = $0.00003125\n    \n* ~1000 read requests = $0.000125\n    \n* &lt;1MB storage = $0.00025\n    \n\n**Cognito**\n\n* 50 MAU (Monthly Active Users) are free (with SAML identity provider).\n    \n* $.015 per MAU after that.\n    \n\n**Step Functions**\n\n~150 state transitions. Within the always-free tier of 4,000 state transitions per month.\n\nOutside the free tier, at $0.025 / 1,000 transitions: $0.025 Ã— 150 / 1000 ~= $0.00375\n\n**EventBridge (Event bus and scheduler)**\n\n* ~10 events per month: ~$0.00001 ($1 / million events)\n    \n* ~10 schedules per month: Well under the 14M schedules free tier; or ~$0.00001 ($1 / million schedule triggers)\n    \n\n**Static Website**\n\n* S3: ~1.4 Mb - $0.023 x 0.0014GB = $0.0000322\n    \n* Cloudfront: CloudFront has an always-free tier of 1TB data transfer and 10M HTTPS requests, which is probably more than enough for this use case.\n    \n\n**ECS Fargate**\n\nProbably the highest cost. This solution uses the lowest configuration possible (0.25 vCPUs and 512 MB of memory). Assuming an average of ~15 minutes execution each time.\n\n* CPU: 0.25 vCPU x 0.25 hours x 10 x $0.03238 = $0.0202375 (ARM architecture)\n    \n* Memory: 0.5GB Ã— 0.25 hours x 10 x $0.00356 = $0.001869\n    \n\n**Total cost**\n\nEven excluding the free tier, the total cost of operation should not go over a few cents per month. Of course, your mileage may vary, depending on the size of your organization, the number of users, and how many times the service is used.\n\n## Itâ€™s Open Source!\n\nThis project is open-source, you can find it on [GitHub](https://github.com/bboure/aws-account-vending-machine-demo)**.** Feel free to fork it and deploy it into your account, share it, and send me your feedback!\n\n## Whatâ€™s Next?\n\nThis solution is basic. I built it both as a PoC and for the challenge. Itâ€™s also good enough for my personal usage and as an MVP. However, I can see a few improvements that could be added:\n\n* **User notifications**: Before an account is destroyed, users might want to get warned a few days before it happens.\n    \n* **Budgets**: I didn't include budget limits in this MVP, but automatically removing costly resources before they increase your AWS bill would be a useful feature.\n    \n* **Time/Budget Extension:** Need more time/budget to work on your project? Request an extension.\n    \n* **Multiple Permission Sets**: The current implementation grants the `AdministratorAccess` permission set by default. One might want to support more than one permission set, depending on the use case the account is created for, or who requests it.\n    \n* **Team accounts**: When working with teams, you might want to assign an account to a team, instead of single users, so that several people can work on it at the same time.\n    \n* **Manager approval**: Companies might want to require approval by a manager before an account is granted to a user or team. Managers could also be able to control expiration times, budgets, permission sets, etc.\n    \n\n## Conclusion\n\nTrying new AWS services, making proofs of concepts, or attending workshops can easily clutter your AWS accounts and incur costs if you forget to clean up after yourself. Ephemeral, self-destructed AWS accounts can help eliminate or mitigate those problems. With a self-service vending machine, users can request sandbox accounts to play with and focus on their projects while the cleanup process is automated.",
      "stars": null,
      "comments": 2,
      "upvotes": 27,
      "read_time": "9 min read",
      "language": null
    },
    {
      "title_en": "Are Web APIs the same as REST APIs? Build More Scalable Web Apps with Web APIs",
      "url": "https://michelle-buchi.hashnode.dev/are-web-apis-the-same-as-rest-apis-build-more-scalable-web-apps-with-web-apis",
      "source": "hashnode",
      "published_at": "2025-01-14T08:37:37.871000+00:00",
      "external_id": null,
      "tags": [
        "projectfugu",
        "Web Development",
        "webdev",
        "Web API",
        "Programming Blogs"
      ],
      "content_length": 9728,
      "content_preview": "To answer the question: **No**, Web APIs are not the same as REST APIs.\n\n**Web APIs** refer to interfaces that allow applications to interact with the web or devices. This term is broad and can refer to Browser APIs (like WebRTC or Geolocation) or Server APIs (like REST or GraphQL).\n\nOn the other hand, **REST APIs** are a specific type of Server API that adhere to REST principles, focusing on statelessness, resource-based communication, and using HTTP methods like GET, POST, and DELETE.\n\n## The ",
      "content_full": "To answer the question: **No**, Web APIs are not the same as REST APIs.\n\n**Web APIs** refer to interfaces that allow applications to interact with the web or devices. This term is broad and can refer to Browser APIs (like WebRTC or Geolocation) or Server APIs (like REST or GraphQL).\n\nOn the other hand, **REST APIs** are a specific type of Server API that adhere to REST principles, focusing on statelessness, resource-based communication, and using HTTP methods like GET, POST, and DELETE.\n\n## The Evolution of the Web\n\nOriginally, the web was designed as a static site for providing information and connecting individuals to relevant resources, regardless of location. However, it has evolved to become interactive and progressive, with certain aspects also turning decentralized. This article focuses on the web being progressive and more user-friendly.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/huv0g6zma3hcbcays5oc.png align=\"left\")\n\n## The Role of Web APIs in Modern Development\n\n**Web APIs** serve as the backbone of modern web development. With APIs like WebRTC, Web Bluetooth, and Service Workers, developers can create highly interactive, native-like web applications without requiring external plugins. These capabilities make **Progressive Web** Apps (PWAs) possible, bridging the gap between web and native apps. Check out my extensive article on progressive web apps.\n\n[https://michelle-buchi.hashnode.dev/how-to-get-started-with-progressive-web-apps](https://michelle-buchi.hashnode.dev/how-to-get-started-with-progressive-web-apps)\n\n**Some Benefits of Progressive Web Apps**\n\n* Works on any browser (cross-platform)\n    \n* Responsive to any device\n    \n* Functions without network connectivity\n    \n* Performs like a mobile app\n    \n* Highly secure\n    \n* Discoverable via search engines\n    \n* Boosts app engagement\n    \n* No installation required\n    \n* Can be shared through a link\n    \n* Optionally available on app stores (e.g., Google Play, Apple Store)\n    \n\n## Web APIs and Project Fugu\n\nWeb APIs here, referring to browser APIs, act as connectors, improvers, and builders for the browser's capabilities. These APIs are inherent to the web and are not third-party APIs purchased from libraries or sites. Instead, they are broadly available and designed to improve the web and enhance products created by developers, reducing boilerplate code and simplifying web interactions.\n\nThe significance of web APIs is underscored by the fact that over 70% of activities conducted online occur through the web, highlighting the need for improvements and scalability.\n\nIn summary, web APIs are crucial for the success of Progressive Web Apps and the future of web development. They act as enablers of advanced functionalities.\n\n## Project Fugu\n\nProject Fugu derives its name from the Japanese word for **pufferfish,** \"fugu.\" Initially, I was curious about why this project was named after a fish until I learned more about it. The fugu is a dangerous fish that can be delicious when prepared correctly; however, if not handled properly, it can be harmful. This metaphor aptly reflects the project's goal of enhancing the web platform's capabilities to match those of native applications. The correlation here is that the Project Fugu APIs aim to enrich the web (the â€˜deliciousâ€™ aspect), and if they are not built correctly (the â€˜preparationâ€™), they can introduce significant security vulnerabilities. Imagine creating an API accessible to billions without adequate security measures; it would expose individuals to various security risks.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3v01mjl4clolpmdg4i72.png align=\"left\")\n\nYou can find more about Project Fugu and its APIs at [https://developer.chrome.com/docs/capabilities/fugu-showcase](https://developer.chrome.com/docs/capabilities/fugu-showcase). Here, you will discover interesting and exciting projects built with web APIs, as well as a comprehensive list of available APIs. Many native app features that were previously unavailable on the web are now accessible, thanks to web APIs.\n\n## Why Web APIs?\n\nWeb APIs are crucial for several reasons, and their importance should not be overlooked. They empower the browser, transforming the web from a simple content viewer into a powerful application platform. With APIs like WebRTC, WebSockets, Service Workers, Web Bluetooth, and Web XR, you can now accomplish tasks on the web that were once limited to native applications.\n\n## Why Should You Start Using Web APIs?\n\nWeb APIs offer numerous benefits, including:\n\n**Access to Native Device Features:** They allow you to access device hardware such as cameras, microphones, sensors, and Bluetooth devices. This means you can make video calls, control robots, and experience augmented reality directly in your browser, without needing to download any apps.\n\n**Offline and Background Functionality:** Service Workers enable features like push notifications and caching, enhancing the user experience even when offline.\n\n**Simplified App Development for Cross-Platforms:** Web APIs facilitate easier development across different platforms.\n\n**How Web APIs Improve the Web**\n\nWeb APIs bring the capabilities of native apps to the web, granting access to powerful features without requiring installations or downloads. They also help future-proof the web; as browsers evolve, more APIs will be developed for seamless integration. Additionally, web APIs are accessible across platforms.\n\n## Is it Easy to Get Started with Web APIs?\n\nYes, it is straightforward. Most web APIs are already integrated into browsers, and supported by robust documentation. Since many are native to the browser, there's no need for third-party libraries or installations. This makes getting started seamless, consistent, and accessible across platforms.\n\n## Some Web APIs\n\nWeb APIs are the cornerstone of the Progressive Web App (PWA) revolution. They form the backbone of improved web functionality and represent the future of web development. Numerous Web APIs are available, and you can find a comprehensive list in the MDN documentation or on [web.dev](http://web.dev). Below are about 10 Web APIs to help you understand how they work. You can find many more in the MDN docs,[web.dev](http://web.dev), or via the project fugu documentation.\n\n**WebRTC API**: Enables real-time audio, video, and data communication directly between browsers without the need for external plugins. It is commonly used for video conferencing and peer-to-peer file sharing.\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1g56swgaakxmionqg625.png align=\"left\")\n\n**Web Bluetooth API**: Allows websites to communicate with Bluetooth-enabled devices, such as fitness trackers, medical devices, and IoT gadgets.\n\n**WebXR API**: Provides support for immersive technologies like Virtual Reality (VR) and Augmented Reality (AR), allowing developers to create engaging 3D experiences directly in the browser.\n\n**WebSocket API**: Facilitates full-duplex communication between the client and server, making it ideal for real-time applications like chat systems, live streaming, and collaborative editing.\n\n**Push Notification API**: Enables websites to send timely and relevant notifications to users even when the website is not open in the browser, enhancing user engagement.\n\n**Web Share API**: Provides a standardized way for web apps to share content using native sharing mechanisms, making it easier to share links, files, or text with other applications.\n\n**Fetch API**: Replaces the older XMLHttpRequest and simplifies asynchronous data fetching.\n\n**Web Payment API**: Simplifies the online payment process by offering a consistent interface for users to make payments using various methods, such as credit cards or digital wallets.\n\n**Geolocation API**: Provides access to the geographical location of the user, enabling location-aware functionality like maps, location-based recommendations, or geofencing.\n\n**File System Access API**: Enables web applications to read from and write to the userâ€™s local file system, creating a native-like experience for file manipulation.\n\n## WebRTC Demo\n\nWebRTC enables real-time communication, including audio, video, and data-sharing capabilities. For a simple WebRTC demo, you will need:\n\nindex.html\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/cznslh245qaglxxpp0lu.png align=\"left\")\n\napp.js\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pf9k0or5aqfa0d717s3m.png align=\"left\")\n\nserver.js\n\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0v8uvpe0sneny0knpc3w.png align=\"left\")\n\nWebSocket or HTTP REST API Inbuilt WebRTC API Local and remote video elements Buttons to manage peer connections WebRTC configurations\n\n## How It Works\n\n**Access User's Camera and Microphone**: Create a function to access the user's camera and microphone, displaying the local stream on the local video element.\n\n**Peer Connection**: Implement a function to handle media exchange.\n\n**Signaling**: Simulate sending and receiving data through the signaling server.\n\n**Remote Stream Handling**: Display the remote track in the remote video element. **ICE Candidates**: Exchange these candidates with peers to establish a connection.\n\nWeb APIs are the cornerstone of modern web development, enabling developers to build powerful, scalable, and engaging applications. Whether you're exploring **WebRTC** for real-time communication or leveraging the File System Access API for native-like experiences, the possibilities are vast. Start experimenting today and contribute to the web's progression.",
      "stars": null,
      "comments": 0,
      "upvotes": 22,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "Must-Have Skills for Upcoming Software Developers and AI Engineers in 2025",
      "url": "https://blog.futuresmart.ai/must-have-skills-for-upcoming-software-developers-and-ai-engineers-in-2025",
      "source": "hashnode",
      "published_at": "2025-01-12T18:48:55.797000+00:00",
      "external_id": null,
      "tags": [
        "internships",
        "General Advice",
        "generative ai",
        "Developer",
        "mentorship"
      ],
      "content_length": 12765,
      "content_preview": "Over my 8+ years of industry experienceâ€”ranging from backend development and data engineering to machine learning and the latest generative AI applicationsâ€”Iâ€™ve seen firsthand what it really takes to add value in a team. As the Founder of [FutureSmart AI](https://www.futuresmart.ai/), Iâ€™ve mentored numerous students and interviewed candidates, and one clear pattern emerges:\n\n**You need solid fundamental skills in coding, APIs, Databases and Communications to thrive in the rapidly evolving tech l",
      "content_full": "Over my 8+ years of industry experienceâ€”ranging from backend development and data engineering to machine learning and the latest generative AI applicationsâ€”Iâ€™ve seen firsthand what it really takes to add value in a team. As the Founder of [FutureSmart AI](https://www.futuresmart.ai/), Iâ€™ve mentored numerous students and interviewed candidates, and one clear pattern emerges:\n\n**You need solid fundamental skills in coding, APIs, Databases and Communications to thrive in the rapidly evolving tech landscape.** This holds true whether you aim to become a traditional software developer or youâ€™re intrigued by the cutting edge of Generative AI. If youâ€™re focused on learning the â€œshinyâ€ new [AI tools](https://aidemos.com/) without first mastering the core foundations, youâ€™ll struggle to deliver real impact.\n\n[Being an Expert-Vetted (Top 1%) on Upwork with over $300K+ earned and a 100% Job Success rate](https://www.linkedin.com/feed/update/urn:li:activity:7242065082073927681/) has further reinforced my understanding of what skills truly matter in the industry.\n\nBelow, Iâ€™ll walk through the essential skills that, in my experience, every aspiring developer or AI engineer should prioritize.\n\n## 1\\. Why Practical Skills Matter\n\nMany newcomers are drawn to the excitement of [advanced AI concepts](https://aidemos.com/ai-playground)â€”like Generative AI, LLMâ€™s, RAG (Retrieval Augmented Generation), or vector databases. While these innovations are undoubtedly game-changing, they can overshadow the day-to-day coding, debugging and deployment tasks that keep a project running smoothly.\n\n**Think about it:** Even the most cutting-edge AI system is useless if it canâ€™t be integrated into a functional application. That requires reliable code, proper database connections, and well-structured APIs. Itâ€™s why companiesâ€”when hiring interns or fresh gradsâ€”emphasize practical software development skills just as much as familiarity with new AI trends.\n\n**Key point:** Whether your goal is traditional software development or specialized AI engineering, youâ€™ll stand out by showing you can handle the basics and add value to the team and not liability. Mastering these fundamentals will make it far easier to adopt advanced [AI tools](https://aidemos.com/) effectively.\n\n## 2\\. Embracing Self-Learning & Adaptability\n\nIn a world where [**ChatGPT**](https://youtu.be/RALmm6flXII?si=w_gJy6bOginm9IiP) and [**YouTube tutorials**](https://www.youtube.com/@FutureSmartAI) are just a click away, memorizing syntax is less important than knowing **how** to discover answers and adapt them to your specific needs. When you face a new challengeâ€”be it **connecting a database** or **troubleshooting an API route**â€”your ability to learn on the fly is what truly sets you apart.\n\n* **Resourcefulness Over Rote Memorization**: Whether youâ€™re copying a snippet from Stack Overflow or ChatGPT, your real value is in customizing and debugging that code for your application.\n    \n* **Hands-On Experience**: Reading documentation is great, but actually **breaking things** and fixing them is how you learn effectively.\n    \n* **Continuous Upskilling**: The AI landscape evolves quickly. If you want to keep up, you need to be comfortable **teaching yourself** new frameworks and libraries.\n    \n\n## 3\\. Mastering Practical Python\n\nWhile you donâ€™t need to be a â€œPython guruâ€ or a competitive programming champion, you do need to write clear, functional code that gets the job done. Hereâ€™s what matters most:\n\n* **Use the Right Data Structures:** Know when to use lists for ordered data, dictionaries for key-value pairs, and tuples for lightweight groupings.\n    \n* **Organize Your Code with Functions and Files:** This makes your code more readable, reusable, and easier to debug. keep files small instead of writing long scripts.\n    \n* **Handle Files and Data:** Most real-world software and AI projects involve ingesting or cleaning data. Be comfortable working with CSV, JSON, or other common formats.\n    \n\n### **Common Pitfalls to Avoid:**\n\nâŒ Writing overly complex, hard-to-read code.\n\nâŒ Ignoring error handlingâ€”always use `try-except` blocks.\n\nâŒ Copy-pasting from Stack Overflow or ChatGPT without understanding the solution.\n\nâŒ Writing long, unstructured Python files instead of modular, function-based code split into small files.\n\n### **Hands-on Challenge:**\n\n> Write a Python script that reads a JSON file, processes it, and writes an updated file. Keep the script modular by splitting different functionalities into separate functions and files.\n\nIn the professional world, clarity and reliability are often more valuable than clever but cryptic code. Once your Python basics are solid, youâ€™ll be able to pick up new libraries or advanced frameworks with minimal fuss.\n\n## 4\\. Working with APIs\n\nThe ability to consume and provide APIs is crucial in modern software developmentâ€”whether youâ€™re building e-commerce backends, [AI-driven chatbots](https://youtu.be/W3FadhIpSmU?si=QQLZyacFcHo70M4v), or anything in between.\n\n### **API Consumption**\n\nMost applications communicate via RESTful APIs. Know how to:\n\nâœ… Send requests (**GET**, **POST**, **PUT**, **DELETE**).\n\nâœ… Handle authentication (e.g., bearer tokens, API keys).\n\nâœ… Parse JSON responses (often deeply nested).\n\n### **Handling Authentication & Security**\n\nExpect to encounter JWT (JSON Web Tokens), cookies, and CORS (Cross-Origin Resource Sharing). Understanding these ensures your app communicates securely and efficiently.\n\n### **Working with JSON**\n\nAPIs typically return JSON, so youâ€™ll deal with Python dictionaries. Knowing how to navigate nested structures and handle edge cases will save you hours of troubleshooting.\n\n### **Practical Use Cases**\n\n* Fetch data from a public APIs (e.g., weather, currency exchange), transform the response, and store it for analytics.\n    \n\n## 5\\. Essential SQL and Database Operations\n\nNo matter what you build, you need a reliable way to store and retrieve data. SQL databases like **MySQL** and **PostgreSQL** are still core technologies in production environments.\n\n* **Basic SQL Queries:** Learn how to `SELECT`, `INSERT`, `UPDATE`, and `DELETE`. These cover most interactions with a relational database.\n    \n* **Connecting with Python:** Use libraries like `psycopg2` or `mysql-connector-python` to integrate your Python application with the database.\n    \n\n### [**Real-World Example**](https://youtu.be/38aMTXY2usU?si=1RyzZVkT95Zcq64O&t=4569)\n\nMany LLMs donâ€™t persist previous user inputs once an API call finishes. To maintain conversation history (for a chatbot or a Generative AI assistant):\n\n1. Assign a **session ID** for each conversation.\n    \n2. Store all messages (both user and AI) in a SQL database.\n    \n3. For subsequent queries, retrieve the conversation history using that session ID.\n    \n4. Pass the history to the LLM for context, then store the new response.\n    \n\nThis simulates â€œmemoryâ€ and vastly improves user experience.\n\n### **Practice Task:**\n\n> Create a simple database that stores user messages with timestamps and session ids. Design a query to retrieve past messages for a specific user session.\n\n## 6\\. [Creating Your Own APIs with FastAPI](https://youtu.be/KVdP4SpWcc4?si=IjXk83spo3qlRSim)\n\nEven if youâ€™re not building complex AI services, knowing how to serve any functionality via an API is invaluable. FastAPI is a popular choice in Python circles due to its simplicity and asynchronous capabilities.\n\n### **Why FastAPI?**\n\n* **Speed and Simplicity:** An asynchronous framework that makes handling concurrent requests more efficient.\n    \n* **Auto-Generated Documentation:** Teammates and clients can easily understand and test your endpoints.\n    \n* **Widely Used in AI & ML:** FastAPI has become a go-to for deploying machine learning models quickly.\n    \n\n### **Key Concepts**\n\n* **Endpoints (Routes):** Clearly define the URLs (e.g., `/predict`) for specific functionalities.\n    \n* **Sync vs. Async:** Leverage asynchronous functions for better performance when scaling out.\n    \n* **Data Validation:** Validate incoming JSON data before processing to prevent runtime errors.\n    \n\n### **Mini Project:**\n\n> Build a FastAPI service that accepts text input and returns a sentiment score using an NLP model.\n\n## 6\\. [Getting Comfortable with Basic Deployment](https://youtu.be/7FVPn25mmEQ?si=JAYGnuiKbHIZbrE3)\n\nWhether youâ€™re a back-end engineer or a budding AI Engineer, your application needs to be accessible to others to deliver real value. Thatâ€™s where deployment comes in.\n\n### **Why Deployment Matters**\n\n* **Accessibility:** A deployed app or API can be accessed by anyone with the right permissions.\n    \n* **Real-World Feedback:** Live usage data and metrics guide iterative improvements.\n    \n* **Collaboration:** Demonstrates you understand the full development lifecycle, not just coding.\n    \n\n### **Common Deployment Options**\n\n1. **Cloud VMs:** AWS, Azure, or Google Cloud let you rent servers and install your software stack.\n    \n2. **PaaS (Platform as a Service):** Services like Heroku or Render manage much of the infrastructure for you.\n    \n3. **Company Infrastructure:** Internships often provide access to enterprise-grade tools, a great way to learn hands-on without personal costs.\n    \n\n### **Hands-on Task:**\n\n> Deploy your FastAPI app to Render or a cloud VM and access it via a public URL.\n\n## 7\\. Where Advanced AI Concepts Fit In\n\nOnce youâ€™ve nailed down the basics, exploring advanced AI topics becomes far more rewarding. Whether itâ€™s Generative AI, RAG, vector databases, or frameworks like LangChain, these cutting-edge tools are best leveraged when you can integrate them seamlessly into real applications.\n\n### **Integration Over Isolation**\n\n* [**RAG (Retrieval Augmented Generation)**](https://youtu.be/38aMTXY2usU?si=AqdP50LzuuWcPokO)**:** Fetch relevant context from a vector database, then pass it to an LLM.\n    \n* [**Vector Databases**](https://youtu.be/5NG8mefEsCU?si=y9AKNx7KsRo2RgnD)**:** Specialized for embedding-based searches, often used in semantic search or question-answering.\n    \n* [**LangChain (and Similar)**](https://youtu.be/NQWfvhw7OcI?si=dv_VfrRQYQKwUw2S)**:** Frameworks that streamline building LLM-powered applications.\n    \n\n## **Real-World Example:**\n\n> Suppose you're developing a document search tool for legal professionals. Using RAG and vector databases allows efficient retrieval of case laws based on user queries, improving productivity compared to traditional keyword-based search. and legal precedents for lawyers. It uses RAG to fetch the right references and LLMs to summarize them.\n\n### **Applied AI Use Case:**\n\nImagine you're building an AI-powered FAQ chatbot. Instead of just **generating random responses**, you: 1ï¸âƒ£ **Retrieve relevant FAQs** from a vector database.\n\n2ï¸âƒ£ **Pass that context to an LLM.**\n\n3ï¸âƒ£ **Generate a response using the AI model.**\n\n4ï¸âƒ£ **Deliver the output via an API.**\n\nThe entire system **depends** on strong coding, API, and database skills.\n\n> Tip: Instead of jumping straight to LangChain, try manually implementing a RAG pipeline to understand how everything connects.\n\n%[https://youtu.be/8sSHg1034r0?si=-q_Q35vXNo6R_FcL] \n\n## 8\\. Conclusion & Next Steps\n\nAfter years of working I can confidently say these foundational skills are what truly enable successâ€”whether youâ€™re a software developer, a data scientist, or an AI specialist. Iâ€™ve interviewed and mentored many candidates, and the ones who excel are those who can **both** understand advanced AI concepts **and** handle the essential coding and deployment work.\n\n### **Key Takeaways**\n\n* **Practice & Portfolio:** Build small, complete projects that demonstrate your ability to write clean Python code, work with APIs, interact with a database, and (ideally) deploy your application.\n    \n* **Seek Internships & Mentorship:** Real-world experience accelerates learning. Youâ€™ll gain exposure to production environments and valuable feedback from peers or senior developers.\n    \n* **Explore Advanced AI Topics:** Once you have the basics locked in, frameworks like LangChain or vector databases can add significant value to your skill set.\n    \n* **Stay Curious & Adaptable:** Technology moves fastâ€”those who keep learning and adapting will always stay relevant.\n    \n\n**Final Thought:**\n\nThe world of techâ€”especially AIâ€”is growing at an unprecedented pace. Make sure you have the fundamentals in place so you can ride that wave rather than getting washed away by it. With a strong foundation, youâ€™ll be ready to contribute meaningfully to any team, whether youâ€™re a software developer, data engineer, or the next generative AI whiz.\n\nFor real-world examples of our work, take a look at our [**case studies**](https://www.futuresmart.ai/case-studies).",
      "stars": null,
      "comments": 1,
      "upvotes": 39,
      "read_time": "8 min read",
      "language": null
    },
    {
      "title_en": "Cookie-based Authentication in Remix",
      "url": "https://tigerabrodi.blog/cookie-based-authentication-in-remix",
      "source": "hashnode",
      "published_at": "2025-01-12T14:42:45.387000+00:00",
      "external_id": null,
      "tags": [
        "Web Development",
        "Security",
        "authentication",
        "React",
        "webdev",
        "Open Source"
      ],
      "content_length": 23519,
      "content_preview": "# Introduction\n\nIn this blog post, I wanna go over cookies and how to implement cookie-based authentication in React Router 7 (as a framework).\n\nYes, the title uses Remix since most are familiar with it. The team has now rebranded it to React Router 7. Which you now use as a library or framework.\n\nI don't assume you're familiar with cookies. We'll start from the basics and move up from there.\n\nI'm a big fan of learning from first principles. We'll dig into source code and specs. It's gonna be a ",
      "content_full": "# Introduction\n\nIn this blog post, I wanna go over cookies and how to implement cookie-based authentication in React Router 7 (as a framework).\n\nYes, the title uses Remix since most are familiar with it. The team has now rebranded it to React Router 7. Which you now use as a library or framework.\n\nI don't assume you're familiar with cookies. We'll start from the basics and move up from there.\n\nI'm a big fan of learning from first principles. We'll dig into source code and specs. It's gonna be a lot of fun.\n\n![A Contagious Smile : r/Naruto](https://preview.redd.it/xzt83rz2ea291.jpg?width=1024&format=pjpg&auto=webp&s=2b857f45d935c9378932cb4920801e13394554ac align=\"left\")\n\nI recommend reading MDN and documentation along the way. Read it in detail. Take your time. Make sure you understand things. Don't just read this post and \"think\" I covered everything.\n\nMy mindset to learning to is to be the \"owner\" of how I consume information. Which is why you'll see me digging into source code, specs, etc.\n\nRemember, nothing is magic. You can learn everything. That fancy library or framework you see, whether itâ€™s written in Rust, TypeScript, Zig, you name it, a human being wrote it.\n\n# What are cookies?\n\nCookies are small pieces of data stored as text strings in the browser. The fundamental format is `name=value`. They can have additional attributes, separated by semicolons.\n\nWhen a server wants to set a cookie, it sends an HTTP response header: `Set-Cookie: session=abc123; HttpOnly; Secure; SameSite=Strict`\n\nThe browser then stores this cookie and **automatically attaches** ([link to spec](https://datatracker.ietf.org/doc/html/rfc6265#section-4.2)) it to future requests to the same domain in the `Cookie` header: `Cookie: session=abc123`.\n\nThe automatic attachment by browsers is important. It's why cookies are good for authentication. You don't need to manually handle this in your client side code.\n\n## Cookie attributes\n\n### HttpOnly\n\nJavaScript can't access the cookie, preventing [XSS attacks](https://owasp.org/www-community/attacks/xss/). XSS stands for Cross Site Scripting, it's when an attacker injects malicious code into a web page.\n\n### Secure\n\nCookie only sent over HTTPS. HTTPS is the secure version of HTTP which uses [TLS encryption](https://en.wikipedia.org/wiki/Transport_Layer_Security) under the hood.\n\n### SameSite\n\nControls when cookie is sent in cross-site requests. This prevents CSRF attacks. A [CSRF attack](https://owasp.org/www-community/attacks/csrf) is when an attacker tricks a user into making a POST request to a website without their knowledge.\n\nFor example, if you get an email with a button that says \"click here to view your account\", and you click it, the browser will automatically send the cookie with the request. The attacker can then use this to access the user's account.\n\nIt's common to set this value to `SameSite=Lax` to allow cookies to be sent in cross-site requests when GET requests are made. This way, they can remain logged in to the site.\n\n### Domain\n\nWhich domains can receive the cookie. This is useful if you want to share cookies between subdomains.\n\n### Path\n\nWhich paths on the domain receive the cookie.\n\n### Expires/Max-Age\n\nWhen the cookie should be deleted.\n\n# Cookies in React Router 7 (as a framework)\n\nIn React Router 7, there are two primary ways to handle cookies: [createCookie](https://api.reactrouter.com/v7/functions/react_router.createCookie) and [createCookieSessionStorage](https://api.reactrouter.com/v7/functions/react_router.createCookieSessionStorage.html).\n\nThe full documentation for both can be found here: [React Router 7 - Sessions and Cookies](https://reactrouter.com/explanation/sessions-and-cookies).\n\nMy goal here isn't to restate everything in the docs. But I do want to go over a few things in a clearer way.\n\n## createCookie\n\nThe source code for `createCookie` can be found here: [cookies.ts - Line 73](https://github.com/remix-run/react-router/blob/a3e4b8ed875611637357647fcf862c2bc61f4e11/packages/react-router/lib/server-runtime/cookies.ts#L73).\n\n`createCookie` is the fundamental building block for dealing with cookies.\n\n* Creates a single cookie with name/value\n    \n* Handles raw cookie operations (parse/serialize)\n    \n* Manages cookie attributes (expires, path, etc.)\n    \n* Can sign cookies for security\n    \n* Just manages ONE single key-value pair in the cookie\n    \n\nWhen is it useful?\n\n* Simple preferences (theme, language)\n    \n* Single pieces of data\n    \n\nHere is a code example:\n\n```typescript\n// createCookie - theme preference\nconst themeCookie = createCookie(\"theme\", {\n  maxAge: 604_800, // one week\n  path: \"/\",\n});\n\n// Usage\nresponse.headers.set(\"Set-Cookie\", await themeCookie.serialize(\"dark\"));\n```\n\n## What is a \"session\"?\n\nIn the documentation, createCookieSessionStorage is described to provide a session-like interface. For someone who isn't familiar with the concept of a session, this can be confusing.\n\nIn traditional web development (e.g. with PHP), **a session refers to a server side storage of data that persists across requests.** For example, when you log into a PHP app, it creates a session file on the server with your data, and sends you (the client) a session ID cookie. The server uses this ID to find your data.\n\nSo it's gonna map the id to the data. On the server, the data can be stored in a database or file system. Files on disk is PHP's default.\n\n## createCookieSessionStorage\n\nThe source code for `createCookieSessionStorage` can be found here: [cookieStorage.ts - Line 26](https://github.com/remix-run/react-router/blob/a3e4b8ed875611637357647fcf862c2bc61f4e11/packages/react-router/lib/server-runtime/sessions/cookieStorage.ts#L26).\n\n`createCookieSessionStorage` uses `createCookie` internally. It stores all your data in one cookie. That's how it is different from the traditional session approach. Now, if you want, you can create a [custom session storage](https://reactrouter.com/explanation/sessions-and-cookies#creating-custom-session-storage).\n\nHere is an example of how to use it from the documentation:\n\n```typescript\nimport { createCookieSessionStorage } from \"react-router\";\n\ntype SessionData = {\n  userId: string;\n};\n\ntype SessionFlashData = {\n  error: string;\n};\n\nconst { getSession, commitSession, destroySession } =\n  createCookieSessionStorage<SessionData, SessionFlashData>(\n    {\n      // a Cookie from `createCookie` or the CookieOptions to create one\n      cookie: {\n        name: \"__session\",\n\n        // all of these are optional\n        domain: \"reactrouter.com\",\n        // Expires can also be set (although maxAge overrides it when used in combination).\n        // Note that this method is NOT recommended as `new Date` creates only one date on each server deployment, not a dynamic date in the future!\n        //\n        // expires: new Date(Date.now() + 60_000),\n        httpOnly: true,\n        maxAge: 60,\n        path: \"/\",\n        sameSite: \"lax\",\n        secrets: [\"s3cret1\"],\n        secure: true,\n      },\n    }\n  );\n\nexport { getSession, commitSession, destroySession };\n```\n\nWe touched on cookie options already e.g. `name`, `maxAge`, `path`, etc. If you want the source code for the types that React Router 7 uses, you can find it [here](https://github.com/jshttp/cookie/blob/master/src/index.ts#L161).\n\nLet's look at two quick snippets. One that demonstrates persisting userId and the other one shows using flash messages.\n\n## Auth example\n\nThis is just a quick example. I don't wanna dig too deep since I'm doing things a bit differently in my own project to keep it type safe.\n\n```typescript\nconst { getSession, commitSession } = createCookieSessionStorage({\n  cookie: {\n    name: \"auth\",\n    secrets: [\"your-secret\"],\n    secure: true,\n    sameSite: \"lax\",\n    maxAge: 60 * 60 * 24 * 30, // 30 days\n  },\n});\n\n// in your action\nasync function login(request: Request) {\n  const session = await getSession(request.headers.get(\"Cookie\"));\n\n  // here you'd get the userId from from the database\n  // im just hardcoding it as demonstration\n  session.set(\"userId\", \"123\");\n\n  return redirect(\"/dashboard\", {\n    headers: {\n      // you always need to commit the session\n      // otherwise the cookie won't be set in the headers\n      // ...and the changes only remain in memory\n      \"Set-Cookie\": await commitSession(session),\n    },\n  });\n}\n\n// Auth check in loaders\nasync function checkAuth(request: Request) {\n  const session = await getSession(request.headers.get(\"Cookie\"));\n  const userId = session.get(\"userId\");\n\n  if (!userId) {\n    return redirect(\"/login\");\n  }\n\n  // ...\n}\n```\n\n## commitSession quickly explained\n\n`commitSession` can be a bit of a confusion. You may wonder why changes aren't set after calling `session.set()`.\n\nIn the [source code](https://github.com/remix-run/react-router/blob/a3e4b8ed875611637357647fcf862c2bc61f4e11/packages/react-router/lib/server-runtime/sessions/cookieStorage.ts#L45), you'll see that commitSession is what actually sets the cookie in the headers:\n\n```typescript\n    async commitSession(session, options) {\n      let serializedCookie = await cookie.serialize(session.data, options);\n      if (serializedCookie.length > 4096) {\n        throw new Error(\n          \"Cookie length will exceed browser maximum. Length: \" +\n            serializedCookie.length\n        );\n      }\n      return serializedCookie;\n    }\n```\n\n## Flash messages example\n\n```typescript\nconst { getSession, commitSession } = createCookieSessionStorage({\n  cookie: {\n    // can be any name you want e.g. toast\n    name: \"flash\",\n    secrets: [\"your-secret\"],\n    sameSite: \"lax\",\n  },\n});\n\n// Action that sets flash\nasync function saveAction(request: Request) {\n  const session = await getSession(request.headers.get(\"Cookie\"));\n  session.flash(\"success\", \"Changes saved successfully!\");\n\n  return redirect(\"/dashboard\", {\n    headers: {\n      \"Set-Cookie\": await commitSession(session),\n    },\n  });\n}\n\n// Loader that reads flash\nasync function loader(request: Request) {\n  const session = await getSession(request.headers.get(\"Cookie\"));\n  // Flash messages are removed after being read\n  const message = session.get(\"success\");\n\n  return json(\n    { message },\n    {\n      headers: {\n        \"Set-Cookie\": await commitSession(session),\n      },\n    }\n  );\n}\n```\n\n# Authentication in my side project\n\nNow it's time for the exciting stuff hihi\n\nI wanna share how I did authentication in my side project. We're gonna cover password based authentication.\n\nNow, I was using [Prisma as ORM](https://www.prisma.io/docs/orm/overview/introduction/what-is-prisma) and [Prisma Postgres](https://www.prisma.io/docs/orm/overview/databases/prisma-postgres) as database. You can use whatever you want though. That's just how we store data.\n\nI'll first talk about how I deal with the cookies & redirects, then later we can look at the password stuff.\n\n## Cookies & redirects\n\nI'm using `createCookieSessionStorage` to handle the cookies. The relevant file for this is under [auth.server.ts](https://github.com/tigerabrodi/narumory/blob/main/app/lib/auth.server.ts).\n\nI like to keep things type safe, so under the constants.ts file, you'll see how I deal with the cookie strings and routes:\n\n```typescript\nexport const ROUTES = {\n  home: '/',\n  roomDetail: '/rooms/:roomCode',\n  roomJoin: '/rooms/:roomCode/join',\n  leaveRoom: '/rooms/:roomCode/leave',\n  login: '/login',\n  register: '/register',\n} as const\n\nexport const COOKIE_KEYS = {\n  setCookie: 'Set-Cookie',\n  getCookie: 'Cookie',\n} as const\n```\n\nFor routes, you may wonder, how do you create a route you need to redirect to? I use `generatePath` React Router. It also infers what you should pass as params. An example: `redirect(generatePath(ROUTES.roomDetail, { roomCode: userRoom.code }))`.\n\nThe cookie keys are there so that I don't ever mistype them. If you're working on a large project in a team, these are VERY helpful.\n\nThe keys for setting and getting cookies are different. It's a bit confusing that the get one is just \"Cookie\", but the set one is \"Set-Cookie\". But that's how it is I guess.\n\n### First bit\n\nLet's go over the first bit of `auth.server.ts`:\n\n```typescript\nconst authCookie: SessionIdStorageStrategy[\"cookie\"] = {\n  name: \"__session\",\n  secrets: [serverEnv.SESSION_SECRET],\n  sameSite: \"lax\",\n  maxAge: 60 * 60 * 24 * 30, // 30 days\n  httpOnly: true,\n  secure: serverEnv.NODE_ENV === \"production\",\n};\n\nconst cookieSchema = z.object({\n  userId: z.string().optional(),\n});\n\nconst sessionStorage = createCookieSessionStorage({ cookie: authCookie });\n\nconst typedAuthSessionStorage = createTypedSessionStorage({\n  sessionStorage,\n  schema: cookieSchema,\n});\n\nexport function getCookieFromRequest(request: Request) {\n  return request.headers.get(COOKIE_KEYS.getCookie);\n}\n```\n\nHere I'm using [createTypedSessionStorage](https://github.com/sergiodxa/remix-utils/?tab=readme-ov-file#typed-sessions) from `remix-utils`. It's a library that provides helpers when working with React Router 7. In this case, it allows you pass a zod schema to the session storage. Keeping things type safe.\n\nThe reason userId is optional is because it could also not be there at all e.g. if user is not logged in. `SessionIdStorageStrategy` is just a type from React Router, where we access the cookie definition.\n\ngetCookieFromRequest is a helper function that gets the cookie from the request headers.\n\nRequests and responses by the way are the normal [requests](https://developer.mozilla.org/en-US/docs/Web/API/Request) and [responses](https://developer.mozilla.org/en-US/docs/Web/API/Response). It's not anything specific to React Router 7. I mean, it's built on top of the web platform after all.\n\n### requireAuth and logout\n\n```typescript\ntype AuthResponse =\n  | {\n      type: \"redirect\";\n      response: Response;\n    }\n  | {\n      type: \"result\";\n      user: User;\n    };\n\nexport async function requireAuth({\n  request,\n}: {\n  request: Request;\n}): Promise<AuthResponse> {\n  const session = await typedAuthSessionStorage.getSession(\n    getCookieFromRequest(request)\n  );\n  const userId = session.get(\"userId\");\n\n  if (!userId) {\n    return {\n      type: \"redirect\",\n      response: redirect(generatePath(ROUTES.login)),\n    };\n  }\n\n  const user = await prisma.user.findUnique({\n    where: { id: userId },\n  });\n\n  // if user doesn't exist for whatever reason\n  // logout\n  if (!user) {\n    return {\n      type: \"redirect\",\n      response: await logout({ request }),\n    };\n  }\n\n  return {\n    type: \"result\",\n    user,\n  };\n}\n\nexport async function logout({ request }: { request: Request }) {\n  const session = await typedAuthSessionStorage.getSession(\n    getCookieFromRequest(request)\n  );\n\n  const headers = new Headers();\n  headers.set(\n    COOKIE_KEYS.setCookie,\n    await typedAuthSessionStorage.destroySession(session)\n  );\n\n  return redirect(generatePath(ROUTES.login), {\n    headers,\n  });\n}\n\n// How I use requireAuth\n// Can be found under e.g. app/routes/resources/api.liveblocks.ts\nconst requireAuthResult = await requireAuth({ request });\n\nif (requireAuthResult.type === \"redirect\") return requireAuthResult.response;\n\nconst { user } = requireAuthResult;\n```\n\nNow, you can do it differently if you want. This is how I prefer to do it. And yes, it's annoying that middleware doesn't exist in React Router 7. It should be out in 2-3 months hopefully. For now, we need to handle every route ourselves.\n\n`requireAuth` returns either a redirect or a user. To be very clear, `redirect` under the hood is just a Response. [A redirect itself is a 3XX response.](https://developer.mozilla.org/en-US/docs/Web/HTTP/Redirections) Source code: [utils.ts](https://github.com/remix-run/react-router/blob/main/packages/react-router/lib/router/utils.ts#L1383).\n\nBy default, it is a 302 response. Which means a temporary redirect. This tells the browser to not cache the redirect permanently. A 301 response would be a permanent redirect, which tells the browser to cache the redirect permanently. The problem there is that if the user logs out and logs in as a different user, the browser will serve a cached redirect to the previous user's destination. Which is not what you want. 301 is suitable for e.g. static assets because they don't change.\n\n---\n\nGoing back to the code:\n\n```typescript\ntype AuthResponse =\n  | {\n      type: \"redirect\";\n      response: Response;\n    }\n  | {\n      type: \"result\";\n      user: User;\n    };\n```\n\n`requireAuth` returns one of the two types.\n\nIf the type is a redirect, we know to just return the response we get. If not, we know the user is authenticated and we can continue with the user object.\n\n---\n\nLet's take a look at two things in the `logout` function.\n\nThe first thing is we destroy the session. Under the hood, this is simply serializing the cookie with an empty string and making sure it expires immediately ([source code](https://github.com/remix-run/react-router/blob/main/packages/react-router/lib/server-runtime/sessions/cookieStorage.ts#L55)).\n\nThe way I use `Headers` is something I plan on doing more often. I just find it cleaner. That's nothing from React Router 7. It's a web API. See [MDN](https://developer.mozilla.org/en-US/docs/Web/API/Headers).\n\n# Password based authentication\n\nYou can use popular libraries like bcrypt. I'm using the [crypto library](https://github.com/nodejs/node/blob/v20.13.1/lib/crypto.js) from node.js and doing it myself.\n\nThe key file to how I handle [password based authentication](https://github.com/tigerabrodi/narumory/blob/main/app/lib/password-service.server..ts):\n\n```typescript\nimport crypto from 'crypto'\n\nexport class PasswordService {\n  private static ITERATIONS = 1000\n  private static KEY_LENGTH = 64\n  private static ALGORITHM = 'sha256'\n\n  static async hashPassword(password: string) {\n    const salt = crypto.randomBytes(16).toString('hex')\n    const hash = await this.generateHash({ password, salt })\n    return { hash, salt }\n  }\n\n  static async verifyPassword({\n    password,\n    storedHash,\n    storedSalt,\n  }: {\n    password: string\n    storedHash: string\n    storedSalt: string\n  }) {\n    const attemptHash = await this.generateHash({ password, salt: storedSalt })\n    return attemptHash === storedHash\n  }\n\n  private static generateHash({\n    password,\n    salt,\n  }: {\n    password: string\n    salt: string\n  }) {\n    return new Promise<string>((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        this.ITERATIONS,\n        this.KEY_LENGTH,\n        this.ALGORITHM,\n        (err, key) => {\n          if (err) reject(err)\n          resolve(key.toString('hex'))\n        }\n      )\n    })\n  }\n}\n```\n\nI use this in:\n\n* login: [https://github.com/tigerabrodi/narumory/blob/main/app/routes/views/auth/login.tsx](https://github.com/tigerabrodi/narumory/blob/main/app/routes/views/auth/login.tsx)\n    \n* create user: [https://github.com/tigerabrodi/narumory/blob/main/app/routes/views/auth/lib/db-queries.ts#L21](https://github.com/tigerabrodi/narumory/blob/main/app/routes/views/auth/lib/db-queries.ts#L21)\n    \n\nYou may look at the code and go, wow, what's going on here?\n\nLet's go over it!\n\n## Storing passwords safely\n\nWe need a way to verify if a user's login password matches what they used when signing up. This means we need to store something. But we can't store the raw password. Because if your database gets leaked, then the attacker can get the passwords.\n\nWhat we need is:\n\n1. A function that always produces the same output for the same input (deterministic).\n    \n2. But is impossible to reverse - you can't go from output back to input.\n    \n3. So even if attackers get our database, they can't get passwords!\n    \n\nThis is what a hash function does. Every time you input \"password123\", you get the same weird output like \"a32ef7...\". But you can't go backwards. That's the beauty of it.\n\nHowever, simple hashing isn't enough:\n\n1. Attackers can pre-compute hashes for common passwords.\n    \n2. If two users have password \"123456\", they'll have identical hashes (this is a problem!).\n    \n3. Modern computers can try billions of hashes per second.\n    \n\nThis is where PBKDF2 ([Password-Based Key Derivation Function 2](https://en.wikipedia.org/wiki/PBKDF2)) comes in. It:\n\n1. Adds a unique random value (salt) to each password before hashing\n    \n    * So `(\"password123\" + \"RANDOM1\") â‰  (\"password123\" + \"RANDOM2\")`\n        \n    * This is why we use `randomBytes(16)` to create a unique salt\n        \n2. Hashes the result thousands of times\n    \n    * Each password guess now requires 1000 hashes\n        \n    * Makes brute force attacks thousands of times slower\n        \n    * This is our constant `ITERATIONS = 1000`\n        \n    * To be clear, after hashing the first time, **we take that result and hash again.** This is what we keep on repeating for each hash output.\n        \n\nWhen a user signs up:\n\n1. Generate random salt\n    \n2. Run PBKDF2(password + salt) 1000 times\n    \n3. Store the result and salt\n    \n\nWhen they login:\n\n1. Get salt from database\n    \n2. Run PBKDF2(login\\_attempt + salt) 1000 times\n    \n3. If result matches stored hash, password is correct (remember, it is deterministic!)\n    \n\n## PBKDF2 parameters\n\nPBKDF2 is recommended by NIST. You can find this publication [here](https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-132.pdf).\n\n---\n\nFor ITERATIONS, a minimum of 1000 is recommended. This is what we use. On page 11 of the paper, it says:\n\n> A minimum iteration count of 1,000 is recommended\n\n---\n\nOn page 10, you'll see:\n\n> All or a portion of the salt shall be generated using an approved Random Bit Generator (e.g., see \\[5\\]). The length of the randomly-generated portion of the salt shall be at least 128 bits.\n\nWhen we do `crypto.randomBytes(16)`, we get 16 bytes. That's 128 bits.\n\n---\n\nALGORITHM (SHA-256) is the specific hash function PBKDF2 uses for each iteration. SHA-256 is a cryptographically secure hash function that produces 256-bit outputs, and [is widely trusted by the security community.](https://nordvpn.com/blog/sha-256/)\n\n---\n\nKEY\\_LENGTH (64 bytes = 512 bits) determines how long the final hash will be.\n\nThis is a bit tricky to understand. I spent like an hour researching and trying to understand what key length is actually doing here lol.\n\nKEY\\_LENGTH serves two purposes:\n\n1. It determines the length of our final output hash\n    \n    * In our code it's 64 bytes (512 bits).\n        \n    * PBKDF2 can create this longer output by running multiple times and merging results. Otherwise, each hash output is 32 bytes (256 bits), because that's the output of SHA-256.\n        \n2. There's a catch though:\n    \n    * Because we're using SHA-256, the real security strength is limited to 256 bits.\n        \n    * Making it longer than 256 bits (32 bytes) does NOT add more security.\n        \n    * This is because HMAC-SHA256 internally works with 256-bit chunks.\n        \n\nWe can ask for longer outputs, the actual security we get is the same as 256 bits. The real security comes from:\n\n* Using a good salt\n    \n* Having enough iterations\n    \n* Users choosing strong passwords\n    \n\n# Conclusion\n\nWe went through a lot here. Way more than I expected lmao. It took me 5 hours to write this. Iâ€™m a bit stubborn when it comes to really understanding how things work.\n\nIf a single takeaway from this post, itâ€™s gotta be the quote from [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) â†’ â€œWeaponize your curiosityâ€\n\n![Pin page](https://i.pinimg.com/originals/43/96/86/4396860a33aa06d15dc4f353fe41e3dc.jpg align=\"left\")",
      "stars": null,
      "comments": 1,
      "upvotes": 18,
      "read_time": "14 min read",
      "language": null
    },
    {
      "title_en": "Understanding the Model Context Protocol (MCP)",
      "url": "https://k33g.hashnode.dev/understanding-the-model-context-protocol-mcp",
      "source": "hashnode",
      "published_at": "2025-01-02T06:28:41.519000+00:00",
      "external_id": null,
      "tags": [
        "ollama",
        "genai",
        "generative ai",
        "llm",
        "mcp",
        "claude.ai"
      ],
      "content_length": 10576,
      "content_preview": "## Definition\n\n> **Pre-requisite**: read my blog post about [\"Tool Calling\" and Ollama](https://k33g.hashnode.dev/tool-calling-and-ollama).\n\nThe MCP is an open protocol that standardizes how applications provide context to large language models (LLMs). It also provides a standardized way to connect AI models with various data sources and tools.\n\nAnthropicÂ [defined this protocol](https://www.anthropic.com/news/model-context-protocol), and one of its first implementations isÂ [**Claude Desktop**](h",
      "content_full": "## Definition\n\n> **Pre-requisite**: read my blog post about [\"Tool Calling\" and Ollama](https://k33g.hashnode.dev/tool-calling-and-ollama).\n\nThe MCP is an open protocol that standardizes how applications provide context to large language models (LLMs). It also provides a standardized way to connect AI models with various data sources and tools.\n\nAnthropicÂ [defined this protocol](https://www.anthropic.com/news/model-context-protocol), and one of its first implementations isÂ [**Claude Desktop**](https://claude.ai/download). This application enables Anthropic's Claude AI model to interact with your computer using the MCP protocol.\n\n## General Architecture\n\nThe Model Context Protocol uses a client-server architecture where a host application can connect to multiple servers:\n\n1. **MCP Hosts**: These are generative AI applications using LLMs that want to access external resources via MCP. Claude Desktop is an example of a host application.\n    \n2. **MCP Clients**: Protocol clients that maintain 1:1 connections with servers (and the client is used by MCP host applications).\n    \n3. **MCP Servers**: Programs that expose specific functionalities through the MCP protocol using local or remote data sources.\n    \n\nThe MCP protocol offers two main transport models: **STDIO** (Standard Input/Output) and **SSE** (Server-Sent Events). Both use JSON-RPC 2.0 as the message format for data transmission.\n\nThe first model, **STDIO**, communicates through standard input/output streams. It's ideal for local integrations. The second model, **SSE**, uses HTTP requests for communication, with SSE for server-to-client communications and POST requests for client-to-server communication. It's better suited for remote integrations.\n\n```mermaid\nflowchart TD\n  subgraph Host[\"MCP Host (AI Application)\"]\n      App[\"Application LLM\\n(e.g., Claude Desktop)\"]\n  end\n\n  subgraph Client[\"MCP Client\"]\n      Protocol[\"Protocol Client\\n1:1 connections\"]\n  end\n\n  subgraph Servers[\"MCP Servers\"]\n      Server1[\"Server 1\\n(Local Data)\"]\n      Server2[\"Server 2\\n(Remote Data)\"]\n  end\n\n  App --> Protocol\n  Protocol <--> |STDIO\\nJSON-RPC 2.0| Server1\n  Protocol <--> |SSE\\nJSON-RPC 2.0| Server2\n```\n\n> The protocol includes provisions for adding additional transport models in the future.\n\n## Operation\n\nThe MCP server exposes functionalities to the host application via the MCP protocol (this could be a list of available tools, such as addition and subtraction).\n\n```json\ntools: [\n    {\n        name: \"add_numbers\",\n        description: \"Add two numbers together\",\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                a: { type: \"number\" },\n                b: { type: \"number\" }\n            },\n            required: [\"a\", \"b\"]\n        }\n    },\n    {\n        name: \"subtract_numbers\",\n        description: \"Subtract two numbers together\",\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                a: { type: \"number\" },\n                b: { type: \"number\" }\n            },\n            required: [\"a\", \"b\"]\n        }\n    }\n]\n```\n\nThe host application formats this list into a similar list that is comprehensible by the LLM:\n\n```json\n\"tools\": [\n    {\n      \"function\": {\n        \"description\": \"Add two numbers\",\n        \"name\": \"add_numbers\",\n        \"parameters\": {\n          \"properties\": {\n            \"number1\": {\n              \"description\": \"The first number\",\n              \"type\": \"number\"\n            },\n            \"number2\": {\n              \"description\": \"The second number\",\n              \"type\": \"number\"\n            }\n          },\n          \"required\": [\n            \"number1\",\n            \"number2\"\n          ],\n          \"type\": \"object\"\n        }\n      },\n      \"type\": \"function\"\n    },\n    {\n      \"function\": {\n        \"description\": \"Subtract two numbers\",\n        \"name\": \"subtract_numbers\",\n        \"parameters\": {\n          \"properties\": {\n            \"number1\": {\n              \"description\": \"The first number\",\n              \"type\": \"number\"\n            },\n            \"number2\": {\n              \"description\": \"The second number\",\n              \"type\": \"number\"\n            }\n          },\n          \"required\": [\n            \"number1\",\n            \"number2\"\n          ],\n          \"type\": \"object\"\n        }\n      },\n      \"type\": \"function\"\n    },\n  ]\n}\n```\n\nUsing the tools list, the host application can generate a prompt for the model. For example, **the tools list +** `\"add 12 and 28\"`.\n\nThe LLM (if it supports tools) can understand the prompt and extract the numbers, recognizing that it's an addition operation, and respond with a JSON result of this type:\n\n```json\n\"tool_calls\": [\n    {\n        \"function\": {\n            \"name\": \"add_numbers\",\n            \"arguments\": {\n                \"number1\": 12,\n                \"number2\": 28\n            }\n        }\n    }\n]\n```\n\nFrom this result, the host application constructs a new message for the MCP server that will execute the `add_numbers` tool:\n\n```json\n{\n  \"name\":\"add_numbers\",\n  \"arguments\": {\n    \"number1\":28,\n    \"number2\":14\n  }\n}\n```\n\nFinally, the MCP server responds with the operation result, which can be used by the host application:\n\n```json\n{\n  \"result\": 42\n}\n```\n\n```mermaid\nsequenceDiagram\n    participant MCP Server\n    participant Host App\n    participant LLM\n\n    MCP Server->>Host App: Expose tools (add_numbers, subtract_numbers)\n    Host App->>Host App: Format tools for LLM\n    Host App->>LLM: Send prompt + formatted tools\n    Note over LLM: Process request\n    LLM->>Host App: Return tool_calls JSON\n    Host App->>MCP Server: Convert and send tool request\n    MCP Server->>Host App: Return operation result\n```\n\n### Key Advantages of MCP:\n\n* Standardization: A uniform way to connect LLMs to different data sources\n    \n* Flexibility / Extensibility:\n    \n    * Ability to change LLM providers\n        \n    * Ability to change, add, or modify MCP servers\n        \n* Security: Data protection within your infrastructure\n    \n* Reusability: MCP servers can be used across different projects\n    \n\n## Using a Docker Hub MCP Server with Claude Desktop\n\nReady-to-use MCP servers are available onÂ **Docker Hub at**Â [https://hub.docker.com/u/mcp](https://hub.docker.com/u/mcp). TheÂ **SQLite**Â MCP server, for example, is available atÂ [https://hub.docker.com/r/mcp/sqlite](https://hub.docker.com/r/mcp/sqlite).\n\nTo use this Docker Hub MCP server with Claude Desktop, you need to add a configuration in a `claude_desktop_config.json` file to tell Claude Desktop how to start the MCP server. Here's an example configuration for the SQLite MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"--rm\",\n          \"-i\",\n          \"-v\",\n          \"mcp-test:/mcp\",\n          \"mcp/sqlite\",\n          \"--db-path\",\n          \"/mcp/test.db\"\n      ]\n    }\n  }\n}\n```\n\n> You can add multiple MCP servers to this configuration. For more information about Claude Desktop configuration, consult the official documentation: [https://modelcontextprotocol.io/quickstart/user](https://modelcontextprotocol.io/quickstart/user)\n\nThis will allow you to see the available tools in **Claude Desktop**:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735798317336/c20f03f9-2105-4075-9dc3-ef8ecf908243.png align=\"center\")\n\nAnd then interact with the available tools:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735798332231/105dfb2c-8e0b-43d4-bfc4-9753f3aa0a43.png align=\"center\")\n\n> In this example, I asked Claude Desktop to create a `buddies` table in the SQLite database and add 3 records to it.\n\nâœ‹ Note that it's entirely possible to do the same thing with a local LLM served by **Ollama**.\n\n## Using the SQLite MCP Server with Ollama and a Local LLM\n\nAs I'm particularly fond of Go, I searched for an MCP CLI written in Go and found `mcphost` by **Mark3Labs** ([https://github.com/mark3labs/mcphost](https://github.com/mark3labs/mcphost)). You can install it using the following command:\n\n```bash\ngo install github.com/mark3labs/mcphost@latest\n```\n\nNext, create a configuration file `mcp.json` to tell `mcphost` how to start the SQLite MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"sqlite\": {\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"--rm\",\n          \"-i\",\n          \"-v\",\n          \"mcp-test:/mcp\",\n          \"mcp/sqlite\",\n          \"--db-path\",\n          \"/mcp/test.db\"\n      ]\n    }\n  }\n}\n```\n\nNow, you can start `mcphost` with the following command:\n\n```bash\nmcphost --config ./mcp.json --model ollama:qwen2.5:3b\n```\n\n> Of course, you'll need to have downloaded the `qwen2.5:3b` model using the command `ollama pull qwen2.5:3b`.\n\nAnd now, you can interact with the SQLite MCP server.\n\nThe CLI will display the available MCP server(s), and you can enter your prompt:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735798361251/decc17f9-d97f-4d40-894e-46045206697b.png align=\"center\")\n\nYou can request the list of available tools using the `/tools` command:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735798391325/891e7957-08ff-4028-88f3-a6db181ed6d9.png align=\"center\")\n\nAnd then interact with the available tools. Here, I'm asking to create a `users` table in the SQLite database and add three records to it:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735798412872/b181a9e0-e0cd-40e3-877f-2423da78b70d.png align=\"center\")\n\nAnd there you have it, the `users` table has been created, and the three records have been added successfully. I can now ask to view the list of the users:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735798432834/a2102f6b-e6cb-4e04-822b-21a670265d1d.png align=\"center\")\n\nAnd here's the result:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1735798584252/5f7ba4c0-76d9-49f2-ac21-b4eb6e88ec4d.png align=\"center\")\n\nThere you have it! You now know how to use the Model Context Protocol with Claude Desktop and Ollama. As you've seen, implementing an MCP server with Docker is extremely simple and quick. You can now explore the various possibilities offered by the MCP servers available on [https://hub.docker.com/u/mcp](https://hub.docker.com/u/mcp) to connect your LLMs to external information sources.\n\n> You can find the source code of the official MCP servers here: [https://github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)\n\nIn upcoming blog posts, I'll explain how to create your own MCP server and how to develop a host application to interact with this MCP server.",
      "stars": null,
      "comments": 2,
      "upvotes": 49,
      "read_time": "6 min read",
      "language": null
    },
    {
      "title_en": "How to Schedule Cloud Functions with Google Cloud Tasks",
      "url": "https://clouds.hashnode.dev/how-to-schedule-cloud-functions-with-google-cloud-tasks",
      "source": "hashnode",
      "published_at": "2025-01-08T23:35:19.230000+00:00",
      "external_id": null,
      "tags": [
        "google cloud",
        "cloud functions",
        "google cloud functions",
        "cloud tasks"
      ],
      "content_length": 15033,
      "content_preview": "Google Cloud Tasks is provided by Google Cloud Platform (GCP). It is a fully managed service that allows you to create, manage, and execute asynchronous tasks in a reliable and scalable way. These tasks are independent pieces of work that are to be processed outside your application flow using handlers that you create. These handlers are essentially the endpoints or services that process your tasks. You can use HTTP, app engine, pub/sub, or even custom backend handlers. In this article, we will ",
      "content_full": "Google Cloud Tasks is provided by Google Cloud Platform (GCP). It is a fully managed service that allows you to create, manage, and execute asynchronous tasks in a reliable and scalable way. These tasks are independent pieces of work that are to be processed outside your application flow using handlers that you create. These handlers are essentially the endpoints or services that process your tasks. You can use HTTP, app engine, pub/sub, or even custom backend handlers. In this article, we will look through the cloud task workflow and how to effectively schedule cloud functions using cloud tasks.\n\n## How does Google Cloud Tasks Work\n\nGoogle Cloud Task is essentially used to manage the execution of asynchronous tasks on a high level. Here is a step-by-step description of its workflow:\n\n### Tasks Creation:\n\nYour application creates a task with specific data (payload) and adds it to a **task queue**.\n\nYou can specify:\n\n* The target handler (HTTP endpoint, App Engine, or Pub/Sub).\n    \n* Task execution time (immediate or delayed).\n    \n* Retry policies (e.g., how often and when to retry failed tasks).\n    \n\n### Tasks Queuing:\n\nTasks are stored in a queue until they are ready to be executed. With queues, you can organize tasks by priority or function.\n\n### Tasks Execution:\n\nWhen a task is ready to execute (immediately or after a specified delay), Google Cloud Tasks sends the task to the specified **handler**.\n\nHandlers can be:\n\n* HTTP(S) endpoints.\n    \n* App Engine services.\n    \n* Pub/Sub topics.\n    \n\n### Task Processing:\n\nThe handler processes the task using the data provided in the task payload. After processing, the handler responds with a success or failure status. If successful, the task is marked as complete and removed from the queue else, the task is retried based on the retry policy.\n\nYou can monitor the status of tasks and queues using the Google Cloud Console or APIs.\n\n## Google Cloud Tasks vs Google Cloud Scheduler\n\nGoogle Cloud Scheduler is a fully managed cron job service provided by Google Cloud Platform (GCP). It allows you to schedule and automate the execution of tasks, such as running scripts, triggering APIs, or executing Cloud Functions, at specified times or intervals.\n\nHere is a comparative analysis of Google Cloud tasks and Google Cloud Scheduler, understanding what these two services offer will help you choose what service is the best fit for your project.\n\n### Comparative Analysis: Google Cloud Scheduler vs. Google Cloud Tasks\n\n| Aspect | Google Cloud Scheduler | Google Cloud Tasks |\n| --- | --- | --- |\n| Purpose | Automates and schedules recurring tasks or cron jobs. | Manages asynchronous or one-off tasks with controlled execution. |\n| Task Execution | Executes tasks based on a fixed schedule (e.g., hourly, daily). | Executes tasks triggered by application events or logic. |\n| Concurrency | Executes tasks one at a time per job. | Can handle multiple tasks concurrently with queue management. |\n| Scalability | Suitable for a moderate number of scheduled jobs. | Highly scalable for managing thousands of tasks. |\n| Example Scenario | Run a database cleanup script every Sunday at midnight. | Queue a task to process an image upload triggered by a user action. |\n\n## Schedule Cloud Function using Cloud Tasks\n\nIn this example, we will go through the process of creating a cloud function that is to be evoked at a later time using cloud tasks.\n\n### **Prerequisites**\n\nTo make the most of this tutorial, you should have the following:\n\n* Google Cloud Platform (GCP) project with billing enabled\n    \n* A good understanding of Cloud functions and Typescript\n    \n\nOnce youâ€™ve checked off all the prerequisites, enable the Google Cloud Tasks API from the Google Cloud console.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736378043322/8d8587a4-21d1-40f9-9053-f859f72bdb43.png align=\"left\")\n\nOnce this has been successfully enabled, create a queue. You can create a queue with Google Cloud (gcloud) CLI or right there in the Google Cloud console. To create a queue via gcloud CLI then you have to first [install](https://cloud.google.com/sdk/docs/install) the gcloud SDK and have it configured to your Firebase project. Once itâ€™s been configured, run this command from the terminal to create your queue.\n\n```typescript\n    gcloud tasks queues create QUEUE_ID --location=LOCATION\n```\n\nReplace *QUEUE\\_ID* and *LOCATION* with your preferred values. For more details on queue creation via gcloud CLI, see [here](https://cloud.google.com/tasks/docs/creating-queues).\n\nTo create a queue directly from Google Console, navigate to Cloud Tasks and click on the create queue option to create your queue.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736378062323/4910eb09-2911-4a3f-b22d-de09681aa11b.png align=\"left\")\n\nNow that Cloud Tasks has been set up you can now use it in your functions. To use, first install the Cloud Task client.\n\n```dart\nnpm install @google-cloud/tasks\n```\n\nIn this example, we will create a cloud function that sends emails to the email addresses in our Firestore collection. Using Google Cloud Task, we will schedule this cloud function to be called at a specified time. Letâ€™s dive in.\n\nInstall `nodemailer`, because we will first create a `sendEmail` function.\n\n```tsx\nnpm install nodemailer\n```\n\nHere is the send email function:\n\n```tsx\nimport * as functions from \"firebase-functions\";\nimport * as admin from \"firebase-admin\";\nimport * as nodemailer from \"nodemailer\";\nimport {OAuth2Client} from \"google-auth-library\";\n\nadmin.initializeApp();\n\nconst transporter = nodemailer.createTransport({\n  service: \"gmail\",\n  auth: {\n    user: \"senderemail@gmail.com\",\n    pass: \"sender pass\",\n  },\n});\n\n/**\n * This function sends emails.\n*/\nexport const sendEmail = functions.https.onRequest(async (req, res) => {\n  const projectId = JSON.parse(process.env.FIREBASE_CONFIG!).projectId;\n  const location = \"us-central1\";\n\n  const authorizationHeader = req.headers.authorization;\n\n  if (!authorizationHeader) {\n    res.status(401).send(\"unauthorized token\");\n    return;\n  }\n\n  // if authorizationHeader is not null access the token\n  const token = authorizationHeader.split(\" \")[1];\n\n  // verify ID token\n  try {\n    await verifyToken(token, location, projectId);\n  } catch (error) {\n    console.log(error);\n    res.status(401).send(\"Unauthorized token\");\n    return;\n  }\n\n  try {\n    const snapshot = await admin\n      .firestore()\n      .collection(\"email_addresses\")\n      .get();\n\n    if (snapshot.empty) {\n      res.status(404).send(\"No email addresses found in the collection.\");\n      return;\n    }\n\n    const emailAddresses: string[] = [];\n    snapshot.forEach((doc) => {\n      const data = doc.data();\n      if (data.email) {\n        emailAddresses.push(data.email);\n      }\n    });\n    if (emailAddresses.length === 0) {\n      res.status(404).send(\"No valid email addresses found.\");\n      return;\n    }\n\n    const promises = emailAddresses.map((email) => {\n      const mailOptions = {\n        from: \"senderemail@gmail.com\",\n        to: email,\n        subject: \"Welcome to Our Service!\",\n        text: `Hello, ${email}! Welcome to our platform. \n        We're excited to have you on board!`,\n      };\n\n      return transporter.sendMail(mailOptions);\n    });\n\n    await Promise.all(promises);\n\n    res.status(200).send(\"Emails sent successfully!\");\n  } catch (error) {\n    console.error(\"Error sending emails:\", error);\n    res.status(500).send(\"An error occurred while sending emails.\");\n  }\n});\n```\n\nThis function extracts the list of email addresses in the collection and then using the `nodemailer`, it sends emails to every one of those email addresses. This function also has an auth guard which prevents it from being called by those that arenâ€™t authorized. This auth guard first checks if the auth token is contained in the header, if not it throws an error. However, if a token is contained, using the `verifyToken` function, it verifies it.\n\nFor the verifyToken function install the google-auth-library. This is what will be used to verify the token.\n\n```tsx\nnpm install google-auth-library\n```\n\n```tsx\n/**\n * This function verifies token\n * @param {string} token\n * @param {string} location\n * @param {string} projectId\n * @return {Promise<object>}\n */\nasync function verifyToken(\n  token: string,\n  location: string,\n  projectId: string,\n): Promise<object> {\n  const client = new OAuth2Client();\n  const ticket = await client.verifyIdToken({\n    idToken: token,\n    audience: `https://${location}-${projectId}.cloudfunctions.net/sendEmail`,\n  });\n\n  const payload = ticket.getPayload();\n\n  if (!payload) {\n    throw new Error(\"Invalid token: Payload is undefined.\");\n  }\n\n  return payload;\n}\n```\n\nNow, to the fun part, scheduling this function using the cloud tasks. The creation of this task will be triggered when a new document is created for the email\\_addresses collection in Firestore. Note that you can use any user-related action to trigger the creation of your task. Your task canâ€™t be created without a trigger.\n\nImport the following to your file:\n\n```tsx\nimport {CloudTasksClient, protos} from \"@google-cloud/tasks\";\nimport * as functions from \"firebase-functions\";\n```\n\nNext, define the Firestore onCreate trigger function\n\n```tsx\nexport const onCreateEmail = functions.firestore\n  .document(\"/email_addresses/{emailId}\")\n  .onCreate(async (snapshot) => {\n    \n  });\n```\n\nNext, add the logic for defining and creating a task\n\n```tsx\nexport const onCreateEmail = functions.firestore\n  .document(\"/email_addresses/{emailId}\")\n  .onCreate(async (snapshot) => {\n    const data = snapshot.data();\n    console.log(data);\n\n    const projectId = JSON.parse(process.env.FIREBASE_CONFIG!).projectId;\n    const location = \"us-central1\";\n    const queue = \"my-scheduler\";\n    const taskClient = new CloudTasksClient();\n    const queuePath: string = taskClient.queuePath(projectId, location, queue);\n    const url = `https://${location}-${projectId}.cloudfunctions.net/sendEmail`;\n    const taskName =\n    `projects/${projectId}/locations/${location}/queues/${queue}/tasks`;\n    const serviceAccountEmail =\n      \"SERVICE-ACCOUNT-EMAIL\";\n\n    const task = {\n      name: `${taskName}/myTask-${Date.now()}`,\n      httpRequest: {\n        httpMethod: protos.google.cloud.tasks.v2.HttpMethod.POST,\n        url: url,\n        headers: {\n          \"Content-Type\": \"application/json\",\n        },\n        oidcToken: {\n          serviceAccountEmail,\n          audience: `https://${location}-${projectId}.cloudfunctions.net/sendEmail`,\n        },\n        body: Buffer.from(JSON.stringify({})).toString(\"base64\"),\n      },\n      scheduleTime: {\n        seconds: Math.floor(Date.now() / 1000) + 2 * 60,\n      },\n    };\n    const request = {parent: queuePath, task: task};\n    const [response] = await taskClient.createTask(request);\n\n    functions.logger.info(\n      `Task ${response.name} scheduled for 2mins later for user`,\n    );\n  });\n```\n\nBreaking down the code snippet above, youâ€™ll notice that:\n\n* We first defined all the variables required to create a task. These variables include\n    \n    * the `projectId` refers to your Firebase `projectId`. You can either hardcode it or use the env value as can be seen above. This is ideal when you have more than one project using the same function.\n        \n    * the `location` and `queue` are the same values that you defined when creating the queue\n        \n    * the `url` which is the cloud function to be executed\n        \n    * the `taskClient` for the cloud task sdk\n        \n    * the `queuePath` which is gotten from the `taskClient`\n        \n    * the `taskName` is created using a hierarchical naming scheme, which allows Google Cloud Tasks to uniquely identify tasks, across multiple projects, locations, and queues. This helps to prevent conflicts.\n        \n    * the `serviceAccountEmail` is used as an extra security layer. This is optional but setting up your service account helps to ensure that the `sendEmail` function can only be called via the authenticated service account.\n        \n        * To get your service account email:\n            \n            * create a service account by going to the **IAM & ADMIN** section in your project on the Google console\n                \n            * select the Service Accounts option, on the sidebar that has all the options under **IAM & ADMIN**\n                \n            * click on **Create Service Account**\n                \n            * name your service account and assign **Cloud Function Invoker, Cloud Tasks Enqueuer,** and, **Service Account Token Creator** roles to it\n                \n            * retrieve your service account email\n                \n            * select the **IAM** option on the sidebar\n                \n            * click on grant access and in the field New Principals add your service account email there and grant the above roles to it\n                \n* Next, we configured the cloud task:\n    \n    * In the task name, we added a `Date.now()` value so that all the created task names are unique\n        \n    * In the `oidcToken` field, we passed the service account email and audience. In the audience field, we pass the endpoint that we initially defined.\n        \n        * Using the endpoint as an audience helps to ensure that the token is used for the right service.\n            \n        * If your endpoint has query params, remove the query params and pass just the endpoint to the audience field.\n            \n        * The service account email generates an OpenId Connect (OIDC) token when the task is about to be executed.\n            \n        * The OIDC token is sent as part of the authorization header. This is then validated by the cloud function which ensures the request is authenticated and authorized. The cloud function also validates the audience value, confirming that it matches its URL and that the token is intended for this service.\n            \n    * the body field is used in passing payload. In this case, we don't need to pass any payload, so it is empty.\n        \n    * in the `scheduleTime` we defined when this task should be executed\n        \n* Finally, using the `taskClient`, we define the `createTask` function.\n    \n\nNow when a new email is added to the email\\_addresses collection, a new task is created and queued which will call the `sendEmail` function at the scheduled time.\n\n## Conclusion\n\nFinally, we have come to the end of this article. So far, we defined the Google Cloud Tasks and its workflow. We also explored the differences between Google Cloud Task and Google Cloud Scheduler, which are similar but offer different services.\n\nWith what you have learned from this article you can schedule cloud functions or any of the other handlers to be called at the specified time, you can also add a service account to your task configuration to ensure security is covered and much more.\n\nIf you found this article helpful, you can support it by leaving a like or comment. You can also follow me for more related articles.",
      "stars": null,
      "comments": 0,
      "upvotes": 25,
      "read_time": "10 min read",
      "language": null
    },
    {
      "title_en": "How React's Render, Effects and Refs work under the hood",
      "url": "https://tigerabrodi.blog/how-reacts-render-effects-and-refs-work-under-the-hood",
      "source": "hashnode",
      "published_at": "2025-01-08T15:29:11.172000+00:00",
      "external_id": null,
      "tags": [
        "React",
        "performance",
        "Web Development"
      ],
      "content_length": 13948,
      "content_preview": "# Introduction\n\nUnderstanding the entire render cycle of React and how it works with the browser isn't easy.\n\nEven if you read the modern React documentation, it can be confusing considering all the visuals.\n\nLet's keep it simple and direct. This is something I'm writing for myself to help me understand the full flow.\n\nLet's start with this snippet to get thoughts rolling:\n\n```javascript\nfunction ExploringReactRefs() {\n  // Why does this ref start as null?\n  // When does it get its actual value?",
      "content_full": "# Introduction\n\nUnderstanding the entire render cycle of React and how it works with the browser isn't easy.\n\nEven if you read the modern React documentation, it can be confusing considering all the visuals.\n\nLet's keep it simple and direct. This is something I'm writing for myself to help me understand the full flow.\n\nLet's start with this snippet to get thoughts rolling:\n\n```javascript\nfunction ExploringReactRefs() {\n  // Why does this ref start as null?\n  // When does it get its actual value?\n  const divRef = useRef<HTMLDivElement>(null);\n\n  // This feels like it should work... but does it?\n  // When exactly does this effect run?\n  useEffect(() => {\n    console.log(\"Effect:\", divRef.current?.getBoundingClientRect());\n  }, []);\n\n  // What's different about this effect?\n  // Why might we need this instead of useEffect?\n  useLayoutEffect(() => {\n    console.log(\"Layout Effect:\", divRef.current?.getBoundingClientRect());\n  }, []);\n\n  // What's special about this callback ref approach?\n  // When does this function actually get called?\n  // See the second div below where handleRef is used.\n  const handleRef = (node: HTMLDivElement | null) => {\n    if (node) {\n      console.log(\"Callback ref:\", node.getBoundingClientRect());\n    }\n  };\n\n  return (\n    <div className=\"flex gap-4\">\n      {/* When can we actually access this element via divRef? */}\n      <div ref={divRef}>Using useRef</div>\n\n      {/* How is this different from useRef? */}\n      <div ref={handleRef}>Using callback ref</div>\n    </div>\n  );\n}\n```\n\n# State update and renders\n\nWhenever you a component's state is updated, React will re-render the component. Re-rendering a component will re-render all of its children (yes, you can optimize this, but that's not the point here).\n\nAnd! Just to be clear, effects only run if their dependencies change. If it's an empty array, it will only run once when the component is mounted (created).\n\nLet's just go over a snippet to be brutally clear about this:\n\n```js\nfunction Component() {\n  // 1. No dependency array - runs on EVERY render\n  useEffect(() => {\n    // Effect runs\n    return () => {\n      /* Cleanup runs before next effect */\n    };\n  }); // Missing dependency array\n\n  // 2. Empty array - runs only on mount/unmount\n  useEffect(() => {\n    // Effect runs once\n    return () => {\n      /* Cleanup runs on unmount */\n    };\n  }, []);\n\n  // 3. With dependencies - runs when deps change\n  useEffect(() => {\n    // Effect runs if count changed\n    return () => {\n      /* Cleanup runs before next effect if count changed */\n    };\n  }, [count]);\n\n  // Same rules apply for useLayoutEffect\n}\n```\n\nMount means the component gets created.\n\nUnmount means the component gets destroyed, or in simpler worlds, removed from the DOM. I used to believe in my younger days that this meant navigating away from the page. **But this can also be the case if you're conditionally rendering a component.**\n\n# Back to the snippet\n\nThere is quite a few things going on in this snippet.\n\nWhen a component is rendered, it goes through two main phases:\n\n1. Render phase.\n    \n2. Commit phase.\n    \n\nWe're gonna break those down into simpler terms.\n\nFor now, understand that every single time a render happens, two phases are executed: Render and Commit.\n\n# Virtual DOM\n\nBefore we dive into Render phase, let's talk about the Virtual DOM.\n\nA lot of people who lack understanding instantly rush towards saying \"Virtual DOM is to make React faster\". It's a bit funny, because that's not really the case. **You've UI libraries today such as Solid.js that don't have a Virtual DOM and are faster than React. So that statement is very confusing and incorrect.**\n\nWhat I'm gonna be explaining is how things work at a high level.\n\nIn actuality, React uses Fiber architecture instead of a simple Virtual DOM. This let's React split work into chunks and prioritize it. This is still good for us to understand the basics.\n\n## What is Virtual DOM?\n\nVirtual DOM is just JavaScript objects. It's a representation of the actual DOM.\n\n```javascript\n// Virtual DOM is just JavaScript objects\nconst virtualElement = {\n  type: \"div\",\n  props: { className: \"container\" },\n  children: [\n    {\n      type: \"span\",\n      props: { children: \"Hello\" },\n    },\n  ],\n};\n\n// Real DOM is actual browser APIs\nconst realElement = document.createElement(\"div\");\nrealElement.className = \"container\";\n```\n\nSo here we noticed the first \"cost\" already. We're storing a representation of the DOM in memory. Now this isn't a big deal. Millions if not billions of websites are using React. **The point is just to look at things from first principles and really understand what's happening instead of just saying things without any understanding.**\n\n## Okay... But why does React need this?\n\nThere are two core philosophies I wanna mention here.\n\n### Different platforms\n\nBy having a virtual DOM, React isn't tied to the browser's DOM.\n\nThis means React can render to different platforms.\n\nThat's why React Native exists and works. Mobile apps are not using the browser's DOM hehe...\n\nJust pseudo code for our enlightenment:\n\n```js\n// React can render to different targets\nfunction render(virtualElement) {\n  switch (environment) {\n    case \"web\":\n      return renderToDOM(virtualElement);\n    case \"mobile\":\n      return renderToNative(virtualElement);\n    case \"server\":\n      return renderToString(virtualElement);\n  }\n}\n```\n\n### Batching updates\n\nAs we discussed before, React re renders the entire component (including its children) whenever a state update happens.\n\nThis means when state updates happen, it could result in a lot of DOM changes in the end.\n\nWith Virtual DOM, React can batch these updates. It can figure out all the changes that it needs to do, and apply them in a single pass when the commit phase is executed.\n\n```javascript\n// Without Virtual DOM\nstate.change1(); // DOM update\nstate.change2(); // DOM update\nstate.change3(); // DOM update\n\n// With Virtual DOM\nstate.change1(); // Update virtual tree\nstate.change2(); // Update virtual tree\nstate.change3(); // Update virtual tree\n// One single DOM update at the end!\n```\n\n# Render phase\n\nLet's finally talk about the render phase.\n\nThis is the first phase of the render cycle.\n\nOne thing that annoys me sometimes when learning is all the terminologies people try to use.\n\nWe can also call this the \"first step\" of going from state change to DOM change.\n\nLet's look at some pseudo code:\n\n```javascript\n// RENDER PHASE\nfunction renderPhase(newState) {\n  // 1. React creates/updates Virtual DOM by calling components\n  function handleStateUpdate() {\n    // Create new Virtual DOM tree\n    const newVirtualDOM = {\n      type: \"div\",\n      props: { className: \"app\" },\n      children: [\n        {\n          type: \"span\",\n          props: { children: newState },\n        },\n      ],\n    };\n\n    // 2. Reconciliation (Diffing)\n    // React compares new Virtual DOM with previous one\n    // Figures out what needs to change in real DOM\n    const changes = diff(previousVirtualDOM, newVirtualDOM);\n    // Results in a list of required DOM operations\n    // [{type: 'UPDATE', path: 'span/textContent', value: newState}]\n  }\n}\n```\n\n1. With new state, React creates a new Virtual DOM tree.\n    \n2. React uses this new Virtual DOM tree to figure out what changes need to be made to the actual DOM.\n    \n3. It does so by comparing the new Virtual DOM tree with the previous one.\n    \n\nNow React knows exactly the changes that need to be made and we don't need to update the full DOM every time a state update happens.\n\nNow, you might think updating the full DOM would be very expensive. The real answer he is that **\"it could be\"**. It depends on what you're building. It can also be good enough. So we can't say for sure that it would be VERY expensive (this is just me thinking very thoroughly, carefully and from first principles here).\n\n# Commit phase\n\nOk. Now we know what changes we need to do.\n\nThe commit phase is often summarized as \"React updates the DOM\". But it's a bit deeper than that.\n\nPS! If you're not familiar with the event loop, I recommend reading up on it before continuing. I've a [free advanced JS book](https://github.com/tigerabrodi/intermediate-to-advanced-js-book/). Chapters 12, 13 and 14 are relevant if you wanna learn more about the event loop. MDN and Youtube are also good resources.\n\nLet's look at some pseudo code:\n\n```javascript\n// 1. React's Commit Phase (Synchronous JavaScript)\n// This runs on the main thread\nfunction commitToDOM() {\n // React calls DOM APIs\n // Each call gets added to the call stack\n mutateDOM() {\n   document.createElement()\n   element.setAttribute()\n   element.appendChild()\n   // ...\n }\n\n // remember useLayoutEffect?\n // Now we'll run all the layout effects\n // this is synchronous\n // the code in here gets added to the call stack too\n runLayoutEffects()\n\n // Queue useEffect for later\n queueMicrotask(() => {\n   runEffects()\n })\n}\n\n// commitToDOM() is done - time for browser to work\n\n// 2. Browser's Work\n// - Calculate Layout\n// - Paint\n// - Composite\n\n// 3. Microtask Queue\n// Now useEffect runs\n```\n\nNow, how browsers work is out of scope for this post. But that is super interesting. It's on my list of things to learn in 2025. I so badly wanna explore it deeply lmao. I've done some research on it where I dug into hidden classes and stuff. Let's go over those points quickly, then get back to the topic:\n\n* **Calculating layout:** Browser calculates exact positions and sizes.\n    \n* **Paint:** Browser converts layout results into visual pixels.\n    \n* **Composite:** Browser combines layers into final screen image.\n    \n\n---\n\nThe first thing: Because we now know the updates we need to make, we run synchronous JavaScript code (`mutateDOM()`).\n\nIf we just look at this small snippet to get a feel for the event loop:\n\n```javascript\nfunction mutateDOM() {\n  document.createElement();\n  element.setAttribute();\n  element.appendChild();\n}\n\nmutateDOM();\n```\n\nEvery call gets added to the call stack. Then the browser clears it from top to bottom.\n\n```javascript\n1. element.appendChild()\n2. element.setAttribute()\n3. document.createElement()\n4. mutateDOM()\n```\n\nA stack is a LIFO (Last In First Out) data structure.\n\n---\n\nWhen we run the layout effects, we're running synchronous JavaScript code. The function call and the ones it contains get added to the call stack. Now, if you've been following along closely, you understand that every time layout effects' dependencies change, they will run again. This MEANS more synchronous code to go through before the browser can do its thing (which is why React recommends to be careful with `useLayoutEffect`).\n\nI'm trying to go through everything in detail and relate to practical points along the way for us to really understand this.\n\n---\n\nWe then run the normal effects. These are queued up with `queueMicrotask()` in our example. HOWEVER, in actuality, React uses its own scheduling system. But I think it helps to view it as a microtask queue to sort of understand the basics.\n\nWhen the browser does its thing, it's gonna first clear the entire call stack before it runs anything from the microtask queue. Then it runs the microtask queue.\n\nNow we've covered everything except the refs.\n\n# Refs\n\nNow, React 19 is out.\n\nI don't plan on diving into the details of that, I'll do it in an upcoming blog post.\n\nAye. Let's focus on the refs from the original snippet.\n\n```javascript\nconst divRef = useRef<HTMLDivElement>(null);\n```\n\nThis ref is created during the render phase. It starts as null because the DOM element doesn't exist during the first render. It gets its actual value after React commits the changes to the DOM. **But you can't know exactly when this happens just by using useRef alone.**\n\nThat's why you always need to check if the ref is null before you use it.\n\n```javascript\nif (divRef.current) {\n  console.log(divRef.current.getBoundingClientRect());\n}\n```\n\n---\n\nWhat happens when you use a callback ref?\n\n```javascript\nconst handleRef = (node: HTMLDivElement | null) => {\n  if (node) {\n    console.log(\"Callback ref:\", node.getBoundingClientRect());\n  }\n};\n```\n\nCalled immediately when the element is attached to the DOM. You can be 100% sure that the callback ref will run at the right time. It is null when the element is removed in case you need to clean up. It runs before useLayoutEffect. It's best for immediate DOM measurements or setup.\n\n# I need to get dimensions of an element... when is the best time to do this?\n\nLet's say you need to know how to position a tooltip:\n\n```javascript\nfunction Tooltip({ text, targetRef }) {\n  const tooltipRef = useRef(null);\n\n  // Wrong: Might cause flicker\n  // Why?\n  // Because this happens after the DOM is painted\n  // You will tooltip in its original position\n  // Then it flickers when this runs\n  useEffect(() => {\n    const targetRect = targetRef.current.getBoundingClientRect();\n    tooltipRef.current.style.top = `${targetRect.bottom}px`;\n  }, []);\n\n  // Better: No flicker\n  // Why?\n  // Because this happens before the DOM is painted\n  // You will see the tooltip in its final position\n  useLayoutEffect(() => {\n    const targetRect = targetRef.current.getBoundingClientRect();\n    tooltipRef.current.style.top = `${targetRect.bottom}px`;\n  }, []);\n\n  // Best: Most direct\n  // Why?\n  // Because this happens immediately after the DOM is attached (layout effect happens AFTER the DOM is attached)\n  const handleRef = (node) => {\n    if (node) {\n      const targetRect = targetRef.current.getBoundingClientRect();\n      node.style.top = `${targetRect.bottom}px`;\n    }\n  };\n\n  return <div ref={handleRef}>{text}</div>;\n}\n```\n\n# When do cleanup functions run?\n\nAfter a render, right BEFORE React runs the effect (useEffect or useLayoutEffect, only if dependencies changed), it runs the cleanup functions with the previous values. Then it runs the new effect with the new values. Or of course if the component unmounts.",
      "stars": null,
      "comments": 5,
      "upvotes": 57,
      "read_time": "10 min read",
      "language": null
    },
    {
      "title_en": "How to Start Technical Writing in 2025",
      "url": "https://blog.lo-victoria.com/how-to-start-technical-writing-in-2025",
      "source": "hashnode",
      "published_at": "2025-01-08T01:00:26.143000+00:00",
      "external_id": null,
      "tags": [
        "Technical writing ",
        "Blogging",
        "Developer",
        "AI",
        "Hashnode"
      ],
      "content_length": 12822,
      "content_preview": "Welcome back everyone! Iâ€™m excited to publish my first article of 2025 with an entry to one of my most beloved series: [Victoriaâ€™s Blogging Tips](https://lo-victoria.com/series/victorias-blogging-tips), featuring a collection of articles on tips on writing/blogging and getting started for new tech bloggers.\n\nA few close friends recently asked me, â€œIs it too late to start a tech blog in 2025?â€ or â€œIs AI going to make technical writing obsolete?â€ That inspired me to write this article because I be",
      "content_full": "Welcome back everyone! Iâ€™m excited to publish my first article of 2025 with an entry to one of my most beloved series: [Victoriaâ€™s Blogging Tips](https://lo-victoria.com/series/victorias-blogging-tips), featuring a collection of articles on tips on writing/blogging and getting started for new tech bloggers.\n\nA few close friends recently asked me, â€œIs it too late to start a tech blog in 2025?â€ or â€œIs AI going to make technical writing obsolete?â€ That inspired me to write this article because I believe thereâ€™s never been a better time to start technical writing. While AI has undoubtedly changed the content landscape, it hasnâ€™t replaced the value of human insight, storytelling, and connection.\n\nIf I were beginning my journey today, hereâ€™s exactly how Iâ€™d start (and thrive) in technical writing this year.\n\n# 1\\. Establish Your Niche\n\nThe key to standing out is having a niche. Donâ€™t try to write about everythingâ€”pick an area where youâ€™re knowledgeable or eager to learn deeply. For example:\n\n* **Frontend Development**: Tutorials, best practices, and trends.\n    \n* **AI/ML**: Insights, practical applications, and ethical considerations.\n    \n* **Open Source Contributions**: Guides, community highlights, and getting started.\n    \n\nA well-defined niche allows you to build authority and attract a loyal audience. Think about what excites you and aligns with your career goals. If youâ€™re unsure, experiment with a few topics and see what resonates with readersâ€”and with you.\n\nðŸ’¡ Tip: As I always say to my readers, **you are your first audience**. Write something that you would want to read over and over again. If it is valuable to you, most likely itâ€™s also a good resource for someone else.\n\n> Read more: [5 Questions to Ask For Your Next Article Topic](https://lo-victoria.com/5-questions-to-ask-for-your-next-article-topic)\n\n# 2\\. Write on Developer Platforms\n\nPlatforms like [Dev.to](https://dev.to) and [Hashnode](https://hashnode.com) are ideal for beginners. These communities value learning and experimentation, making them welcoming spaces for technical writers.\n\nWhen writing for these platforms, you can:\n\n* Share personal experiences/learnings, like solving a tricky bug or learning a new framework.\n    \n* Write tutorials that guide readers through specific challenges or how to integrate/use certain technologies\n    \n* Build a demo with a new technology that you want to share.\n    \n* Share your personal opinions/analysis on tech trends\n    \n\nðŸ’¡ Tip: Remember to **engage with readers through comments and feedback**. Itâ€™s a great way to refine your skills and build connections within the developer community.\n\nðŸ’¡ Tip 2: Itâ€™s also a good idea to add canonical URLs to cite the original source of the article when syndicating to many sites. This helps with SEO.\n\nðŸ’¡ Tip 3: Personal blog vs using a writing platform? There are pros and cons for each choice. Iâ€™ll write about this on a separate article. Ultimately, choose the one youâ€™re comfortable writing and publishing on first.\n\n> More developer blogging platforms on [this list](https://lo-victoria.com/best-blogging-platforms-for-developers-in-2023)\n\n# 3\\. Share Your Content\n\nAfter publishing on developer platforms, extend your reach by sharing on your socials. If I had to pick one platform to focus on, it would be LinkedIn. This professional network is a powerful tool for promoting your work and establishing your voice in the industry. Plus, there tend to be more people interested in reading in this platform than in other platforms where there are more images or video-based content.\n\nTo maximize engagement:\n\n* Add a personal anecdote or key takeaway from your article.\n    \n* Use relevant hashtags to increase visibility.\n    \n* End your post with a question to spark discussions.\n    \n\nðŸ’¡ Tip: LinkedIn isnâ€™t just about visibility; itâ€™s a space to build meaningful connections. Engage with comments, and donâ€™t shy away from reaching out to other professionals whose work inspires you. Also, feel free to share other writersâ€™ content with your audience.\n\nðŸ’¡ Tip 2: Besides LinkedIn, feel free to explore other socials to increase your reach. X or Instagram are popular options within the tech community.\n\n# 4\\. Publish 1 Article Per Week\n\nConsistency is the backbone of a successful technical writing career. A weekly schedule not only builds momentum but also establishes you as a reliable voice in your niche. As you gain more experience, you can change how often you publish, just like I now publish 2-3 articles per month. But if Iâ€™m starting out, I would stick to 1 per week to maximize growth.\n\nIf the idea of writing weekly feels overwhelming, start small:\n\n* Reflect on recent challenges or successes in your work.\n    \n* Turn a solved problem into a step-by-step tutorial.\n    \n* Share insights from conferences, webinars, or books youâ€™ve read.\n    \n* Keep article length byte-sized, it doesnâ€™t have to be an essay.\n    \n\nðŸ’¡ Tip: Each article you publish is a stepping stone toward expertise and audience growth. Remember, quality is more important than quantity, but consistency builds trust.\n\nðŸ’¡ Tip 2: Keep a backlog and you can schedule your articles weekly. I tend to keep at least 5-6 articles in the backlog. That way, you donâ€™t have to write 1 article a week. You can write as many as you want in a grind session and schedule weekly publishing.\n\n# **5\\. Contribute to Established Platforms**\n\nWhen I first started my technical writing journey, I write beyond personal blogs and developer platforms. Writing for established sites can amplify your reach and credibility. Here are some great platforms to consider:\n\n* [**freeCodeCamp**](http://freecodecamp.org): Share in-depth tutorials and guides with a global audience.\n    \n* [**Smashing Magazine**](https://www.smashingmagazine.com/): Focus on web development, design, and UX.\n    \n* [**CSS-Tricks**](https://css-tricks.com/): Perfect for frontend-focused content.\n    \n* [**Hackernoon**](https://hackernoon.com/): Covers a variety of tech-related topics, from tutorials to industry insights.\n    \n* [**Towards Data Science**](https://towardsdatascience.com/): Great for AI, machine learning, and data analysis.\n    \n* [**LogRocket Blog**](https://blog.logrocket.com/): Ideal for deep dives into web and mobile app development.\n    \n* [**DZone**](https://dzone.com/): Explores topics like DevOps, programming languages, and cloud computing.\n    \n* [**InfoQ**](https://www.infoq.com/): Targets experienced professionals with content on software architecture and agile practices.\n    \n* [**GeeksforGeeks**](https://www.geeksforgeeks.org/): Offers opportunities to write tutorials and solutions aimed at students and beginners.\n    \n* [OpenReplay Blog](https://blog.openreplay.com/): Focus on front-end development tips, tutorials and technologies.\n    \n\nðŸ’¡ Tip: Each platform has its submission guidelines, so take time to understand their audience and tailor your content accordingly. Writing for these platforms not only expands your reach but also helps you learn from experienced editors and contributors.\n\n# 6\\. Explore Freelance Writing\n\nFreelancing can be a great way to earn while building your portfolio. Start by looking for small gigs on platforms like Upwork, Fiverr, or tech-specific job boards. As you gain experience, you can pitch directly to companies or startups that need technical content.\n\nSome ideas for freelance technical writing projects include:\n\n* Writing tutorials and how-to guides for developer tools.\n    \n* Creating technical documentation for products or APIs.\n    \n* Contributing guest posts to company blogs.\n    \n\nðŸ’¡ Tip: Deliver high-quality work consistently, and donâ€™t hesitate to ask for testimonials from satisfied clients, especially on LinkedIn. These testimonials can help you secure higher-paying gigs over time.\n\nðŸ’¡ Tip 2: Always leave your contact information everywhere so it is easy for companies to reach out to you for freelancing. Have it on your website, LinkedIn and other socials.\n\n# 7\\. Leverage AI to Enhance Your Writing\n\nI started technical writing way before ChatGPT exists. It would take me an average of 2-3 full days to complete an article. If I am starting a technical writing career in 2025, I would leverage AI tools like ChatGPT.\n\nThe key, however, is to use these tools thoughtfully because you do not want to lose authenticity. Hereâ€™s how AI can enhance your writing process:\n\n* **Outlining**: Generate topic ideas or structure your content.\n    \n* **First Drafts**: Use AI to create a rough draft, then refine it with your expertise and other relevant details.\n    \n* **Editing**: Leverage tools like Grammarly for grammar, tone/style improvements, spell checks and so on.\n    \n* **Research**: Summarize complex concepts or find relevant code snippets quickly.\n    \n\nðŸ’¡ Tip: Remember to not overuse AI tools. Your voice, perspective, and authenticity are irreplaceable. Use AI as a helpful assistant, not a shortcut. You need to be able to develop your own writing style over time.\n\n> Read how to [Develop Your Own Writing Style](https://lo-victoria.com/developing-a-writing-style-as-a-technical-writer) and how to [Make Your Blog More Readable in 3 Simple Ways](https://lo-victoria.com/3-simple-ways-to-make-your-blog-more-readable)\n\n# 8\\. Knowledge Is Everything\n\nConnecting from the previous point on leveraging AI tools for research, itâ€™s your expertise and experience that make your content valuable and unique. Avoid the temptation to rely solely on AI to generate articles about topics you barely understand.\n\nInstead:\n\n* Test and validate AI-generated content with your own knowledge.\n    \n* Write about topics youâ€™ve worked on or are actively learning.\n    \n\nðŸ’¡ Tip: Readers can sense authenticity. Sharing insights from your personal experiences builds trust and sets your content apart. Your blog and writing should reflect your voice, knowledge and experiences.\n\n# 9\\. Network with Other Writers and Developers\n\nTechnical writing isnâ€™t just about creating content; itâ€™s also about engaging with a community. Join forums, Slack channels, or Discord groups where writers and developers share ideas. Platforms like X and LinkedIn are also great for networking.\n\nIf joining all these communities are too overwhelming, pick one. I recommend LinkedIn as it is a professional platform. The more writers you connect with on LinkedIn, the more mutual connections you'll discover, creating a snowball effect. Also, LinkedIn is a platform where you can:\n\n* Gain inspiration for new topics.\n    \n* Collaborate with other writers or developers.\n    \n* Stay updated on trends in your niche.\n    \n* Ask for feedback and seek out mentors for writing.\n    \n* Receive testimonials from clients or collaborators.\n    \n\nðŸ’¡ Tip: Building a strong network can open doors to mentorship, collaborations, and opportunities to grow your influence.\n\n# 10\\. Track Your Progress\n\nUse analytic tools to monitor how your articles perform. Are certain topics resonating more with your audience? Are there areas for improvement? What are some SEO strategies you can optimize?\n\nAdditionally you should strive to:\n\n* Respond to comments and questions on your articles as frequently as possible.\n    \n* Seek feedback from editors, fellow writers and readers.\n    \n* Facilitate discussions within your community by using your article as the starting point.\n    \n* Analyse which part of your writing process can be improved.\n    \n\nThis iterative process will help you refine your writing and grow as a thought leader.\n\n# Final Thoughts: Is Starting a Technical Writing Career Too Late in 2025?\n\nShort answer: No. AI is a powerful tool that can speed up processes, but it cannot replicate the depth, creativity, or personal touch of a skilled writer. In the 2024 Stack Overflow Developer Survey, technical documentation, written tutorials and blogs are within the top 5 preferred online resources to learn how to code for programmers. This shows the demand for authentic, insightful, and practical wrriten content is still very relevant.\n\n![https://survey.stackoverflow.co/2024/](https://cdn.hashnode.com/res/hashnode/image/upload/v1735882444583/903695b8-6e25-41b9-8adf-9d720b9437d6.png align=\"center\")\n\nIf I were starting my technical writing journey today, Iâ€™d focus on building genuine expertise, sharing consistently, and embracing AI as an enabler, **not a replacement**. With dedication and authenticity, thereâ€™s no limit to how far you can go as a technical writer.\n\nThanks for reading this article! If you are technical writer yourself, feel free to share any additional thoughts in the comments below and letâ€™s connect! Cheers!\n\n### **Let's Connect!**\n\n* [**Twitter**](https://twitter.com/lo_victoria2666)\n    \n* [**LinkedIn**](https://www.linkedin.com/in/victoria2666/)\n    \n* [**GitHub**](https://github.com/victoria-lo)",
      "stars": null,
      "comments": 5,
      "upvotes": 123,
      "read_time": "9 min read",
      "language": null
    },
    {
      "title_en": "Advanced GIT - Top 6 Version Control Hacks That Saves You Time",
      "url": "https://khushitrivedi.hashnode.dev/advanced-git-top-6-version-control-hacks-that-saves-you-time",
      "source": "hashnode",
      "published_at": "2025-01-07T04:58:40.907000+00:00",
      "external_id": null,
      "tags": [
        "community",
        "GitHub",
        "Git",
        "Gitcommands",
        "version control",
        "hacking",
        "hacks",
        "advanced git",
        "tech ",
        "technology",
        "Technical writing ",
        "Pull Requests",
        "Hashnode",
        "version control systems",
        "khushitrivedi"
      ],
      "content_length": 5259,
      "content_preview": "Here we are with another blog featuring **Git**. My previous write-up on version-control showed a pretty good response- which was a beginner-guide to learn the basics of Git Commands and Controls.\n\nWell, this blog is about some Git-Productivity hacks which helped me save hours. Take a look yourself!\n\n## 1\\. Clone only a specific branch of the repository\n\nWhile cloning any repo from GitHub, we directly download the main (default) branch to our systems. Hereâ€™s what we can do in-case weâ€™re preferri",
      "content_full": "Here we are with another blog featuring **Git**. My previous write-up on version-control showed a pretty good response- which was a beginner-guide to learn the basics of Git Commands and Controls.\n\nWell, this blog is about some Git-Productivity hacks which helped me save hours. Take a look yourself!\n\n## 1\\. Clone only a specific branch of the repository\n\nWhile cloning any repo from GitHub, we directly download the main (default) branch to our systems. Hereâ€™s what we can do in-case weâ€™re preferring any specific branch:\n\n**SYNTAX :** `git clone --single-branch -b <branch-name> \"<repo-link>\"`\n\n**EXAMPLE :** In case weâ€™d like to clone a specific branch named â€œfix-blog-linksâ€ from Appwriteâ€™s GitHub Repository, we can use this command on our terminal\n\n`git clone --single-branch -b fix-blog-links â€œ`[`https://github.com/appwrite/appwrite.git`](https://github.com/appwrite/appwrite.git)`â€`\n\n---\n\n## 2\\. Link your PR to any Issue\n\nWe can actually link Pull requests with any GitHub Issue. Plus, we can automatically close that issue by just editing our PRâ€™s description or comment. Hereâ€™s how we do it:\n\n* ### **Link the issue within the same repo:** (in which youâ€™re raising the PR)\n    \n    **SYNTAX :** `<your-keyword> #issue-number`\n    \n    **EXAMPLE :** `Closes #9` , `fixed #13`\n    \n* ### **Link the issue which is on a different repository:**\n    \n    **SYNTAX :** `keyword <repo-owner>/<repo-name>#issue-number`\n    \n    **EXAMPLE :** In-case of a repository name: portfolio, and the owner: [trivedi-khushi](https://www.github.com/trivedi-khushi) we can mention the comment as: `fixes portfolio/trivedi-khushi#5` in which, 5 indicates the issue number on the portfolio repository.\n    \n* ### **Link the Multiple Issues:**\n    \n    **SYNTAX :** use full syntax for each issue, and separate it by comma `,`\n    \n    **EXAMPLE :** `resolves #149, resolves #6, resolves portfolio/trivedi-khushi#7`\n    \n\n---\n\n## 3\\. Search within your entire Git Histroy\n\nEver wanted to search throughout your commit history for a specific keyword? We can actually do it by adding a `-S` flag to the `git log` command. Itâ€™s certainly more than what a normal text search (Ctrl+F) could do.\n\n**Using this comamnd, we can view all the additions and deletions made throughout the code changes** in the Git Commit History. Also, this search command is **case-sensitive,** so make sure to type in the correct string while searching.\n\n**SYNTAX :** `git log -S \"<your-keyword>\"`\n\n**EXAMPLES :** `git log -S \"initializeDatabase\"`\n\nIf you understood the purpose of the search command, it would be interesting to know about:\n\n* **Search within a range of commits:** We can easily use an updated command `git log -S â€œ<your-keyword>â€ <start-commit>..<end-commit>`. A simple example for this could be: `git log -S \"initializeDatabase\" HEAD~10..HEAD`\n    \n* **Search Within any Specific File:** Hereâ€™s a syntax in order to search based on your specific file: `git log -S <your-keyword> -- <file-path>`. This command comes with a disadvantage of not supporting files after their rename. For that, we can specifically use a `-follow` flag to this command. For instance, `git log -S \"initializeDatabase\" --follow -- path/to/file`.\n    \n\n---\n\n## 4\\. Tag any Specific Commit\n\nTags in Git could be used in order to part a couple of places to be important within your project. We can use tags for any reason such as:\n\n* releases,\n    \n* feature-complete,\n    \n* experiemntal feature,\n    \n* milestone, etc.\n    \n\nWell, you can only use tagging in Git if youâ€™ve already made a commit. Hereâ€™s how we can use git tag on our terminal:\n\n**SYNTAX :** `git tag <tag-name> <commit-id>`\n\n**EXAMPLE :** Letâ€™s consider tagging a commit to remember it with first release. We can use: `git tag v1.0 1a2b3c4`\n\n---\n\n## 5\\. List Remote Branches without Cloning them\n\nWe can easily list down the branches within a repository without cloning any of them. However, this only list down branch-names and not their contents like branch-specific files/folders.\n\n**SYNTAX :** `git ls-remote --heads <repo-link>`\n\n**EXAMPLE :** `git ls-remote --heads` [`https://github.com/appwrite/appwrite.git`](https://github.com/appwrite/appwrite.git)\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">No, <strong>double quotes</strong> are not required for the repository link in the <code>git ls-remote</code> command unless the link contains special characters or spaces (which is rare in Git URLs).</div>\n</div>\n\n---\n\n## 6\\. Undo any Local Commit\n\nWe can easily undo a Local Commit made, without losing any changes. Hereâ€™s how we do it:\n\n**SYNTAX :** `git reset --soft HEAD~1`.\n\nThe flag here `--soft` removes the changes from the commit, but they will be still present in the staging area. We can use this to rather remake suitable chnages and commit later. Meanwhile, `HEAD~1` indicates most recent commit done (the one weâ€™re undoing).\n\n---\n\nHope it added value for you. Thereâ€™s no particular source where these all could be found listed. so, feel free to comment any other hacks you use.If these hacks were impressive, and the blog was a nice resource for you, make sure to support me by following my [hashnode profile](https://hashnode.com/@trivedi-khushi)!â¤ï¸",
      "stars": null,
      "comments": 2,
      "upvotes": 22,
      "read_time": "4 min read",
      "language": null
    },
    {
      "title_en": "Mastering UserDefaults: The Right Way to Use",
      "url": "https://swiftable.hashnode.dev/mastering-userdefaults-the-right-way-to-use",
      "source": "hashnode",
      "published_at": "2025-01-04T12:36:08.616000+00:00",
      "external_id": null,
      "tags": [
        "iOS",
        "Swift",
        "UserDefaults",
        "UIkit",
        "storage"
      ],
      "content_length": 11012,
      "content_preview": "With UserDefaults, you can store small pieces of data persistently in iOS. Basically, it is a key-value store that saves user preferences, settings, and other lightweight data that needs to persist across app launches.\n\nUsing UserDefaults is simple and efficient, which makes it important. Because it provides a straightforward API, itâ€™s a great choice for anyone looking to store small, non-sensitive data quickly.\n\n**Avoid using UserDefaults** for storing large data sets, sensitive information (li",
      "content_full": "With UserDefaults, you can store small pieces of data persistently in iOS. Basically, it is a key-value store that saves user preferences, settings, and other lightweight data that needs to persist across app launches.\n\nUsing UserDefaults is simple and efficient, which makes it important. Because it provides a straightforward API, itâ€™s a great choice for anyone looking to store small, non-sensitive data quickly.\n\n**Avoid using UserDefaults** for storing large data sets, sensitive information (like passwords or tokens), or complex objects. For those, consider using the Keychain or Core Data.\n\nNext, you will see the following sections:\n\n* Why meaningful names are important for keys?\n    \n* How to organise the keys across the app?\n    \n* How to store and retrieve values in a right way?\n    \n* How to manage modifications at single point?\n    \n* How to organise UserDefaults using extension?\n    \n\n## **Use Meaningful Keys**\n\nOne of the most important aspects of using UserDefaults effectively is creating **meaningful and descriptive keys**. Using poorly named keys can lead to confusion, collisions, and bugs in your app. Always aim for names that clearly describe the data being stored.\n\nAlways use descriptive and consistent keys. This not only improves readability but also reduces the likelihood of key collisions. For example:\n\n```swift\n// âŒ - Not recommended\nlet imageData = \"imageData\"\nlet loginStatus = \"loginStatus\"\nlet accessToken = \"accessToken\"\nlet mobileNumber = \"mobileNumber\"\nlet launchDate = \"launchDate\"\n\n// âœ… - Recommended\nlet userProfileImageData = \"userProfileImageData\"\nlet isUserLoggedIn = \"isUserLoggedIn\"\nlet userAccessToken = \"userAccessToken\"\nlet userPhoneNumber = \"userPhoneNumber\"\nlet appFirstLaunchDate = \"appFirstLaunchDate\"\n```\n\n**Why** â€œimageDataâ€ is too vague? By specifying the prefix â€œuserProfile,â€ itâ€™s clear this is the data related to the userâ€™s profile image, avoiding confusion with other potential image data in the app.\n\n**Also,** the â€œlaunchDateâ€ could refer to many things, such as a product launch or session start. The â€œappFirstLaunchDateâ€ makes it clear that this key stores the date when the app was first launched by the user.\n\n> *Clear and consistent names make your code more intuitive and efficient to work with. It helps you understand the purpose of key.*\n\n## **Group Related Keys**\n\nIn large projects, itâ€™s common to have several keys that are related to specific features or parts of your app. Grouping related keys together not only helps with code organisation but also makes your app more scalable and maintainable. Instead of scattering keys throughout your codebase, consider grouping them logically by feature or functionality.\n\nIf you have multiple keys related to the same feature, consider grouping them to keep your code organised.\n\nWhen managing user-related data, grouping keys related to the user profile makes it easier to track user preferences and information in one place. Like this:\n\n```swift\nstruct UserProfileKeys {\n    static let userProfileImageData = \"userProfileImageData\"\n    static let userPhoneNumber = \"userPhoneNumber\"\n    static let userAccessToken = \"userAccessToken\"\n    static let userDisplayName = \"userDisplayName\"\n    static let userEmailAddress = \"userEmailAddress\"\n}\n```\n\nFor keys related to app configuration or settings, it makes sense to group them under a dedicated settings struct. Like this:\n\n```swift\nstruct AppSettingsKeys {\n    static let appFirstLaunchDate = \"appFirstLaunchDate\"\n    static let isUserLoggedIn = \"isUserLoggedIn\"\n    static let appThemeMode = \"appThemeMode\"\n    static let notificationsEnabled = \"notificationsEnabled\"\n}\n```\n\nFor onboarding-related features, you might have specific keys to track the userâ€™s progress or completion of onboarding. Like this:\n\n```swift\nstruct OnboardingKeys {\n    static let hasSeenOnboarding = \"hasSeenOnboarding\"\n    static let onboardingStepCompleted = \"onboardingStepCompleted\"\n}\n```\n\nWhen all related keys are in one place, itâ€™s easier to manage and update them without scattering them throughout the code. As your app grows, grouping helps avoid key collisions and keeps your code structured.\n\nYou should make key groups according to feature modules. You can create a key group called â€œAppSubscriptionKeysâ€ for information related to App Subscriptions. In another example, suppose you have a bookmarking feature in your app and need to store different information in UserDefaults. You can create a key group named â€œBookmarkKeysâ€.\n\n> *Your team members will find it much easier to navigate the code when related keys are logically grouped together.*\n\n## **Store and Retrieve Values**\n\nYou can take advantages of Swift fundamentals for storing and retrieving the information in UserDefaults. Yes, you can prefer **Computed Properties** to do that.\n\nUsing computed properties (setters and getters) to store and retrieve values from **UserDefaults** provides a clean and organised way to handle data, improving readability and maintainability of your code.\n\nHere are some example:\n\n```swift\n// Access and assign a boolean value\nvar isUserLoggedIn: Bool {\n    set {\n        self.set(newValue, forKey: UserDefaults.AppSettingsKeys.isUserLoggedIn)\n    } get {\n        return self.bool(forKey: UserDefaults.AppSettingsKeys.isUserLoggedIn)\n    }\n}\n\n// Access and assign a string value\nvar userPhoneNumber: String? {\n    set {\n        self.set(newValue, forKey: UserDefaults.UserProfileKeys.userPhoneNumber)\n    }\n    get {\n        return self.value(forKey: UserDefaults.UserProfileKeys.userPhoneNumber) as? String\n    }\n}\n\n// Access and assign data value\nvar userProfileImageData: Data? {\n    set {\n        self.set(newValue, forKey: UserDefaults.UserProfileKeys.userProfileImageData)\n    }\n    get {\n        return self.value(forKey: UserDefaults.UserProfileKeys.userProfileImageData) as? Data\n    }\n}\n```\n\nThere are other ways to store and retrieve the information to UserDefaults, but I personally prefer using the computed properties to do that. Why is this the case? Let me explain.\n\n* It **encapsulates** the logic for storing and retrieving values from UserDefaults in one place. This means you donâ€™t need to call separate methods all over your codebase.\n    \n* You ensure **type safety** when retrieving values from UserDefaults. For example, in the `isUserLoggedIn` property, the getter always returns a `Bool` type, which prevents type-related bugs.\n    \n* If you need to change the key or the method of data storage, you only need to update it in the computed property.\n    \n* If you need to **migrate** from UserDefaults to another storage solution, like Keychain, you only need to modify the getter and setter of the properties.\n    \n\n## **Modify Values**\n\nSuppose you are implementing the bookmarking feature for articles, where you need to store the article IDs that the user bookmarks. Weâ€™ll create functions to add and remove articles from the bookmark list, all encapsulated within a **UserDefaultsâ€™s extension**. This keeps the code organised and easy to maintain.\n\nHow? Letâ€™s walk through the implementation.\n\nThe first step is to create a key group that will store information related to bookmark articles. Like this:\n\n```swift\n// MARK: - BookmarkKeys\nstruct BookmarkKeys {\n    static let bookmarkArticleIdentifiers = \"bookmarkArticleIdentifiers\"\n}\n\n// Add more keys if needed for bookmark feature.\n```\n\nNow you can create some functions to add an article to the bookmark list and to remove it from the same list. Like below:\n\n```swift\n// MARK: - Bookmark Data\nextension UserDefaults {\n    \n    var bookmarkedArticles: [String]? {\n        set {\n            self.setValue(newValue, forKey: UserDefaults.BookmarkKeys.bookmarkArticleIdentifiers)\n        }\n        get {\n            self.object(forKey: UserDefaults.BookmarkKeys.bookmarkArticleIdentifiers) as? [String]\n        }\n    }\n    \n    func addBookmarkArticle(articleId: String) {\n        \n        // create temp array\n        var identifiers: [String] = []\n        \n        // store bookmarked articles into temp array if stored any\n        if let articleArray = bookmarkedArticles {\n            identifiers = articleArray\n        }\n        \n        // append article identifier\n        // check before append to avoid duplicate entries\n        // or you can use Set here to avoid duplication\n        identifiers.append(articleId)\n        \n        // store updated array of identifier\n        self.bookmarkedArticles = identifiers\n    }\n    \n    func removeBookmarkArticle(articleId: String) {\n        \n        // create temp array\n        var identifiers: [String] = []\n        \n        // write code to remove the article from bookmark list\n        \n        // store updated array of identifier\n        self.bookmarkedArticles = identifiers\n    }\n}\n```\n\nThroughout your app, you need to call **addBookmarkArticle()** function to add an article in the bookmark list. In the same way, call **removeBookmarkArticle()** function to remove an article from the list.\n\n***Why should we do this? From a single point, itâ€™s easy to navigate and modify the storage logic rather than creating functions in other files.***\n\n## **UserDefaults+Extension:**\n\nIt is likely that you will be using UserDefaults a lot in the real app. To manage the storage from multiple files, it is recommended to create an extension (e.g. UserDefaults+Extension) and keep all logic in this file. The following example shows how to structure your code related to UserDefaults.\n\n```swift\nextension UserDefaults {\n    // MARK: - UserProfileKeys\n    \n    // MARK: - AppSettingsKeys\n    \n    // MARK: - OnboardingKeys\n}\n\n// MARK: - Common Utilities\nextension UserDefaults {\n    // Define common utilities here\n}\n\n// MARK: - UserProfile Data\nextension UserDefaults {\n    // Define properties or functions here related to user profile info.\n}\n\n// MARK: - AppSettings Data\nextension UserDefaults {\n    // Define properties or functions here related to app settings info.\n}\n\n// MARK: - Onboarding Data\nextension UserDefaults {\n    // Define properties or functions here related to onboarding info.\n}\n```\n\nIn large projects, managing UserDefaults becomes more complex and critical. Without proper management, you can quickly run into issues like key conflicts, data loss, or difficulty debugging. As your app scales, the number of keys in UserDefaults can grow, making it harder to maintain and understand.\n\nProviding a simple way to store and retrieve small amounts of data, UserDefaults are fundamental to iOS development. To make it work effectively, especially in large projects, it is important to follow best practices. By carefully managing your keys, understanding when and where to use UserDefaults, and adhering to good practices, you can ensure your app remains performant and maintainable.\n\n---\n\nThank you for taking the time to read this article! If you found it helpful, donâ€™t forget to like, share, and leave a comment with your thoughts or feedback. Your support means a lot!\n\nKeep reading,  \nâ€” Swiftable",
      "stars": null,
      "comments": 0,
      "upvotes": 10,
      "read_time": "7 min read",
      "language": null
    },
    {
      "title_en": "Getting started at BLoC architecture",
      "url": "https://rishi2220.hashnode.dev/getting-started-at-bloc-architecture",
      "source": "hashnode",
      "published_at": "2024-12-13T17:04:59.075000+00:00",
      "external_id": null,
      "tags": [
        "BLoC",
        "Flutter",
        "Dart",
        "Future"
      ],
      "content_length": 34964,
      "content_preview": "### Asynchronous data, streams, and futures:\n\nThink of a busy restaurant where the waiter takes your order and then goes to serve other tables while your food is being prepared. You donâ€™t have to wait for your meal to be ready before the waiter can take other orders. This is exactly asynchronous data. Just like the waiter can serve multiple tables, asynchronous data allows a computer or application to handle several tasks at once.\n\nNow letâ€™s understand the technicalities:\n\n* In asynchronous tran",
      "content_full": "### Asynchronous data, streams, and futures:\n\nThink of a busy restaurant where the waiter takes your order and then goes to serve other tables while your food is being prepared. You donâ€™t have to wait for your meal to be ready before the waiter can take other orders. This is exactly asynchronous data. Just like the waiter can serve multiple tables, asynchronous data allows a computer or application to handle several tasks at once.\n\nNow letâ€™s understand the technicalities:\n\n* In asynchronous transmission, each character or byte of data is framed with a start bit and a stop bit. The start bit signals the beginning of the data transmission, while the stop bit indicates its end. For example, when sending an ASCII character, it might be represented as 10 bits: 1 start bit + 8 data bits + 1 stop bit.\n    \n* This is cool, but it increases the memory that needs to be transferred because of the extra start and stop bits. And for the same reason, it also increases the processing overhead.\n    \n* Data is sent one character at a time, as opposed to blocks or frames used in synchronous transmission. This means that each character is transmitted independently, allowing for irregular intervals between transmissions.\n    \n* Unlike synchronous transmission, where data is sent in fixed intervals dictated by a clock signal, asynchronous transmission allows for variable timing between characters. The receiver does not know when the next character will arrive until it detects the start bit.\n    \n\nAnd a stream is like a hose that delivers water continuously. You can fill up a bucket (process the data) as the water flows out. The data sent by the stream is essentially asynchronous data.\n\nLetâ€™s understand futures in dart:\n\nFuture in Dart represents a value that may not be immediately available but will be at some point in the future and hence used to handle asynchronous data in Flutter.\n\nA **Future** object can be in one of two states:\n\n* **Uncompleted**: The operation is still ongoing.\n    \n* **Completed**: The operation has finished, either successfully with a value or with an error.\n    \n\nHence the main thread of the app is not blocked using future datatype. Here is some code to understand futures:\n\n```dart\nimport \"dart:async\";\n\nvoid main() {\n  final myFuture = Future(() {\n    print(\"Creating the future\");\n    return 12;\n  });\n\n  print(\"end of main\");\n}\n```\n\nHere we are constructing a `Future` that takes a function as an argument. This function is executed asynchronously when the Future is scheduled to run. The function you pass to the `Future` constructor is queued to be executed in the event loop. After the creation of a `Future`, Dart schedules that task to run after all synchronous code has been completed. The event loop checks for tasks that are ready to be executed (like your Future) and runs them when it can.\n\nHence the output is something like this\n\n```bash\nend of main\nCreating the future\n```\n\n```plaintext\nPrint \"end of main\" -> (synchronous)\nEnd of main() -> (synchronous)\nExecute Future's function -> (asynchronous)\nPrint \"Creating the future\" -> (asynchronous)\n```\n\nA good use of `Future` can be seen in this code, where we are mocking an `http` request:\n\n```dart\nimport \"dart:async\";\n\nvoid main() {\n  Future<int>.delayed(Duration(seconds: 3), () {\n    return 100;\n    Duration.zero refers to a duration of zero time. When you use Future.delayed(Duration.zero), you are essentially creating a future that completes immediately, but not before the current event loop iteration finishes.\n\n  }).then((value) {\n    print(\"value: ${value}\");\n  });\n  print(\"waiting for a value..\");\n}\n```\n\n* `Future<int>.delayed` creates a `Future` that will be completed after a specified duration. In this case, it will be completed after **3 seconds**.\n    \n* The function passed as the second argument returns the integer `100`. This value will be available when the Future is completed (after 3 seconds)\n    \n* Then we can use the `.then()` method to specify what should happen once the Future is completed successfully.\n    \n* When the Future completes after 3 seconds, it passes its result (the value `100`) to this callback function, which prints `\"value: 100\"` to the console.\n    \n\nEnough Future! Now Letâ€™s implement whatever we know and yield some data using a stream.\n\n```dart\nStream<int> dataStream() async* {\n//async data generator function\n  for (int i = 1; i <= 10; i++) {\n    print(\"sent data: \" + i.toString());\n    await Future.delayed(Duration(seconds: 2));\n    yield i;\n  }\n}\n\nvoid main() async {\n  Stream<int> stream = dataStream();\n  stream.listen((receivedData) {\n    print(\"recieved data: \" + receivedData.toString());\n  });\n}\n```\n\nIn the following code, we are generating some data every 2 seconds (to simulate useful streams) and yielding that data to the output. We are listening to the data from the stream in the main function.\n\nHere is the breakdown of some keywords:\n\n`async`:\n\n* When you declare a function with `async`, it returns a `Future`. This means that the function is expected to perform some asynchronous operation and eventually provide a single value (or no value) when it completes.\n    \n* `async` is used when you need to perform an operation that will complete with a single result, such as fetching data from an API or performing a calculation.\n    \n* When you call an `async` function, it starts executing immediately but may pause at any `await` keyword until the awaited operation is completed. After the awaited operation finishes, execution resumes from where it paused.\n    \n* When we use `await` before a `Future`, the execution of the async function is paused at that point until the `Future` completes. This does not block the entire thread; instead, it allows other code (including other asynchronous operations) to run while waiting for the `Future` to resolve.\n    \n\n`async*`:\n\n* When you declare a function with `async*`, it returns a `Stream`. This indicates that the function will yield multiple values over time, rather than returning a single value at once. You use the `yield` keyword to emit values one at a time.\n    \n* Each time a value is yielded, execution pauses until the next value is requested by a listener on the stream.\n    \n* `async*` is used when you want to produce a series of values over time, such as generating data in intervals, where each value can be processed as it arrives.\n    \n\nState management of an app can be simplified using streams. By listening to a stream, widgets can rebuild themselves automatically when new data is emitted, making it easier to manage complex states.\n\nTo manage the state of our app more effectively and make the app more scalable, we might want to use a state management solution called BLoC. It listens for events dispatched from the UI, processes them, and emits new states based on the logic defined within the Bloc.\n\n### Basics of Cubit & BLoC\n\nBut to simplify things, We might want to first learn about **Cubit**. Cubit is a simplified version of Bloc that does not rely on events. Instead, it uses methods to emit new states directly. (This will be understood once we write some code.)\n\nFirst, we need to add the `flutter_bloc` package to your `pubspec.yaml` file:\n\n```yaml\ndependencies:\n  flutter:\n    sdk: flutter\n  flutter_bloc: ^8.0.0 # Check for the latest version\n```\n\nNow we can create a Cubit:\n\n```dart\nimport 'package:flutter_bloc/flutter_bloc.dart';\n\n// Define the CounterCubit\nclass CounterCubit extends Cubit<int> {\n  // Initial state is set to 0\n  CounterCubit() : super(0); \n\n  // Method to increment the counter\n  void increment() => emit(state + 1); \n  // Method to decrement the counter\n  void decrement() => emit(state - 1); \n}\n```\n\nHere we are simply inheriting our own class from a pre-existing `Cubit` class which is managing an integer here. The class constructor is defaulted and delegated to the Cubit class with the initial state set as 0 using the `super` keyword.\n\n`state` is a public getter that returns the current state of the Cubit. It represents the value that the Cubit is managing. `emit` is a public method used to update the state of the Cubit. When you call `emit(newState)`, it sets the Cubit's state to `newState` and notifies any listeners that the state has changed.\n\nNow this code can be used just like any single class.\n\n```dart\nvoid main() {\n  final cubit = CounterCubit();\n  print(cubit.state); //0\n  cubit.increment();\n  print(cubit.state); //1\n  cubit.decrement();\n  print(cubit.state); //2\n}\n```\n\nSee! We successfully separated our business logic from UI components. As the application grows, having a clear separation between the UI and business logic can make the codebase easier to maintain.\n\nNow, As we know `Cubit` is essentially a stream-emitting state, we can listen to it using a `Stream`. Letâ€™s listen:\n\n```dart\n\nFuture<void> main() async {\n  final cubit = CounterCubit();\n\n  final streamSubscription = cubit.listen(\n      print); \n  cubit.increment();\n  cubit.increment();\n\n  await Future.delayed(Duration.zero);\n  await streamSubscription.cancel();\n  await cubit.close();\n}\n```\n\nAs we are using `await` aka asynchronous tasks, we might want to make the main function asynchronous using `async` keyword. As we already discussed, when we declare a function `async` it returns a `Future`.\n\nAfter creating an instance for the `Cubit`, We are subscribing to a stream to listen to the changes.\n\n```dart\nfinal streamSubscription = cubit.stream.listen(print);\n```\n\n* Here, We are accessing the `stream` property of the `cubit` instance, which is a stream of integers representing the current state of the Cubit.\n    \n* The `listen(print)` method subscribes to this stream. This means that every time a new state is emitted (when you call `emit()` in your Cubit), the `print` function will be called with that new state.\n    \n\n```dart\nawait Future.delayed(Duration.zero);\n```\n\nZero Duration delay is simply used to let all the microtasks finish before the asynchronous tasks continues. We are letting the main thread yield some control.\n\nNow we can simply unsubscribe to the stream, to donâ€™t print any further state changes on the console.\n\n```dart\nawait streamSubscription.cancel();\n```\n\n```dart\nawait cubit.close();\n```\n\nThis line closes the Cubit instance. Closing a Cubit releases any resources it may be holding and signals that it will no longer emit any states.\n\nThis was cool!\n\nCubit emits a stream, but we need to call methods directly to emit new states. But BLoC is different, it is event-driven. It relies on events to trigger state changes. We send events to the BLoC, which processes them and emits new states.\n\nBLoC is essentially a function, which take events as input and omits a state as the output.\n\n```dart\nimport 'package:bloc/bloc.dart';\n\nenum CounterEventState { increment, decrement }\n\nclass CounterBloc extends Bloc<CounterEventState, int> {\n  CounterBloc() : super(0) {\n    // Registering event handlers\n    on<CounterEventState>((event, emit) {\n      switch (event) {\n        case CounterEventState.increment:\n          emit(state + 1); // Emit new state for increment\n          break;\n        case CounterEventState.decrement:\n          emit(state - 1); // Emit new state for decrement\n          break;\n      }\n    });\n  }\n}\n```\n\nFrom the above build-up, this code is easily digestible. Here We converted a Cubit to a BLoC by creating an enumeration with only two possible values, increment or decrement.\n\nNow similar to a Cubit, we are inheriting our custom class from BLoC, where we need to provide the enumeration and the data we want to manage. In the class constructor, we can register an event handler with `on<EventType>` which accepts an anon function with 2 parameters, the `event` and `emit`.\n\nNow with a simple switch case block, we can determine what to emit after changing the state on different enum values.\n\n```dart\nFuture<void> main() async {\n  final bloc = CounterBloc();\n\n  final streamSub = bloc.stream.listen(print); \n    // Listen to emitted states\n\n  // Adding events to the BLoC\n  bloc.add(CounterEventState.increment);\n  bloc.add(CounterEventState.decrement);\n  bloc.add(CounterEventState.increment);\n\n  await Future.delayed(Duration.zero); \n    // Allow pending operations to complete\n  await streamSub.cancel(); // Cancel subscription\n  await bloc.close(); // Close the BLoC\n}\n```\n\nNow similar to Cubit we can listen to the events after subscribing to a stream and we can use `.add` the method to add events to the event handler. This dispatches the event to the BLoC which is added to the queue of the correct event handler which then determines and emits that new state, then the event is removed from the queue. BLoC does not retain a record of processed events; it only maintains the current state and listens for new events.\n\n### BLoC Provider\n\nBLoC provider is a widget that provides the BLoC instance so that we are not required to create instance for each use. BLoC provider provides a single instance of a bloc to the subtree below it using the instance we will inject into the provider. This is called a dependency injection:\n\nImagine you have a car that needs fuel to run. Instead of the car having to go to the gas station and fill itself up, you have a friend (the injector) who brings the fuel to the car whenever it needs it. This way, the car doesnâ€™t need to worry about where the fuel comes from; it just knows that it gets what it needs when it needs it.\n\nBlocProvider uses lazy initialization by default, which means an object is created only when it is needed, rather than upfront. It saves memory and processing power by avoiding the creation of unused objects.\n\nLetâ€™s start by implementing a cubit with the bloc provider and the BLoC provider.\n\nTo get started, Visual Studio Code has an extension named `bloc` which is very helpful in creating cubits and blocs and for wrapping widgets with bloc providers or other bloc widgets. Letâ€™s create a cubit with a name `counter`\n\n```plaintext\nlib\n|-- cubit\n|   |-- counter_cubit.dart\n|   |-- counter_state.dart\n|-- main.dart\n```\n\nhere is `counter_state` file:\n\n```dart\npart of 'counter_cubit.dart';\n\nclass CounterState {\n  int counterValue;\n  CounterState({required this.counterValue});\n}\n```\n\nIn initial examples, we were managing an integer, but here we made our own custom class `CounterState` to be managed. Creating a separate state class promotes separation between business logic (managed by the Cubit) and the state representation. This makes the code cleaner and more maintainable, as changes to the state structure can be managed independently from the logic that operates on it. Here we have a normal `counterValue` integer with an initializer.\n\n```dart\nimport 'package:bloc/bloc.dart';\npart 'counter_state.dart';\n\nclass CounterCubit extends Cubit<CounterState> {\n  CounterCubit() : super(CounterState(counterValue: 0));\n}\n```\n\nHere we are similarly creating an extended cubit class, asking the cubit to manage the custom class we made, and creating a simple initializer with initial `counterValue` as 0.\n\nWe can add the following functions to the cubit:\n\n```dart\nvoid increment() => \nemit(CounterState(counterValue: state.counterValue + 1));\nvoid decrement() => \nemit(CounterState(counterValue: state.counterValue - 1));\n```\n\nNow our Cubit is ready to be implemented with the UI and BlocProvider.\n\nAs we already discussed the bloc provider provided the instance of the bloc or cubit to the tree nested below it. Because our counter app starts `MaterialApp` widget, we will wrap that with `BlocProvider`:\n\n```dart\nclass MyApp extends StatelessWidget {\n  const MyApp({super.key});\n  @override\n  Widget build(BuildContext context) {\n    return BlocProvider<CounterCubit>(\n      create: (context) => CounterCubit(),\n      child: MaterialApp(\n        title: 'Flutter bloc',\n        //theme: ThemeData(...),\n        home: const MyHomePage(title: 'Bloc'),\n      ),\n    );\n  }\n}\n```\n\nUnderstanding the code:\n\n`BlocProvider<CounterCubit>`: The generic type `<CounterCubit>` tells Flutter that this provider will manage an instance of `CounterCubit`.\n\n`create`: This parameter is used to define how the BLoC instance will be created. It takes a function that returns an instance of the BLoC.\n\n`(context) => CounterCubit()`: This is a lambda function that takes `context` (The current BuildContext provides information about the location of this widget in the widget tree.) as an argument and returns a new instance of `CounterCubit` (This calls the constructor of the `CounterCubit` class, creating a new instance that will manage the counter's state and logic).\n\nNow the instance of the `CounterCubit` and its functions can be accessed within the widget tree like this:\n\n```dart\nonPressed: () {\n   //accessing context of the bloc provider:\n   BlocProvider.of<CounterCubit>(context).decrement();\n}\n```\n\nNote that this will retrieve the closest instance of `CounterCubit` from the widget tree.\n\nNow we change the state of the Cubit via UI. But might want to also view that, to that that we will use `BlocBuilder`.\n\n### BLoC Builder\n\nIt listens for state changes and rebuilds the widget whenever a new state is emitted.\n\nThe builder function can be called multiple times, so it's important to ensure that it efficiently rebuilds only the parts of the UI that need updating, which means we will only wrap the text that will show the counter, instead of the whole UI. This helps avoid performance issues associated with unnecessary rebuilds.\n\n```dart\nBlocBuilder<CounterCubit, CounterState>(\n   builder: (context, state) {\n      return Text(state.counterValue.toString());\n   },\n)\n```\n\nThe declaration of a `BlocBuilder` widget takes two things, one is the type of BLoC (here cubit, remember cubit is the subtype of BLoC but not vice versa) that it will listen to and state `CounterCubit` emits.\n\nThe builder function, a [pure function](https://en.wikipedia.org/wiki/Pure_function), also takes two arguments, which are the `BuildContext` and the state emitted by the `CounterCubit` (represents the latest state of the cubit).\n\nNow we are able to see the value of our `counterValue` variable in the `CounterState` class in the UI.\n\nBloc Builder also provides a function `buildWhen` that allows us to control when the builder function should be called based on the state changes of the BLoC. The `buildWhen` function takes two parameters:\n\n* `previousState`: The state that was emitted before the current state.\n    \n* `currentState`: The state that is currently being emitted.\n    \n\nIt returns a boolean on which it is decided if the widget will be called or not.\n\n```dart\nbuildWhen: (previousState, currentState) {\n    // Only rebuilds if the counter value has changed\n    return previousState.counterValue \n    != currentState.counterValue;\n  }\n```\n\nCool! But in `BlocBuilder`, the build function is not called once on a single state change, though it flutter engine may call this multiple times to rebuild the state on a function call. Hence you canâ€™t push to another another screen or show a snackbar on a state change inside `BlocBuilder`. So solve this, we might need to move on to another BLoC concept called `BlocListener`.\n\n### BLoC Listener and Consumer\n\nTo solve the stated issue, Bloc Listener has a `listen` function which is only called once per state (not including the initial state). Similarly we also have a `listenWhen` function in the `BlocListener` as well,\n\n> Note that: `BlocListener` doesnâ€™t rebuild the UI.\n> \n> `BlocListener` is designed for executing side effects based on state changes, such as displaying a `SnackBar`, showing a dialog, or navigating to another page. These actions typically should not cause a rebuild of the widget tree, making `BlocListener` more suitable for these scenarios.\n\nLetâ€™s change some code.\n\n`counter_state.dart`\n\n```dart\nclass CounterState {\n  int counterValue;\n  bool? ifIncremented;\n  CounterState({\n    required this.counterValue,\n    this.ifIncremented\n  });\n}\n```\n\n`counter_cubit.dart`\n\n```dart\nclass CounterCubit extends Cubit<CounterState> {\n  CounterCubit() : super(CounterState(counterValue: 0));\n  void increment() => emit(\n      CounterState(counterValue: state.counterValue + 1, \n      ifIncremented: true));\n  void decrement() => emit(\n      CounterState(counterValue: state.counterValue - 1, \n      ifIncremented: false));\n}\n```\n\nWe will show a snackbar based on if the the counter was incremented or decremented using a `BlocListener`.\n\n```dart\nbody: BlocListener<CounterCubit, CounterState>(\n        listener: (context, state) {\n          if (state.ifIncremented == true) {\n            ScaffoldMessenger.of(context).showSnackBar(\n            const SnackBar(content: Text('value incremented')));\n          }\n          if (state.ifIncremented == false) {\n            ScaffoldMessenger.of(context).showSnackBar(\n            const SnackBar(content: Text('value decremented')));\n          }\n        },\nchild: ....\n);\n```\n\nNow we are using both `BlocBuilder` and `BlocListener`, But we have another BLoC Concept, called `BlocConsumer` which combines both Listener and Builder into single widget.\n\n```dart\nBlocConsumer<CounterCubit, CounterState>(\n   builder: (context, state) {\n        return Text(state.counterValue.toString());\n   },\n   listener: (context, state) {\n        if (state.ifIncremented == true) {\n            SnackBar(..);\n        }\n        if (state.ifIncremented == false) {\n            SnackBar(..);\n        }\n   },\n)\n```\n\nWe are up with basic BLoC concepts. Letâ€™s move on to BLoC artitechture.\n\n## BLoC Architecture\n\nThe BLoC architecture consists of three primary layers that work together to manage the flow of data and business logic in a Flutter application.\n\n* UI/Presentation layer:\n    \n    * It listens to state changes from the BLoC and updates the UI accordingly.\n        \n    * It handles user input and triggers events that are sent to the BLoC for processing.\n        \n* BLoC (Business logic layer)\n    \n    * Intermediary between the UI and Data layers which processes incoming events from the UI, applies business logic, and emits new states based on those events.\n        \n    * It manages the flow of data and ensures that the UI reflects the current state of the application.\n        \n* Data Layer\n    \n    * It fetches, stores, and updates data as required by the business logic.\n        \n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1728665522146/d8e77399-34eb-4d89-b602-fd183ceda5b5.png align=\"center\")\n\nLetâ€™s start with the Data layer first and build upon it to the UI layer.\n\n### Data Layer\n\nThe data layer is further divided into 3 parts\n\n* Models\n    \n* Data Providers\n    \n* Repositories\n    \n\nLetâ€™s zoom into Models to simplify things.\n\nModels are like blueprints or templates for the data that your application uses. They help define what information you need to work with and how that information is organized. Itâ€™s essentially a Class.\n\nThink of a model as a way to describe specific types of information an app needs. For example, we might have a **Weather model** that includes details like Temperature, Humidity, wind speed, city, etc.\n\nModels often help convert data from one format to another. When a weather app fetches data from an API (like a JSON response), the model can turn that data into something the app can use easily. For example, converting the JSON fields for temperature and condition into properties of your Weather model.\n\nNow Letâ€™s talk about Data providers and repositories:\n\nIt is essentially an API for your app. It has multiple methods to access data and communicate with the app layer. These methods wonâ€™t return the Model, but rather the type of raw data received from the API (letâ€™s say a JSON string).\n\nRepositories are basically a wrapper around Data providers to encapsulate them. The Repository abstracts these interactions, meaning it handles the details of how data is retrieved or stored without exposing those complexities to the rest of the app. Itâ€™s only here, that we will instantiate the model object. and this will be send to the business logic layer.\n\n### BLoC & UI layer\n\nNow the bloc can be depended on the repository layer to build and emit a state. The interesting thing is that Blocs can communicate with one another. Letâ€™s say we have two blocs, the first one about weather data, and another one about the internet state. We can subscribe to the stream of internet status in the weather bloc so that we can omit different states if there is no internet while accessing weather data, though we might be required to close the stream manually to prevent stream leaks.\n\nHere are some naming conventions according to what we learned:\n\n```plaintext\nlib/\nâ”œâ”€â”€ main.dart\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ data_providers/\nâ”‚   â””â”€â”€ repositories/\nâ”œâ”€â”€ business_logic/\nâ”‚   â”œâ”€â”€ cubits/\nâ”‚   â””â”€â”€ blocs/\nâ””â”€â”€ presentation/\n    â”œâ”€â”€ screens/\n    â””â”€â”€ pages/\n```\n\n## BLoC Testing\n\nTesting helps identify defects early in the development lifecycle when they are cheaper to fix. This reduces risks related to software quality, security, and performance.\n\nTest files are created under a folder called `test` at the same hierarchy as the `lib` folder. For our app, here is the directory hierarchy:\n\n```plaintext\ntest/\nâ”œâ”€â”€ cubit/\nâ”‚   â”œâ”€â”€ counter_cubit_test.dart\n```\n\nTo get started we will create a `group` function, itâ€™s used to create a named group of tests that share a common context or setup. By grouping related tests together, you make it easier for anyone reading the test file (including your future self) to understand the purpose of each test and how they relate to one another.\n\n```dart\nvoid main() {\n  group(\"CounterCubit\", () {});\n}\n```\n\nIt takes two parameters, first is the description and second is an anonymous function.\n\n```dart\ngroup(\"CounterCubit\", () {\n    CounterCubit? counterCubit;\n    setUp(() {\n      counterCubit = CounterCubit();\n    });\n    tearDown(() {\n      counterCubit!.close();\n    });\n  });\n```\n\nWe need an instance of the Counter cubit. Next, we need two functions, `setUp` and `tearDown`.\n\n* The `setUp` function initializes a new instance of `CounterCubit` before each test run.\n    \n* The `tearDown` function closes the cubit after each test to clean up resources.\n    \n\nHere is a test:\n\n```dart\ntest(\"checking for initial state\", () {\n  expect(counterCubit!.state, CounterState(counterValue: 0));\n});\n```\n\n`test` function also accepts a description and a function, in the closure, we have a `except` function, The `expect` function is used to assert that a certain condition holds true. It takes two arguments:\n\n* The first argument is the actual value you want to checkâ€”in this case, `counterCubit!.state`.\n    \n* The second argument is the expected valueâ€”here, itâ€™s `CounterState(counterValue: 0)`.\n    \n\nBut we will experience something like this as an error:\n\n```dart\nExpected: <Instance of 'CounterState'>\nActual: <Instance of 'CounterState'>\n```\n\n* The test is comparing two different instances of `CounterState`. Even if both instances are the same `counterValue`, they are stored in different memory locations.\n    \n* Since the default implementation of `==` checks for reference equality, it returns `false` when comparing these two different instances, leading to a failed test.\n    \n\nTo solve this, we need to override the `==` operator and the `hashCode` getter in the `CounterState` class.\n\n```dart\n@override\nbool operator ==(Object other) {\n  if (identical(this, other)) return true; \n  // Check if they are the same instance\n  if (other is! CounterState) return false; \n  // Check if 'other' is not a CounterState\n  return counterValue == other.counterValue; \n  // Compare based on counterValue\n}\n```\n\nNow the test should successfully run. The next step is to write Bloc tests.\n\nHere is a `blocTest`\n\n```dart\nblocTest<CounterCubit, CounterState>(\n  'emits [CounterState(count: 1)] when increment event is added',\n   build: () => counterCubit!,\n   act: (cubit) => cubit.increment(),\n   expect: () => [CounterState(counterValue: 1, \n                  ifIncremented: true)],\n);\n```\n\nThe `blocTest` function is part of the `bloc_test` package in Dart and Flutter, designed to simplify the process of unit testing BLoCs\n\n* **Build**: It initializes a new instance of `CounterCubit`.\n    \n* **Action**: It simulates an increment action by calling the `increment()` method on the cubit.\n    \n* **Expectation**: It checks that after performing the increment action, the cubit emits a new state with specific values (`counterValue: 1` and `ifIncremented: true`).\n    \n\n## Navigation with BLoC\n\nAs we know, when we wrap a widget with `BlocProvider`, the Bloc instance is available in the widget tree below it. However when the new screen is pushed, a new `BuildContext` is generated, hence the bloc instance is no longer available. Hence we need to manually pass the instance using `BlocProvider.value`.\n\nWe can make a new screen and change the file hierarchy with respect to the previously discussed BLoC artitechture.\n\n```plaintext\nlib/\nâ”œâ”€â”€ logic/\nâ”‚   â””â”€â”€ cubit/\nâ”‚       â”œâ”€â”€ counter_cubit.dart\nâ”‚       â””â”€â”€ counter_state.dart\nâ”œâ”€â”€ presentation/\nâ”‚   â”œâ”€â”€ home.dart\nâ”‚   â””â”€â”€ second_screen.dart\nâ””â”€â”€ main.dart\n```\n\nBefore proceeding, one should have basic knowledge about navigating in flutter, including, anonymous, name, and generative navigators.\n\n### Anonymous navigators\n\nHere is the implementation for anonymous navigators:\n\n```dart\nonPressed: () {\n   Navigator.pushReplacement(\n            context,\n            MaterialPageRoute(\n               builder: (_) => BlocProvider.value(\n                   value: BlocProvider.of<CounterCubit>(context),\n                   child: const SecondScreen(\n                             title: \"second screen\",\n                             color: Colors.redAccent),\n   )));\n}\n```\n\nHere we are simply injecting the old instance of the cubit into the new screen with a new context.\n\nHere are some Naive mistakes:\n\n**#1:**\n\n```dart\nonPressed: () {\n   Navigator.pushReplacement(\n            context,\n            MaterialPageRoute(\n               builder: (_) => BlocProvider.value(\n                   value: CounterCubit(),\n                   child: const SecondScreen(...),\n   )));\n}\n```\n\nHere one might accidentally provide a new instance of `CounterCubit` instead of the pre-existing one, leading to creation of a new Cubit, leakage of memory and undefined behavior.\n\n**#2:**\n\n```dart\nonPressed: () {\n   Navigator.pushReplacement(\n            context,\n            MaterialPageRoute(\n               builder: (context) => BlocProvider.value(\n                   value: BlocProvider.of<CounterCubit>(context),\n                   child: const SecondScreen(...),\n   )));\n}\n```\n\nIf we carefully watch, we are accessing the cubit with `BuildContext` name `context`. The `BlocProvider` will naturally look for the nearest context, and find the `BuildContext` in which we are building the new screen which is also named `context` and within that context, we donâ€™t have any Cubit defined. Hence we either need to change the name of the context which is being built with some other name, or just assign `_` as an argument, because this `BuildContext` wonâ€™t be used in this screen. The correct code can be referred to as the original one.\n\n### Named navigators\n\nHere is a simple implementation of named navigation\n\nCreate a cubit instance inside the `MyApp` widget where we will define our routes.\n\n```dart\nfinal CounterCubit _cubit = CounterCubit();\n```\n\ndefine the `routes` parameter inside `MaterialApp`:\n\n```dart\ninitialRoute: '/',\nroutes: {\n  '/': (context) => BlocProvider.value(\n   value: _cubit,\n   child: const HomeScreen(title: \"home\", \n             color: Colors.blueAccent)),\n  '/second': (context) => BlocProvider.value(\n   value: _cubit,\n   child: const SecondScreen(title: \"second\", \n             color: Colors.redAccent)),\n},\n```\n\nSimply change the push replacement navigator to the named one.\n\n```dart\nNavigator.pushNamed(context, '/second');\n```\n\nBut as we manually created an instance of the cubit, we need to manually dispose the cubit to avoid a memory leak.\n\n```dart\n  void dispose() {\n    _cubit.close();\n    super.dispose();\n  }\n```\n\n### Generative navigators\n\nGenerative navigtors can also be implemented with `BlocProvider.value`. Make a folder and two files for the router to encapsulate everything. Generative navigators are the best kind for the same reason.\n\nHere is the Router constants class\n\n```dart\nclass RouterConstants {\n  static const String home = '/';\n  static const String secondScreen = '/second';\n}\n```\n\nHere is the Route Generator itself:\n\n```dart\nclass RouteGenerator {\n  final CounterCubit _cubit = CounterCubit();\n  Route<dynamic> generate(RouteSettings settings) {\n    final args = settings.arguments;\n    final name = settings.name;\n\n    switch (name) {\n      case RouterConstants.home:\n        return MaterialPageRoute(\n            builder: (_) =>\n                BlocProvider.value(\n                    value: _cubit, \n                    child: const HomeScreen()));\n      case RouterConstants.secondScreen:\n        if (args is Color?) {\n          return MaterialPageRoute(\n              builder: (_) => BlocProvider.value(\n                  value: _cubit, \n                  child: SecondScreen(color: args as Color)));\n        }\n        break;\n    }\n    return MaterialPageRoute(\n        builder: (_) =>\n            BlocProvider.value(\n                value: _cubit, \n                child: const HomeScreen()));\n  }\n  void dispose() {\n    _cubit.close();\n  }\n}\n```\n\nSimilarly, a Cubit instance can be initialized within the class, and the instance can be fed to, `BlocProvider.value` wrapped on the screen where we are pushing the screen.\n\nThe old route map can be removed completely, and can be replaced with\n\n```dart\nonGenerateRoute: _router.generate\n```\n\nafter the instance of the router generator class is created:\n\n```dart\nfinal RouteGenerator _router = RouteGenerator();\n```\n\nThe old navigator can also be replaced and the args can also be passed:\n\n```dart\nNavigator.pushNamed(context, '/second',\n                    arguments: Colors.redAccent);\n```\n\nThis blog post serves as the first part of a two-part series focusing on the concepts of streams, futures, and state management using BLoC (Business Logic Component) and Cubit. Looking ahead, the second part of this series, titled **\"Getting Cracked at BLoC,\"** will explore advanced BLoC patterns and their integration into Flutter applications. This will include practical implementations and deeper insights into managing application state effectively.\n\nStay tuned for these upcoming discussions as we continue to unravel the intricacies of asynchronous programming and state management in Dart.\n\nYou can download the PDF format for this blog at [Gumroad](https://rishiahuja.gumroad.com/l/bloc1). Feel free to connect with me on my social media platforms:\n\n* [X](https://x.com/rishi2220)\n    \n* [LinkedIn](https://www.linkedin.com/in/rishi-ahuja-b1a224310/).\n    \n\nYour thoughts and feedback are always appreciated!",
      "stars": null,
      "comments": 0,
      "upvotes": 10,
      "read_time": "23 min read",
      "language": null
    },
    {
      "title_en": "Why Android Apps Don't Convert Java/Kotlin to ELF: The Power of ART and Dalvik",
      "url": "https://itsmequasim.hashnode.dev/why-android-apps-dont-convert-javakotlin-to-elf-the-power-of-art-and-dalvik",
      "source": "hashnode",
      "published_at": "2024-12-20T10:54:15.218000+00:00",
      "external_id": null,
      "tags": [
        "Android runtime",
        "Dalvik",
        "Android",
        "android app development",
        "Kotlin",
        "Java"
      ],
      "content_length": 6818,
      "content_preview": "### **Introduction**\n\nHave you ever wondered why Android apps, built with Java or Kotlin, rely on the **Android Runtime (ART)** or **Dalvik Virtual Machine** instead of converting directly to ELF binaries for native execution? At first glance, skipping ELF might seem like sacrificing speed, but this design choice empowers Android to be flexible, portable, and developer-friendly. In this blog, we'll explore why ART and Dalvik are at the heart of Android's architecture and how they strike the perf",
      "content_full": "### **Introduction**\n\nHave you ever wondered why Android apps, built with Java or Kotlin, rely on the **Android Runtime (ART)** or **Dalvik Virtual Machine** instead of converting directly to ELF binaries for native execution? At first glance, skipping ELF might seem like sacrificing speed, but this design choice empowers Android to be flexible, portable, and developer-friendly. In this blog, we'll explore why ART and Dalvik are at the heart of Android's architecture and how they strike the perfect balance between performance and usability.\n\n### **What Are ELF Files and How Do They Work in Android?**\n\nELF (Executable and Linkable Format) is a standard file format used in Linux-based systems for executables, shared libraries, and core dumps. In Android, ELF files are primarily used for **native libraries** (e.g., `.so` files) and system binaries. These files provide a structured way to store machine code, making them essential for executing **low-level operations** and interfacing with the hardware. While your Java/Kotlin code runs on ART/Dalvik, it relies on ELF files for accessing native functionality and system-level features.\n\n### **Java/Kotlin Code in Android Apps**\n\n1. **Compilation Process**:\n    \n    * Java/Kotlin code is first compiled into **bytecode** (as `.class` files).\n        \n    * The Android build system (e.g., Gradle) uses the **D8/R8 compiler** to convert these `.class` files into **DEX (Dalvik Executable)** files. These `.dex` files are packaged into the APK (or AAB).\n        \n2. **Execution Environment**:\n    \n    * When you install the app on your Android device:\n        \n        * The Android Runtime (**ART**) or the older Dalvik Virtual Machine (**DVM**) takes the `.dex` files and **interprets or compiles** them into **machine code** that can run on the device's CPU.\n            \n        * ART typically performs **Ahead-of-Time (AOT)** or **Just-in-Time (JIT)** compilation to translate DEX bytecode into machine code.\n            \n\n### **Benefits of Using ART/Dalvik**\n\nART/Dalvik provides a **managed runtime environment** that brings several advantages:\n\n#### 1\\. **Cross-Platform Compatibility**\n\n* **Java/Kotlin Bytecode Independence**: Bytecode (`DEX` format) is platform-independent, meaning it can run on any architecture (e.g., ARM, x86) without recompilation.\n    \n* **Portability**: With ART/Dalvik, the app doesnâ€™t need to be recompiled into architecture-specific ELF binaries. Instead, the runtime handles this, making it easier to support multiple device architectures.\n    \n\n#### 2\\. **Dynamic Features**\n\n* **Reflection and Dynamic Loading**: Features like reflection, dynamic class loading, and runtime code execution (e.g., `Class.forName()`) are easier to implement in a virtual machine.\n    \n* **Hot Code Patching**: The runtime enables dynamic updates to apps without requiring ELF recompilation or app restarts.\n    \n\n#### 3\\. **Efficient Memory Management**\n\n* **Garbage Collection (GC)**: ART/Dalvik provides automatic memory management via garbage collection, reducing the risk of memory leaks and ensuring efficient memory usage.\n    \n* If the app were converted into an ELF binary, managing memory would need to be done manually, increasing complexity and the likelihood of bugs.\n    \n\n#### 4\\. **Optimized Execution**\n\n* **JIT (Just-In-Time) Compilation**:\n    \n    * During execution, ART/Dalvik can optimize code based on runtime conditions (e.g., frequently used methods are compiled and optimized dynamically).\n        \n* **AOT (Ahead-Of-Time) Compilation**:\n    \n    * ART allows pre-compilation of bytecode into machine code (stored in `.oat` files) to improve startup time and execution speed.\n        \n\n#### 5\\. **Development Speed**\n\n* **Ease of Debugging**: The managed runtime provides better debugging, profiling, and exception-handling mechanisms.\n    \n* **Rapid Iteration**: Developers can write code once, and ART/Dalvik handles compilation and optimization at runtime, without needing separate ELF binaries for each platform.\n    \n\n### **Why Not Convert Bytecode to ELF?**\n\nWhile converting Java/Kotlin bytecode to ELF binaries may **potentially increase raw execution speed**, it introduces significant trade-offs:\n\n#### 1\\. **Loss of Platform Independence**\n\n* If bytecode were converted to ELF, developers would need to build separate ELF binaries for each supported architecture (e.g., ARM, x86, ARM64). This process would increase complexity and make apps harder to distribute and maintain.\n    \n\n#### 2\\. **Reduced Flexibility**\n\n* ELF binaries are static and lack the dynamic capabilities of bytecode. This would limit features like dynamic class loading, runtime code injection, and the use of reflection.\n    \n\n#### 3\\. **Increased Development Overhead**\n\n* Without a managed runtime, developers would need to handle memory management, threading, and other low-level tasks manually, increasing the risk of bugs and making development slower.\n    \n\n#### 4\\. **Increased Storage Requirements**\n\n* ELF binaries for multiple architectures would increase APK size, especially for apps that need to support a wide range of devices.\n    \n\n#### 5\\. **Runtime Optimization Benefits**\n\n* ART can optimize code at runtime based on actual usage patterns, something static ELF binaries cannot do. For example, ART can inline frequently called methods or eliminate unused code during execution.\n    \n\n### **When Does ART/Dalvik Compile to Machine Code?**\n\nART/Dalvik does compile the bytecode into machine code for performance reasons, but it doesnâ€™t produce ELF files. Instead, it uses:\n\n* **Ahead-of-Time (AOT) Compilation**:\n    \n    * At install time, ART compiles bytecode into machine code stored in `.oat` or `.odex` files.\n        \n    * These are architecture-specific but not full ELF binaries, as they rely on the runtime for execution.\n        \n* **Just-In-Time (JIT) Compilation**:\n    \n    * During app execution, ART dynamically compiles parts of the bytecode to machine code based on runtime needs.\n        \n\n### Performance vs. Flexibility: The Android Balancing Act\n\n* **Direct ELF Conversion**: Would make the app slightly faster (because thereâ€™s no runtime interpretation or JIT overhead), but it sacrifices flexibility and portability.\n    \n* **ART/Dalvik Approach**: Trades off a bit of raw speed for huge gains in compatibility, dynamic features, and ease of development.\n    \n\n### **Conclusion**\n\nUsing ART/Dalvik instead of directly converting bytecode to ELF is a design choice aimed at balancing performance with developer productivity, app portability, and runtime flexibility. While ELF binaries might offer faster execution in specific cases, the trade-offs in terms of compatibility, memory management, and dynamic features make ART/Dalvik the better choice for the diverse Android ecosystem.",
      "stars": null,
      "comments": 0,
      "upvotes": 13,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "8 Modern No-code Tools to Boost Your Developer Workflow ðŸ§‘â€ðŸ’»ðŸš€",
      "url": "https://madza.hashnode.dev/8-modern-no-code-tools-to-boost-your-developer-workflow",
      "source": "hashnode",
      "published_at": "2024-12-17T14:16:14.751000+00:00",
      "external_id": null,
      "tags": [
        "Web Development",
        "nocode",
        "tools",
        "Productivity",
        "coding",
        "Beginner Developers",
        "learning",
        "AI",
        "#ai-tools",
        "No Code",
        "Programming Blogs",
        "webdev",
        "technology",
        "JavaScript",
        "HTML5"
      ],
      "content_length": 9283,
      "content_preview": "In today's rapid-paced digital landscape, developers often face increasingly challenging requests to create and deliver robust solutions under tight deadlines.\n\nThankfully no-code tools are there to help and they could help us to bridge the gap between the the requirements of technical knowledge and the output of the execution speed.\n\nIn this article, I've manually compiled some of my favorite no-code tools that you can use in your workflow and boost productivity without a lot of tech expertise ",
      "content_full": "In today's rapid-paced digital landscape, developers often face increasingly challenging requests to create and deliver robust solutions under tight deadlines.\n\nThankfully no-code tools are there to help and they could help us to bridge the gap between the the requirements of technical knowledge and the output of the execution speed.\n\nIn this article, I've manually compiled some of my favorite no-code tools that you can use in your workflow and boost productivity without a lot of tech expertise and coding skills.\n\nThe categories include various diversified tech domains from form creation and website building to workflow automation, multilingual translation, documentation, and much more.\n\nI hope that everyone regardless of their expertise will find something valuable to improve their toolkit and that it would ultimately help to stay ahead in the competitive market.\n\nEach tool will include a direct link, description, key features, and image preview so you can get the initial impression on the go! Alright, let's dive in!\n\n---\n\n## 1\\. [Jotform Workflows](https://www.jotform.com/products/workflows/) - Automate online form processes\n\nJotform is a powerful no-code tool for building online forms, surveys, and workflows. It offers an intuitive drag-and-drop interface and hundreds of templates to get started quickly.\n\nDevelopers can use Jotform to automate form creation and data collection processes, saving valuable time on tasks like user feedback, registration, and more.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733893911682/b764a05c-36ad-4dc6-98ee-a8e8c6a27375.png align=\"center\")\n\n**Some of the most awesome features include:**\n\nðŸ”¥ **Drag and drop workflow builder**: Create customized workflows effortlessly using a drag-and-drop interface to automate tasks, processes, and approvals.\n\nðŸ“ˆ **Workflow tracking and insights**: Monitor the progress of workflows in real-time and gain insights about the completion statuses, ensuring that no task is overlooked.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733877445378/4331e730-b2fc-4166-91c4-4f6e97ff298d.png align=\"center\")\n\nðŸ¤ **Integration with 3rd party apps**: Connect seamlessly with apps like Google Drive, Slack, and Dropbox to streamline processes and data flow.\n\nâœ¨ **Pre-built templates for inspiration**: Automate your processes with 200+ pre-built workflow templates that can be fully customized to meet your unique requirements.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733967194931/b31a8945-700a-49cf-9306-2997b35dd5b5.png align=\"center\")\n\nCreate, automate, and track powerful form-based processes with Jotform Workflows. [Sign up for free today](https://www.jotform.com/products/workflows/) and start building smarter workflows in just minutes!\n\nðŸŒ Website link: [https://jotform.com](https://jotform.com)\n\nThanks to the Jotform team for sponsoring this article.\n\n---\n\n## 2\\. [V0](https://v0.dev) - Create full-stack apps with AI prompts\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733876041720/1436ed65-a4ff-4ec2-b538-b4d867688d21.png align=\"center\")\n\n[V0.dev](http://V0.dev) enables developers to create full-stack applications using AI-generated prompts. The tool helps you draft, edit, and deploy applications with minimal coding effort.\n\nUsers can also provide further prompts to edit specific parts of the generated code, describe new features that need to be added, or update the design of the application.\n\n**Key features & why to use it:**\n\n* AI-powered code generation for rapid prototyping.\n    \n* Collaborate, preview, and deploy the project instantly.\n    \n* Supports multiple programming languages and frameworks.\n    \n\nðŸŒ Website link: [https://v0.dev](https://v0.dev)\n\n---\n\n## 3\\. [Bildr](https://www.bildr.com) - Build scalable, data-driven web apps\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733872430044/fa12e568-f241-46b3-85e3-ad8b1193411d.png align=\"center\")\n\nBildr allows you to create dynamic web applications without writing code. It combines design and development tools into one platform, enabling pixel-perfect designs and robust app functionality.\n\nDevelopers can use Bildr to accelerate project delivery while maintaining creative control, saving weeks of development time.\n\n**Key features & why to use it:**\n\n* WYSIWYG editor with instant preview.\n    \n* Use logic and conditionals to handle any scenario.\n    \n* Integrate apps like Stripe, Sendgrid, and OpenAI.\n    \n\nðŸŒ Website link: [https://www.bildr.com](https://www.bildr.com)\n\n---\n\n## 4\\. [Eraser](https://www.eraser.io) - Draw complex technical diagrams\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733872562182/b4f7d9f0-20c1-42a6-a328-a4cae41c560c.png align=\"left\")\n\nEraser is a no-code tool for creating technical design diagrams. It lets you quickly draft, refine, and integrate diagrams into your documentation or repositories.\n\nDevelopers can use Eraser to visualize architectures, document workflows, and improve project clarity during the planning and execution phases.\n\n**Key features & why to use it:**\n\n* Create diagrams quickly with AI assistance.\n    \n* Embed diagrams directly into Markdown documents.\n    \n* Two-way syncing with GitHub for version control.\n    \n\nðŸŒ Website link: [https://www.eraser.io](https://www.eraser.io)\n\n## 5\\. [Rows](https://rows.com/product) - Leverage the full power of spreadsheets\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733962449802/fb9f948c-bead-4d20-b9ed-1e47718bc24f.png align=\"center\")\n\nRows is a no-code spreadsheet tool with a focus on collaboration and automation. It allows users to automate tasks within spreadsheets with custom scripts or pre-built templates.\n\nDevelopers can leverage Rows to dramatically increase productivity by automating data entry, creating real-time dashboards, integrating with various data sources, and more.\n\n**Key features & why to use it:**\n\n* Spreadsheet-database hybrid functionality.\n    \n* Automated data import and sync.\n    \n* Customizable dashboards for data visualization.\n    \n\nðŸŒ Website link: [https://rows.com/product](https://rows.com/product)\n\n---\n\n## 6\\. [Super](https://super.so/) - Create websites from Notion pages\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733873049104/0f1e823c-8d80-4634-bc1b-db8d7e1c6113.png align=\"center\")\n\nSuper turns Notion pages into fully functional websites with a custom domain, themes, and responsive design options. It simplifies web creation for developers and non-developers alike.\n\nDevelopers can use Super to quickly launch content-focused websites or prototypes, significantly reducing development time.\n\n**Key features & why to use it:**\n\n* Instant website creation from Notion.\n    \n* Custom themes and styling.\n    \n* Fast, responsive performance.\n    \n\nðŸŒ Website link: [https://super.so](https://super.so)\n\n---\n\n## 7\\. [Weglot](https://www.weglot.com) - Translate & manage multilingual websites\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733872948283/9309a465-6349-4bba-950f-e8e11c258dfc.png align=\"center\")\n\nWeglot is a no-code translation platform that allows developers to add multilingual support to websites in minutes. It supports numerous languages and integrates with platforms like WordPress and Shopify.\n\nDevelopers can leverage Weglot to expand their websiteâ€™s audience globally without dealing with complex language setups.\n\n**Key features & why to use it:**\n\n* Automatic website translation into multiple languages.\n    \n* SEO-friendly multilingual URLs.\n    \n* User-friendly translation management dashboard.\n    \n\nðŸŒ Website link: [https://www.weglot.com](https://www.weglot.com)\n\n---\n\n## 8\\. [ReadMe](https://readme.com) - Create interactive API documentation\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1733873008276/16a24883-341e-4df2-addb-529dd0f92b9a.png align=\"center\")\n\nReadMe is a tool developers use to create, customize, and manage API documentation. It enhances the developer experience with real-time API testing and personalized guides.\n\nDevelopers can use ReadMe to deliver clear, interactive API documentation, making it easier for teams and external users to work with their APIs.\n\n**Key features & why to use it:**\n\n* Customizable API documentation interface.\n    \n* Interactive code samples and tutorials.\n    \n* Support for versioning and access control.\n    \n\nðŸŒ Website link: [https://readme.com](https://readme.com)\n\n---\n\n### Did you like the resources? Here is more ðŸ‘‡\n\nJoin 6000+ others to receive the best DEV resources, tools, productivity tips, and career growth advice I discover by subscribing to [my newsletter](https://madzadev.substack.com/)!\n\n[![](https://cdn.hashnode.com/res/hashnode/image/upload/v1725354811266/0608a49b-dfc5-4c32-b61e-800fcb02fb8a.png?auto=compress,format&format=webp&auto=compress,format&format=webp&auto=compress,format&format=webp&auto=compress,format&format=webp align=\"left\")](https://madzadev.substack.com/)\n\nAlso, connect with me on [Twitter](https://twitter.com/madzadev), [LinkedIn](https://www.linkedin.com/in/madzadev/), and [GitHub](https://github.com/madzadev)!\n\nWriting has always been my passion and it gives me pleasure to help and inspire people. If you want to get featured or partner up, feel free to [get in touch](https://www.madza.dev/contact)!",
      "stars": null,
      "comments": 2,
      "upvotes": 39,
      "read_time": "5 min read",
      "language": null
    },
    {
      "title_en": "You don't need to Server-Side Render everything",
      "url": "https://sandrovolpicella.hashnode.dev/you-dont-need-to-server-side-render-everything",
      "source": "hashnode",
      "published_at": "2024-12-17T13:42:49.195000+00:00",
      "external_id": null,
      "tags": [
        "Next.js",
        "Developer"
      ],
      "content_length": 3925,
      "content_preview": "Speaking not as a dev here, just as a consumer of web applications. I feel Iâ€™m often pretty annoyed by some web applicationâ€™s UX. And I think this is often due to poorly implemented server-side rendering (SSR).\n\nIn this post, I want to give a few thoughts on SSR and why I think a simple single-page application (SPA) with client-side rendering (CSR) is the way to go.\n\nWith the rise of Next.js (I assume at least that is the reason) more and more web applications are fully server-side rendered.\n\nAn",
      "content_full": "Speaking not as a dev here, just as a consumer of web applications. I feel Iâ€™m often pretty annoyed by some web applicationâ€™s UX. And I think this is often due to poorly implemented server-side rendering (SSR).\n\nIn this post, I want to give a few thoughts on SSR and why I think a simple single-page application (SPA) with client-side rendering (CSR) is the way to go.\n\nWith the rise of Next.js (I assume at least that is the reason) more and more web applications are fully server-side rendered.\n\nAnd I get why. It is simpler. You don't have any loading states, no UI that is jumping around, and your DB access is a simple `getServerSideProps` away. **But the UX suffers in my opinion**.\n\nLet's head over to an example of LemonSqueezy.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1725778409077/87462b76-da35-4099-9066-9c9fee3e5fc0.gif align=\"center\")\n\nClicking around on LemonSqueezy somehow feels clunky. In my perspective, this is the fault of SSR. Each time you click, the server renders the page, sends it back to the client, and the client renders the full HTML.\n\nThere is no immediate feedback for the user. So, it often happens that I click a second or third time to make sure something happens.\n\nIf you compare that with Stripe it feels much more snappier.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1725778468266/5cd72c0d-cb41-4c10-ac85-c9e0c7e3210d.gif align=\"center\")\n\nYou click on something and as a consumer, you get instant feedback. You still have to wait until the content is loaded. But it feels much snappier.\n\nIn my opinion, if two-page switches take the same time it feels much faster if I get instant feedback instead of waiting for a whole page reload (or almost a whole page reload)\n\n## Why is SSR so popular?\n\nI think this has two main reasons:\n\n1. Next.js pushes a lot for it\n    \n2. It is easier\n    \n\n### Next.js pushes for it\n\nOnce you work with Next.js you will be overwhelmed by how many ways there are to fetch data. Doing everything client-side like in a good old SPA is not the favourite option often.\n\nThis is why many developers take SSR now for granted and use it everywhere.\n\n### It is easier\n\nThe second point what I think why this is SSR is so commonly used is because it is easier. You don't need to think about loading states, error handling, etc.\n\nAlso for accessing private data, you don't need to think about APIs, API keys, etc. - hint: you still should think about securing your API.\n\nYou simply put it in your fetcher function (like `getServerSideProps`) and it takes care of it. In my perspective, this comes at the cost of the UX.\n\n## When should you use SSR?\n\nOf course, there are cases when it makes sense to render your page on the server side. A good example is [Hashnode](https://hashnode.com).\n\nBlog posts need to be server-side rendered because Google doesn't like client-side rendered content.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ðŸ’¡</div>\n<div data-node-type=\"callout-text\">Not 100% true see this one. But it is probably ranked better</div>\n</div>\n\n%[https://x.com/sandro_vol/status/1839290566363824498] \n\nWhile it can read it it is often much better to have server-side rendered (or cached) content.\n\nIn general, if you have a content-heavy page it makes sense to let it SSR or statically generated during build time.\n\nIf your content doesn't change often you can make use of caching. At hashnode, we cache our content for a long period of time and only let it regenerate if something changes (e.g. title or content changes).\n\nThere are also great ways to mix server-side rendering and client-side rendering. Remix or the layout approach in the Next.JS app router are great approaches for that.\n\n**However,** I think for many applications (especially B2B admin/dashboard apps) a single-page application is still the way to go. It requires often fewer resources, is simpler to build, and is simpler to deploy.",
      "stars": null,
      "comments": 2,
      "upvotes": 36,
      "read_time": "3 min read",
      "language": null
    },
    {
      "title_en": "Demystifying Memory Management: From Deadlocks to Page Replacement Policies",
      "url": "https://aakashi.hashnode.dev/demystifying-memory-management-from-deadlocks-to-page-replacement-policies",
      "source": "hashnode",
      "published_at": "2024-12-15T16:33:18.904000+00:00",
      "external_id": null,
      "tags": [
        "Banker's Algorithm",
        "operating system",
        "memory-management",
        "storage",
        "Devops",
        "Linux",
        "Developer",
        "iOS",
        "Android",
        "Windows",
        "Blockchain",
        "Blogging",
        "technology",
        "software development",
        "Software Engineering"
      ],
      "content_length": 29301,
      "content_preview": "![Deadlock System model - GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20210501102429/img2.JPG align=\"left\")\n\nA **deadlock** occurs when two or more processes are stuck waiting for each other to release resources, causing them all to stop functioning. Imagine two people trying to cross a narrow bridge from opposite sides; if they both refuse to back up for the other, they will remain stuck indefinitely. In computing, this situation can significantly hinder system performance",
      "content_full": "![Deadlock System model - GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20210501102429/img2.JPG align=\"left\")\n\nA **deadlock** occurs when two or more processes are stuck waiting for each other to release resources, causing them all to stop functioning. Imagine two people trying to cross a narrow bridge from opposite sides; if they both refuse to back up for the other, they will remain stuck indefinitely. In computing, this situation can significantly hinder system performance.\n\n## **Deadlock Characteristics**\n\nDeadlocks are characterized by four specific conditions that must all be true for a deadlock to occur:\n\n1. **Mutual Exclusion**: This means that at least one resource (like a printer or memory) is held in a way that only one process can use it at a time. If one process is using a resource, others must wait.\n    \n2. **Hold and Wait**: Processes that are holding resources can request additional resources without releasing what they already have. For example, if Process A is using a printer and requests access to a scanner while still holding onto the printer, it creates a potential deadlock.\n    \n3. **No Preemption**: Resources cannot be forcibly taken away from a process holding them. If Process A has the printer and is waiting for the scanner, the system cannot take the printer from Process A to give it to another process.\n    \n4. **Circular Wait**: There exists a circular chain of processes where each process is waiting for a resource held by the next process in the chain. For instance, if Process A waits for a resource held by Process B, Process B waits for one held by Process C, and Process C waits for one held by Process A, they are all deadlocked.\n    \n    ## **Deadlock Prevention**\n    \n    ![Introduction of Deadlock in Operating System - GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/06/deadlock.png align=\"left\")\n    \n    To prevent deadlocks from occurring, we can alter how resources are allocated:\n    \n    * **Eliminate Mutual Exclusion**: Whenever possible, allow resources to be shared among processes. For example, multiple processes could print simultaneously if the printer supports it.\n        \n    * **Eliminate Hold and Wait**: Require processes to request all resources they need at once instead of holding onto some while waiting for others. This way, no process holds onto resources while waiting.\n        \n    * **Allow Preemption**: If a process requests a resource that cannot be granted immediately, the system can take away some of its currently held resources (if possible) and allocate them to other processes.\n        \n    * **Eliminate Circular Wait**: Create an ordering of resources and require processes to request resources in this predefined order. This prevents cycles from forming.\n        \n    \n    ## **Deadlock Avoidance: Banker's Algorithm**\n    \n    ![Banker's Algorithm - Tutorial](https://static.takeuforward.org/content/-s5iFbk01 align=\"left\")\n    \n    The **Banker's Algorithm** is a method used to avoid deadlocks by ensuring that resource allocation does not lead to an unsafe state:\n    \n    * The algorithm keeps track of how many resources each process might need (maximum needs) and how many they currently hold (allocated).\n        \n    * When a process requests resources, the system checks if granting this request would still allow all processes to complete eventually (safe state). If it would lead to an unsafe state where deadlocks could occur, the request is denied until it can be safely granted.\n        \n    \n    This proactive approach ensures that the system remains stable while allowing processes to run efficiently.\n    \n    **Deadlock Detection and Recovery**\n    \n    In computing, deadlocks can significantly hinder system performance, making it essential for operating systems to have effective mechanisms for detecting and recovering from these situations. When processes become deadlocked, they are unable to proceed because each is waiting for resources held by another process. This creates a cycle of dependencies that can halt system operations. Hereâ€™s a closer look at how deadlocks are detected and the recovery strategies used to resolve them.\n    \n    ## **Deadlock Detection Mechanisms**\n    \n    One of the primary methods for detecting deadlocks is through the use of**Wait-for Graphs (WFG)**. In this graphical representation, processes are represented as nodes, and directed edges indicate resource requests between them. For example, if Process A is waiting for a resource held by Process B, an edge is drawn from A to B. If there is a cycle in this graphâ€”meaning that you can start at one process and follow the edges to return to the same processâ€”it indicates a deadlock situation.To confirm whether a deadlock exists, algorithms can be applied to analyze the WFG.\n    \n      \n    These algorithms check for cycles within the graph and can help identify which processes are involved in the deadlock. For instance, if three processes are waiting on each other in a circular manner, the detection algorithm will reveal this cycle, allowing the operating system to take appropriate actions.\n    \n    ## **Recovery Strategies**\n    \n    Once a deadlock is detected, the operating system must implement recovery strategies to restore normal operation. Here are three common approaches:\n    \n    1. **Process Termination**: This strategy involves killing one or more processes involved in the deadlock. By terminating a process, its resources are released back to the system, allowing other processes to continue their execution. The choice of which process to terminate can depend on various factors such as priority levels or how long they have been running.\n        \n    2. **Resource Preemption**: In this approach, the operating system temporarily takes resources away from one process and allocates them to another process that needs them more urgently. This can help break the cycle of dependencies causing the deadlock. However, preemption must be handled carefully to avoid further complications or resource starvation for processes that are preempted.\n        \n    3. **Rollback**: Rollback involves restoring processes to their previous states before they entered the deadlocked condition. This method often requires maintaining checkpoints of process states so that if a deadlock occurs, the system can revert back to a known good state. This allows processes to restart their operations without being stuck indefinitely.\n        \n    \n    These recovery strategies are crucial for maintaining system stability and ensuring that resources are utilized efficiently even in the face of potential deadlocks. By implementing effective detection mechanisms and recovery methods, operating systems can minimize downtime and improve overall performance.  \n    \n    ### **Memory Management**\n    \n    ## **Basic Memory Management**\n    \n    ![Memory Management in Operating System - GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20221116104505/1white-660x453.png align=\"left\")\n    \n    Memory management refers to how an operating system handles its memory resourcesâ€”allocating space for programs when they need it and freeing that space when theyâ€™re done. Efficient memory management is crucial for overall system performance.\n    \n    ## **Logical and Physical Address Space**\n    \n    ![Logical and Physical Address Space in OS - Scaler Topics](https://www.scaler.com/topics/images/logical-and-physical-address-space-in-os-thumbnail.webp align=\"left\")\n    \n    * **Logical Address**: This is the address generated by the CPU during program execution. Itâ€™s what programs use when accessing memory.\n        \n    * **Physical Address**: This refers to actual locations in RAM where data resides. The operating system translates logical addresses into physical addresses so that programs can access their data properly.\n        \n    \n    This mapping allows programs to operate without needing to know where exactly in physical memory their data is stored.\n    \n    ## **Memory Allocation Strategies**\n    \n    Memory allocation strategies determine how memory is divided among running processes:\n    \n    1. **Contiguous Memory Allocation**:\n        \n        * **Fixed Partitioning**: The operating system divides memory into fixed-sized blocks (partitions). Each block can hold one process. While this method is simple, it can lead to internal fragmentation if processes do not fully utilize their allocated space.\n            \n        * **Variable Partitioning**: Memory is allocated based on the actual needs of each process. This method can lead to external fragmentation over time as free space becomes scattered across memory.\n            \n    2. **Fragmentation Types**:\n        \n        * **Internal Fragmentation**: Occurs when allocated memory blocks contain unused space because they are larger than needed.\n            \n        * **External Fragmentation**: Happens when free memory is scattered throughout; even though there may be enough total free space available, it may not be contiguous enough for larger requests.\n            \n    3. **Compaction**: A technique used periodically by the operating system to eliminate fragmentation by moving processes around in memory so that free spaces become contiguous again.\n        \n        ### **Paging**\n        \n        Paging is an advanced memory management technique that eliminates the need for contiguous allocation:\n        \n        * **Principle of Operation**: The physical memory (RAM) is divided into fixed-size blocks called frames, while logical memory (the program's view) is divided into pages of the same size. When programs run, their pages are loaded into any available frames in RAM.\n            \n        * **Page Allocation**: Pages do not need to be loaded in contiguous blocks; they can be scattered throughout physical memory. This flexibility minimizes fragmentation issues common with contiguous allocation methods.\n            \n            ###   \n              \n            **Hardware Support for Paging**\n            \n            Paging requires hardware support from components like the Memory Management Unit (MMU), which translates logical addresses into physical addresses using page tables maintained by the operating system. These tables store information about which pages are currently loaded into which frames in RAM.\n            \n            ## **Protection and Sharing**\n            \n            Paging allows multiple processes to share physical memory while keeping them isolated from each other. Each process has its own page table that maps its logical addresses to physical addresses securely, preventing unauthorized access between processes.\n            \n            ## **Disadvantages of Paging**\n            \n            While paging has many advantages, it also has some drawbacks:\n            \n            * Increased overhead due to managing page tables.\n                \n            * Performance issues may arise if many pages are accessed frequently (a situation known as thrashing), where constant loading and unloading of pages slow down overall performance.\n                \n            \n            ## **Virtual Memory**\n            \n            ![Operating Systems: Virtual Memory](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter9/9_01_VirtualMemoryLarger.jpg align=\"left\")\n            \n            ## **Basics of Virtual Memory**\n            \n            Virtual memory extends the available memory beyond what physically exists in RAM by using disk storage as an overflow area. This allows systems to run larger applications than would otherwise fit into physical RAM alone.\n            \n            ## **Hardware and Control Structures**\n            \n            Virtual memory relies on hardware support from components like MMUs for translating logical addresses into physical addresses via page tables. Control structures manage this translation efficiently during program execution so that applications run smoothly without interruption.\n            \n            ## **Locality of Reference**\n            \n            Programs tend to access only a small portion of their address space at any given timeâ€”a principle known as locality of reference. Virtual memory takes advantage of this behavior by keeping frequently accessed data in RAM while less-used data resides on disk storage until needed again.\n            \n            ## **Page Faults**\n            \n            A page fault occurs when a program tries to access data not currently loaded into RAM. When this happens:\n            \n            1. The operating system pauses the program.\n                \n            2. It locates the required page on disk.\n                \n            3. It loads this page into RAM.\n                \n            4. Finally, it resumes program execution with access now available.\n                \n            \n            This mechanism allows programs to use more memory than what physically exists in RAM but may introduce delays during execution when page faults occur frequently.\n            \n            ## **Working Set Model**\n            \n            The working set model defines which pages are actively used by a process within a specific time frameâ€”essentially identifying which pages should remain in RAM for optimal performance. Keeping these pages resident minimizes page faults and improves efficiency during program execution.\n            \n            ## **Dirty Page/Dirty Bit**\n            \n            A dirty page refers to a page that has been modified but not yet written back to disk storage (the original copy). The dirty bit indicates whether a page has been modified since it was loaded into RAM; this helps optimize write operations during page replacement because only dirty pages need attention when being swapped out of RAM.\n            \n            ## **Demand Paging**\n            \n            Demand paging only loads pages into RAM when they are needed rather than preloading all pages at once when an application starts running. This approach conserves memory usage and improves load times since only necessary pages occupy valuable RAM space at any given moment.\n            \n            ## **Page Replacement Policies**\n            \n            When a computer's physical memory is full, it cannot accommodate new pages that need to be loaded. This is where**page replacement policies**come into play. These policies determine which existing pages should be evicted to make room for new ones. Hereâ€™s a closer look at some of the most commonly used page replacement policies:\n            \n            ## **Optimal (OPT)**\n            \n            The**Optimal page replacement policy**is considered the best possible algorithm for managing page replacements because it minimizes the number of page faults. The principle behind this policy is straightforward: it replaces the page that will not be used for the longest time in the future.For example, suppose a program needs to load a new page, but all available frames in memory are occupied. The operating system will look ahead at the future requests and determine which of the currently loaded pages will not be needed for the longest duration.\n            \n            By evicting that page, the system ensures that it is making the most efficient use of memory resources.While this policy can theoretically provide optimal performance, it is impractical in real-world scenarios because it requires future knowledge of page requests, which is not typically available. However, it serves as a benchmark against which other algorithms can be compared, providing insights into their efficiency.\n            \n            ## **First In First Out (FIFO)**\n            \n            The\\*\\*First In First Out (FIFO)\\*\\*page replacement algorithm is one of the simplest and most straightforward methods. In FIFO, pages are organized in a queue, and when a page fault occurs (meaning a requested page isnâ€™t currently in memory), the oldest page in memoryâ€”the one that has been there the longestâ€”is removed first.This method is akin to a line of people waiting for service; the person who has been waiting the longest gets served first.\n            \n            While FIFO is easy to implement and understand, it doesnâ€™t always lead to optimal performance. For instance, it can lead to situations where frequently accessed pages are removed simply because they were loaded earlier, which might not be efficient if those pages are still needed.\n            \n            ## **Second Chance (SC)**\n            \n            The\\*\\*Second Chance (SC)\\*\\*algorithm builds on FIFO by giving pages a \"second chance\" before eviction. When a page fault occurs, instead of immediately removing the oldest page, SC checks whether that page has been used recently. Each page has an associated reference bit that indicates whether it has been accessed since it was last loaded.If the oldest page has its reference bit set to 1 (indicating it has been used), it gets its bit reset to 0 and is given another chance to stay in memory.\n            \n            The algorithm then moves on to check the next oldest page. If that one also has its reference bit set to 1, it too gets reset and is given another chance. This process continues until a page with a reference bit of 0 is found and evicted.This method helps improve performance by retaining frequently accessed pages while still following an orderly eviction process.\n            \n            ## **Not Recently Used (NRU)**\n            \n            The\\*\\*Not Recently Used (NRU)\\*\\*algorithm classifies pages based on their recent usage patterns and employs this classification to decide which pages should be evicted. Pages are categorized into four classes based on two bits: whether they have been referenced and whether they have been modified.\n            \n            1. **Class 0**: Not referenced and not modified.\n                \n            2. **Class 1**: Not referenced but modified.\n                \n            3. **Class 2**: Referenced but not modified.\n                \n            4. **Class 3**: Referenced and modified.\n                \n            \n            When a page fault occurs, NRU selects a victim from the lowest non-empty class. This means that pages that have not been used recentlyâ€”and especially those that have not been modifiedâ€”are prioritized for eviction. This approach helps ensure that frequently accessed data remains in memory while minimizing unnecessary writes back to disk.\n            \n            ## **Least Recently Used (LRU)**\n            \n            The\\*\\*Least Recently Used (LRU)\\*\\*policy operates on the assumption that pages that have not been accessed for the longest time are less likely to be needed in the near future. When a page fault occurs, LRU replaces the page that has not been accessed for the longest [period.To](http://period.To) implement LRU effectively, operating systems maintain a record of when each page was last accessedâ€”this could involve timestamps or counters associated with each frame in memory.\n            \n            When deciding which page to evict, LRU looks at these records and selects the one with the oldest access time.Although LRU can provide better performance than FIFO or SC by keeping frequently accessed pages in memory longer, its implementation can be more complex due to tracking access times or maintaining sorted lists of pages.Each of these policies offers different trade-offs between complexity and performance, making them suitable for various scenarios depending on system requirements and expected workloads. Understanding these policies helps developers optimize memory management strategies within their applications and operating systems effectively\n            \n            ## **Storage Management**\n            \n            ![What Is Memory Management in OS and How Does It work? | by åˆ˜ç»´ | Medium](https://miro.medium.com/v2/resize:fit:730/0*aa8pAuTpAoMO6sfk.png align=\"left\")\n            \n            ## **File Concepts**\n            \n            Files represent collections of related data treated as single entities within an operating systemâ€”like documents or images stored on your computerâ€™s hard drive or SSD:\n            \n            ## **Access Methods**\n            \n            Access methods define how data is read from or written to files:\n            \n            * **Sequential Access**: Data must be read or written in order; think of reading a book chapter by chapter.\n                \n            * **Random Access**: Data can be accessed directly without needing sequential reads; imagine being able to jump straight to any chapter in your book without flipping through all previous ones first.\n                \n            \n            ## **Directory Structure**\n            \n            Directories organize files hierarchically within file systemsâ€”similar to folders on your computerâ€”allowing users easy navigation through their stored files based on categories or types rather than having everything lumped together randomly.\n            \n            ## **File System Mounting**\n            \n            Mounting makes file systems accessible at specific points in your directory structure so users can interact with files stored across different devices seamlesslyâ€”like plugging in an external hard drive or USB stick and having its contents appear alongside your internal files automatically!\n            \n            ## **File Sharing and Protection**\n            \n            File sharing enables multiple users or processes access files simultaneously while protection mechanisms ensure unauthorized access is prevented through permissions settingsâ€”like who can read or edit certain documents based on user roles within an organizationâ€™s network environment!\n            \n            ## **Implementing File Systems**\n            \n            Implementing file systems involves managing how data gets stored on storage devices effectively:\n            \n            * Directory implementation defines how directories are structured and accessed so users find what they need quickly.\n                \n            * Allocation methods determine how files occupy space on disksâ€”whether contiguously (all together) or through linked lists/indexes (scattered).\n                \n            * Free space management tracks available storage efficiently so new files can be added without wasting existing capacity unnecessarily!\n                \n            * Recovery mechanisms ensure data integrity during failures or crashesâ€”like backing up important documents regularly just in case something goes wrong!\n                \n            \n            ## **Mass Storage Structure**\n            \n            ## **Overview of Mass Storage Structure**\n            \n            Mass storage provides long-term data retention beyond volatile memory (RAM)â€”think hard drives (HDDs) or solid-state drives (SSDs)â€”allowing users access their information even after turning off their computers!\n            \n            ## **Disk Structure and Attachment**\n            \n            ![Operating Systems: Mass-Storage Structure](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter10/10_01_DiskMechanism.jpg align=\"left\")\n            \n            Disks organize themselves into tracks and sectors where data gets stored efficiently! Disk attachment refers specifically how these disks connect with computer systemsâ€”such as using SATA cables versus SCSI connectionsâ€”which impacts performance reliability depending on what type you choose!\n            \n            ## **Disk Scheduling**\n            \n            ![Disk Scheduling Algorithms - GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/disc-scheduling-algorithms.png align=\"left\")\n            \n            Disk scheduling algorithms optimize read/write operations by determining which requests should be handled first based on various criteria like urgency! Common algorithms include:\n            \n            * FCFS (First-Come First-Served): Processes requests strictly based on arrival order.\n                \n            * SSTF (Shortest Seek Time First): Prioritizes requests closest physically located next within disk surface area reducing overall seek times improving throughput!\n                \n            \n            ## **Swap Space Management**\n            \n            ![Swap-Space Management in Operating system - GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20200525125013/3164-1.png align=\"left\")\n            \n            Swap space allows operating systems extend RAM using disk space as virtual memory during high load periods! This enables more applications run simultaneously without exhausting physical resources leading smoother user experiences even under heavy workloads!\n            \n            ## **RAID Structure**\n            \n            ![RAID (Redundant Arrays of Independent Disks) - GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20191125021305/Data-transfer-in-Parallel.png align=\"left\")\n            \n            RAID stands for Redundant Array of Independent Disksâ€”a technology combining multiple disks into single units improving performance/redundancy through configurations like RAID 0 (striping) enhancing speed but risking data loss versus RAID 1 (mirroring) ensuring redundancy protecting against failures!\n            \n            ## **Stable Storage Implementation**\n            \n            Stable storage ensures data integrity even during failures using techniques like journaling/replication maintaining copies critical states safeguarding against unexpected crashes ensuring users donâ€™t lose important work unexpectedly!\n            \n            ## **Tertiary Storage Structure**\n            \n            Tertiary storage includes removable media such as tapes/optical disks primarily used backup archival purposes providing long-term solutions lower costs compared primary options like HDDs/SSDs making them ideal choices organizations looking retain historical information securely!\n            \n            ## **Case Study**\n            \n            ## **Real-Time Operating System (RTOS)**\n            \n            Real-Time Operating Systems cater specifically applications requiring timely processing predictable response times! They prioritize tasks based deadlines rather than traditional scheduling methods found general-purpose OS ensuring critical functions execute reliably without delays impacting performance negatively!\n            \n            ## **iOS Architecture and Features**\n            \n            ![iOS | Uxcel](https://img.uxcel.com/tags/ios-1721717446446-2x.jpg align=\"left\")\n            \n            iOS operates Unix-based architecture optimized mobile devices! Key features include multitasking capabilities allowing background processing without draining battery life excessively alongside security enhancements sandboxing protecting user data malicious applications ensuring smooth operation across various device types!\n            \n            ## **Applications of iOS Operating System**\n            \n            iOS supports diverse applications ranging productivity tools document editors games leveraging user-friendly interface robust performance capabilities tailored specifically mobile environments making it popular choice users seeking seamless experiences across devices!\n            \n            ## **Unix Operating System**\n            \n            ![UNIX operating system - javatpoint](https://images.javatpoint.com/linux/images/unix-operating-system.png align=\"left\")\n            \n            Unix known multiuser capabilities portability across different hardware platforms employing hierarchical file systems powerful command-line tools facilitating efficient administration tasks diverse environments enabling developers create robust applications easily adaptable various settings!\n            \n            ## **Windows Operating System**\n            \n            Windows OS offers extensive application support focusing user interface design allowing compatibility various software ecosystems providing robust security features user account controls regular updates enhancing overall stability keeping systems secure against threats evolving landscape technology!\n            \n            ## **Android OS**\n            \n            ![Android Digital Signage: Superior Performance and Features](https://navori.com/wp-content/uploads/2023/06/Logo-Android.jpg align=\"left\")\n            \n            Android open-source mobile operating system based Linux supporting diverse hardware configurations offering extensive customization options through application framework making popular among developers seeking flexibility app development fostering innovation creativity across industry sectors!\n            \n            This comprehensive exploration covers essential concepts related deadlocks memory management virtual storage management various operating systems architectures! Understanding these topics equips individuals foundational knowledge crucial advancing computer science software engineering fields paving way exciting careers future technological advancements!",
      "stars": null,
      "comments": 1,
      "upvotes": 34,
      "read_time": "16 min read",
      "language": null
    },
    {
      "title_en": "Creating a Chat Application with Go, Gorilla WebSocket, and jQuery",
      "url": "https://arya2004.hashnode.dev/creating-chat-application-go-gorilla-websocket-jquery",
      "source": "hashnode",
      "published_at": "2024-12-15T21:38:44.208000+00:00",
      "external_id": null,
      "tags": [
        "Go Language",
        "websockets",
        "jQuery",
        "JavaScript",
        "golang"
      ],
      "content_length": 31834,
      "content_preview": "In this blog, we will explore how to build a **real-time chat application** using **Go (Golang)**, **Gorilla WebSocket**, and **jQuery**. Real-time chat applications require quick and seamless communication between users, and WebSockets make this possible by enabling continuous, two-way communication.\n\n## **Why WebSockets?**\n\nWebSockets are an advanced technology that allows a **persistent, bidirectional connection** between the client (e.g., browser) and the server. Unlike traditional HTTP, whe",
      "content_full": "In this blog, we will explore how to build a **real-time chat application** using **Go (Golang)**, **Gorilla WebSocket**, and **jQuery**. Real-time chat applications require quick and seamless communication between users, and WebSockets make this possible by enabling continuous, two-way communication.\n\n## **Why WebSockets?**\n\nWebSockets are an advanced technology that allows a **persistent, bidirectional connection** between the client (e.g., browser) and the server. Unlike traditional HTTP, where a connection is opened for each request and response, WebSockets establish a single connection that remains open throughout the session. This reduces overhead and ensures low-latency communication.\n\n## **WebSocket Workflow**\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734294893977/bc02fecb-1b40-4cad-a759-2d554bac57b4.webp align=\"center\")\n\nThe WebSocket protocol works as shown in the diagram:\n\n1. **HTTP Connection (Upgrade Request):**  \n    The process begins with a standard HTTP connection where the client sends an \"Upgrade\" request to the server, asking to switch to the WebSocket protocol.\n    \n2. **101 Switching Protocols:**  \n    If the server supports WebSockets, it responds with an HTTP `101 Switching Protocols` status code. This confirms that the connection is upgraded.\n    \n3. **WebSocket Communication:**  \n    Once the connection is upgraded, **bidirectional communication** starts. Both the client and server can send messages to each other over the same connection without repeatedly re-establishing connections.\n    \n4. **Connection Close:**  \n    When the communication ends, either the client or the server can close the WebSocket connection.\n    \n\nThis workflow ensures efficient and fast communication, making WebSockets ideal for applications like chats, live notifications, or real-time updates.\n\n# **Setting Up the Project**\n\nTo build a real-time chat application using **Go**, **Gorilla WebSocket**, and **jQuery**, itâ€™s important to set up the project directory structure correctly. Below is the step-by-step guide and methods to include all dependencies and necessary files.\n\n## **Project Directory Structure**\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734294985153/5f2c2515-e2bc-4bf0-9e10-18a485d5232c.png align=\"center\")\n\n### **1\\. Initializing the Project**\n\nStart by creating the Go module and installing required dependencies.\n\n1. **Initialize the Go Module**:  \n    Run the following command to initialize a Go module in the project root:\n    \n\n```bash\ngo mod init go-websocket-blog\n```\n\n2. **Install Dependencies**:  \n    Install the necessary libraries:\n    \n\n```bash\ngo get github.com/gorilla/websocket\ngo get github.com/bmizerany/pat\ngo get github.com/CloudyKit/jet/v6\n```\n\n### **2\\. Adding Reconnecting WebSocket**\n\nTo include the `reconnecting-websocket.min.js` library for automatic reconnections, you can fetch it either **automatically** (Linux/Mac) or **manually** (Windows).\n\n#### **Automatic Download (Linux/Mac)**\n\nRun the following script to download the file into the correct directory:\n\n```bash\n#!/bin/bash\n\n# Create required directories\nmkdir -p assets/javascript\n\n# Download reconnecting-websocket.min.js\ncurl -o assets/javascript/reconnecting-websocket.min.js \\\nhttps://raw.githubusercontent.com/joewalnes/reconnecting-websocket/master/reconnecting-websocket.min.js\n\necho \"reconnecting-websocket.min.js has been downloaded successfully!\"\n```\n\n#### **Manual Download (Windows)**\n\n1. Go to the following URL:  \n    [https://github.com/joewalnes/reconnecting-websocket/blob/master/reconnecting-websocket.min.js](https://github.com/joewalnes/reconnecting-websocket/blob/master/reconnecting-websocket.min.js)\n    \n2. Copy the **entire code** from the file.\n    \n3. Create a file named `reconnecting-websocket.min.js` inside the `assets/javascript/` directory.\n    \n4. Paste the copied code into the file and save it.\n    \n\n### **3\\. Full Bash Script to Generate the Project Structure**\n\nTo automate the entire setup, use the following script:\n\n```bash\n#!/bin/bash\n\n# Create project structure\nmkdir -p go-websocket-blog/assets/css\nmkdir -p go-websocket-blog/assets/javascript\nmkdir -p go-websocket-blog/cmd\nmkdir -p go-websocket-blog/pkg/handlers\nmkdir -p go-websocket-blog/pkg/routes\nmkdir -p go-websocket-blog/templates\n\ncd go-websocket-blog\n\n# Create files\ntouch assets/css/styles.css\ntouch assets/javascript/main.js\ntouch cmd/main.go\ntouch pkg/handlers/handlers.go\ntouch pkg/routes/routes.go\ntouch templates/home.html\n\n# Initialize Go module\ngo mod init go-websocket-blog\n\n# Add dependencies\ngo get github.com/gorilla/websocket\ngo get github.com/bmizerany/pat\ngo get github.com/CloudyKit/jet/v6\n\n# Fetch reconnecting-websocket.min.js\ncurl -o assets/javascript/reconnecting-websocket.min.js \\\nhttps://raw.githubusercontent.com/joewalnes/reconnecting-websocket/master/reconnecting-websocket.min.js\n\n# Print success message\necho \"Project structure created successfully with dependencies and reconnecting-websocket!\"\n```\n\n# Core WebSocket Handlers\n\nThe `handlers.go` file serves as the heart of this application, managing WebSocket connections, communication, and rendering templates. We'll break down the code function by function, explaining its purpose and behavior. Letâ€™s get started!\n\n### **Setting Up WebSocket Connections and Templates**\n\n```go\nvar (\n\twsChannel       = make(chan WsPayload)\n\tclients         = make(map[*websocket.Conn]string)\n\tviews           = jet.NewSet(\n\t\tjet.NewOSFileSystemLoader(\"./templates\"),\n\t\tjet.InDevelopmentMode(),\n\t)\n\tupgradeConnection = websocket.Upgrader{\n\t\tReadBufferSize:  1024,\n\t\tWriteBufferSize: 1024,\n\t\tCheckOrigin: func(r *http.Request) bool { return true },\n\t}\n)\n```\n\n**Explanation:**\n\n* `wsChannel`: A Go channel for passing messages between WebSocket handlers.\n    \n* `clients`: A map to track active WebSocket connections and their associated usernames.\n    \n* `views`: Configures Jet template engine to load templates from the `./templates` directory.\n    \n* `upgradeConnection`: Configures WebSocket settings, allowing all origins (`CheckOrigin` returns `true`).\n    \n\nThis setup enables managing WebSocket connections and rendering HTML templates dynamically.\n\n### **Home Page Handler**\n\n```go\nfunc Home(w http.ResponseWriter, r *http.Request) {\n\tlog.Println(\"Rendering home page\")\n\terr := renderPage(w, \"home.html\", nil)\n\tif err != nil {\n\t\tlog.Println(\"Error rendering home page:\", err)\n\t}\n}\n```\n\n**Explanation:**\n\n* The `Home` function renders the `home.html` template using the `renderPage` helper function.\n    \n* If rendering fails, it logs the error. This provides a simple way to serve the application's main interface.\n    \n\n### **WebSocket Endpoint**\n\n```go\nfunc WsEndpoint(w http.ResponseWriter, r *http.Request) {\n\tlog.Println(\"Client attempting to connect to WebSocket endpoint\")\n\n\tws, err := upgradeConnection.Upgrade(w, r, nil)\n\tif err != nil {\n\t\tlog.Println(\"WebSocket upgrade failed:\", err)\n\t\treturn\n\t}\n\n\tlog.Println(\"Client successfully connected to WebSocket endpoint\")\n\n\tresponse := WsJsonResponse{\n\t\tMessage: \"<em><small>Connected to server</small></em>\",\n\t}\n\n\tclients[ws] = \"\"\n\terr = ws.WriteJSON(response)\n\tif err != nil {\n\t\tlog.Println(\"Error writing JSON response:\", err)\n\t\treturn\n\t}\n\n\tgo ListenForWs(ws)\n}\n```\n\n**Explanation:**\n\n* **Upgrade Connection**: Converts an HTTP connection into a WebSocket connection using `websocket.Upgrader`.\n    \n* **Initial Response**: Sends a confirmation message to the client.\n    \n* **Client Management**: Adds the WebSocket connection to the `clients` map.\n    \n* **Message Listening**: Launches the `ListenForWs` function in a goroutine to handle client messages asynchronously.\n    \n\n### **Listening to Client Messages**\n\n```go\nfunc ListenForWs(conn *websocket.Conn) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tlog.Printf(\"Recovered from error: %v\", r)\n\t\t}\n\t}()\n\n\tvar payload WsPayload\n\tfor {\n\t\tif err := conn.ReadJSON(&payload); err != nil {\n\t\t\tlog.Println(\"Error reading WebSocket message:\", err)\n\t\t\treturn\n\t\t}\n\n\t\tpayload.Conn = conn\n\t\twsChannel <- payload\n\t}\n}\n```\n\n**Explanation:**\n\n* Reads JSON messages from the client and stores them in the `payload`.\n    \n* Adds the clientâ€™s connection to the `payload` and sends it to `wsChannel` for processing.\n    \n* Uses `defer` to gracefully handle panics and clean up.\n    \n\n### **Handling Messages on the WebSocket Channel**\n\n```go\nfunc ListenToWsChannel() {\n\tfor {\n\t\te := <-wsChannel\n\t\tvar response WsJsonResponse\n\n\t\tswitch e.Action {\n\t\tcase \"username\":\n\t\t\tclients[e.Conn] = e.Username\n\t\t\tresponse.Action = \"list_users\"\n\t\t\tresponse.ConnectedUsers = getUserList()\n\t\t\tbroadcastToAll(response)\n\n\t\tcase \"left\":\n\t\t\tdelete(clients, e.Conn)\n\t\t\tresponse.Action = \"list_users\"\n\t\t\tresponse.ConnectedUsers = getUserList()\n\t\t\tbroadcastToAll(response)\n\n\t\tcase \"broadcast\":\n\t\t\tresponse.Action = \"broadcast\"\n\t\t\tresponse.Message = fmt.Sprintf(\"<strong>%s</strong>: %s\", e.Username, e.Message)\n\t\t\tbroadcastToAll(response)\n\t\t}\n\t}\n}\n```\n\n**Explanation:**\n\n* **Listening to** `wsChannel`: Continuously waits for messages sent through the channel.\n    \n* **Message Handling**:\n    \n    * `username`: Updates the client's username and broadcasts the user list.\n        \n    * `left`: Removes a client and updates the user list.\n        \n    * `broadcast`: Sends the clientâ€™s message to all connected users.\n        \n\n## **Helper Functions**\n\n### **Retrieve User List**\n\n```go\nfunc getUserList() []string {\n\tvar userList []string\n\tfor _, username := range clients {\n\t\tif username != \"\" {\n\t\t\tuserList = append(userList, username)\n\t\t}\n\t}\n\tsort.Strings(userList)\n\treturn userList\n}\n```\n\n**Purpose**: Collects all usernames from `clients`, sorts them alphabetically, and returns the list.\n\n### **Broadcast Messages**\n\n```go\nfunc broadcastToAll(response WsJsonResponse) {\n\tfor client := range clients {\n\t\tif err := client.WriteJSON(response); err != nil {\n\t\t\tlog.Printf(\"Error sending message to client: %v\", err)\n\t\t\t_ = client.Close()\n\t\t\tdelete(clients, client)\n\t\t}\n\t}\n}\n```\n\n**Purpose**: Sends a JSON response to all connected WebSocket clients. Removes clients that fail to receive the message.\n\n### **Render Templates**\n\n```go\nfunc renderPage(w http.ResponseWriter, tmpl string, data jet.VarMap) error {\n\tview, err := views.GetTemplate(tmpl)\n\tif err != nil {\n\t\tlog.Println(\"Error loading template:\", err)\n\t\treturn err\n\t}\n\n\tif err := view.Execute(w, data, nil); err != nil {\n\t\tlog.Println(\"Error executing template:\", err)\n\t\treturn err\n\t}\n\treturn nil\n}\n```\n\n**Purpose**: Fetches and renders an HTML template with dynamic data using Jet.\n\nHereâ€™s the complete `handlers.go` file for you to copy and use\n\n```go\npackage handlers\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"sort\"\n\n\t\"github.com/CloudyKit/jet/v6\"\n\t\"github.com/gorilla/websocket\"\n)\n\n// Global variables for managing WebSocket connections\nvar (\n\twsChannel       = make(chan WsPayload)                // Channel for handling WebSocket messages\n\tclients         = make(map[*websocket.Conn]string)   // Map of connected clients and their usernames\n\tviews           = jet.NewSet(                        // Template engine configuration\n\t\tjet.NewOSFileSystemLoader(\"./templates\"),\n\t\tjet.InDevelopmentMode(),\n\t)\n\tupgradeConnection = websocket.Upgrader{\n\t\tReadBufferSize:  1024,\n\t\tWriteBufferSize: 1024,\n\t\tCheckOrigin: func(r *http.Request) bool { return true }, // Allow all origins\n\t}\n)\n\n// Home renders the home page of the application\nfunc Home(w http.ResponseWriter, r *http.Request) {\n\tlog.Println(\"Rendering home page\")\n\terr := renderPage(w, \"home.html\", nil)\n\tif err != nil {\n\t\tlog.Println(\"Error rendering home page:\", err)\n\t}\n}\n\n// WsJsonResponse represents the JSON structure for WebSocket responses\ntype WsJsonResponse struct {\n\tAction         string   `json:\"action\"`\n\tMessage        string   `json:\"message\"`\n\tMessageType    string   `json:\"message_type\"`\n\tConnectedUsers []string `json:\"connected_users\"`\n}\n\n// WsPayload represents the payload received from WebSocket clients\ntype WsPayload struct {\n\tAction   string          `json:\"action\"`\n\tUsername string          `json:\"username\"`\n\tMessage  string          `json:\"message\"`\n\tConn     *websocket.Conn `json:\"-\"` // Exclude from JSON serialization\n}\n\n// WsEndpoint upgrades HTTP connections to WebSocket and initializes communication\nfunc WsEndpoint(w http.ResponseWriter, r *http.Request) {\n\tlog.Println(\"Client attempting to connect to WebSocket endpoint\")\n\n\tws, err := upgradeConnection.Upgrade(w, r, nil)\n\tif err != nil {\n\t\tlog.Println(\"WebSocket upgrade failed:\", err)\n\t\treturn\n\t}\n\n\tlog.Println(\"Client successfully connected to WebSocket endpoint\")\n\n\t// Initial connection response\n\tresponse := WsJsonResponse{\n\t\tMessage: \"<em><small>Connected to server</small></em>\",\n\t}\n\n\tclients[ws] = \"\" // Add the new connection to clients map\n\n\terr = ws.WriteJSON(response)\n\tif err != nil {\n\t\tlog.Println(\"Error writing JSON response:\", err)\n\t\treturn\n\t}\n\n\t// Start listening for messages from this client\n\tgo ListenForWs(ws)\n}\n\n// ListenToWsChannel listens for messages on the WebSocket channel and handles them\nfunc ListenToWsChannel() {\n\tfor {\n\t\te := <-wsChannel\n\t\tvar response WsJsonResponse\n\n\t\tswitch e.Action {\n\t\tcase \"username\":\n\t\t\t// Handle username assignment\n\t\t\tclients[e.Conn] = e.Username\n\t\t\tresponse.Action = \"list_users\"\n\t\t\tresponse.ConnectedUsers = getUserList()\n\t\t\tbroadcastToAll(response)\n\n\t\tcase \"left\":\n\t\t\t// Handle client disconnection\n\t\t\tresponse.Action = \"list_users\"\n\t\t\tdelete(clients, e.Conn)\n\t\t\tresponse.ConnectedUsers = getUserList()\n\t\t\tbroadcastToAll(response)\n\n\t\tcase \"broadcast\":\n\t\t\t// Handle broadcast messages\n\t\t\tresponse.Action = \"broadcast\"\n\t\t\tresponse.Message = fmt.Sprintf(\"<strong>%s</strong>: %s\", e.Username, e.Message)\n\t\t\tbroadcastToAll(response)\n\t\t}\n\t}\n}\n\n// getUserList returns a sorted list of connected usernames\nfunc getUserList() []string {\n\tvar userList []string\n\tfor _, username := range clients {\n\t\tif username != \"\" {\n\t\t\tuserList = append(userList, username)\n\t\t}\n\t}\n\tsort.Strings(userList)\n\treturn userList\n}\n\n// broadcastToAll sends a WebSocket response to all connected clients\nfunc broadcastToAll(response WsJsonResponse) {\n\tfor client := range clients {\n\t\tif err := client.WriteJSON(response); err != nil {\n\t\t\tlog.Printf(\"Error sending message to client: %v\", err)\n\t\t\t_ = client.Close()\n\t\t\tdelete(clients, client)\n\t\t}\n\t}\n}\n\n// ListenForWs listens for messages from a specific WebSocket client\nfunc ListenForWs(conn *websocket.Conn) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tlog.Printf(\"Recovered from error: %v\", r)\n\t\t}\n\t}()\n\n\tvar payload WsPayload\n\tfor {\n\t\tif err := conn.ReadJSON(&payload); err != nil {\n\t\t\tlog.Println(\"Error reading WebSocket message:\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Assign connection to payload and send it to the channel\n\t\tpayload.Conn = conn\n\t\twsChannel <- payload\n\t}\n}\n\n// renderPage renders a template with the given data\nfunc renderPage(w http.ResponseWriter, tmpl string, data jet.VarMap) error {\n\tview, err := views.GetTemplate(tmpl)\n\tif err != nil {\n\t\tlog.Println(\"Error loading template:\", err)\n\t\treturn err\n\t}\n\n\tif err := view.Execute(w, data, nil); err != nil {\n\t\tlog.Println(\"Error executing template:\", err)\n\t\treturn err\n\t}\n\treturn nil\n}\n```\n\nNow that weâ€™ve covered the core functionality in `handlers.go`, letâ€™s look at how the routing and server setup are handled in `routes.go` and `main.go`. These files configure the applicationâ€™s routes and initialize the server.\n\n# Application Routing\n\n### `routes.go`\n\n```go\npackage routes\n\nimport (\n\t\"net/http\"\n\n\t\"github.com/arya2004/gobanter/pkg/handlers\"\n\t\"github.com/bmizerany/pat\"\n)\n\n\nfunc Routes() http.Handler {\n\tmux := pat.New()\n\n\n\tmux.Get(\"/\", http.HandlerFunc(handlers.Home))\n\n\n\tmux.Get(\"/ws\", http.HandlerFunc(handlers.WsEndpoint))\n\n\n\tfileServer := http.FileServer(http.Dir(\"./assets/\"))\n\tmux.Get(\"/assets/\", http.StripPrefix(\"/assets/\", fileServer))\n\n\treturn mux\n}\n```\n\n**Explanation:**\n\n1. **Router Initialization**:\n    \n    * [`pat.New`](http://pat.New)`()` creates a new router using the `pat` package, which provides a simple pattern-based routing mechanism.\n        \n2. **Home Page Route**:\n    \n    * The `\"/\"` route maps to the `Home` handler, which serves the home page.\n        \n3. **WebSocket Route**:\n    \n    * The `\"/ws\"` route is connected to the `WsEndpoint` handler, establishing WebSocket communication.\n        \n4. **Static File Serving**:\n    \n    * Serves static files (e.g., CSS, JS, images) from the `assets` directory using `http.FileServer`.\n        \n    * `http.StripPrefix` removes the `/assets/` prefix from the URL path before serving files.\n        \n\n# Application Entry Point\n\n### `main.go`\n\n```go\npackage main\n\nimport (\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/arya2004/gobanter/pkg/handlers\"\n\t\"github.com/arya2004/gobanter/pkg/routes\"\n)\n\n\nfunc main() {\n\n\tmux := routes.Routes()\n\n\n\tlog.Println(\"Starting WebSocket channel listener...\")\n\tgo handlers.ListenToWsChannel()\n\n\n\tlog.Println(\"Server starting on port 8080...\")\n\tif err := http.ListenAndServe(\":8080\", mux); err != nil {\n\t\tlog.Fatalf(\"Error starting server: %v\", err)\n\t}\n}\n```\n\n**Explanation:**\n\n1. **Initialize Routes**:\n    \n    * The `routes.Routes()` function sets up all application routes and returns a `http.Handler`.\n        \n2. **WebSocket Channel Listener**:\n    \n    * `handlers.ListenToWsChannel()` is launched in a goroutine to handle WebSocket messages asynchronously.\n        \n3. **Start HTTP Server**:\n    \n    * The server listens on port `8080` using `http.ListenAndServe` with the router (`mux`) handling incoming requests.\n        \n    * Logs errors if the server fails to start.\n        \n\n# User Interface for Chat Application\n\nThis HTML file defines the structure of the chat application's user interface. It includes the layout for sending messages, displaying the chat history, and listing online users. It uses Bootstrap for styling and incorporates custom CSS and JavaScript for functionality. The file is loaded as the home page of the application.\n\nCopy and paste the above code into an `home.html` file in your project for the chat application interface.\n\n```xml\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Chat Application</title>\n    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\"\n        integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\" />\n\n    <link rel=\"stylesheet\" href=\"/assets/css/styles.css\">\n</head>\n\n<body>\n    <div class=\"container mt-5\">\n        <header class=\"text-center mb-4\">\n            <h1 class=\"display-4 fw-bold\">Welcome to <span class=\"text-primary\">GoBanter</span></h1>\n            <p class=\"text-muted fs-5\">Connect and communicate with ease</p>\n            <hr class=\"mt-4\">\n        </header>\n        <div class=\"row\">\n            <div class=\"col-md-8\">\n                <div class=\"card shadow\">\n                    <div class=\"card-header bg-primary text-white\">\n                        <div class=\"chat-header\">Chat</div>\n                    </div>\n                    <div class=\"card-body\">\n                        <div class=\"mb-3\">\n                            <label for=\"username\" class=\"form-label\">Username</label>\n                            <input type=\"text\" id=\"username\" class=\"form-control\" placeholder=\"Enter your username\">\n                        </div>\n                        <div class=\"mb-3\">\n                            <label for=\"message\" class=\"form-label\">Message</label>\n                            <input type=\"text\" id=\"message\" class=\"form-control\" placeholder=\"Type your message\">\n                        </div>\n                        <div class=\"d-flex justify-content-between\">\n                            <button id=\"sendButton\" class=\"btn btn-primary\">Send Message</button>\n                            <div id=\"status\"></div>\n                        </div>\n                        <div id=\"output\" class=\"chatbox mt-3\"></div>\n                    </div>\n                </div>\n            </div>\n            <div class=\"col-md-4\">\n                <div class=\"card shadow\">\n                    <div class=\"card-header bg-secondary text-white\">\n                        <div class=\"chat-header\">Who's Online</div>\n                    </div>\n                    <div class=\"card-body\">\n                        <ul id=\"online_users\"></ul>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n\n    <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js\"\n        integrity=\"sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM\"\n        crossorigin=\"anonymous\"></script>\n\n    <script src=\"https://code.jquery.com/jquery-3.7.1.slim.min.js\"\n        integrity=\"sha256-kmHvs0B+OpCW5GVHUNjv9rOmY0IvSIRcf7zGUDTDQM8=\" crossorigin=\"anonymous\"></script>\n\n    <script src=\"/assets/javascript/reconnecting-websocket.min.js\"></script>\n\n    <script src=\"/assets/javascript/main.js\"></script>\n\n\n\n</body>\n\n</html>\n```\n\n# **Chat Application Styling**\n\nThis CSS file defines the styles for the chat application, enhancing the layout and user interface elements. It includes styling for the chatbox, headers, and online user list. Use this file to ensure your application has a clean and responsive design.\n\nCopy the code below and save it as `styles.css`:\n\n```css\n.chatbox {\n    border: 1px solid #ddd;\n    border-radius: 0.5rem;\n    min-height: 200px;\n    max-height: 400px;\n    overflow-y: auto;\n    padding: 1em;\n    background-color: #f8f9fa;\n}\n\n.chat-header {\n    font-size: 1.5rem;\n    font-weight: bold;\n}\n\n#online_users {\n    list-style: none;\n    padding: 0;\n}\n\n#online_users li {\n    background: #e9ecef;\n    margin-bottom: 0.5em;\n    padding: 0.5em;\n    border-radius: 0.5rem;\n}\n```\n\n# jQuery-Powered WebSocket Communication\n\nThis `main.js` file contains the client-side logic for interacting with the WebSocket server using jQuery. It includes functions to handle WebSocket events, update the UI dynamically, and manage user interactions. Below, weâ€™ll break the code into sections and explain each part.\n\n## **WebSocket Initialization**\n\n```javascript\nlet socket = null;\n\n$(document).ready(function () {\n    const offline = `<span class=\"badge bg-danger\">Not connected</span>`;\n    const online = `<span class=\"badge bg-success\">Connected</span>`;\n\n    const $statusDiv = $(\"#status\");\n    const $output = $(\"#output\");\n    const $userField = $(\"#username\");\n    const $messageField = $(\"#message\");\n    const $onlineUsers = $(\"#online_users\");\n\n    socket = new ReconnectingWebSocket(\"ws://localhost:8080/ws\", null, { debug: true, reconnectInterval: 3000 });\n```\n\n**Explanation:**\n\n* **Variables**:\n    \n    * `offline` and `online`: HTML badges indicating connection status.\n        \n    * `$statusDiv`, `$output`, `$userField`, `$messageField`, `$onlineUsers`: Cached jQuery selectors for UI elements.\n        \n* **WebSocket Connection**:\n    \n    * `ReconnectingWebSocket` is used to establish a persistent WebSocket connection with auto-reconnect enabled.\n        \n\n## **Handling WebSocket Events**\n\n```javascript\n    socket.onopen = function () {\n        console.log(\"connected!!\");\n        $statusDiv.html(online);\n    };\n\n    socket.onclose = function () {\n        console.log(\"connection closed!\");\n        $statusDiv.html(offline);\n    };\n\n    socket.onerror = function () {\n        console.log(\"there was an error\");\n        $statusDiv.html(offline);\n    };\n```\n\n**Explanation:**\n\n* `onopen`: Updates the UI to show a \"Connected\" status when the WebSocket connection opens.\n    \n* `onclose`: Changes the status to \"Not connected\" when the connection closes.\n    \n* `onerror`: Handles connection errors by logging them and updating the UI.\n    \n\n## **Receiving WebSocket Messages**\n\n```javascript\n    socket.onmessage = function (msg) {\n        const data = JSON.parse(msg.data);\n        console.log(\"Action:\", data.action);\n\n        switch (data.action) {\n            case \"list_users\":\n                $onlineUsers.empty();\n                if (data.connected_users.length > 0) {\n                    $.each(data.connected_users, function (index, user) {\n                        $onlineUsers.append(`<li class=\"list-group-item\">${user}</li>`);\n                    });\n                }\n                break;\n\n            case \"broadcast\":\n                $output.append(`<div>${data.message}</div>`);\n                $output.scrollTop($output.prop(\"scrollHeight\"));\n                break;\n        }\n    };\n```\n\n**Explanation:**\n\n* `onmessage`:\n    \n    * Parses incoming WebSocket messages as JSON.\n        \n    * **Switch Cases**:\n        \n        * `list_users`: Updates the \"Who's Online\" list with connected usernames.\n            \n        * `broadcast`: Appends broadcast messages to the chatbox and scrolls to the latest message.\n            \n\n## **User Interaction: Username Update**\n\n```javascript\n    $userField.on(\"change\", function () {\n        const jsonData = {\n            action: \"username\",\n            username: $(this).val()\n        };\n        console.log(jsonData);\n        socket.send(JSON.stringify(jsonData));\n    });\n```\n\n**Explanation:**\n\n* Listens for changes in the username field.\n    \n* Sends a `username` action to the server with the new username as JSON.\n    \n\n## **Sending Messages**\n\n```javascript\n    $messageField.on(\"keydown\", function (event) {\n        if (event.key === \"Enter\") {\n            if (!socket || socket.readyState !== WebSocket.OPEN) {\n                alert(\"No connection\");\n                return false;\n            }\n\n            if (!$userField.val() || !$messageField.val()) {\n                alert(\"Enter username and message!\");\n                return false;\n            } else {\n                sendMessage();\n            }\n\n            event.preventDefault();\n        }\n    });\n\n    $(\"#sendButton\").on(\"click\", function () {\n        if (!$userField.val() || !$messageField.val()) {\n            alert(\"Enter username and message!\");\n            return false;\n        } else {\n            sendMessage();\n        }\n    });\n```\n\n* **Enter Key**:\n    \n    * Triggers `sendMessage` when Enter is pressed in the message field.\n        \n    * Validates that a username and message are provided and checks WebSocket connection status.\n        \n* **Send Button**:\n    \n    * Performs the same validation and triggers `sendMessage` when the \"Send Message\" button is clicked.\n        \n\n## **Disconnecting on Page Unload**\n\n```javascript\n    $(window).on(\"beforeunload\", function () {\n        console.log(\"leaving ;(\");\n        if (socket && socket.readyState === WebSocket.OPEN) {\n            const jsonData = { action: \"left\" };\n            socket.send(JSON.stringify(jsonData));\n        }\n    });\n```\n\n**Explanation:**\n\n* Listens for the browser's `beforeunload` event (e.g., closing the tab or refreshing the page).\n    \n* Sends a `left` action to notify the server that the user has disconnected.\n    \n\n## **Sending JSON Data to WebSocket**\n\n```javascript\n    function sendMessage() {\n        const jsonData = {\n            action: \"broadcast\",\n            username: $userField.val(),\n            message: $messageField.val()\n        };\n        socket.send(JSON.stringify(jsonData));\n        $messageField.val(\"\");\n    }\n```\n\n**Explanation:**\n\n* Constructs a JSON object with the `broadcast` action, username, and message.\n    \n* Sends the JSON to the server through the WebSocket connection.\n    \n* Clears the message field after sending.\n    \n\n## **Full Code**\n\nHereâ€™s the full `main.js` file without comments for you to copy and use:\n\n```javascript\nlet socket = null;\n    \n$(document).ready(function () {\n    const offline = `<span class=\"badge bg-danger\">Not connected</span>`;\n    const online = `<span class=\"badge bg-success\">Connected</span>`;\n\n    const $statusDiv = $(\"#status\");\n    const $output = $(\"#output\");\n    const $userField = $(\"#username\");\n    const $messageField = $(\"#message\");\n    const $onlineUsers = $(\"#online_users\");\n\n    // Reconnecting WebSocket Initialization\n    socket = new ReconnectingWebSocket(\"ws://localhost:8080/ws\", null, { debug: true, reconnectInterval: 3000 });\n\n    // WebSocket Events\n    socket.onopen = function () {\n        console.log(\"connected!!\");\n        $statusDiv.html(online);\n    };\n\n    socket.onclose = function () {\n        console.log(\"connection closed!\");\n        $statusDiv.html(offline);\n    };\n\n    socket.onerror = function () {\n        console.log(\"there was an error\");\n        $statusDiv.html(offline);\n    };\n\n    socket.onmessage = function (msg) {\n        const data = JSON.parse(msg.data);\n        console.log(\"Action:\", data.action);\n\n        switch (data.action) {\n            case \"list_users\":\n                $onlineUsers.empty();\n                if (data.connected_users.length > 0) {\n                    $.each(data.connected_users, function (index, user) {\n                        $onlineUsers.append(`<li class=\"list-group-item\">${user}</li>`);\n                    });\n                }\n                break;\n\n            case \"broadcast\":\n                $output.append(`<div>${data.message}</div>`);\n                $output.scrollTop($output.prop(\"scrollHeight\"));\n                break;\n        }\n    };\n\n    // Username Field Change\n    $userField.on(\"change\", function () {\n        const jsonData = {\n            action: \"username\",\n            username: $(this).val()\n        };\n        console.log(jsonData);\n        socket.send(JSON.stringify(jsonData));\n    });\n\n    // Message Field Enter Key\n    $messageField.on(\"keydown\", function (event) {\n        if (event.key === \"Enter\") {\n            if (!socket || socket.readyState !== WebSocket.OPEN) {\n                alert(\"No connection\");\n                return false;\n            }\n\n            if (!$userField.val() || !$messageField.val()) {\n                alert(\"Enter username and message!\");\n                return false;\n            } else {\n                sendMessage();\n            }\n\n            event.preventDefault();\n        }\n    });\n\n    // Send Button Click\n    $(\"#sendButton\").on(\"click\", function () {\n        if (!$userField.val() || !$messageField.val()) {\n            alert(\"Enter username and message!\");\n            return false;\n        } else {\n            sendMessage();\n        }\n    });\n\n    // WebSocket Disconnect on Page Unload\n    $(window).on(\"beforeunload\", function () {\n        console.log(\"leaving ;(\");\n        if (socket && socket.readyState === WebSocket.OPEN) {\n            const jsonData = { action: \"left\" };\n            socket.send(JSON.stringify(jsonData));\n        }\n    });\n\n    // Function to Send Message\n    function sendMessage() {\n        const jsonData = {\n            action: \"broadcast\",\n            username: $userField.val(),\n            message: $messageField.val()\n        };\n        socket.send(JSON.stringify(jsonData));\n        $messageField.val(\"\");\n    }\n});\n```\n\n# **Conclusion**\n\nBuilding a real-time chat application using Go, Gorilla WebSocket, and jQuery is a great way to explore modern web development with WebSocket-based communication. By combining a robust backend in Go with jQuery on the frontend, you can achieve a seamless and interactive user experience.\n\nThe chat application features a user-friendly design as shown below:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734298232765/b6827fd8-d7dd-4bbb-b575-f429f3e0713b.png align=\"center\")\n\nThis simple chat application demonstrates real-time user updates, message broadcasting, and client-server communication. To access the full source code and dive deeper into the implementation, visit the GitHub repository: [https://github.com/arya2004/gobanter](https://github.com/arya2004/gobanter).\n\nHappy coding!",
      "stars": null,
      "comments": 1,
      "upvotes": 35,
      "read_time": "15 min read",
      "language": null
    },
    {
      "title_en": "RAG \"from Scratch\" with Go and Ollama",
      "url": "https://k33g.hashnode.dev/rag-from-scratch-with-go-and-ollama",
      "source": "hashnode",
      "published_at": "2024-12-13T07:12:50.956000+00:00",
      "external_id": null,
      "tags": [
        "RAG ",
        "ollama",
        "golang",
        "AI",
        "llm"
      ],
      "content_length": 27762,
      "content_preview": "Today, we'll discuss RAG (Retrieval Augmented Generation). This is a way to provide knowledge that an LLM (Large Language Model) doesn't have, allowing it to answer better questions about the Bible, your documentation, a particular domain of knowledge, and so on. And this without having to retrain it (which is costly in time and energy). Some large models have an extensive knowledge base, but they don't know everything, particularly about information created after their training or documentation",
      "content_full": "Today, we'll discuss RAG (Retrieval Augmented Generation). This is a way to provide knowledge that an LLM (Large Language Model) doesn't have, allowing it to answer better questions about the Bible, your documentation, a particular domain of knowledge, and so on. And this without having to retrain it (which is costly in time and energy). Some large models have an extensive knowledge base, but they don't know everything, particularly about information created after their training or documentation from your brain that won't be public. And this is even more true for smaller models (these \"Tiny models\" that I'm so fond of).\n\nWith the help ofÂ [Claude.AI](http://Claude.AI), I have createdÂ content that describes an entire fantastic bestiary oriented towards heroic fantasy, which would be used for an entirely fictional role-playing game (Chronicles of Aethelgard). This content is available on [Chronicles of Aethelgard](https://github.com/ollama-tlms-golang/02-rag-first-contact/blob/main/content/chronicles-of-aethelgard.md).\n\nQuick summary of this content (thanks to Claude for the writing ðŸ˜˜): *The Chronicles of Aethelgard present a fantastic world populated by numerous intelligent species, divided into several categories: noble species (humans, elves, and dwarves), wild species (orcs, halflings, and gnomes), and exotic species (such as drakeid and demons-descended tieflings). Each species has its own biological characteristics, unique culture, and traditions, with varying lifespans ranging from 60-80 years for humans to over 500 years for elves. The relationships between these different species are complex and constantly evolving, alternating between cooperation, distrust, and conflicts, with humans often playing a central intermediary role thanks to their great adaptability and ability to reproduce with several other species.*\n\nSo, I will try to feed \"Baby Qwen\" with this content to see if it can answer questions about this fictional world.\n\nLet's get started!\n\n> Of course, it's better for your understanding if you've read the previous blog posts.\n\n## Let's try in \"classic\" mode.\n\nMy first attempt was to load the content into a variable, add it to the prompt for the model, and ask it a question: \"Explain the biological compatibility of the Human species\". This question involves all parts of the document, as the model must check for compatibility with each described species.\n\nHere's the code in its entirety, and then I'll explain it in detail:\n\n```golang\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\n\t\"github.com/ollama/ollama/api\"\n)\n\nvar (\n\tFALSE = false\n\tTRUE  = true\n)\n\nfunc main() {\n\tctx := context.Background()\n\n\tvar ollamaRawUrl string\n\tif ollamaRawUrl = os.Getenv(\"OLLAMA_HOST\"); ollamaRawUrl == \"\" {\n\t\tollamaRawUrl = \"http://localhost:11434\"\n\t}\n\n\turl, _ := url.Parse(ollamaRawUrl)\n\n\tclient := api.NewClient(url, http.DefaultClient)\n\n\tsystemInstructions := `You are a role playing games expert like D&D.\n\tYou are the dungeon master of the Chronicles of Aethelgard game.\n\tIf you need information about the Aethelgard and its species, refer only to the provided content.\n\t`\n\n\tcontext, err := os.ReadFile(\"../content/chronicles-of-aethelgard.md\")\n\tif err != nil {\n\t\tlog.Fatalln(\"ðŸ˜¡\", err)\n\t}\n\n\tquestion := \"Explain the biological compatibility of the Human species\"\n\n\t// Prompt construction\n\tmessages := []api.Message{\n\t\t{Role: \"system\", Content: systemInstructions},\n\t\t{Role: \"system\", Content: \"CONTENT:\\n\" + string(context)},\n\t\t{Role: \"user\", Content: question},\n\t}\n\n\treq := &api.ChatRequest{\n\t\tModel:    \"qwen2.5:0.5b\",\n\t\tMessages: messages,\n\t\tOptions: map[string]interface{}{\n\t\t\t\"temperature\":    0.0,\n\t\t\t\"repeat_last_n\":  2,\n\t\t\t\"repeat_penalty\": 1.8,\n\t\t\t\"top_k\":          10,\n\t\t\t\"top_p\":          0.5,\n\t\t},\n\t\tStream: &TRUE,\n\t}\n\n\terr = client.Chat(ctx, req, func(resp api.ChatResponse) error {\n\t\tfmt.Print(resp.Message.Content)\n\t\treturn nil\n\t})\n\n\tif err != nil {\n\t\tlog.Fatalln(\"ðŸ˜¡\", err)\n\t}\n\tfmt.Println()\n}\n```\n\n### Some explanations\n\nThis is a program that interacts with a LLM (`qwen2.5:0.5b`) using Ollama's Go API to create a specialized role-playing assistant.\n\nThe section below defines the system instructions for \"Baby Qwen\" (the LLM):\n\n```go\nsystemInstructions := `You are a role playing games expert like D&D...`\n```\n\nThese instructions define the role and context in which the model should operate.\n\nThe program then reads a content file containing information about the Aethelgard universe:\n\n```go\ncontext, err := os.ReadFile(\"../content/chronicles-of-aethelgard.md\")\nif err != nil {\n    log.Fatalln(\"ðŸ˜¡\", err)\n}\n```\n\nThen, the request construction is done by creating an array of messages:\n\n```go\nmessages := []api.Message{\n    {Role: \"system\", Content: systemInstructions},\n    {Role: \"system\", Content: \"CONTENT:\\n\" + string(context)},\n    {Role: \"user\", Content: question},\n}\n```\n\nThis structure follows the classic format of conversations with language models: system instructions, context, and user questions.\n\nThe request configuration is particularly interesting:\n\n```go\nreq := &api.ChatRequest{\n    Model:    \"qwen2.5:0.5b\",    // Uses the Qwen 2.5 model (0.5 billion parameters)\n    Messages: messages,\n    Options: map[string]interface{}{\n        \"temperature\":    0.0,    // Maximum determinism\n        \"repeat_last_n\":  2,      // Tries to avoid repetitions\n        \"repeat_penalty\": 1.8,    // Strong penalty for repetitions\n        \"top_k\":          10,     // helps keep focus on most likely responses\n        \"top_p\":          0.5,    // helps keep focus on most likely responses\n    },\n    Stream: &TRUE,  // Activates response streaming\n}\n```\n\nFinally, the program sends the request and handles the response through streaming:\n\n```go\nerr = client.Chat(ctx, req, func(resp api.ChatResponse) error {\n    fmt.Print(resp.Message.Content)\n    return nil\n})\n```\n\nThis part displays the response as the LLM generates it.\n\n#### Parameters `top_k`, `top_p`, `repeat_last_n` and `repeat_penalty`\n\nBefore running the program, let's take a moment to discuss some of the request parameters:\n\n* `top_k`: Reduces the probability of generating incoherent content. A higher value (e.g., 100) will give more diverse responses, while a lower value (e.g., 10) will be more conservative. (Default: 40)\n    \n* `top_p`: Works in conjunction with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\n    \n* `repeat_last_n`: Determines how far back the model should look to avoid repetitions.\n    \n* `repeat_penalty`: Defines the intensity of repetition penalization. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more tolerant.\n    \n\nThese parameters are essential for controlling the quality and coherence of responses generated by the model. But even though there are recommendations, it's important to adjust them according to your specific needs, especially according to the model (in my opinion).\n\n### Let's ask that question!\n\nSo, let's run the program and see what \"Baby Qwen\" says about the biological compatibility of the human species in the Aethelgard universe.\n\n```bash\ngo run main.go\n```\n\nAnd here's \"Baby Qwen's\" response:\n\n```plaintext\nThe Human species is highly compatible with other species, including the Aethelgard. Humans are known for their intelligence, adaptability, and ability to communicate effectively with others. They are also known for their ability to adapt and evolve over time, making them a valuable and adaptable species.\n```\n\nWell, well, well, while the response seems correct, it's very generic, and ultimately, I'm no more informed about the biological compatibility of humans in the Aethelgard universe.\n\n**Warning: anthropomorphic remark ahead!**\n\nIn fact, I think \"Baby Qwen\" has trouble ingesting all the content and processing it correctly. So, Baby Qwen cannot have a global vision of the Aethelgard universe and ultimately keep only the relevant information to answer the question.\n\nWe'll, therefore, need to reduce the context provided to \"Baby Qwen\" with only information related to the question so it can better process the information and respond more precisely to the question. ðŸ¤”\n\nTo put it another way, we need to extract information from the document that talks about humans, as well as those where we talk about humans, other species, and biological compatibility. Put all this in a smaller and more precise context so that \"Baby Qwen\" can better focus.\n\nThis is where RAG (Retrieval Augmented Generation) comes into play. ðŸš€\n\n## Let's talk about RAG\n\nDoing RAG means searching for similarities in content based on the user's question.\n\nThe principle is as follows:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1734073448782/3c6e22e5-eec6-448e-a2f9-48aca96db51b.png align=\"center\")\n\nRAG is there to overcome the limitations of LLMs.\n\nFor example, if you want to discuss your documentation via Ollama, the model will probably \"confess\" its ignorance on the topic.\n\nThis is where RAG (Retrieval Augmented Generation) comes in, once again, a solution to enrich the model's knowledge without retraining it.\n\nThe process takes place in several steps:\n\n* Fragmentation: Your documents are cut into small coherent segments (called **chunks**).\n    \n* Vector Transformation: Each fragment (chunk) is converted into a \"unique mathematical fingerprint\" (a vector, also called **embeddings**) that captures the semantics of its content. **The embeddings are calculated by specialized LLMs capable of returning a set of vector coordinates from text**.\n    \n* Organization in Vector Database: These vectors are saved in a specialized database (we'll work with in-memory data for now).\n    \n\nThen, when a user asks a question, the system will work as follows:\n\n* The question is first transformed into a vector\n    \n* The system searches for the most relevant fragments (chunks) in its library. **Since the fragments are linked to vectors, we can calculate the distances between vectors and keep only the closest ones**.\n    \n* These fragments are integrated into the context of the question\n    \n* The whole thing is transmitted to the language model, which can then draw from this new knowledge to formulate a precise and relevant response (in theory).\n    \n\n### How do we fragment the document?\n\nSo, \"chunking\" is the process of breaking down a document into smaller coherent segments (chunks) to facilitate their processing by language models.\n\nThere are various cutting strategies; here are the main ones:\n\n1. By fixed size\n    \n\n* Cutting into segments of a fixed number of characters or tokens\n    \n* Simple but risks cutting in the middle of sentences or ideas\n    \n\n2. By structure\n    \n\n* Respecting natural divisions of the document (paragraphs, sections)\n    \n* It better preserves context but can create chunks of uneven sizes\n    \n\n3. By meaning\n    \n\n* Cutting while preserving coherent semantic units\n    \n* It is more complex but gives better results for understanding\n    \n\nWe should try to:\n\n* Maintain an optimal chunk size (neither too big nor too small)\n    \n* **Ensure overlap between chunks to not lose context** (in the case of the 1st strategy)\n    \n* Preserve the logical structure of the document (strategies 2 and 3)\n    \n* Preserve important metadata (titles, references) to facilitate similarity search (we create sorts of links or bookmarks).\n    \n\n### How do we calculate embeddings?\n\nWe'll use Ollama and a specialized language model to calculate the embeddings of our chunks (We call these **embedding models**). Ollama provides an API for this, which allows text to be transformed into vectors. I'll let you read this blog post that explains it in detail: [Embedding models](https://ollama.com/blog/embedding-models).\n\nFor our first experiments, we'll use [`snowflake-arctic-embed:33m`](https://ollama.com/library/snowflake-arctic-embed:33m) as the embedding model. It's minimal (67MB) but reasonably sufficient for our needs.\n\nSo don't forget to download it:\n\n```bash\nollama pull snowflake-arctic-embed:33m\n```\n\n### How do we calculate distances between vectors?\n\nWe'll do some coding to calculate distances between vectors and use the **\"Cosine Similarity\"** method.\n\nIn the context of embeddings (the vectors):\n\n* **Vectors are close (or similar)** when the **cosine similarity** is **close to 1**.\n    \n* **Vectors are distant (or dissimilar)** when the **cosine similarity** is **close to 0** (or when the cosine distance is close to 1).\n    \n\n**Key Points**:\n\n1. **Cosine Similarity** measures the angle between two vectors:\n    \n    * If the vectors are **aligned (pointing in the same direction)**, the similarity is `1`.\n        \n    * If the vectors are **perpendicular**, the similarity is `0`.\n        \n    * If the vectors are **opposite**, the similarity is `-1`.\n        \n2. **Cosine Distance** (it's the inverse of similarity):\n    \n    * It's simply `1 - cosine similarity`.\n        \n    * When vectors are very similar, the cosine distance will be close to `0`.\n        \n    * When vectors are very dissimilar, the cosine distance will be close to `1`.\n        \n3. **\"Proximity\" Threshold**:\n    \n    * There is no universal threshold, but typically:\n        \n        * **Cosine Similarity â‰¥ 0.8**: Vectors are considered close (very similar).\n            \n        * **Cosine Distance â‰¤ 0.2**: Vectors are considered close.\n            \n\n**So, for example**:\n\n* If you calculate a cosine similarity of `0.95` between two embeddings, this means the vectors are very close and probably represent similar concepts.\n    \n* Conversely, a cosine **similarity** of `0.2` or a cosine **distance** of `0.8` indicates they are pretty far apart.\n    \n\n> This is important. At first, I would reverse or mix up the concepts of similarity and distance.\n\n**Practical Use**: In the context of embeddings for tasks like retrieval-augmented generation (RAG):\n\n* **Close vectors** often mean the embeddings are semantically similar, probably representing related ideas or contexts.\n    \n* **Distance thresholds** are generally context-specific and may require experimentation to be finely tuned.\n    \n\nYes, I know, that's a lot, but it's important for what follows. ðŸ¤“\n\nLet's code now!\n\n## Let's code the RAG\n\nOnce again, I'll start with the complete code and then explain the details. Today, we'll use chunking strategy number 1, and we'll see other strategies in future blog posts to improve the relevance of responses.\n\n```golang\npackage main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"math\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"sort\"\n\n\t\"github.com/ollama/ollama/api\"\n)\n\nvar (\n\tFALSE = false\n\tTRUE  = true\n)\n\nfunc main() {\n\tctx := context.Background()\n\n\tvar ollamaRawUrl string\n\tif ollamaRawUrl = os.Getenv(\"OLLAMA_HOST\"); ollamaRawUrl == \"\" {\n\t\tollamaRawUrl = \"http://localhost:11434\"\n\t}\n\n\turl, _ := url.Parse(ollamaRawUrl)\n\n\tclient := api.NewClient(url, http.DefaultClient)\n\n\tsystemInstructions := `You are a role playing games expert like D&D.\n\tYou are the dungeon master of the Chronicles of Aethelgard game.\n\tIf you need information about the Aethelgard and its species, refer only to the provided content.\n\t`\n\n\tquestion := \"Explain the biological compatibility of the Human species\"\n\n\tcontext, err := os.ReadFile(\"../content/chronicles-of-aethelgard.md\")\n\tif err != nil {\n\t\tlog.Fatalln(\"ðŸ˜¡\", err)\n\t}\n\n\tchunks := ChunkText(string(context), 1024, 256)\n\n\tvectorStore := []VectorRecord{}\n\t// Create embeddings from documents and save them in the store\n\tfor idx, chunk := range chunks {\n\t\tfmt.Println(\"ðŸ“ Creating embedding nb:\", idx)\n\t\tfmt.Println(\"ðŸ“ Chunk:\", chunk)\n\n\t\tembedding, _ := GetEmbeddingFromChunk(ctx, client, chunk)\n\n\t\t// Save the embedding in the vector store\n\t\trecord := VectorRecord{\n\t\t\tPrompt:    chunk,\n\t\t\tEmbedding: embedding,\n\t\t}\n\t\tvectorStore = append(vectorStore, record)\n\t}\n\n\tembeddingFromQuestion, _ := GetEmbeddingFromChunk(ctx, client, question)\n\n\t// Search similarites between the question and the vectors of the store\n\t// 1- calculate the cosine similarity between the question and each vector in the store\n\tsimilarities := []Similarity{}\n\n\tfor _, vector := range vectorStore {\n\t\tcosineSimilarity, err := CosineSimilarity(embeddingFromQuestion, vector.Embedding)\n\t\tif err != nil {\n\t\t\tlog.Fatalln(\"ðŸ˜¡\", err)\n\t\t}\n\n\t\t// append to similarities\n\t\tsimilarities = append(similarities, Similarity{\n\t\t\tPrompt:           vector.Prompt,\n\t\t\tCosineSimilarity: cosineSimilarity,\n\t\t})\n\t}\n\n\t// Select the 5 most similar chunks\n\t// retrieve in similarities the 5 records with the highest cosine similarity\n\t// sort the similarities\n\tsort.Slice(similarities, func(i, j int) bool {\n\t\treturn similarities[i].CosineSimilarity > similarities[j].CosineSimilarity\n\t})\n\n\t// get the first 5 records\n\ttop5Similarities := similarities[:5]\n\n\tfmt.Println(\"ðŸ” Top 5 similarities:\")\n\tfor _, similarity := range top5Similarities {\n\t\tfmt.Println(\"ðŸ” Prompt:\", similarity.Prompt)\n\t\tfmt.Println(\"ðŸ” Cosine similarity:\", similarity.CosineSimilarity)\n\t\tfmt.Println(\"--------------------------------------------------\")\n\n\t}\n\n\t// Create a new context with the top 5 chunks\n\tnewContext := \"\"\n\tfor _, similarity := range top5Similarities {\n\t\tnewContext += similarity.Prompt\n\t}\n\n\t// Answer the question with the new context\n\n\t// Prompt construction\n\tmessages := []api.Message{\n\t\t{Role: \"system\", Content: systemInstructions},\n\t\t{Role: \"system\", Content: \"CONTENT:\\n\" + newContext},\n\t\t{Role: \"user\", Content: question},\n\t}\n\n\treq := &api.ChatRequest{\n\t\tModel:    \"qwen2.5:0.5b\",\n\t\tMessages: messages,\n\t\tOptions: map[string]interface{}{\n\t\t\t\"temperature\":    0.0,\n\t\t\t\"repeat_last_n\":  2,\n\t\t\t\"repeat_penalty\": 1.8,\n\t\t\t\"top_k\":          10,\n\t\t\t\"top_p\":          0.5,\n\t\t},\n\t\tStream: &TRUE,\n\t}\n\n\tfmt.Println(\"ðŸ¦„ question:\", question)\n\tfmt.Println(\"ðŸ¤– answer:\")\n\terr = client.Chat(ctx, req, func(resp api.ChatResponse) error {\n\t\tfmt.Print(resp.Message.Content)\n\t\treturn nil\n\t})\n\n\tif err != nil {\n\t\tlog.Fatalln(\"ðŸ˜¡\", err)\n\t}\n\tfmt.Println()\n\tfmt.Println()\n\n}\n\nfunc ChunkText(text string, chunkSize, overlap int) []string {\n\tchunks := []string{}\n\tfor start := 0; start < len(text); start += chunkSize - overlap {\n\t\tend := start + chunkSize\n\t\tif end > len(text) {\n\t\t\tend = len(text)\n\t\t}\n\t\tchunks = append(chunks, text[start:end])\n\t}\n\treturn chunks\n}\n\ntype VectorRecord struct {\n\tPrompt    string    `json:\"prompt\"`\n\tEmbedding []float64 `json:\"embedding\"`\n}\n\ntype Similarity struct {\n\tPrompt           string\n\tCosineSimilarity float64\n}\n\nfunc GetEmbeddingFromChunk(ctx context.Context, client *api.Client, doc string) ([]float64, error) {\n\tembeddingsModel := \"snowflake-arctic-embed:33m\"\n\n\treq := &api.EmbeddingRequest{\n\t\tModel:  embeddingsModel,\n\t\tPrompt: doc,\n\t}\n\t// get embeddings\n\tresp, err := client.Embeddings(ctx, req)\n\tif err != nil {\n\t\tlog.Println(\"ðŸ˜¡:\", err)\n\t\treturn nil, err\n\t}\n\treturn resp.Embedding, nil\n}\n\n// CosineSimilarity calculates the cosine similarity between two vectors\n// Returns a value between -1 and 1, where:\n// 1 means vectors are identical\n// 0 means vectors are perpendicular\n// -1 means vectors are opposite\nfunc CosineSimilarity(vec1, vec2 []float64) (float64, error) {\n\tif len(vec1) != len(vec2) {\n\t\treturn 0, errors.New(\"vectors must have the same length\")\n\t}\n\n\t// Calculate dot product\n\tdotProduct := 0.0\n\tmagnitude1 := 0.0\n\tmagnitude2 := 0.0\n\n\tfor i := 0; i < len(vec1); i++ {\n\t\tdotProduct += vec1[i] * vec2[i]\n\t\tmagnitude1 += vec1[i] * vec1[i]\n\t\tmagnitude2 += vec2[i] * vec2[i]\n\t}\n\n\tmagnitude1 = math.Sqrt(magnitude1)\n\tmagnitude2 = math.Sqrt(magnitude2)\n\n\t// Check for zero magnitudes to avoid division by zero\n\tif magnitude1 == 0 || magnitude2 == 0 {\n\t\treturn 0, errors.New(\"vector magnitude cannot be zero\")\n\t}\n\n\treturn dotProduct / (magnitude1 * magnitude2), nil\n}\n```\n\n### Explanations (grab yourself a coffee â˜•ï¸)\n\nThe program uses embeddings to find the most relevant parts of a document before answering a question.\n\nLet's understand the overall structure of the program:\n\n#### Important Data Structures:\n\nI defined two key data structures to store embeddings and similarities:\n\n```go\ntype VectorRecord struct {\n    Prompt    string    `json:\"prompt\"`\n    Embedding []float64 `json:\"embedding\"`\n}\n\ntype Similarity struct {\n    Prompt           string\n    CosineSimilarity float64\n}\n```\n\nThese structures are essential for storing and comparing embeddings. VectorRecord associates a text (the chunk content) with its vector embedding, while Similarity stores a text and its calculated similarity with the question.\n\n#### Utility Functions:\n\n##### Chunking\n\nThe `ChunkText` function splits the text into fixed-size pieces with overlap:\n\n```go\nfunc ChunkText(text string, chunkSize, overlap int) []string {\n    chunks := []string{}\n    for start := 0; start < len(text); start += chunkSize - overlap {\n        end := start + chunkSize\n        if end > len(text) {\n            end = len(text)\n        }\n        chunks = append(chunks, text[start:end])\n    }\n    return chunks\n}\n```\n\nThis function is essential because it allows the processing of long texts by breaking them into pieces that slightly overlap (overlap), which helps maintain context between chunks.\n\n##### Embeddings\n\nThe `GetEmbeddingFromChunk` function converts text into a vector using Ollama's Go API (`EmbeddingRequest`):\n\n```go\nfunc GetEmbeddingFromChunk(ctx context.Context, client *api.Client, doc string) ([]float64, error) {\n    embeddingsModel := \"snowflake-arctic-embed:33m\"\n\n    req := &api.EmbeddingRequest{\n        Model:  embeddingsModel,\n        Prompt: doc,\n    }\n    // get embeddings\n    resp, err := client.Embeddings(ctx, req)\n    if err != nil {\n        log.Println(\"ðŸ˜¡:\", err)\n        return nil, err\n    }\n    return resp.Embedding, nil\n}\n```\n\nThis function uses the **\"snowflake-arctic-embed:33m\"** model to transform the text into a vector that captures its semantic meaning.\n\n##### Similarities\n\nThe `CosineSimilarity` function calculates the similarity between two vectors:\n\n```go\nfunc CosineSimilarity(vec1, vec2 []float64) (float64, error) {\n    if len(vec1) != len(vec2) {\n        return 0, errors.New(\"vectors must have the same length\")\n    }\n\n    // Calculate dot product\n    dotProduct := 0.0\n    magnitude1 := 0.0\n    magnitude2 := 0.0\n\n    for i := 0; i < len(vec1); i++ {\n        dotProduct += vec1[i] * vec2[i]\n        magnitude1 += vec1[i] * vec1[i]\n        magnitude2 += vec2[i] * vec2[i]\n    }\n\n    magnitude1 = math.Sqrt(magnitude1)\n    magnitude2 = math.Sqrt(magnitude2)\n\n    // Check for zero magnitudes to avoid division by zero\n    if magnitude1 == 0 || magnitude2 == 0 {\n        return 0, errors.New(\"vector magnitude cannot be zero\")\n    }\n\n    return dotProduct / (magnitude1 * magnitude2), nil\n}\n```\n\nThis function implements the cosine similarity measure, which provides a value between -1 and 1, indicating how similar two vectors are.\n\n#### Now, the Main Program\n\nFirst, the program loads and chunks the context:\n\n```go\ncontext, err := os.ReadFile(\"../content/chronicles-of-aethelgard.md\")\nchunks := ChunkText(string(context), 1024, 256)\n```\n\nThe program reads the context file and splits it into pieces of 1024 characters with a 256-character overlap.\n\nNext, we have the creation of embeddings:\n\n```go\nvectorStore := []VectorRecord{}\nfor idx, chunk := range chunks {\n    embedding, _ := GetEmbeddingFromChunk(ctx, client, chunk)\n    record := VectorRecord{\n        Prompt:    chunk,\n        Embedding: embedding,\n    }\n    vectorStore = append(vectorStore, record)\n}\n```\n\nEach piece of text is converted into an embedding and stored in memory in an array or slice of `VectorRecord`.\n\nWe can now launch the similarity search:\n\n```go\nembeddingFromQuestion, _ := GetEmbeddingFromChunk(ctx, client, question)\n\n// Search similarities between the question and the vectors of the store\n// 1- calculate the cosine similarity between the question and each vector in the store\nsimilarities := []Similarity{}\n\nfor _, vector := range vectorStore {\n    cosineSimilarity, err := CosineSimilarity(embeddingFromQuestion, vector.Embedding)\n    if err != nil {\n        log.Fatalln(\"ðŸ˜¡\", err)\n    }\n\n    // append to similarities\n    similarities = append(similarities, Similarity{\n        Prompt:           vector.Prompt,\n        CosineSimilarity: cosineSimilarity,\n    })\n}\n```\n\nThe program calculates the similarity between the question and each piece of text and then selects the 5 most relevant pieces.\n\n```go\n// Select the 5 most similar chunks\n// retrieve in similarities the 5 records with the highest cosine similarity\n// sort the similarities\nsort.Slice(similarities, func(i, j int) bool {\n    return similarities[i].CosineSimilarity > similarities[j].CosineSimilarity\n})\n\n// get the first 5 records\ntop5Similarities := similarities[:5]\n\nfmt.Println(\"ðŸ” Top 5 similarities:\")\nfor _, similarity := range top5Similarities {\n    fmt.Println(\"ðŸ” Prompt:\", similarity.Prompt)\n    fmt.Println(\"ðŸ” Cosine similarity:\", similarity.CosineSimilarity)\n}\n```\n\n**Now we can move on to generating the response**:\n\nThe program then uses these 5 most relevant pieces to build a new, more focused context, which is provided to the language model along with the question:\n\n```go\n// Create a new context with the top 5 chunks\nnewContext := \"\"\nfor _, similarity := range top5Similarities {\n    newContext += similarity.Prompt\n}\n\n// Answer the question with the new context\n\n// Prompt construction\nmessages := []api.Message{\n    {Role: \"system\", Content: systemInstructions},\n    {Role: \"system\", Content: \"CONTENT:\\n\" + newContext},\n    {Role: \"user\", Content: question},\n}\n```\n\nAnd then we can launch our request as before.\n\nLet's go!\n\n### Let's run the program.\n\n```bash\ngo run main.go\n```\n\nThe program will cut the document into about a hundred chunks and then calculate the embeddings for each chunk. Then, it will calculate the similarity between the question and each chunk. It will then select the 5 chunks most similar to the question, concatenate them to form a new more targeted context, and finally ask \"Baby Qwen\" the question.\n\nIn my trials, the five most similar chunks had a cosine similarity of `0.80` to `0.84`. That's a good score, but it can be adjusted. You need to experiment to find the right similarity threshold and also try with other LLMs that calculate embeddings.\n\nAnd here's \"Baby Qwen's\" response:\n\n```plaintext\nThe Human species is characterized by its unique genetic flexibility, which allows it to produce viable offspring with various humanoid species. This adaptability has profound implications\n\n- Half-Evans: The most common mixed heritage, often serving as bridges between human and elven communities.\n- Half-orcs: They emerge from either peaceful unions or historical conflicts, facing varying degrees of acceptance.\n- Rarer combinations (human-dwarf, human-halfling) occur but typically require magical intervention.\n\nThis biological adaptability influences human society, leading to complex inheritance laws and social structures accommodating mixed-heritage individuals.\n```\n\nThis is much better! The response is more precise and detailed, and it addresses specific aspects of humans' biological compatibility with other species in the Aethelgard universe.\n\nNow, we could see if taking more results, like the top 10, makes the response even more precise. That's up to you to try!\n\nToday, we've covered the basics to start experimenting with RAG. In future blog posts, we'll explore other chunking strategies and other language models, as well as some concepts about vector databases for storing embeddings.\n\nSee you soon for new adventures! ðŸš€\n\nOf course, you'll find the complete code on [ollama-tlms-golang/02-rag-first-contact](https://github.com/ollama-tlms-golang/02-rag-first-contact)",
      "stars": null,
      "comments": 0,
      "upvotes": 27,
      "read_time": "17 min read",
      "language": null
    }
  ]
}